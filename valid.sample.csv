"Nearly one quarter of Germany's population of 83 million has a migration background. Adolescents and young adults aged 15 to 30 years make up about 13 million with 4.5 million having a migration background [1] . Mental illnesses in adolescence and young adulthood are steadily increasing [2] . Based on data from a health insurance company, in Germany mental illnesses rose by 38% to 1.9 million between 2005 and 2016 [3] . Adolescents are particularly vulnerable to mental health issues, especially to internet-related addictions such as computer and smartphone addiction, which is positively associated with anxiety, depression, and loneliness [4, 5] . Depression is an important risk factor for suicide [6] , which is the second leading cause of death in adolescents and young adults [7] . Many children and adolescents who have a significant mental health disorder have a higher probability to experience negative life Individuals who experience mental health issues may have feelings of shame and fear stigmatization from their family, friends, or society. Young people may be reluctant to ask for help or to get treatment and are therefore likely to isolate themselves from the world. That is why it is important to provide digital opportunities in which young people themselves search for information. However, it is important to address the dangers of the internet as well, as non-serious sources can be accessed by those affected [33] . For example, false information can circulate on the Internet, which can also be triggering. In addition online offerings that address psychological topics are not exclusively aimed at young people, and the target group of young people with a migration background is hardly addressed at all. In recent years, an increasing number of people have migrated to Germany with most of the applicants being younger than 30 years old [34] . The migration experience can be a stressful process; hence, the migrant population is more vulnerable to have mental disorders due to traumatic experiences and they may experience post-traumatic stress disorder, adaptation problems, anxiety, and depression [35] . Nevertheless, there is a lack of adequate information and treatment options available. Consequently, offering insight to mental illnesses and the promotion of mental health through digital sources have a high relevance particularly for vulnerable adolescents and young adults and especially for those with migration background. Jorm and colleagues introduced and shaped the construct of Mental Health Literacy (MHL) [36] due to the neglect of mental disorders in the context of health literacy promotion [37] . The construct of MHL represents an extension of health literacy [36] . According to this construct, MHL comprises the ideas and knowledge about mental disorders, which in turn form the basis for recognition, coping, or prevention. It is characterized by seven components: ability to recognize specific mental disorders; knowledge of how to obtain mental health information; knowledge and beliefs regarding risk factors and causes; knowledge and beliefs regarding self-intervention strategies; knowledge and beliefs regarding available professional services; attitudes that promote recognition; and appropriate seeking of help [36, 38] . Improving the target group's ability of to recognize signs of mental disorders at an early stage, to classify symptoms correctly and to be able to obtain reliable information about contact points, could increase the motivation to seek help and treatment, what substantially leads to better mental health outcomes and a better quality of life in the long term. Digital technologies offer an ideal opportunity for a low-threshold platform that can be easily adapted to specific target groups for people with and without a migration background. The current project aims to design a multilingual website in German and English that will enable young people to obtain comprehensive, appealing, and comprehensible information about mental illness. To design this website according to the needs of young people, this study aims to find out what kind of health information is needed and how this information should best be presented. The main objectives of this study were to explore the presence and relevance of mental health problems in the life of young people with and without a migration background and how they deal with it, to conclude their health literacy. Focus groups were set up to examine, from the perspectives of young people, their experiences with mental health problems, challenges, and needs. The results should form the basis for the development and design of a website. With the development of a target group-oriented website, the following goals should be achieved:@story_separate@The health literacy for the mental health of adolescents and young adults with and without a migration background is to be promoted through the information on the website. The target group should be sensitized to the relevance of psychological problems and illnesses regarding themselves and their environment (peers), thus preventing chronification. Through the presentation of diverse case studies, an understanding of the individual approach to mental health and illness is achieved, which in turn leads to a reduction in stigmatizing behavior and an increase in the compatibility of the topic of mental health with the everyday life of young people. The website should provide an impression of trustworthiness and professionalism so that visitors are encouraged to deepen their knowledge about mental illnesses and are able to cope with previously challenging or overwhelming situations. In the long term, this should help to improve the health and quality of life of the target groups. • This study refers mainly to Step 1 and summarizes ideas from its results that will be incorporated into Step 2 (see Figure 1 ). The target group should be sensitized to the relevance of psychological problems and illnesses regarding themselves and their environment (peers), thus preventing chronification. Through the presentation of diverse case studies, an understanding of the individual approach to mental health and illness is achieved, which in turn leads to a reduction in stigmatizing behavior and an increase in the compatibility of the topic of mental health with the everyday life of young people. The website should provide an impression of trustworthiness and professionalism so that visitors are encouraged to deepen their knowledge about mental illnesses and are able to cope with previously challenging or overwhelming situations. In the long term, this should help to improve the health and quality of life of the target groups. This study refers mainly to Step 1 and summarizes ideas from its results that will be incorporated into Step 2 (see Figure 1 ). Overview about steps, aims, and methods of the research project. Focus group discussions are often used in qualitative research and are structured by the application of a guideline. This guideline is developed for a specific research question to be able to capture its subject of interest [39] . Regardless of the fact that Focus Groups are relatively resource-saving [40] , they were chosen because they are appropriate for the investigation of the reality of life, experiences, or opinions and are therefore particularly suitable for the exploration of health care research [41, 42] . They have also proved to be successful in an intercultural context, which requires aspects of cultural sensitivity and understanding on the part of the moderator [43, 44] . The structuring of the FGD by a guideline aims to enable the comparison of different FGD and at the same time the collection of  Focus group discussions are often used in qualitative research and are structured by the application of a guideline. This guideline is developed for a specific research question to be able to capture its subject of interest [39] . Regardless of the fact that Focus Groups are relatively resource-saving [40] , they were chosen because they are appropriate for the investigation of the reality of life, experiences, or opinions and are therefore particularly suitable for the exploration of health care research [41, 42] . They have also proved to be successful in an intercultural context, which requires aspects of cultural sensitivity and understanding on the part of the moderator [43, 44] . The structuring of the FGD by a guideline aims to enable the comparison of different FGD and at the same time the collection of a broad spectrum of opinions. To achieve this comparability, the FGD is guided by the moderator. At the beginning, the participants are told which aspects of the topic will be dealt with and how the FGD will proceed. However, variations of topics can ensure that a discussion among the participants takes place and the conversation does not fall silent prematurely. At best, the participants feel stimulated by the statements of others to deeper considerations and expressions of their own. The moderator has to recognize the group dynamics and give them free rein without losing sight of the focused topic. The interview guideline was created by the research team based on previous research on relevant aspects (including research on existing websites on mental illness, and health literacy). In addition to an introductory question, several questions on the aspects of interest were formulated in order to loosely supervise the conversation or point towards left out topics which, in the end, lead to more detailed information. The questions refer to the following aspects: • Everyday life of young people (challenges; the reality of life): The participants were generally asked how they were doing in everyday life, whether there are things at school, at work, or in the family that stress them. They were also asked how they deal with possible problems and challenges. • Procurement and understanding of health-relevant information: It was discussed where and how the participants obtain information on health topics. In addition, specific information formats such as texts, videos, etc., were addressed in order to determine preferences, among other things. • Importance of mental health (experiences and handling): Previous experiences with mental illnesses in the personal environment and general interest in psyche were discussed. In particular, the need for information and the evaluation of the information found with regard to comprehensibility and usefulness were addressed. • Demands on digital services in general and in the context of mental health: Participants were asked for websites and apps that are used in everyday life. They were also asked what they like about these diverse digital possibilities and possibly also media people, e.g., on YouTube, and what they do not like and to what extent there is room for improvement. Of particular interest was the question of the seriousness, trustworthiness, and quality of online offerings. Additionally they were asked about their use of chat rooms or forums as well as quizzes or playful offers on websites. The study was approved by the Ethics Committee of the Medical Faculty of the University of Cologne (n. . The first FGD was conducted as a pretest. The pretest allows to test relevant aspects for the execution of a focus group. Testing the guideline is of primary importance, because it can provide information on whether the composition of the group is optimally aligned with the research questions and the goal of the project, and nevertheless because it offers the opportunity for the moderator to train techniques of discussion leadership in a real focus group situation. During the implementation of this first FGD no difficulties occurred and therefore there was no need for modifications regarding questions, settings, or moderator behavior. For this reason, the data of this FGD, originally announced as pretest, were the first to be included in the present analyses. In the present study nine FGD had to be conducted until saturation was reached, i.e., an additional group is not expected lead to substantially new results [45, 46] . Through our associated partners, the project researchers had the opportunity to advertise the project in schools and cultural associations (with special offers for young people) with posters. One school offered the possibility to take part in the FGD during a project week. Students contacted the teachers if they were interested and were given the information and consent forms to sign in advance. This procedure was used in four FGDs. At another school, the research project was also posted on notice boards for several weeks, so that students could contact the secretary's office if they were interested. A similar procedure was implemented for the associations. In order to obtain results relevant to the target group, the groups were kept as homogeneous as possible. In particular, these were groups that were more or less familiar with each other from the school or club context. The age range within the respective groups was relatively small. Male and female participants were represented in the FGDs that took place in schools. In the four groups that took place in association for social youth work and in a cultural association, only the last group with refugee participants had mixed genders. The other three groups were separated by gender, one purely male and two purely female. The participants were interested persons who responded to the study announcements in schools or at clubs. The participants from schools-a total of 5 constructed focus groups-were mixed in terms of age, migrant background, and gender, as befits everyday school life. The participants from the clubs were homogenous and natural groups [47] , which means that the participants are connected to each other's in everyday life. One of the female groups, a regular leisure group, consisted of Arabic participants from North Africa. The other female group, consisting of Turkish adolescents, meets regularly at the mosque and undertakes leisure activities. Another group consisted of 5 Turkish and one North African male participants, who already knew each other from previous meetings. Since the participants in this study were part of the study voluntarily (in the sense of self-recruitment), no statement can be made about how many people refused to participate. There were no participants who neither left focus group discussions nor subsequently revised participation. Before each group discussion, consent forms had to be signed and collected. For those under 18, a legal guardian had to sign in addition to the student him-or herself. Since, as already mentioned, the associated partners (teachers and association staff) handed out the information material and the declarations of consent to the interested parties in advance, there were only a few exceptions to the fact that the information material was read and signed before the FGD began. They were given sufficient time to read the material before signing the consent form. Despite these preparations, the information was repeated by the moderator before the start of the FGD and sociodemographic questionnaires with questions on gender, age, migration background, and current educational situation were distributed. Two researchers and one assistant participated in every focus group. One person moderated the discussions, the other researcher documented relevant aspects for later recaps and spontaneously participated in the conversation if the situation required so. Here, the female researcher Ü.S.S. was in all focus groups on the role of the leading moderator. The assistant (psychology student) kept minutes of the speaker sequence in order to be able to assign the statements to the participants after transcribing the material. To avoid technical problems and to secure the recording, two audio recording devices were used. The standardized procedure was as follows: The researchers welcomed the participants, introduced themselves and outlined the project plan. In order to start and maintain a positive atmosphere, the following aspects were asked to be considered: that the discussion remains confidential, that any opinion is welcome, that questions are allowed at any time, and that the focus group can be terminated at any time without giving reasons. At the beginning, the participants were asked to introduce themselves and to give an introductory statement about what interests them about the topic and/or what motivates them to participate. The sound recordings were started before this round of introductions. In order to actively promote the participants' willingness to discuss, the moderator clarified open aspects by asking questions, verified the participants' own understanding by paraphrasing, and in some cases made summaries to guide the course of the discussion. As proof of their participation, the participants received a certificate from the University Hospital Cologne and the TH Köln University of Applied Sciences. In this certificate, the project and its goals were outlined and the focus group discussion was described, including information on how long it lasted. The participants were interested in receiving this certificate to use it in their CV as active voluntary support of a research project. After the focus groups had been carried out, the material, i.e., the tape recordings from two audio devices, were saved as files. One of the files was used as the basis for the transcription. The other file served as a backup file and was used when there were acoustic difficulties in understanding the spoken word. The discussions were transcribed verbatim (in German). After their completion, the analysis of the material was done with MAXQDA, a software for qualitative data analysis [48, 49] . This research is based on the Consolidated criteria for reporting on qualitative studies [50] . For the qualitative content analysis, categories were formed both deductively and inductively [51] . This combined approach has advantages, among others: The deductive categories are based on the guideline and thus reflect the research questions, which serves their exploration. The development of inductive categories from the material makes it possible to generate new aspects that were not previously considered. The first two focus group transcripts (1 group with male participants with a migration background and 1 group with mixed-gender participants with and without a migration background) were coded by the researcher (Ü.S.S.) using MAXQDA (VERBI Software, Berlin, Germany). Here, the aspects of the interview guideline that follow the research questions served as the basis for forming the deductive categories. In addition, inductive categories were formed from the material. After completing the coding of the first two transcripts, another researcher (M.S.) checked the codings and the category system for traceability and completeness. After completion of this check of the analysis results, some subcategories were created and this slightly modified version was discussed and consolidated as a basis for the further coding. Based on this modified category system, Ü.S.S. coded the remaining seven transcripts. After the coding was completed, researcher M.S. again checked all the material for comprehensibility and completeness. There were no significant changes, so that the present analysis has been substantiated by two researchers, both of whom were also in the role of moderator and co-moderator. The quotations of the participants used in this manuscript were translated by a fluent English speaking researcher. Nine FGD with young people were conducted between July 2019 and February 2020. A total of 68 young adults participated in the FGD. Among them were 25 persons of male gender (36.8%) and 43 of female gender (63.2%). The average age of male participants was 23.8 years (SD = 3.2 years) and that of female participants was 22.5 years (SD = 4.0 years). There were 28 participants (41.2%) who had no migration background, whereas 40 participants (58.8%) had a migration background, which was recorded using a sociodemographic questionnaire. There was one group with only female participants of Arab origin (North Africa), and another group with only male participants of Turkish origin and one Arabic participant from North Africa. The second last FGD of a youth group of a mosque was only with female participants of Turkish origin. The FGD in schools were mixed gender and intercultural. The characteristics of the participants are presented in Table 1 . The participants showed great interest in the topic and usually spent 90 min discussing the aspects. Personal experiences with psychological topics were shared with the other participants in a confidential setting and very tangible ideas and wishes were expressed regarding digital offerings. An overview of the categories formed is shown in Table 2 . As already mentioned, these were generated deductively and inductively. The most important categories cover research questions relating to the everyday lives of young people, such as media use, challenges in everyday life and experiences with mental health problems. The general use of media and its importance were also surveyed, as well as media use for information needs on health issues. Following the model of Sorensen et al. [19] , the coding of the material was also carried out with regard to the health literacy components. In the following, a selection of the results is presented as an example based on the participants' quotes.  Many participants had experiences with mental health problems and were aware of similar experiences of relatives and friends. In the FGD, the participants consistently named various psychological symptoms and illnesses, including personality disorders such as borderline, eating disorders, self-harming behavior, sleep disorders, schizophrenia, anxiety disorders, depression, and suicidal tendencies. Three participants even reported about relatives or friends who committed suicide. The topic of mental stress and illness is omnipresent in the target group. There was a great willingness to talk about the topic. Answers range from every day, ""little problems"", to many years of experience with mental illness. Some were satisfied with the treatment, others were very frustrated. Participants who have not yet had any connection to the topic were willing to inform themselves about it (""Sad to know so little"") or would have been grateful for an appropriate offer in case of a confrontation. In terms of reasons were mentioned: ""generational problems"", lack of planning; fast pace of life; dullness; social pressure, comparisons with others or ""false"" ideals (Instagram and celebrities), superficiality, bullying, ""nobody reflects on themselves anymore"", anonymity, hardly any social contacts outside the social media; even in personal meetings ""everybody is hanging on the cell phone."" The majority of respondents expressed psychological problems as a relevant problem in their social environment and at the same time expressed the wish that this should receive more attention. ""I can imagine that every second person in this school has problems or knows people who have problems with something like that. I think it would be really helpful if there was a psychology subject in school. Then you already learn about it in young age. You would be able to help those affected in some way."" (female, 18 years old, with migration background (wMB) ""Well, I just wanted to say again that I am in favor of everyone seeing a psychologist. And I also think that you should not necessarily call it a psychologist, because the word is always a bit negatively afflicted with wrong ideas. Instead, everyone should be entitled to a kind of life coach. This person should of course have a professional training, because you can get a lot out of personalities if you work on them for a bit. It doesn't necessarily always have to be that someone only has one blatant problem but that you also find something else to work on."" (female, 29 years old, without migration background) The participants reported almost unanimously from their own experiences or those of their relatives that it is problematic to get a psychotherapy place. Both the procedure in general of overcoming oneself and finding someone and the fact that it takes a long time to get a therapy place. ""If you have to go to a psychologist or something like this, you often have to go to a general practitioner first to get a medical certificate. Additionally you have to wait for months until you get an appointment. And people often need acute care. Sometimes it is really urgent, and I think in this case using the internet as an interim measure until an appointment is actually made-there is nothing wrong with that. But as I just said, it is often the case that during this time so many influences can have a negative impact, that it is sometimes too late . . . three months ago it was too late for a friend of mine and she threw herself in front of a train. She waited. She had an appointment . . . and it was so acute and yes, that's when she decided to commit suicide. And I simply believe an interim solution definitely has to be provided."" (female, 28 years old, without migration background) ""I have a friend who came to see me last week. And one night he got some bad news: His friend had killed himself. He is from Afghanistan, and his asylum application has been rejected. And I think he had been in Germany for four or five years. My friend told me, 'Yes, he suffers from depression.' For already, I think, a year or something like that. [ . . . ]. For example, 'I feel very bad and I need someone who always listens.' But my friend told me, 'You know what, when you listen to him, you feel bad yourself.' And because of that, no one wants to help him. Because there's a bad feeling coming back. But after he had killed himself, all friends were like, 'We wish we could listen to him just once more one time,' and things like this. But now it's all over and . . . I think that these people, who suffer from depressions, always need someone who listens and comforts. But I think refugees have no one like that, and that's why it s so hard."" (male, 23 years old, with migration background and refugee experience) Some participants expressed that the topic of the psyche is more present in society, but admitted that it is still repressed and is hardly ever talked about in school or in private contexts. Parents usually do not want to believe that their child has psychological problems. Experiences were reported in which parents adopted a defensive attitude and also related the presence of a psychological problem personally to their upbringing: ""That can't be. That is not the way we raised our child. It has to be healthy."" Parents see external circumstances such as having a good life as a guarantee for physical well-being. It was also mentioned that parents generally lack understanding, and that psychological illnesses or symptoms that limit active participation in life (especially school or job) are associated with laziness. A consequence that relatives who don't understand or simply underestimate mental problems is that the affected children lose the courage to turn to others, e.g., professionals, because they expect the same unsupportive, indifferent and questioning attitude. The experience of a lack of understanding within the family often leads to frustration and resignation and is subsequently generalized: ""In some cultures . . . there is simply no such thing. So as soon as you start talking about not being well, even though there is no particular reason or you have dark thoughts or even think about death, they you say, 'That's bullshit. You are fantasizing it. You are simply imagining it to yourself. That is not true and so on. You have everything.' You are accused of such things . . . . If you are already introverted and sad anyway, if you have depressions or something like that, then you tend not to say anything anymore. Also to friends or to doctors or somebody else. Because it's your own parents. So if they already speak against you, what else should a doctor do."" (female, 18 years old, with migration background) ""That's exactly how it was with me. I didn't tell my mother either and tried to do it myself, because I knew that she would say that I am just crazy. Of course I am crazy. that literally was the problem."" (female, 29 years old, without migration background) The question of whether mental illness is still a taboo subject today was answered very differently. Some said that nowadays people deal with them much more openly and that this is particularly evident in subcultures and in music. Comparisons were made with the past, but today there is greater acceptance and knowledge. It was emphasized even stronger that the topic has only rudimentarily arrived in society and should be given much more attention. The following quotation illustrates this need again: ""I personally think this topic is still too quiet. Far too quiet. And I think society needs to be sensitized much more to this topic. I also think that these people deserve to be heard and that they should not be treated with contempt. That they are taken seriously. And that people shouldn't dismiss it and say, 'Oh come on, he has his little ailments there' or 'But I also have my problems'."" (female, 28 years old, without migration background) There were also fears of rejection or criticism of one's own person by others, as this person pointed out: ""Yes, you can also offer a target in some way. Even if you don't expect to be attacked by your family or friends, but still. . . . You give something of yourself away."" (female, 21 years old, without migration background) The majority of participants use desktop PCs/laptops as well as smartphones and tablets. All participants use at least one social medium; the most common are: • WhatsApp (communication medium; ""You can't do without it""; otherwise you are excluded; what makes it special is that it is free and fast) • Instagram (serves as a distraction; but also: dangerous ideals are presented there-both in terms of body ideals and lifestyles such as vacation photos; ""perfect"", exciting life) • YouTube (both as ""problem solver"" and for fun) • Facebook (named as a contrast to Instagram: Networking through groups, events and sales; criticism: flood of information that doesn't interest you at all) The opinions of the participants differ on the following points: Some said, ""You would be happier without apps"", others tended in the direction of, ""Apps make everything easier"". In addition, the overstimulation and waste of time was seen by some as critical and the observation was made that personal contact has decreased significantly. The most important criterion for good media offerings in general is that new content is constantly being delivered. Regarding the planned website, it was said that it must be of a high quality compared to others. Chat forums are viewed critically because ""Anyone can answer and pretend to be a professional"". At the same time it was mentioned that trust could be increased by the hurdle of login. Nevertheless, communication ""in private"" is desired and proof that you are dealing with a professional person and anonymity is maintained. ""Well, I think such forums would make sense, because there is such a thing, where you can call, no matter what the topic is. There is still a bit of inhibition threshold, if you really call this anonymous number or not. And you don't know who is on the phone right now. Does the person really understand you? Then sometimes it is perhaps easier to explain in writing how you feel than on the phone."" (female, 28 years old, without migrant background) There are culturally colored views on mental disorders that do not see any disease value in the symptoms. Above all, the picture changes depending on the gender of the person concerned: ""Again, about this thing in our home country, so I think in most countries, which is so Muslim, when a woman as an unmarried woman-if you are depressed or crying somehow-everyone says: 'Yes, sure! She longs for a man.' And then when she is married, everyone says, 'Yes, she doesn't like her husband. Apparently, she has a lover.' In other words, it's always viewed from a different perspective, a different position on the matter, and not what's really there, what her problem is."" (male, 23 years old, with migration background and refugee experience) Concerning men, the picture is as follows: ""So they say, 'Oh God, the poor man has worked so hard and suffered so much! He took care of the whole family and made sure they got something to eat. Of course you go crazy.' Yes, with men it's always a matter of course, 'Oh, the poor guy!', and with women, 'Are you in love? Well, are you longing for a man? Would you like a man right away?' Or, if you already have a man, 'Well, are you in love with his brother?' Right, it's always like that. They don't realize what their problems really are."" (male, 23 years old, with migration background and refugee experience) About the belief in ""evil eyes"" or Jinn as a cause of illness ""So there is a girl, she is about 25 years old. She has already studied the whole Koran. She was finished and took several courses, so . . . language courses. And has her diploma and everything. . . . At that age it's marriageable age there. . . . She still hasn't married. And she was acting a little bit strange. And all her parents and her family, her neighbors see her as mentally ill because she behaves a little bit different. And they say okay, I say 'eye' or it's magic, witchcraft. Or because she didn't marry . . . that she has a jinn in her body."" (female, 20 years old, with migrant background) The participants reported extensively on various media that are an integral part of their everyday lives. Reasons for media use were mentioned: Learning/researching; leisure time, including entertainment, relaxation and pursuing interests; communication/networking; being able to keep up or be up-to-date; practical help in everyday life. In the context of psyche, self-diagnosis or diagnosis for others was also mentioned. First of all, it was emphasized that it is difficult to obtain reliable information and that it always involves an effort. This contributes to the fact that one does not search at all and puts back his needs and complaints: ""Sometimes there is the short thought, 'I need help', but then it's gone again. The hurdle is too big, to get information would be too much effort."" (female, 23 years old, without migration background) Basically, searches in search engines are viewed critically and as something that is difficult: ""When searching on google, the problem is that google shows those things first an average user concerns most or what has been viewed most. This means, when starting something new you need the help of our main media providers for advertisement to be found."" (male, 22 years old, without migration background) In general, participants reported that they explicitly call up pages where it is evident that professionals, e.g., doctors, produce content or respond to questions, but do not feel addressed: ""Well, I first looked for the diagnosis and then for what was available on the Internet. And then there were sites like Netdoktor or Onmeda. There they focus on medication, on symptoms and then at the bottom they provide a reference: So that you can contact a specialist or something like that. There also exist selfhelp groups and stuff like that. But, yes it wasn't really personal. So I did not feel addressed."" (male, 20 years old, with migration background) Various problems were raised in the search for help. For example, the fundamental doubts as to whether one's own research and assessments are correct. These doubts then lead to further obstacles ""Am I allowed to take advantage of this now at all? What if I misdiagnosed myself?"" A further problem is that the technical language would be a deterrent. The participants reported that they did not know who was helping with which problem. At this point, participants with a migrant background mentioned that there can be elementary difficulties when filling out the questionnaires. Some participants mentioned that things ""you just need to know"" are not communicated (such as the possibility of four ""trial"" sessions before starting therapy). Furthermore, there are no helpful hints, only many telephone numbers, which implies a lot of selfinitiative is needed. Many participants also mentioned that ""Dr. Google"" should be avoided. Looking for second opinions on the net would only confuse and instill fear From the perspective of the helping person it is difficult to point the person concerned to the appropriate offer-""No one can be expected to be consolidated as a caregiver/reference person."" Various needs were expressed regarding media content in general and the website on the subject of psyche. First of all, that it has to be a serious offer with contents of concerned persons and professionals, which are personally presented. Because only personal stories from other affected people can help and inspire, but are not enough. A professional person has to provide content as well. The identification with the offer must be simple, the question, ""Am I right here?"" should be answered directly. When linking to regional offers sufficient information is very important! • It is important to remember that telephoning can be difficult or that the surroundings of an institution or how to get there can be crucial for the decision for or against this institution. A lack of knowledge about the content and process of a therapy was expressed: ""My sister already had therapeutic treatments. . . . And I ask myself, What is actually discussed in these hours? Is there a dialogue or a monologue taking place?"" (female, 28 years old, without migration background) Seriousness is recognized very differently. There were participants who based the trustworthiness of a site on how many people use the site and how well known the offer is: ""The more people use it, the greater my trust in it"". Additionally, decisive for some is the look of the page/app. As long as a professional design can be recognized, such as a reasonable structure and coherent menu, no advertising or annoying banners/popups and the information about the operator and promoter of the digital offer, trust is placed. The design for a serious-looking site is coherently described by the participants: clear, ""good"" design; no frills; after all, it is a serious offer; little distraction (""I have never watched a video on a website before"") and a trustworthy choice of color. Some participants demonstrated very good media skills, which ensured that they were able to assess the quality of the content in a much more differentiated way. The idea was put forward that, taking into account the fact that some families have no understanding of mental health problems and that there are also care deficits in the health care system, online services could act as an interim solution. ""Okay I don't have this backbone at home. For whatever reasons, you should at least have the possibility to get help elsewhere if you want to. If it is not given at home. Because I think you have to get help somewhere and I see it as a must. So as I said, this is not only a privilege, it should really be part of it like breathing, eating, sleeping. Just like first aid. If I feel bad, I call an ambulance. That is with us if I feel bad, I call the ambulance. And I think this help . . . should not only be physical help, but also mental help."" (female, 28 years old, without migration background) Barriers when talking to your parents/others about mental problems and anticipated benefits a website would hold: ""That you even open up a third section where the children really say ""Mom, watch out. Ehm maybe you don't quite understand it all. Here is this page. You can simply write it down, give it to them and the parents will be able to visit this page. The page alone will be very meaningful and afterwards, when the parents have read it, they might be like, 'aha my daughter or my son is not the only one. And there seems to be something wrong.' This way you have the possibility to communicate with your parents. It's sad when you don't have it face to face. . . . But maybe this is the first step."" (female, 28 years old, without migration background) The question of the advantages or disadvantages of remote diagnosis (self vs. others) via the Internet was extensively and controversially discussed. As advantage was argued: ""In my opinion, diagnoses do have a value. They can indeed be helpful. At first you don't know what your problem is, and therefore don't know how to go against it. You simply don't know where to start. The diagnosis gives you a name to work with. Now you can start to get rid of it. That's why it is a good starting point. If you don't understand what's wrong, you first need to identify the basic problem, which is a problem in itself."" (male, 25 years old, with migration background) As disadvantage was argued: ""I also think so. In my opinion, it is very, very dangerous to only use the internet, especially when you immediately trust everything you read. When it comes to mental problems it can be very complex and experts are needed to watch over it. You can't try to identify and understand every term by yourself, just to estimate your own position. I don't like that. Most of all, if you do it this way, you automatically apply a negative identity to yourself and get into it way too much. You could develop even more symptoms, just because they fit the diagnosis you just found. Your behavior will change depending on the illness you think you have."" (female, 24 years old, with migration background) Experiences were reported on media content having very disturbing effects. It was mentioned that these are dangers that can often be found on the Internet. Something like romanticizing suicide; or encouraging each other to unhealthy behavior (e.g., anorexia) or self-harming behavior. ""I think that really was the worst thing I could do at that time . . . . Well, my friends and others teased me in school always saying that I'm depressive. That was when I first entered the word depression into Tumblr. Immediately images of arms that where cut open appeared. Below it where words like, 'It did so good. I feel better I'm not sad anymore, the pressure's off.' And as a 14-yearold, when you look at something like that, you think, 'Ah okay, maybe it'll help me. I've been struggling with this for so long. This is the way out.' Now when you're a little bit older of course you think differently, but when you're that deep in it at that age, everything pulls you along. Today I would do it completely different . . . accordingly I would not even enter something like that. Especially not on pages like Instagram, Tumblr, Facebook, or such. Even on Google. If you type in depression and go to pictures, you will immediately get such pictures of self-harm. Immediately. You won't get a table with things you can do to distract yourself or with skills that help you if there is the pressure to hurt yourself. There are only these pictures of self-harm. Pictures of people crying. Things like that."" (female, 18 years old, with migration background) Mental disorders already manifest themselves in adolescence and young adulthood and persist into adulthood or represent preliminary stages of initial manifestation [52, 53] . In accordance with WHO [15] findings, our research showed that adolescents and young adults, despite their young age, already have mental health experiences and problems. In particular, severe mental disorders such as bipolar disorders, depression, alcohol-related disorders, and schizophrenia have an early age of manifestation [53] . Some participants of the FGD expressed the need that the topic of mental health should be included in the life course, i.e., should be dealt with at school, if not in kindergarten. Stigmatizations related to mental illness are still present [10] and interventions to reduce it exist [54] . Empirical findings show that young people in particular experience stigmatization and shame, problems in recognizing symptoms in the sense of low MHL and a preference for autonomy in coping with mental health problems as the main obstacles to seeking help [55] . Studies show that mental health, help-seeking efficiency, and positive attitudes towards people with mental illness can be strengthened by promoting MHL [28, [55] [56] [57] [58] . For example, through the targeted promotion of mental health competence in the form of training, the quality of the intention to help depressed persons, and the self-confidence in providing help could be improved. Likewise the stigmatization and the desire for social distance towards depressive persons decreased [59] . Another study showed that increased knowledge about schizophrenia combined with increased personal social contact was associated with improved attitudes toward people with schizophrenia [60] . In summary, the findings indicate that MHL is related to health and social outcomes. In addition, MHL can be promoted in a targeted manner through interventions. A stronger focus on MHL among others in prevention can reduce health inequalities [61, 62] . There are multiple definitions of a migration background, either depending on nationality, migration, or both. Categorizations are generally problematic, as they can be stigmatizing and may lead to exclusions [63, 64] . Evidence of experiences of discrimination in the education system, in vocational training, in the media, in the housing market in the context of migration background is available [64] . The refugees in particular are facing many challenges. Young people with a history of migration are a population that has to acquire skills for themselves and work their way through systems. They often act as translators for their parents, in everyday life and also during visits to the doctor [65] , which is why promoting their health literacy is particularly crucial for the entire migrant population. However, in Germany in particular there is a lack of data on Health literacy among migrants [66] . Research on Turkish migrants in Germany has shown that the first generation of migrants of Turkish origin has a low sociodemographic and socioeconomic status and has more psychological problems than the native Germans. The first generation has higher rates of depression, panic attacks, and suicidal thoughts, while the second generation has better sociodemographic characteristics and comparable psychological problems to native Germans [67] . In particular, some migrant populations, such as those of Turkish origin, suffer more from mental disorders than the general population [68] . At the same time, there is evidence that the motivation for psychotherapy in the group of Turkish immigrants in Germany is low and that psychotherapeutic treatment in this subgroup is not as effective as in the general German population [68] . In keeping with the experiences and observations of young people with a migrant background reported in this study-especially in the FGD, which took place outside the school context-the Muslim faith in this case has an influence on the view of illness. It became apparent that, in connection with religion, the notions of illness with regard to mental health problems varied from those of the non-Muslim participants in the other focus groups in the schools. The authors Laabdallaoui and Rüschoff [69] , both working in psychiatric-psychotherapeutic fields, summarized on the basis of religious sources what is considered to cause illness according to Islam. In addition to the medical causes of illness, there are other triggers such as the ""evil eye"", magic and other spirit beings, the so-called ""jinn"", which are to be understood as follows: Faith in the effect of the evil eye Accordingly, Muslims believe that the gaze of a person can have a negative effect and can harm the person opposite. The evil eye is said to cause any kind of illness, but depressive symptoms, tension, constant yawning and general malaise are attributed to the effect of the evil eye, to which one is at the mercy of. Belief in the effect of magic or sorcery Abusive practice, which is strictly forbidden in religion, but is nevertheless quite present. Particularly in the case of mental and psychiatric illnesses, the possibility of a cause by magic is not excluded Faith in the effect of other beings, the jinn There is a belief that when jinn are disturbed in their tranquility, they can cause a disturbance to humans [61, 70] . In particular with mental illnesses, psychosis fainting fits, and paralysis phenomena an obsession by a jinn is assumed [71] . Against the background of these ideas of illness, in the case of mental illness the affected persons or their relatives additionally consult a religious scholar, imam, or hodja, who is supposed to support the healing process through Koran recitations or special invocations [69, 72] . According to Laabdallaoui and Rüschoff (2010) , knowledge of the presumed subjective causes of disease is of great importance for the success of treatment [69] . In the study conducted, it was noticed that female participants are more involved in the mental health issues of their relatives and thus have more experiences with mental illnesses. Furthermore, perceptions, attitudes, prevalence, and management of mental illness differ between men and women. Young women have a higher prevalence and a higher severity of depressive symptoms. Brettschneider et al. (2018) investigated the prevalence of depression in the German population and underlined that the 12-month prevalence in women aged 18-34 years rose by about twice as much in the years 1997-1999 and 2009-2012 from 8.8% to 15.6%, although the prevalence of depression in the general population did not increase. The conclusion is that an increased distribution of prevalence among young women is a risk in this specific subgroup [23] . Other research results show that depression in adolescence entails a greater risk of suicidal tendencies among young women; those affected conduct more suicide attempts than men [73] . However, suicide attempts are more often fatal for men [74] . Another gender difference relates to the symptoms of depression: women show typical symptoms such as sadness, withdrawal, increased appetite, weight gain, sleep disorders, somatoform disorders, increased crying, and feelings of guilt. They report more frequent and more pronounced symptoms than men, while men more often suffer from health problems, insomnia, overall listlessness and agitation [75] . Migration, acculturation and mental disorders cause different stress and adaptation processes in men and women. It was found that female migrants with depression or anxiety disorders have higher levels of the migration-related stressor than male migrants, i.e., stress associated with migration and acculturation. In addition, feelings of guilt and self-condemnation are stronger and more often pronounced among female migrants than among male migrants and native Germans. These two manifestations could be related to mental disorders [76] . In this context the term acculturation is understood as a process of cultural identity adaptation when there is long-term contact between members of two different cultures. Morawa and Erim (2014) divide acculturation into four components: integration, assimilation, separation, and marginalization [77] . They tested correlations to depression symptoms with the result of integration being the lowest and marginalization the highest correlated. In contrast to previously stated studies, gender had no influence on acculturation, but on depressive symptoms. Women have a higher level of depressive symptoms than men. Moreover, the first generation of migrants shows more depressive symptoms than the second [77] . Among migrants of Turkish origin, stigma is associated with depression and psychological stress. Patients who are more depressed and show a higher level of psychological stress experience their condition even more stigmatized. Depression and symptoms of other mental disorders influence the concerns and complaints of stigma, which results in even more or stronger concerns. The fear of stigma can lead to misunderstandings, wrong diagnoses and renewed stigmatization [78] . Depending on whether woman or a men is affected in mental illness, there may be general culture-dependent differences in the way people deal with it, what shapes understanding and acceptance (see results). As became clear from the focus group discussions, the media play an important role in the lives of young people. They use the internet for a variety of purposes and see many advantages in it, for example in the context of school for learning purposes, but also for researching things that concern them personally. The following study also showed the role of the media: An online survey on ""COVID-19 Health Literacy"" (COVID-HL), in which 14,895 students from 130 universities in Germany participated, focused on the evaluation of online health information, digital health literacy, and mental health [79] . Results show that the Internet has a special significance as a source of information and that every fifth student has already looked for information on dealing with mental stress. Students found it difficult to judge how reliable the health information on the Web is [79] . The observation that media are very important in the lives of young people has led to the proposal to include the aspect of media literacy in the model of health literacy especially among young people [25] . From the statements of the participants, it became clear that existing offers/possibilities were not confidence-inspiring enough. According to this, a special offer should be created for this purpose, as the existing offers are not sufficiently target-oriented or are not adapted to the target group and its wishes. In order to overcome language barriers, graphic and audio-visual content accompanying the text should be integrated in addition to a simple yet comprehensive provision of information as text [80, 81] . Graphs can be used to illustrate both complex issues and the fundamental importance of mental health in society-an under-represented topic compared to physical health in traditional media, leading to a distorted picture of the relevance, distribution and impact of mental illness [82] . Audiovisual content also has an increased effect on the recipients' motivation to deal with the topic, to question their own behavior and to integrate mental health promotion measures into their everyday lives [83] . The gap between the intended and actual future behavior of the recipients can be closed by using understandable, positive language in the videos [84] , so that in addition to the increase in knowledge, the goal of removing taboos and stigmatization is achieved. Interactive design allows to guide the recipients through the website and to provide information at the right time and in the right amount. This principle is considered a factor for a successful improvement of health literacy [85, 86] . Insights into production and reception as well as long-term changes in attitudes and actions are considered to be insufficiently researched [87] , and can also be gained through the planned evaluation. The destigmatization of mental illness also plays a central role in the preparation of all information and topic-specific content and is pursued through the use of target groupspecific language and education. Prejudices and stigma are lowered by the fact that a broad and well-founded education takes place and the focus is not on warning/fear appeals, but on serious and realistic advice. The innovation and model character of the project lies in the fact that a broad group (age, gender, migration background, and educational level) of teenagers and young people can be reached with the digital offer. Due to the multilingualism, particularly vulnerable groups are taken into account. Through cooperation with the associated partners and references to the project on their websites as well as social media channels, the website is to be made widely known on a long-term basis. A long-term use and perpetuation of the project is guaranteed by the cooperation with the associated partners in the schools, therapy institutes, and migrant organizations, thus covering the target groups and the typical contact points and ensuring that the existing offer continuously reaches the target group. Based on the results from the FGD, the following elements will be implemented on the website: To improve the mediation of psychological content, which can be quite complex, multimodal processing is necessary. This made it clear that texts and videos are in demand within the target group. Explicitly redundant content, which is, however, presented multimodally-i.e., via different media formats-is desired. For this reason, video clips are to be created with experts who convey complicated content in a comprehensible manner, thus supporting the texts on the website. For each presented disorder, especially depression, anxiety disorders, personality disorders, psychoses and addictions and possibly other disorders (e.g., eating disorders and obsessive-compulsive disorders), several short videos will be produced on the aspects of symptoms, causes, handling, and therapy. Another result of the FGD is the demand for graphics. These are to be produced to present complex contents in a simplified way. Just like the videos, they serve to support the text-based content and are created and integrated according to the layout of the site. Presentation of all contact points for mentally ill persons and their relatives: The goal is to present the offers on the website in a kind of ""profile"", if necessary with photos of the contact points and/or the respective contact persons. In the FGD it became clear that existing information on the Internet was not manageable and could not be checked for seriousness; besides, the needs or hardships of people with acute psychological problems were not addressed. During the FGD, the target group expressed the wish for an English version of the website. Here, participants with refugee experience identified English as a common and largely understandable language among young people. Therefore, the content should be provided in English. The information on the website will be kept identical for all. In line with the long-term project goals, it is planned to translate the content into Arabic and, if necessary, Turkish. The aim is to make the necessary information regarding mental health and illness equally accessible to people with and without language barriers. In detail, the website aims to enable young people to obtain information quickly and easily about: [36] . The method of recording MHL via short vignettes described above is the most popular recording method, but it is criticized because the vignettes can only cover a small number of disorders and not all areas of MHL [28] . It is also argued that the use of vignettes neglects the ability to distinguish mental disorders from a mental health problem in the sense of everyday stress (Kutcher et al., 2016) . In addition, reference is made to the limited generalizability and replicability of the findings due to the use of different and mostly culture-bound vignettes [88] . As part of the evaluation of the new website, the acceptance of the website in terms of design, layout, structure, and comprehensibility is to be recorded with young people with and without a migration background. This will be done by a combination of qualitative interviews and quantitative questionnaires. Overall, a recursive approach is needed, intermediate products are evaluated and adjusted, which requires further evaluation to check whether the changes were in the interest of the target group. In addition, the website's contribution to increasing the level of information is to be recorded in a pre-post comparison. For this purpose, a small sample of young people should be asked to write down their knowledge about disorders and their most relevant parts (pre-test), then the website should be presented. After reception a new request for relevant information should be made (posttest), so that a possible increase in knowledge can be quantified. Finally, these results will be incorporated into the improved version of the website. One limitation of this study is that the nine focus groups were implemented in Cologne and the surrounding area and that a local dependency may exist with regard to the experiences in everyday life or in society as well as the view of the care situation. However, since it was not the aim of the study to achieve representative results or even generalizations to the target group of young people with and without a migration background, the local restriction has no significance. After all, the goal of qualitative research is to capture the participants' individual perspectives on a particular topic and therefore serves as a means to better understand phenomena in a holistic manner [89] . Overall, each of the focus groups was uniquely constructed in its composition. The focus groups varied in terms of age, migration background, gender and religion of the participants. There could be a selection bias here, as participants were recruited via announcements posted at schools and clubs, whereupon interested individuals could sign up. It is possible that those who had a particular concern or interest in this topic and were willing to participate in the discussion within the group setting were more likely to do so. In contrast, there may be individuals who were affected by mental health issues and may have felt more deterred to participate in a group discussion on this sensitive topic. It could therefore be that possibly relevant aspects of affected persons, which would have been substantial to explore the topic, are not reflected in the material. Another limitation may be the low variation in school types. Both schools that participated in our project-a total of five FGDs-were educational institutions that offer adults the opportunity to obtain a technical college entrance qualification and a general higher education entrance qualification via the second educational pathway. Originally, secondary schools from the ninth grade (15 and 16-year-old students) were also to be involved. However, during the course of the project, it became apparent that the requested schools were unable to participate in our study due to their own capacity limitations. Moreover, due to the current pandemic (COVID- 19) , no additional schools could be recruited. Consequently, for the purpose of feasibility in terms of schools, we limited ourselves to those schools with willingness to participate with adult students. However, this implies that the perspective of adolescents may have been somewhat neglected, despite the fact that there were also adolescent participants from clubs in our study. This section is not mandatory, but may be added if there are patents resulting from the work reported in this manuscript.@story_separate@Our research results show how widespread experiences with mental illness are in the target group of adolescents and young adults with and without migration background, who formed the study sample. It also became clear that media are indispensable in the everyday life of the participating young people and that health information on the Internet is an important resource for understanding the problem and finding solutions. With regard to the difficulties in handling and treatment, it became clear that there are very basic barriers to addressing psychological problems openly, since mental illness is still met with incomprehension and prejudice. In addition to these problems on the individual level, there are also deficits in health care, such as long waiting times for a place on a treatment program, which affect the society as a whole and make it difficult to receive timely and appropriate help. Young refugees in this sample are exposed to further barriers and care deficits in addition to the stresses and strains of fleeing, as they do not receive adequate help due to language-barriers and treatment options are hardly available. In the long term, these care deficits lead to chronification and irreversible consequences such as suicide. Adolescents and young adults with a migrant background (participants of the FGD 1, 7 and 8), who are now living in Germany in the third generation, also reported experiences of discrimination and further burdens. Muslim participants of this study reported a culturally colored view of mental illness. It also became clear that young people want more openness and reliable information. The need to implement the topic of mental health into the education system on an institutional level was expressed. The need for reliable information on the Internet to help in an acute situation-at least as an interim solution-was also expressed. At the same time, negative aspects and dangers of media content were noted and clear indicators/markers of quality, reliability and professionalism, i.e., trustworthiness, were called for. In summary, there is potential for the media to communicate this important topic of mental health in a sensitive, target group-oriented and professional manner. This could give rise to approaches in the health care system to focus even more strongly on this topic and to address the deficits in mental health care. At the societal level, raising awareness for mental illness could be made an integral part of the curriculum in educational institutions and barriers could be broken down. Further research is needed on young people's perspectiveespecially the perspective of those with a migration background-on mental illness and the evaluation of existing offerings on the Internet.","Mental illnesses in adolescence and young adulthood are steadily increasing. Thus, mental disorders represent an individual and societal challenge and an enormous health economic burden, creating an urgent need for research and action. Mental health problems are omnipresent in the life of young people and the internet is the first resource, which helps them to understand their situation. Young people with migration background often have more difficulties accessing health care services. Digital technologies offer an ideal opportunity for a low-threshold platform that addresses the needs of young people. The current project “GeKo:mental” aims to design a multilingual website for Cologne-based adolescents and young adults that will enable them to obtain comprehensive information about mental illness and health, treatment options and first contact points. To design this website, this study aims to find out what kind of health information is needed and how it should best be presented. Nine focus group discussions with adolescents and young adults with and without migration background (N = 68) were conducted; the focus group discussions took place at schools, in an association for social youth work and in an cultural association, which is linked to a mosque in Cologne, Germany. A qualitative content analysis was conducted on the gathered material. The participants reported concrete challenges and needs. The results will form the basis for the development and design of a website."
"Infectious disease (ID) is a leading cause of death and premature death worldwide, despite a decreasing trend [1] . In 2017, nearly 9 million people died and more than 420 thousand years of life were lost due to IDs, accounting for 15% of all deaths and 25% of years of life lost, respectively [1] . Particularly, ID is the leading cause of death among children under 15 years of age, comprising almost half of the total deaths [1] . Although the burden of IDs is disproportionately concentrated in low-and middle-income countries [1] , it remains a major public health threat in high-income countries [2, 3] . In South Korea (hereafter referred to as Korea), ID mortality was on a decline until the mid-1990s; however, it has been continuously increasing since the mid-2000s after a temporary increase during the 1997 economic crisis [2] . An increase in mortality due to pneumonia and sepsis has led to a recent unfavorable trend [2, 4] . In 2018, pneumonia was the third leading cause of death, while sepsis was the 9th and 10th leading cause of death in women and men, respectively [4] . These diseases, along with tuberculosis, continue to contribute to the socioeconomic gap in mortality and life expectancy [2, 5] . Mortality trends can be affected by biological changes associated with aging, social impact at a particular time, and shared experiences among a specific generation [6] . Vulnerable age groups differed depending on the type of ID. Although the influenza mortality rate showed a trimodal age distribution; high in children, young adults, and the elderly [7], overall ID mortality was reported to increase incrementally with age [8] . Social changes 2 of 12 over time, such as economic development and crisis [8] [9] [10] , urbanization [11] , and enhancement of healthcare and public health systems [3, 8, 9] , might influence ID mortality. The risk of death from IDs tended to be lower among those born during periods with improved living conditions than those who were not [8, 9] . Therefore, disentangling age, period, and cohort (APC) effects would help comprehend the possible reasons underlying ID mortality trends. Research on ID mortality trends has primarily focused on temporal changes [2, 3, 8] . Several studies that examined the APC effects on ID mortality were focused only on specific IDs like influenza and pneumonia [7, 9] . Research concerning the APC effects on overall ID mortality is scarce [8] and has never been undertaken in Korea. The findings of a previous study that analyzed the temporal trend of ID mortality in Korea were inappropriate for the interpretation of temporal changes and international comparisons because of the use of crude mortality rates [2] . Korea is among the world's fastest aging [4] and most educated societies [12] . It has achieved rapid economic growth and suffered from economic crisis [13] . The Korean government has established various measures and policies for ID control and prevention, including legislations, the implementation of national immunization programs, and the expansion of health coverage [14, 15] . The study aimed to describe ID mortality trends and estimate APC effects on ID mortality in Korea from 1983 to 2017, thereby exploring potential factors that might affect ID mortality trends.@story_separate@Using data from the national death certificate and the population census of Statistics Korea between 1983 and 2017, the annual number of ID deaths and the corresponding mid-year census population estimates were obtained by sex and age [4] . The underlying causes of death in Korea were classified according to the International Classification of Diseases (ICD) 9th Revision until 1994 and have been coded using ICD-10 since 1995. For the number of deaths from IDs that were coded with ICD-9 from 1983 to 1994, Statistics Korea publicly provides the number of ID deaths changed into the ICD-10 codes using the World Health Organization ICD-9/ICD-10 translator [16] . Accordingly, ID deaths were defined by ICD-10 codes of A00-B99, G00, G03, G04, I00-I09, J09-J18, J86, and M86 [2] , which were the codes available in aggregate death certificate data among the codes covering IDs [17] . A total of 440,154 ID deaths were included in the analysis, excluding 126 deaths from IDs from an unknown age group. Age-standardized mortality rates (ASMRs) from IDs per 100,000 person-years were calculated by the direct method, using the world standard population as the reference population [18] , for the overall population, both sexes, and age groups (0-4, 5-24, 25-44, 45-64, and ≥65 years). Joinpoint regression analysis was used to identify significant changes in the slope of the ID mortality trend, using the Monte Carlo permutation method [19] . The estimated annual percent change (EAPC) was calculated for each linear trend to measure the pace of change in ASMR and the corresponding 95% confidence intervals (CIs) were computed [19] . To assess whether APC components affected ID mortality, data on ID deaths and midyear populations were aggregated into 5-year age groups (0-4, 5-9, . . . , ≥80 years) and 5-year periods (1983-1987, 1988-1992, . . . , 2013-2017) . Partially overlapping birth cohorts were derived (1903, 1908, . . . , 2013) by subtracting the mean age from the mean period, which represented nine birth-years (e.g., the 2013 cohort included those born between 2009 and 2017). To evaluate the independent effects of APC components while avoiding linear dependence (i.e., period = age + cohort), APC models estimated using the intrinsic estimator (IE), based on log-linear Poisson regression, were fitted with the ""apc_ie"" module in Stata [6] . The APC coefficient estimates from the IE models were exponentiated to produce rate ratios (RRs), which indicate the risk of ID deaths of a specific APC category relative to the average risk of all APC categories combined. The goodness-of-fit statistics suggested that a full APC model was preferable over any combination of APC components (Supplementary Table S1 24 .7 in 1990 (EAPC −10.1, 95% CI −12.3 to −7.7), then slowed down (EAPC −1.5, 95% CI −2.3 to −0.7) with a temporary rise in the late 1990s, and has gradually increased since around the mid-2000s (EAPC 4.0, 95% CI 2.8 to 5.2). The ASMRs by age group exhibited concave upward trends with a steep to moderate negative slope in the <65 age groups. However, the elderly group has had a positive slope since the mid-2000s and was the only group with no ID mortality reduction (EAPC 1.4, 95% CI −0.7 to 3.5). There were upward inflections around 1998 in the ≥25 age groups, which were more pronounced among the ≥65 age group and men. Sex differences in ASMRs by age groups were variable but minimal in the 0-24 age groups, whereas in the ≥25 age groups, ASMRs in women were consistently lower than in men or the entire population. linear dependence (i.e., period = age + cohort), APC models estimated using the intrinsic estimator (IE), based on log-linear Poisson regression, were fitted with the ""apc_ie"" module in Stata [6] . The APC coefficient estimates from the IE models were exponentiated to produce rate ratios (RRs), which indicate the risk of ID deaths of a specific APC category relative to the average risk of all APC categories combined. The goodness-of-fit statistics suggested that a full APC model was preferable over any combination of APC components (Supplementary Table S1 Figure 1 , Table 1, and Supplementary Table S2 present trends and changes in ASMRs due to IDs in Korea from 1983-2017. The findings of the joinpoint regression models are shown in Table 1 . During the study period, the ASMR declined annually by 1.8%, plummeted from 56.4 in 1983 to 24.7 in 1990 (EAPC −10.1, 95% CI −12.3 to −7.7), then slowed down (EAPC −1.5, 95% CI −2.3 to −0.7) with a temporary rise in the late 1990s, and has gradually increased since around the mid-2000s (EAPC 4.0, 95% CI 2.8 to 5.2). The ASMRs by age group exhibited concave upward trends with a steep to moderate negative slope in the <65 age groups. However, the elderly group has had a positive slope since the mid-2000s and was the only group with no ID mortality reduction (EAPC 1.4, 95% CI −0.7 to 3.5). There were upward inflections around 1998 in the ≥25 age groups, which were more pronounced among the ≥65 age group and men. Sex differences in ASMRs by age groups were variable but minimal in the 0-24 age groups, whereas in the ≥25 age groups, ASMRs in women were consistently lower than in men or the entire population. The findings of the IE models in Figure 2 and Supplementary The findings of the IE models in Figure 2 and Supplementary Table S3 suggested that each of the APC components substantially influenced ID mortality. The age effect showed a J-shaped concave upward curve with the lowest mortality in the 20-24 age group (RR 0.18, 95% CI 0.16 to 0.22), a surge in the 70-74 age group (RR 5.46, 95% CI 5.04 to 5.91), and the highest in the ≥80 age group (RR 44.68, 95% CI 40.75 to 48.98). The age effect gradually increased in men from 30s to 50s, but did not change significantly in women from 15 to 49 years. The period effect trend followed the concave upward tendency of the join-  −10.1 (−12.3, −7.7) - 1983-1990 −8.4 (−10.2, −6.6) - 1983-1990 −11.2 (−13.0, −9.4) - 1990-2007 −1.5 (−2.3, −0.7) <0.0001 1990-1996 −3.1 (−6.6, 0.5) 0.0114 1990-1997 −3.2 (−6.3, −0.0) 0 To examine factors that may have potentially affected ID mortality changes in Korea from 1983-2017, we analyzed the ID mortality trends and evaluated the effects of APC components on ID mortality. The ID mortality trend showed a W-shaped curve with an upward inflection in the late 1990s. APC components independently influenced this trend significantly. The J-shaped age effects reflected changes in the immune response to IDs over a lifetime, as the immune system is relatively immature at birth, matures with growth and development, and then declines in function in the elderly [20] . Old age, especially ≥70 years, was a critical factor for ID deaths in Korea, which could be attributed to biological susceptibility to IDs, but might also be ascribed to the socioeconomic vulnerability of the elderly. For instance, Korea's relative poverty rate (proportion of population with incomes below 50% of the median income) for the elderly has been around 45%, over three times higher than the OECD and national averages [21] . Among the elderly, the rate was even higher among those who were older, lived alone, female, and less educated [21] . In 2017, 58.4% of the elderly were undereducated (no education to primary education), and 30.9% were working mostly in precarious employment, including self-employment, to make a living [4] . The socially disadvantaged status of the elderly in Korea might have contributed to a high risk of ID deaths [2, 22, 23] . The ASMRs in men were higher than in women among the ≥25 age groups and little change in age effect was observed among women of reproductive age. These findings could be explained by behavioral (e.g., unhealthy behaviors of men such as smoking and alcohol intake) or biological (e.g., regulatory role of the X chromosome for immune functioning, immunosuppressive effects of male sex hormones, and immunoprotective effects of female sex hormones) differences between the sexes [24] . The ASMR from IDs in Korea had declined until the mid-1990s, increased temporarily during the 1997 economic crisis, and then gradually increased. Although it was difficult to directly compare the ID mortality trends due to differences in the definition and epidemiology of IDs and the standard population, the ID mortality rates remained unchanged in the United States from 1980-2014 [25] , declined overall from 1980-2011 in Spain [17] , and decreased from 1958 to the mid-1990s, increased from 1997-2003, and then decreased in Thailand [26] . In these countries, changes in mortality due to the epidemics of HIV/AIDS were a main factor affecting the ID mortality trends [17, 25, 26] . On the contrary, in Korea, HIV/AIDS mortality has been low [27] , and changes in tuberculosis and pneumonia mortalities played a major role in the trend in ID mortality [2, 4] . The dramatic drop in period effects in the 1980s could be primarily attributed to rapid economic growth, which meant an increase in available resources for individuals, improved public health measures, and significant progress towards universal health coverage [2, 8, 9, 11] . Previous research reported that the reduction of ID deaths in the 1980s was driven by decreases in infant and tuberculosis-related deaths [2] . Implementation of the free mandatory immunization program for children in 1974 and better sanitation and hygiene alongside urbanization might have contributed to the reduction of infant ID mortality by decreasing mortality rates from vaccine-preventable and intestinal infections [4, 11, 15] . Increased access to healthcare by expanding health coverage (29.6% in 1980 to the entire population in 1989) might have made a substantial contribution to reducing ID mortality risk, including tuberculosis, pneumonia, and infant ID mortality [2, 15] . The decline in tuberculosis mortality, the leading cause of ID deaths until the 1990s, could also be attributed to the enhanced control of tuberculosis, such as vaccination, early detection, and effective treatment (e.g., use of rifampicin in treatment regimens from 1980, which increased the recovery rate by reducing the duration of anti-tuberculosis treatment by 50%) [2, 15] . The slowdown in the ASMR decline in the early 1990s might be linked to changes in socioeconomic inequality. Korea's economic growth until the 1980s led to improvements in both the average income and income distribution. However, globalization and economic restructuring led to a rise in income inequality from the early 1990s [28, 29] . Growing socioeconomic inequality may negatively influence ID mortality by increasing the risk of exposure to IDs (e.g., overcrowding and unhygienic environment), susceptibility to IDs (e.g., reduced immunity due to high stress levels and poor nutritional status), and disease severity of IDs (e.g., decreased access to quality healthcare and unhealthy behaviors) [10] . The 1997 economic crisis (December 1997 to August 2001) had devastating consequences for the socioeconomic contexts in Korea. Comprehensive structural re-adjustments and the subsequent neoliberal economic policies strikingly increased labor market flexibility, layoffs, job insecurity, and precarious employment, thereby increasing income inequality [13, 28, 30] . The adverse effects of the economic crisis on ID mortality were more prominent in the middle-aged male economically active population and the elderly, probably due to massive layoffs of workers in their 40s to 50s, low female labor-force participation rate, and a large proportion of older people with low socioeconomic status [4, 30] . Although ID mortalities among the <65 age groups have decreased slowly since the early 1990s, they are still affected by the economic crisis in the economically active age groups. However, ID mortalities have been steadily decreasing until recently, consistent with previous findings that the economic crisis may not have long-term effects on ID mortality [10] . The findings may be attributed to improved health and hygiene awareness and continued advances in healthcare and public health measures [15] . However, given the limited social safety net in Korea, the elderly suffered the most due to the economic crisis, with little social protection [2, 13] . Notwithstanding the Korean government's efforts to expand and reinforce the social safety net in response to the economic crisis, they were still insufficient to ameliorate socioeconomic disadvantages of the elderly [30] , thereby increasing ID mortality risk during the economic crisis and thereafter [23] . Moreover, Korea became an aging society with a 7.2% share of the elderly population in 2000, and an aged society with 14.3% in 2018 [4] . Combined with an epidemiological transition to non-communicable diseases and increasing socioeconomic inequality [23] , population aging has led to an increase in elderly people with immunocompromising conditions [2] . The proportion of older people with multimorbidity (having three or more chronic conditions) increased from 30.7% in 2008 to 51.0% in 2017 [4] . An increasing proportion of elderly people with multimorbidity might have increased vulnerability to IDs, possibly raising the ASMR of the elderly [22] . On the other hand, a rise in nosocomial infections and antimicrobial resistance from the 1990s might have contributed to increasing ASMRs from IDs such as pneumonia and sepsis, especially in the elderly [2, 15] . As Korea has the world's fastest aging population [4] , there is a possibility that the burden of IDs in the elderly will persistently increase [31] . Therefore, policies to mitigate ID mortality in the elderly should be strengthened, including social protection policies (e.g., public income transfers), and should not be limited to healthcare policies [22, 23] . The cohort effect showed an inverted U-shape. The increasing RR of ID deaths among the cohorts born between 1903 and 1943 might be the consequences of deteriorating living standards of the Korean people during the 1910-1945 Japanese forced occupation period [29] . It has been noted that low socioeconomic circumstances, poor nutritional conditions, and exposure to IDs in early life may be associated with an increase in ID mortality in later life [32, 33] . Throughout this period, Koreans experienced worsening nutritional status, decreased average height, lack of educational opportunities, widening socioeconomic inequality, and increasing ID mortality [14, 29, 34] , possibly resulting in an increased ID mortality risk. Extreme exploitation of material and human resources by Japan to secure war supplies for the China-Japan War and World War II between 1931 and 1945 exacerbated the living standards for the Koreans [29] , which might have caused the peak of ID mortality risk in the 1933-1943 cohorts. Among the 1933-1943 cohorts, the RR slightly decreased in men after peaking in the 1933 cohort, but gradually increased in women, peaking in the 1943 cohort. Although the primary education enrolment rate increased in the mid-1930s, widespread sex disparities meant the rate was 60% for males and 20% for females in 1940 [35] . Sex disparities in educational experiences might have contributed to the early emergence of the peak in men compared to women [22] . After the independence in 1945, extreme socioeconomic inequality was rapidly alleviated by the land reform that lowered the high percentage rent rate and brought the Japanese-owned farmland back to Korean farmers [29] . The release of suppressed educational demand with the full implementation of compulsory primary education in 1954 led to an increase in enrolment rates for primary education [34] , probably reducing the RR in the 1948 cohort. The absence of any significant reduction in RR in the 1953 cohort could be attributed to the devastating influence of the 1950-1953 Korean War, consistent with previous findings that children born during the war had an elevated mortality risk at older ages than adjacent cohorts [36, 37] . The war created a disrupted social system, further leading to a poor nutritional status, inadequate sanitation, overcrowded living conditions, and impaired access to healthcare and public health services, leading to ID epidemics such as smallpox, typhus, and tuberculosis [15, 22] . These detrimental war-related experiences in the 1953 cohort might have increased the risk of ID deaths [33, 37] . However, the Korean War seemed to have affected only the male ID mortality risk, which might be ascribed to a higher vulnerability to IDs (e.g., tuberculosis) in men than women [24] , selective survival bias due to the lack of parental investment in unhealthy female children, and greater effects of husband's socioeconomic status on women's health in later life rather than the birth year of women [37] . The steep decline in RR among the cohorts born in the 1960s to the early 1980s could be attributed to rapid socioeconomic developments since the 1960s [8, 9, 15] . The economy grew swiftly with an annual average rate of nearly 10%, restoring the infrastructure damaged by the war in the 1950s, while keeping the level of inequality relatively low [28] . The gross national income per capita increased about 18-fold from USD 120 in 1962 to USD 2150 in 1983 [4] . The primary education enrolment rate had already reached 96% in 1959 and the consistent increase in the enrolment rate and the advancement rate to secondary or higher education made Korea one of the world's most educated societies in the late 1990s [12, 34] . Additionally, the government implemented various programs and policies to control and prevent IDs, including the enactment and amendment of ID-related legislations, the implementation of a parasite eradication program, the expansion of a national immunization program, and the establishment of an ID management system [14, 15] . The accumulation of physical and human capital and the government's efforts might have contributed to the reduction of RR in the 1963-1983 cohorts [22] . Although ID-related programs and policies had been strengthened in the 1980s and afterwards [14, 15] , the RR decline slowed down from the 1988 cohort. Similarly, the total fertility rate in Korea steeply decreased from 4.53 in 1970 to 1.74 in 1984, maintaining this level until the late 1990s, and fell to 1.18 in 2002, the lowest-low fertility level, and has continues to decline to the present [4] . Previous studies have shown that the reduction in fertility rate is associated with a decrease in childhood ID mortality, due to fewer opportunities for transmission and more parental care for each offspring [31, 38] , which might partly explain the decreasing trend of RR. Several limitations should be noted. First, the cause-of-death data in Korea do not have a very high proportion of deaths registered to a well-defined cause (75.8% from 1985-2016) [39] . The classification accuracy of ID deaths was reported to be 66.7% [40] . The misclassification bias might have underestimated the ID mortality. Second, since only aggregate data on ID deaths were available, some IDs could not be included in the analysis, such as perinatal infections (P23, P35-39), cardiac infections (I30, I33), and genitourinary infections (N10-13) [2] . Third, it was not possible to trace the trajectory of ID mortality over the whole life course of a particular cohort. Fourth, the findings of this study do not demonstrate any association or causation between ID mortality and a specific policy or socioeconomic event, but might have implications for inferring potential reasons for ID mortality changes. Fifth, the classification accuracy of the cause of death may have changed over time due to advances in diagnostic technology, which might make it difficult to compare ID mortalities between time periods [41] . Moreover, changes in the ICD coding scheme may have affected the ID mortality trend, therefore, the trend might reflect a change in the cause-of-death coding scheme rather than a change in the actual ID mortality [41] . Lastly, despite discussing potential drivers for ID mortality changes using available evidence, there might be other explanations for these changes. Further research is needed to better understand ID mortality trends.@story_separate@The ID mortality trends in Korea from 1983 to 2017 seem to have been affected by various socioeconomic events, including educational expansion, socioeconomic inequality changes, economic growth and crisis, demographic transition, the Korean War, enhanced access to healthcare, and the improvement of ID control and prevention measures, via period and cohort effects. In difficult times, such as the economic crisis or deepening socioeconomic inequality, a vulnerable population with low education or income might be more at risk of dying from IDs. However, the expansion of education and health coverage and strengthening public health might have had an impact on mitigating these risks at the population level. The recent increasing ID mortality trend suggests an urgent need to protect elderly people from the negative consequences of IDs. Alongside healthcare and public health policies, social protection policies should be further improved to reduce the growing burden of ID deaths. Supplementary Materials: The following are available online at https://www.mdpi.com/1660-460 1/18/3/906/s1, Table S1 : The goodness-of-fit statistics for the combination of age, period, and cohort components. Table S2 : Age-standardized mortality rates from infectious diseases by sex and age group in South Korea from 1983 to 2017. Table S3 : Rate ratios based on intrinsic estimator coefficients for age, period, and cohort for infectious disease deaths in South Korea from 1983 to 2017.","We aimed to describe the infectious disease (ID) mortality trends and evaluate age–period–cohort (APC) effects on ID mortality in Korea. Using cause-of-death and census population estimates data from 1983–2017, age-standardized ID mortality trends were investigated by joinpoint regression analysis. The APC effects on ID mortality were estimated using intrinsic estimator models. The age effect showed a J-shaped concave upward curve. Old age, especially ≥70 years, was a critical factor for ID deaths. Similar to the W-shaped period curve, ID mortality rapidly decreased due to economic development and the expansion of health coverage in the 1980s, decelerated with increasing inequality, surged due to the 1997 economic crisis, and has gradually increased since the mid-2000s. The cohort effect showed an inverted U-shape. The increasing cohort effect due to the deterioration of living standards led to a decreasing trend after the independence of Korea. Notwithstanding the slowdown during the 1950–1953 Korean War, educational expansion, economic growth, fertility reduction, and the improvement of ID-related policies might have led to a continued decline among the cohorts born since the 1960s. Diverse socioeconomic events may have influenced ID mortality trends in Korea via period and cohort effects. Policies to reduce the growing burden of ID deaths should be further improved."
"Central nervous system (CNS) demyelination occurs when the myelin responsible for the insulation of axons is damaged, resulting in poor conduction of action potentials, impaired neuronal signalling and, in some cases, partial or complete neuronal loss. Adaptive immunity enables a rapid and intensive response against subsequent exposures to previously encountered antigens. B and T lymphocytes are the key mediators of this branch of the immune system and are responsible for the humoral and cell-mediated adaptive immune response, respectively. Antibodies or immunoglobulins (Igs) are specialised proteins produced by B cells with a precise affinity and specificity for their target antigen. Historically, the brain and spinal cord were perceived as immuneprivileged sites, lacking the conventional lymphatic system accessible to the remainder of the body. 1 Recent decades have shed light upon the complexity of ongoing immune trafficking across the blood-brain and blood-cerebrospinal fluid (CSF) barriers. 2 This intricate interplay between the CNS and the immune system has highlighted the notion that certain neurological demyelinating disorders are attributable to an inflammatory autoimmune pathophysiology. In this review, we explore the complex role of autoimmunity in CNS demyelinating disorders, namely multiple sclerosis (MS), neuromyelitis optica spectrum disorder (NMOSD) and myelin oligodendrocyte glycoprotein antibody-associated disease (MOGAD). Firstly, we investigate the similarities and differences in the clinical characteristics and pathologies between the three disease entities. We then consider hypotheses regarding the autoimmune aetiology of these disorders and the mechanisms involved in disease pathogenesis. Lastly, we discuss trends in the diagnosis and therapy of these disorders as well as future directions in the field of autoimmune demyelination.@story_separate@Multiple sclerosis is a chronic inflammatory neurological disorder characterised by numerous white matter lesions or plaques throughout the CNS. The prevalence of MS is rising globally with an estimated 35.9 people per 100 000 affected by MS which varies significantly depending on geography and ethnicity. 3 Of those diagnosed with MS, there is a clear female preponderance with females 2-4 times as likely to be affected than males. 3 Relapsing-remitting MS (RRMS) is the most common form of the disease and involves the onset of symptoms that are alleviated during periods of recovery until an eventual subsequent attack. While some patients experience recovery from symptoms during remission phases, others persist with residual disability following attacks. 4 Relapse rates can vary, with reported annualised relapse rates of 0.27-1. 66 relapses per year in treatment-na€ ıve MS patients. 5 Between 50 and 80% of patients with RRMS develop debilitating symptoms which worsen in a progressive manner. 6 At this stage, relapsing-remitting patients are recognised as having transitioned to secondary progressive MS (SPMS). There are limited clinical and pathological indicators that identify the transition of RRMS to SPMS. 4 Approximately 15% of MS patients are diagnosed with primary progressive MS and experience persistent accumulation of neurological deficits from disease onset. 7 Disease-modifying therapeutics are often inefficacious at alleviating symptoms of the progressive forms of MS. 8 Diagnosis of MS has been achieved by expert consensus as defined by 2010 McDonald criteria and its subsequent revision in 2017 and particularly emphasises the clinical presentation of demyelinating episodes or attacks that are disseminated in space and time. 9 These episodes are complemented by paraclinical findings of typical white matter lesions observed using magnetic resonance imaging (MRI), abnormalities in visually evoked potentials (VEPs) which evaluate the function of visual pathways, and laboratory CSF testing including the presence of oligoclonal bands (OCBs). 10 There are currently no clinical biomarkers that can predict or distinguish between different MS disease courses. 4, 11 The pathological features of MS lesions strongly support the notion that inflammation-driven mechanisms contribute to the disease. Most demyelinating sites form confluent, perivenous focal lesions observed throughout the CNS in both white and grey matter regions with variable axonal loss and reactive gliosis. 12 Spatial distribution of MS lesions is partial to periventricular, juxtacortical white matter, infratentorial areas such as the cerebellum and the pons, and short lesions of the cervical and thoracic spinal cord. 13 Lesion pathologies are heterogeneous and vary across MS patients, and are composed of mainly activated macrophages and microglia and CD8 + T lymphocytes, with smaller populations of CD4 + T cells, B lymphocytes and plasma cells 12, 14 (Figure 1 ). Although distinct neuropathological patterns have been classically defined, 14 recent recommendations have been made to classify lesions based on the distribution and contents of activated macrophages and microglia. 15 Active lesions can be temporally divided into early or late demyelinating lesions, with the latter characterised by macrophages and microglia containing the more abundant, and thus less readily cleared myelin proteins such as myelin basic protein and myelin proteolipid protein (PLP), while the former also contains these in addition to less abundant, more readily cleared proteins such as myelin oligodendrocyte glycoprotein (MOG) and myelin-associated glycoprotein. 15 Contrastingly, inactive lesions have significantly reduced numbers of macrophages and microglia, are mostly devoid of oligodendrocytes and predominate in MS patients with extended disease durations and progressive forms of MS. 15, 16 Neuromyelitis optica spectrum disorders Neuromyelitis optica spectrum disorders, previously known as Devic's disease or neuromyelitis optica (NMO), refer to a class of demyelinating syndromes characterised by the cooccurrence of inflammation in the optic nerves and spinal cord. The prevalence of NMOSD ranges between 0.5 and 4 people affected per 100 000 17 and has a significantly greater female preponderance compared to MS with a female:male ratio of up to 9:1, particularly in Afro-Caribbean populations. 18, 19 Although a subtype of MS with optic nerve and spinal cord lesions was historically identified in the Asian population, termed 'opticospinal MS', this has since been included under the diagnosis of NMOSD. 20 The age of onset of NMOSD is higher compared to MS and is rarely seen in paediatric patients. 21 NMOSD patients relapse more frequently compared to MS and accumulate greater disability as measured by median expanded disability status scale (EDSS) scores. 21 Furthermore, longitudinally extensive spinal cord lesions were more frequent in NMOSD than in MS. 21 The discovery that the large majority of NMOSD patients harboured IgG antibodies targeting the aquaporin-4 (AQP4) water channel on astrocytes has since become pivotal in NMOSD diagnosis. 22, 23 Consensus diagnostic criteria for NMOSD have been defined by Wingerchuk and colleagues in 2007 24 and refined in 2015 20 subdividing patients into AQP4 antibody (Ab)-positive or AQP4 antibody-negative NMOSD, with the former making up between 60 and 70% of NMOSD cases. 25 Core clinical characteristics defined for NMOSD include optic neuritis (ON), acute myelitis, area postrema syndrome, acute brainstem syndrome, and observation of NMOSD-typical brain lesions with diencephalic clinical syndrome and symptomatic cerebral syndrome. 20 Diagnosis of NMOSD requires the observation of at least one of the aforementioned clinical characteristics upon discovery of AQP4 Ab while more stringent diagnostic requirements need to be met if AQP4 Ab serostatus is negative or unknown. 20 The cooccurrence of other autoantibody-mediated diseases such as systemic lupus erythematosus, Sjogren's syndrome and myasthenia gravis is observed more frequently in NMOSD compared to MS and can strengthen the diagnosis of NMOSD. 20 Pathology of NMOSD lesions is driven by the binding of pathogenic AQP4 Ab on astrocytic endfeet surrounding endothelial cells and has been long identified as a demyelinating disease that is secondary to a primary astrocytopathy. 26 In contrast to the CD8 + T cell predominance in MS lesions, activated CD4 + T cells infiltrating the CNS are central mediators in NMOSD lesion formation 27 (Figure 1 ). Preferential localisation of NMOSD lesions in the spinal cord and optic nerves can be accounted for by the higher expression of AQP4 in these regions relative to the brain. 28 Lesions in NMOSD can be present with relative preservation of myelin or as classically demyelinated lesions. Demyelinated lesions are characterised by infiltration of macrophages containing myelin and astrocyte debris, marked axonal loss, astrocytic damage, decreased AQP4 expression, granulocytic inflammation, immunoglobulin and complement deposition, and vacuolated myelin. 26, 29, 30 MOG antibody-associated disease Myelin oligodendrocyte glycoprotein (MOG) has been extensively studied as a candidate autoantigen in demyelination initially because of its involvement in animal studies of experimental autoimmune encephalitis (EAE), a leading in vivo rodent model of MS. The development of assays which presented MOG in its native conformation was the first to uncover the presence of high titres of MOG Ab in paediatric demyelination cohorts, particularly prominent in children with acute Figure 1 . Pathological features of lesions in autoimmune demyelination. Demyelinating lesions commonly consist of immune cell infiltrates predominated by activated macrophages and microglia, lymphocytes, and varying degrees of immunoglobulin and complement deposition. CD4 + T cells outnumber CD8 + T cells in MOGAD and NMOSD while CD8 + T cells predominate in MS. Granulocytic infiltration is seen in MOGAD and NMOSD while not frequently observed in MS lesions. Axon and astrocyte loss is profound in NMOSD while astrocytes and axons are largely preserved in MS and MOGAD. AQP4 downregulation is observed in NMOSD while conflicting reports of MOG internalisation have been seen in MOGAD. Ab, antibody; AQP4, aquaporin-4 water channel; Ig, immunoglobulin; MOG, myelin oligodendrocyte glycoprotein; MOGAD, MOG Abassociated disease; MS, multiple sclerosis; NMOSD, neuromyelitis optica spectrum disorders. disseminated encephalomyelitis (ADEM). [31] [32] [33] [34] In particular, the live cell-based assay which involves the recombinant expression of conformational MOG on the cell surface of eukaryotic cell lines has become the gold standard detection method for MOG Ab. Further investigations using these methods revealed the presence of MOG Ab in 20-50% of adults with AQP4 Ab-negative NMOSD. 25, [35] [36] [37] This has sparked the current consensus to distinguish patients with MOG Ab as a separate disease entity from NMOSD, termed MOGAD. Despite numerous international recommendations, determination of consensus diagnostic criteria for MOGAD is still ongoing. The typical clinical characteristics of MOGAD include ON, myelitis, and encephalitis, and these phenotypes can be monophasic or relapsing in disease course. 38 The clinical spectrum of MOGAD is continually expanding with emerging reports of MOG Ab associated with seizures and cortical encephalitis. 39 Interestingly, despite the lack of MOG expression in the peripheral nervous system (PNS), MOG Ab has been described in patients with co-existing inflammatory PNS syndromes. 40 The co-incidence of these PNS symptoms and their direct association with MOGAD warrants further investigation. Prevalence studies involving MOGAD cohorts are scarce. In a recent UK study, the prevalence of MOGAD was approximately two per 100 000 with the female:male ratio estimated at 1.8:1, proportionally affecting more men compared to NMOSD and MS. 41 Patients with MOGAD have been reported as having less residual functional impairment compared to AQP4 Ab-positive NMOSD, 25, 42, 43 although this is not universal with some patients, particularly those with a relapsing disease course, suffering permanent visual loss or paraplegia. 44 The co-existence of MOG Ab and AQP4 Ab in the same individual is seldom reported and occurs in as low as 0.06-8% of patients with demyelination. 45, 46 A small subset of MS patients are seropositive for MOG Ab. [47] [48] [49] In contrast to MS, CSF findings in adult and paediatric MOGAD cohorts show that OCBs are less commonly observed in MOGAD compared to MS, and are seen in 10-30% of patients. 50, 51 Radiological differences highlight that MOGAD and NMOSD patients with ON are more likely to have bilateral and longitudinally extensive optic nerve lesions compared to MS patients with ON. 52 Comparatively, MOGAD patients with ON more frequently experienced lesions in the anterior visual pathway relative to NMOSD patients with ON in which lesions were observed posteriorly including the optic chiasm and optic tract. 52 Area postrema involvement is less common in MOGAD and more predominant in NMOSD. 53 Similar to NMOSD, longitudinally extensive transverse myelitis (TM) is observed in MOGAD and rarely observed in MS. 54 Short lesions of TM similar to those seen in MS also occur in MOGAD. 55 While there is currently no clinical predictor of relapse in MOGAD patients, persistent MOG Ab seropositivity has been associated with a relapsing disease course while those with transient seropositivity were more likely to be monophasic. 56, 57 MOGAD lesions are characterised by perivenous, confluent demyelination, axonal preservation, reactive astrocyte gliosis, prominent intracortical demyelination, with immune cell infiltrates, complement deposition and evidence of oligodendrogliopathy. 58,59 Granulocytes and CD4 + T cell infiltrates are observed in high frequencies in MOGAD lesions with astrocyte populations preserved and AQP4 expression sustained in contrast to NMOSD 58 (Figure 1 ). Intracortical lesions were more frequent in MOGAD than in MS and NMOSD. 58 Similar to MS, subpial demyelination was also observed in MOGAD. 58,59 Complement and Ig deposition were less frequently observed in MOGAD lesions compared to NMOSD. 59 Although AQP4 internalisation is an established pathogenic mechanism in NMOSD, some reports have observed MOG-dominant myelin loss at MOGAD lesions, 59 while others have shown evidence of complement deposition without selective loss of MOG expression. 58 While clinical presentations are similar between MS, NMOSD and MOGAD, differences in the origin of the autoimmune response in each disease remain to be explored and these have distinct implications to our understanding of each disease entity. Currently, the location of the initiating pathogenic event in demyelination remains contentious. In the 'outside-in' model, disease pathogenesis begins peripherally, in which autoreactive immune cells traffic into the CNS and elicit an autoimmune response. EAE rodent models are a classical demonstration of this, whereby demyelination is initiated through immunisation of adjuvant-administered myelin proteins resulting in expansion and activation of myelin-reactive CD4 + T cells. 60-64 The likelihood of peripheral immune-mediated pathogenesis is strengthened by large genome-wide association studies which have shown MS susceptibility to be enriched in genes associated with B and T lymphocytes, natural killer cells and microglia. 65, 66 Conversely, an 'inside-out' model posits that disease is resultant of pre-existing damage to oligodendrocytes and myelin, prompting immune cell recruitment to sites of injury, following which an inflammatory response proceeds. This is exemplified in cuprizone-induced models of demyelination which directly elicits oligodendrocyte cytotoxicity and subsequent inflammatory demyelination. 67 Furthermore, alterations to myelin-related genes support this 'inside-out' model of disease pathogenesis, as seen in PLP1 missense mutations found in a subset of MS patients and cause oligodendrocyte apoptosis. 68 While continued investigation is warranted into the compartment of disease onset, several genetic and environmental factors have been commonly investigated across these diseases. The presence of HLA-DRB1*15:01 and the absence of HLA-A*02 have been frequently associated with a significantly increased risk of developing MS. 69 In some cases, MS has been postulated to be a neurodegenerative disorder where the inflammatory response is secondary to neurodegeneration. 70, 71 However, in a multinational genome-wide association study of 9772 cases of MS, genes relevant to inflammationindependent neurodegenerative pathways lacked an association to MS, while genes for CD4 + T cell differentiation were over-represented and implicated in disease pathogenesis. 66 In a nextgeneration sequencing study of 31 Japanese NMOSD patients, HLA-DQA1*05:03 was found to be significantly associated with NMOSD. 72 More recently, in a cohort of 165 NMOSD patients, HLA-DRB1*08:02 and HLA-DPB1*05:01 were found to be susceptibility alleles while HLA-DRB1*09:01 was found to be protective. 73 In contrast, HLA-A*01, HLA-B*08 and HLA-DRB1*03 were associated with NMOSD in a recent Dutch study while no association between HLA and MOGAD was observed. 74 This study, however, did not separate adult and paediatric MOGAD cohorts, whereas in a recent Chinese study of 95 MOGAD patients (51 paediatric, 44 adult), HLA-DQB1*05:02-DRB1*16:02 alleles were associated with paediatric onset of MOGAD while no association between adult-onset MOGAD and HLA genotype was observed. 75 Infectious prodrome Viral infection is among many environmental factors that have been commonly linked to or seemingly preceding autoimmune disease. Arguably, most renowned is the association between MS and Epstein-Barr virus (EBV) with almost all MS patients having been previously infected with EBV. 76 EBV Ab seropositivity is observed at higher rates in MS patients and is strongly correlated with disease onset. 77, 78 Additionally, EBV-induced infectious mononucleosis is found to be associated with a twofold risk of developing MS. 77 Autoreactive B cells latently infected with EBV are proposed to contribute to MS pathogenesis by evading elimination of CD8 + T cells and accumulating within the CNS, producing myelin-reactive antibodies and survival signals for T cells. 78 Induction of MOGAD by a viral infection has also been reported, with the most common occurrence observed in paediatric patients who develop ADEM following viral infection. Postinfectious MOGAD cases include HSV-1 infection followed by ADEM 79 and ON 80 and rubella infection followed by ON. 81 Up to 40% of patients experience a nonspecific viral prodrome with or without fever, prior to clinical onset. Most recently emerging, two reported cases of SARS-CoV-2 infection were followed by MOG Ab-associated ON 82 and NMOSD. 83 Despite these cases, the association between viral infection and demyelination remains unclear and leading hypotheses regarding molecular mimicry of myelin proteins because of their structural overlap with viral proteins remain largely unvalidated in humans. Other than viral infection, there is also emerging evidence regarding non-infectious clinical prodromes preceding autoimmune demyelination. Within 5 years of disease onset, patients with MS were reported to have greater fatigue, sleep disorders, anaemia and pain compared to healthy controls. 84 Neuropathic pain has been reported in MOG Aband AQP4 Ab-positive patients, 85 with another study identifying prodromal headaches in almost 50% of patients with MOG Ab-positive ON. 86  Low vitamin D levels, insufficient UV exposure and higher latitudes have each been linked to an increased risk of developing MS. 87 In a recent study of two independent multicentre cohorts, lower disease severity was seen in MS patients with higher vitamin D levels. 88 Although UV exposure is correlated with the maintenance of sufficient levels of vitamin D, the role of sun exposure in the development of MS may be vitamin D-independent. 89 For instance, while whole-body UV irradiation of mice prevents the development of EAE, 90 UV-induced suppression of EAE was still observed in mice lacking the vitamin D receptor. 91 Interestingly, in a recent crosssectional study of 29 NMOSD patients, increased sun exposure and serum calcifediol (a vitamin D metabolite) were seen in AQP4 Ab-negative NMOSD patients compared to AQP4 Ab-positive patients, potentially implicating a role of vitamin D in AQP4 Ab synthesis. 92 Presently, associations of vitamin D and sun exposure have yet to be explored in MOGAD cohorts. 93  A paraneoplastic association has been described in a few cases of MOGAD and AQP4 Ab-positive NMOSD and may be associated with the pathogenesis of demyelination. More recently, expression of oligodendrocyte markers, including MOG, and CD4 + and CD8 + T cell infiltration have been described in the teratoma tissue of a MOG Ab-positive ON patient with ovarian teratoma. 94 This is in contrast to the more frequently associated lung and breast adenocarcinomas in a small number of patients with NMOSD 95 ; however, cases of ovarian teratomas in NMOSD have also been reported. 96 A patient positive for CSF MOG Ab and presenting with longitudinally extensive TM, BON, and brainstem encephalitis coincidentally with lung adenocarcinoma has been recently reported 97 ; however, it is clear that the paraneoplastic nature of demyelination is rare and warrants further investigation. Intestinal microbiota influence both local and systemic immune responses through its interaction with the gut-brain axis and offer several hypotheses regarding the origin of the autoimmune response. A recent study found that RRMS patients with active disease had increased numbers of intestinal Th17 cells, and expansion of this population was reliant on the composition of gut microbiota. 98 An increased relative abundance of anti-inflammatory Prevotella strains was seen in healthy controls and RRMS patients without active disease. 98 More recently, clonally expanded IgAexpressing B cells have been found to actively traffic to the CNS with specificity against certain phyla of gut microbiota and were detected in the CSF and tissue of active MS-associated lesions and could therefore serve as a systemic biomarker for active disease in MS. 99 Recent investigations into the involvement of gut microbiota in NMOSD pathogenesis have revealed the role of Clostridia perfringens in the disease. This strain was enriched in NMOSD patients and has been implicated in the regulation of Treg and Th17 cell populations. Specifically, AQP4-specific T cells in NMOSD patients exhibited a Th17 phenotype and cross-reacted with a C. perfringens peptide sequence. 100 A newly isolated Erysipelotrichaceae bacterium has been recently described that when co-cultured with Lactobacillus reuteri which has homology to MOG peptides, Th17 inflammatory responses were induced from MOG-specific Th17 cells and increased EAE severity. 101 Other instances of molecular mimicry to MOG have been demonstrated in a small proportion of MS patients harbouring antibodies targeting MOG peptides which cross-reacted with the bovine milk protein butyrophilin. 102 Additionally, butyrophilin has been shown to induce a MOG-specific T-cell response in rats, while also having the potential to produce a tolerogenic response and alleviate EAE. 103 As this was investigated in the context of MS, which is now known to comprise a small minority of MOG Ab-positive patients, these findings have yet to be investigated in a MOGAD cohort. While MS has been classically considered a T celldriven disease, the efficacy of B-cell-targeted therapies has warranted recent investigation into the regulatory or antigen-presenting role of B cells. [104] [105] [106] Conversely, even though NMOSD and MOGAD have a clear antibody association, the role of antigen-specific T cells in these disorders  The contribution of pathogenic T cells has been considerably explored in the context of MS which has been classically described as a T cell-mediated disease. In particular, in EAE, the role of CD4 + Th17 cells is well established as being the primary mediators of disease pathogenesis 107 with recent findings showing increased frequencies of distinct Th17 subsets in the blood and CSF of patients with RRMS compared to SPMS. 108 Regardless of disease course, CD8 + T cells in MS lesions far outweigh the frequency of other infiltrating lymphocytes such as B cells and CD4 + T cells. 109 CD8 + T cells are enriched in the CSF of RRMS patients early in disease, 110 and these CD8 + T cells found in the CSF shared the same repertoire as CNS-resident CD8 + T cells. 111 However, the role these CD8 + T cells play in disease is not well understood. For instance, myelin-specific CD8 + T cells from MS patients have demonstrated a proinflammatory profile when expanded in vitro and expressed CD20 in greater proportions compared to control subjects. 105 These myelinspecific CD20 + CD8 + T cells were preferentially depleted by anti-CD20 therapy which has been shown to be effective in MS treatment. 105 On the contrary, a subset of clonally expanded regulatory CD8 + T cells have been recently described to suppress the pathogenicity of CD4 + T cells in EAE, 112 and thus, CD8 + T cells may play a multifaceted role as either a regulatory or pathogenic mediator of human demyelination. Antigen-specific T cells in NMOSD and MOGAD have been seldom detected despite central dogma for the necessity of these cells in their role for autoantibody production and pathogenicity. Interestingly, AQP4-specific CD4 + T cells were detected in AQP4 Ab-positive patients, with T cell reactivity observed in a peptide sequence at amino acids 156-170 which overlaps with the epitope recognised by AQP4 Ab. 113, 114 Contrastingly, the same study could not detect MOG-specific T cells in MOG Ab-positive patients and this was postulated to be because of the failure of synthetic peptides in mimicking antigen processing and MHC presentation. 113 Indeed, a previous study using Escherichia coli-expressed MOG coupled to fluorescent beads was able to detect MOG-specific CD4 + T cells producing IFNgamma, IL-22 and IL-17A in 16 out of 29 MS patients. 115 However, MOG Ab was detected in only one of these patients reinforcing the notion that MOG Ab seropositivity in MS patients is rare and that the role of MOG-specific T cells in MS patients remains difficult to explore. 115 Although antigen-specific T cells in demyelination have been challenging to isolate, the cytokine profile of demyelination patients implicates the role of specific T cell subpopulations. The proinflammatory Th17-related cytokines IL-6 and G-CSF were elevated in paediatric MOGAD patients compared to MOG Ab-negative patients with demyelination. 116 In a cross-sectional study, adult and paediatric MOGAD patients were found to have an upregulation of Th17, Th1-related and Treg-related cytokines. 117 These cytokines were similar to those upregulated in the CSF of AQP4 Ab-positive NMOSD patients; however, they differed significantly from MS patients. 117  The initiation of an autoimmune response against CNS antigens can lead to the breach of the blood-brain barrier (BBB) to allow leukocyte infiltration, antibody deposition and inflammation in the local CNS region. [118] [119] [120] Autoantibodies are implicated in the exacerbation of autoantigen destruction through executing effector functions such as the activation of complement-dependent cytotoxicity (CDC) or antibody-dependent cellular cytotoxicity (ADCC). 121 Even though a central autoantigen in MS remains unidentified, evidence exists for the contribution of humoral mechanisms in disease pathogenesis in addition to the T cell-driven response. 109, 122 Intrathecal OCBs are a hallmark feature of MS, in which the presence and quantity are strongly associated with disease severity and disability in patients. [123] [124] [125] [126] [127] Several studies have evidenced the role of B cells in the production of OCBs, [128] [129] [130] thus implicating the importance of the humoral response in promoting characteristic MS demyelination. This is further supported by the observation that B cells are responsible for mediating brain homing Th1 cell autoproliferation, and B-cell depletion by anti-CD20 therapies reduces T cells and inflammation both in vitro and in patients with MS. 104 MS patients commonly display elevated intrathecal Ig, specifically IgG1 and IgG3. 131 These findings have been supported by recent studies observing higher serum IgG3 may be predictive of the progression of CIS towards MS. 132, 133 Subsequent functional assays on antibody pathogenicity in animal and human models have confirmed MS lesion observations. Injection of CSF from MS patients was observed to induce demyelination and axonal damage in mice. 134 Evidencing the specific role of antibodies in this process, recombinant IgG1 from MS patient CSF was observed to initiate CDC, rapid demyelination and microglia activation on mouse organotypic cerebellar slices. 135 In addition to CSF antibodies, serum autoantibodies have the capacity to disrupt the BBB, target brain microvessels and initiate destruction of myelin. 136 Defining a putative autoantibody responsible for MS demyelination is encumbered by the potential heterogeneity of target autoantigens in MS. Various autoantibody targets have been investigated (Table 1) . For instance, the presence of isotype-switched antibodies against myelin PLP is significantly elevated in patients with RRMS and SPMS who have specific HLA types, and PLP Ab titres correlated with disease severity. 137 Additionally, a subgroup of MS patients with brainstem and cerebellar lesions have been identified with PLP antibodies. 138 In other cases, the pathogenic potential of these autoantibodies has been evidenced through the initiation of CNS inflammation following the passive transfer to rodent models 139 ; however, validation of their pathogenic roles in humans remains unprecedented. Distinct subgroupings of MS patients with precise disease course, clinical and radiological features may facilitate discovery of new autoantigenic targets and discrete clinical entities, as has been the case for the discovery of AQP4 Ab and MOG Ab. The mechanisms of AQP4 Ab pathogenesis have been investigated extensively through functional assays in animals and human tissue. An early in vitro study revealed that serum-derived IgG from NMOSD patients binds to astrocytes and increases BBB permeability to permit autoimmune reactivity within the CNS. 140 Further, human AQP4 Ab binding to astrocytes initiates injury, secondary oligodendrocytopathy and demyelination through CDC, ADCC or induction of inflammation through granulocyte activation. [141] [142] [143] The process by which astrocytopathy leads to demyelination in NMOSD remains unclear. Bystander formation of membrane attack complexes in the process of CDC has been shown in neurons and oligodendrocytes co-cultured with astrocytes. 144, 145 Other studies demonstrate internalisation and downregulation of the astrocytic glutamate receptor, EAAT2, alongside AQP4 upon Ab binding, 146 and have therefore implicated glutamate excitotoxicity to oligodendrocytes as a contributing factor in disease pathogenesis. 147 The production of recombinant antibodies from NMOSD patient periphery- [148] [149] [150] and CSF-derived 149, 151, 152 B cells revealed that AQP4 Ab initiates pathogenic effects both in vitro and in vivo because of defective central and peripheral B cell tolerance mechanisms. Recombinant AQP4 Ab was observed to specifically bind to the H101/L104 epitope and initiate CDC. 152 Supporting these findings, recent clinical trials indicate that eculizumab, a complement inhibitor, reduces relapse rates in patients. 153 Additionally, the IL-6 cytokine pathway has been strongly implicated in NMOSD pathogenesis [154] [155] [156] and using antibodies against the IL-6 receptor has demonstrated a strong potential to block this pathway through reducing the survival of AQP4 Ab-secreting plasmablasts. 157 Evidence of disease initiation and exacerbation following passive transfer of AQP4 Ab further outlines the pathogenic role of these autoantibodies. Immunisation with AQP4 peptides exacerbated experimental autoimmune myasthenia gravis symptoms in mice, including fatigue, weakness and decreased nerve responsiveness. 158 The administration of recombinant AQP4 Ab to rodents has provided strong evidence to support in vivo studies. Recombinant AQP4 Ab generated from NMOSD patient CSF-derived plasma cells was observed to initiate perivascular astrocyte depletion, myelinolysis, and Ig and complement deposition in EAE rats. 151 In line with functional assays on pathogenicity, the observation that seropositivity is associated with increased chances of opticospinal involvement, longitudinally extensive spinal cord lesions, relapse rate and lower EDSS scores further evidences the role of the autoantibodies in driving demyelinating pathogenesis. 159 Although the mechanism of pathogenesis initiation within the CNS is still debated, current evidence points towards a peripherally initiated response in NMOSD. For instance, intrathecal production of AQP4 Ab is rarely observed in NMOSD patients. 160 Additionally, a recent report demonstrated that Injection of AQP4 peptide to mice with experimental autoimmune myasthenia gravis aggravates disease symptoms 158 Rats immunised with mimotopes of conformational AQP4 epitopes produce AQP4 Ab detectable in sensitive cell-based assays; however, no concurrent pathology was observed 218 Encephalomyelitic syndrome was initiated when Rag1 À/À mice reconstituted with mature T cells of Relapse rate higher in MOG Ab-seropositive patients 131, 219 AQP4 Ab seropositivity is predictive of relapse and titres increase during relapse [220] [221] [222] High titres associated with increased disease activity (complete blindness or extensive CNS involvement) 223 Treatment with rituximab causes significant improvement of disease activity most patients, 224 and AQP4 Ab concentration decreases in serum after immunosuppressive therapy 222 Chimeric, high avidity AQP4 Ab blocks patient IgG binding to AQP4, thus preventing CDC in vitro 225 Coexistence of AQP4 and ANA Ab associated with more severe disease phenotypes 196 Disease ameliorated upon treatment with steroids and rituximab 44 High MOG Ab titres are predictive of a more severe, relapsing course and increase during active disease 56, 168 Ab, antibody; ADCC, antibody-dependent cellular cytotoxicity; ADEM, acute disseminated encephalomyelitis; ANA, anti-nuclear antibody; AQP4, aquaporin-4; CDC, complement-dependent cytotoxicity; CSF, cerebrospinal fluid; EAE, experimental autoimmune encephalitis; EAMG, experimental autoimmune myasthenia gravis; G-CSF, granulocyte colony-stimulating factor; HERV-W, systemic injection of AQP4 Ab has a number of entry sites into the CNS and produced NMOSDlike lesions. 161 Although some studies report the necessity for T cells in AQP4 Ab lesion formation, 162 others suggest that AQP4 Ab alone can induce NMOSD pathology without T cell help. 161 It is likely that both pathways are required in NMOSD pathogenesis, as indicated in a study which observed that while immunisation with AQP4 initiates encephalomyelitis in Rag1 À/À mice with the presence of T cells alone, lesions are only produced in the presence of both antibodies and T cells against AQP4. 163 MOG Ab has been shown to contribute to CNS demyelination through CDC or ADCC-mediated lysis of MOG-expressing cells. 31, 164, 165 These findings are supported by the observation that the majority of MOG Ab are IgG1, 33,166-169 a subclass efficient in fixing complement and binding to Fc receptors for ADCC initiation although the potency of these effector mechanisms may be dependent on autoantibody affinity. 170 Further studies suggest murine and human MOG Ab induce cytoskeleton disruption and microtubule destabilisation. 37, 171 Several studies have further observed CNS inflammation and complement-dependent lysis of MOG-expressing cells following passive transfer of high-affinity patient MOG Ab to rodents. 172, 173 A recent report demonstrated the pathogenicity of an antibody derived from the RNA of postmortem human brain tissue and showed similar demyelinating activity to a prototypical murinederived MOG Ab. 174 However, these studies use rodent-reactive antibodies which are only present in a minor proportion of MOGAD patients, with between 70 and 80% of human MOG Ab recognising a conformational epitope at position 42 of the extracellular domain of MOG where proline exists in humans as opposed to serine in rodents. 168, 175 Autoantibody pathogenicity in a primate model has since been demonstrated in an in vivo macaque model in which complementdependent vacuolisation of myelin, lesion pathology, EAE and MOG Ab were observed after injection of recombinant human MOG. 169  With ongoing attempts to make the delineation between MS, NMOSD and MOGAD progressively clearer, discernment of the correct disease entity remains reliant on evaluation of clinical presentations and accurate paraclinical testing. Increasingly evident is the need to standardise assay methodologies for the accurate detection of demyelination-associated autoantibodies. 109, 176 Several multicentre studies over recent years have evaluated the accuracy, sensitivity and specificity of antibody testing in NMOSD and MOGAD. A multicentre study of 21 AQP4 Ab assays across 15 European centres found that cell-based assays using microscopy produced the highest sensitivities and specificities. 177 It also further reinforced the use of the M23 isoform of AQP4, with improved sensitivity in contrast to the M1 isoform, 177 attributed to findings that formation of orthogonal array of particles (OAPs) exclusive to the M23 isoform, is important for pathogenic human AQP4 Ab binding. 141 Recent validation studies have demonstrated lower concordance in MOG Ab testing when determining the presence of MOG Ab in patients with borderline and low MOG Ab titres tested across international centres. 178 Differences in assay methodologies, data analyses and cut-offs for MOG Ab positivity largely account for these discrepancies. 179, 180 Moreover, studies on the isoform of MOG have shown that C-terminal truncation of the full-length protein decreases binding of human MOG Ab 181 and is likely because of the effect of truncation on the second hydrophobic domain of MOG which has recently been revealed to be important in the bivalent binding of human MOG Ab. 170 Although major human MOG Ab epitopes have been described in the extracellular domain of the protein, 168, 175 it has been postulated that the transmembrane domains and cytoplasmic tail may play an important role in the surface expression and oligomerisation of the antigen. Further, the use of fixatives in cell-based assays has been shown to alter the native conformation of MOG and significantly decrease the sensitivity of the MOG Ab test. 168, 178 Hence, cell-based assays using live cells are the current gold standard in the detection of these antibodies, 38, 178, 182 and therefore, translation and standardisation of these assays for routine diagnostic testing are highly warranted. Recent studies are also shedding light on the cooccurrence of disease-associated autoantibodies in CNS neurological disorders. For instance, the coexistence of NMDAR Ab and other autoantibodies associated with autoimmune encephalitis in MOGAD and AQP4 Ab-positive NMOSD patients has been observed. 183, 184 AQP4 Ab-positive NMOSD has been found to occur in patients with AChR Abpositive myasthenia gravis with NMOSD onset most commonly occurring after diagnosis of MG. 185 This was further reflected in antibody titres, with AChR Ab decreasing as AQP4 Ab increased over the disease course. Predicting disease course remains a significant challenge as residual disability and worsening of clinical outcomes often accumulates with further relapses in MS, NMOSD and MOGAD. Intriguingly, a recent study found an association between epitope binding patterns and likelihood to relapse, with 75% of adult MOGAD patients that did not recognise the Proline42 epitope, the major immunodominant binding region of human MOG Ab, presented with a relapsing disease course. 168 This provides evidence that characterising the epitope of autoantibodies in demyelination may prove useful in predicting disease course, thereby enabling refinement of therapeutic pathways. Characterisation of binding patterns in AQP4 Ab-positive NMOSD showed a change in AQP4 Ab epitope in only a minority of patients over the disease course. 186 Similarly, the immunodominant epitope in MOGAD patients also remains stable over time. 168 Additionally, while MOGAD patients exhibited the same Ab characteristics in serum and CSF, paired CSF-serum samples containing AQP4 Ab taken at the same time exhibited different binding patterns potentially suggesting the contribution of two independent sources of antibody production in NMOSD. 186 Given the exemplars of MOG Ab and AQP4 Ab, the discovery and validation of new biomarkers may have significant implications in predicting disease features. Axonal damage is variable across demyelinating diseases and can be measured by neurofilament levels. Recent findings have shown elevated blood and CSF neurofilament levels in RRMS patients associated with disease activity, disability score and more frequent relapse. 187 Additionally, in 18 MOGAD patients followed longitudinally, elevated neurofilament light-chain levels at baseline remained stable or decreased over time, signifying peak axonal damage occurring at disease onset. 188 Interestingly, similar associations have been reported between elevated serum levels of glial fibrillary acidic protein (GFAP) 189 and NMOSD, in which higher GFAP levels were correlated with greater disability and shorter time to relapse. 190  Evolution of new technologies such as single-cell RNA sequencing has shed light on impairments to immune tolerance checkpoints that result in CNS autoreactivity (reviewed in Zou et al. 150 ). Delivery of mRNA to lymphoid tissues in order to mimic and correct peripheral tolerance mechanisms has recently been demonstrated in an EAE mouse model. 191 In this study, vaccination using MOG peptide-encoding mRNA enabled the expansion of MOG-specific Foxp3 + Treg cells and prevented the development and worsening of symptoms in MOG-induced EAE mice. 191 Single-cell RNA sequencing revealed that the expansion of effector Tregs played a significant role in suppressing proinflammatory Th1 and Th17 cells that would otherwise cause disease. Further, the importance of PD-1 and CTLA-4, which are inhibitory receptors on activated T cells, were emphasised when blockade of these receptors abolished the protective effect of MOG mRNA vaccination. Overall, the study demonstrated the first instance of a protective mRNA vaccine in an autoimmune disease, and showed that pathogenic T cells are inhibited rather than deleted and were reliant on PD-1 and CTLA-4 signalling.@story_separate@In summary, MS, NMOSD and MOGAD can present with similar and often overlapping clinical and radiological characteristics that have made the differential diagnoses of these disorders challenging. The discovery of AQP4 Ab and MOG Ab has become essential in the diagnoses of NMOSD and MOGAD, respectively. Remarkably, despite expression of these autoantigens on distinct cell types of the CNS with heterogeneous pathologies, their immunopathogenic mechanisms lead to similar clinical features. Although the pathogenic role of AQP4 Ab has been more thoroughly investigated, evidence of MOG Ab pathogenicity is emerging. While current focus lies on the autoantibodies in NMOSD and MOGAD, there remains a need to clarify the contribution of T cells in pathogenesis. Although extensive research in MS disease models has demonstrated a primary pathogenic role of CD4 + T cells, B cell and antibody ª 2021 The Authors. Clinical & Translational Immunology published by John Wiley & Sons Australia, Ltd on behalf of Australian and New Zealand Society for Immunology, Inc involvement in MS is being investigated, with a present demand to define patient subgroups in MS with regard to specific autoantigens. Along with associations to environmental factors such as infection, vitamin D and UV exposure, advancing research in the contribution of the gut microbiome provides a potential avenue for novel therapeutic interventions and for modifiable factors involved in brain health. As immunosuppressive and immunomodulatory therapies differ in efficacy across these disorders, a deeper understanding of the aetiology and pathogenic mediators of disease is essential to determine targeted therapeutic approaches to improve treatment decision-making and patient outcomes.","Autoimmunity plays a significant role in the pathogenesis of demyelination. Multiple sclerosis (MS), neuromyelitis optica spectrum disorders (NMOSD) and myelin oligodendrocyte glycoprotein antibody‐associated disease (MOGAD) are now recognised as separate disease entities under the amalgam of human central nervous system demyelinating disorders. While these disorders share inherent similarities, investigations into their distinct clinical presentations and lesion pathologies have aided in differential diagnoses and understanding of disease pathogenesis. An interplay of various genetic and environmental factors contributes to each disease, many of which implicate an autoimmune response. The pivotal role of the adaptive immune system has been highlighted by the diagnostic autoantibodies in NMOSD and MOGAD, and the presence of autoreactive lymphocytes in MS lesions. While a number of autoantigens have been proposed in MS, recent emphasis on the contribution of B cells has shed new light on the well‐established understanding of T cell involvement in pathogenesis. This review aims to synthesise the clinical characteristics and pathological findings, discuss existing and emerging hypotheses regarding the aetiology of demyelination and evaluate recent pathogenicity studies involving T cells, B cells, and autoantibodies and their implications in human demyelination."
"Single-shot spinal, standard epidural, combined spinal epidural (CSE), and dural puncture epidural (DPE) are the most effective techniques to alleviate the pain of childbirth. Initiation of neuraxial blockade in laboring patients provides reliable and rapid onset of high-quality pain relief with minimal serious side effects to the mother and fetus. Catheterbased techniques (epidural, CSE, DPE) also provide a means of rapid conversion to surgical anesthesia for operative delivery. In situ catheters that can be utilized successfully for cesarean delivery anesthesia minimize the need for induction of general anesthesia (GA) and thereby reduce the likelihood of exposure to the risks of GA in this patient population (Table 1) . This article provides an up-to-date, evidence-based review of current trends in neuraxial labor analgesia, including strategies for optimizing and customizing analgesia. We will also include a discussion of how to navigate providing neuraxial labor analgesia during the COVID-19 pandemic.@story_separate@While our understanding of COVID-19 is still evolving, the pandemic has changed several aspects of the management of obstetric patients. Preparation for a neuraxial procedure now requires an understanding of how to screen and prepare for patients with SARS-CoV-2 infection. All patients should be screened for COVID-19 symptoms remotely (i.e., by phone or video) before entrance to the labor and delivery floor. Ideally, universal testing should be employed so that the SARS-CoV-2 status of all hospitalized patients is known. Healthcare providers should use contact and droplet precautions with eye protection (gown, gloves, surgical mask, face shield) when in contact with a patient with known or suspected COVID-19 and when undertaking neuraxial procedures for labor, in accordance with several obstetric anesthesia society recommendations. Airborne precautions with an N95 mask and face shield (or a powered airpurifying respirator (PAPR) if these have an insufficient seal) should be used during aerosol-generating procedures such as intubation [1] . Equipment in rooms should be limited to necessary drugs and labor analgesia and intubating supplies (""COVID kits"") to avoid contamination of anesthesia carts and drug-dispensing machines. If feasible, it is also prudent to limit the number of healthcare providers caring for COVID-19 patients; a log should be maintained of every staff member who goes in and out of the labor and operating rooms. Early neuraxial labor analgesia is recommended for obstetric patients with COVID-19 unless otherwise contraindicated, as it may reduce the need for GA should emergency cesarean delivery become necessary. Because patients with COVID-19 may require venous thromboembolism (VTE) prophylaxis, consideration of the timing and dose of the last anticoagulation medication may be required [2••] . It is also advisable to check a platelet count prior to initiation of neuraxial procedures, although thrombocytopenia is typically only reported in patients with severe COVID-19 illness. However, severe thrombocytopenia in laboring patients that precludes neuraxial analgesia and anesthesia is rare, except in the presence of other etiologies of thrombocytopenia, disseminated intravascular coagulation (DIC), additional comorbidities, or severe disease. Neuraxial procedures in obstetric patients have a strong safety record, but mild, transient complications and rare, serious, life-threatening complications can occur. Postdural puncture headache (PDPH) is a common, potentially severe complication of neuraxial labor procedures, with an incidence of roughly 0.7-1% [3, 4] . PDPH develops in an estimated 52-60% of obstetric patients after accidental dural puncture (ADP) with an epidural needle and is far less frequent after spinal techniques (0.5-2% with small pencil-point spinal needles) [5] . Recent studies suggest that PDPH is, rarely, associated with increased risk of major neurologic complications (e.g., cerebral venous thrombosis, subdural hematoma, bacterial meningitis, persistent low back pain, chronic headache) [6•] . High neuraxial blockade has been identified as one of the most common serious complications of neuraxial anesthesia and analgesia in obstetric patients, with an incidence of roughly 1/4336 neuraxial procedures [3] . This adverse event most often occurs in the labor suite as a result of unrecognized spinal catheter placement during labor epidural procedures but can also occur when spinal anesthesia for surgical delivery is administered after a failed epidural anesthetic (which may complicate up to 14% of epidural placements) [7] . It is reasonable to discuss with patients that optimal placement of the epidural catheter for labor analgesia and/or surgical anesthesia cannot be guaranteed and that the catheter may require replacement if pain relief is partial or failed or should cesarean delivery become necessary. Growing awareness of factors that contribute to failed labor epidural conversion to cesarean delivery anesthesia (Table 2) , as well as active management of in situ epidural catheters and early replacement of nonfunctioning catheters, may help reduce the incidence of high neuraxial blockade and limit the use of GA in obstetrics [8, 9] . Given the rarity of serious neurologic complications of neuraxial blockade in obstetric patients, accurate estimates of the incidence of such events are difficult to gauge. Infection (meningitis, spinal epidural abscess) is a rare cause of serious neurologic sequelae [10] . The risk of direct spinal cord or permanent nerve root damage is extremely low in obstetric patients. To try to minimize trauma during neuraxial procedures, we require that patients report any pain or paresthesias that may occur during the neuraxial procedure and then move the needle or catheter accordingly. In addition, ultrasound can assist in more accurately identifying the lower lumbar interspaces (e.g., L4,5 or L3,4) that are more likely to be below the conus medullaris (the tapered, lower end of the spinal cord which typically occurs near lumber vertebral levels 1 and 2 but may be lower). Spinal epidural hematoma (SEH), the most feared complication of neuraxial anesthesia, has traditionally been associated with anticoagulation therapy, severe thrombocytopenia, traumatic procedures requiring multiple attempts, existing spinal pathology (e.g., spinal stenosis), and thoracic procedures in non-obstetric patients [11] . Obstetric patients are at a lower risk for SEH formation when compared with the general population, with an estimated incidence of 1:200,000-1:250,000 versus 1:3600 in the elderly orthopedic female [11] . Potential reasons include a compliant epidural space (especially in the lumbar region where neuraxial labor analgesia is performed), the hypercoagulable state of pregnancy, a relatively low incidence of spinal pathology, and a lower likelihood of therapeutic anticoagulation. Thrombocytopenia affects up to 12% of obstetric patients, with~1% having a platelet count < 100,000 × 10 6 /L. [12] High-quality data to guide anesthesia providers about whether to proceed with neuraxial techniques in thrombocytopenic obstetric patients are limited. A recent meta-analysis reporting lumbar neuraxial procedures (i.e., lumbar puncture; spinal, epidural, CSE procedures; epidural catheter removal) in thrombocytopenic patients (defined as platelet count < 100,000 × 10 6 /L) across populations (e.g. pediatric and adult oncology patients, obstetric patients) concluded that SEH is rare [13••] . Thirty-three cases of SEH were identified from a total of 7476 procedures between the years 1947 and 2018, most commonly after lumbar punctures (75.8%) with platelet counts < 50,000 × 10 6 /L (61%). Five cases of SEH in obstetric patients with thrombocytopenia were reported: one in a patient who was coagulopathic at the time of accidental epidural catheter removal, one who had an underlying spinal arteriovenous malformation (AVM), two with hemolysis, elevated liver enzymes, and low platelets (HELLP) syndrome; and one with eclampsia. The authors concluded that the sample probability of spinal epidural hematoma formation for all neuraxial procedures is likely low, above an imprecise range of platelet count ≥ 70-75,000 × 10 6 /L [0.097% (95% CI 0.002, 0.2%)]. Subsequently, a Society for Obstetric Anesthesia and Perinatology (SOAP) interdisciplinary consensus statement on neuraxial procedures in thrombocytopenic obstetric patients concluded that if the platelet count is ≥ 70,000 × 10 6 /L and there are no additional contraindications or risk factors, then there is likely to be a low risk of spinal epidural hematoma and it is reasonable to proceed with a neuraxial procedure if clinically indicated (class IIa and level C-LD) [2••] . In the subgroup with a known etiology of thrombocytopenia and a platelet count between 50,000 and 70,000 × 10 6 /L, an individualized risk/benefit analysis within the clinical context is required to determine the appropriateness of neuraxial anesthesia. If the platelet count is below 50,000 × 10 6 /L, then there may likely be an increased risk of SEH, and it may be reasonable to avoid neuraxial procedures (class IIb and level C-LD). Obstetric patients who present to the labor and delivery floor with newly recognized thrombocytopenia may require additional work-up before initiation of neuraxial labor analgesia if the platelet count is < 70,000 × 10 6 /L. The SOAP consensus statement is not intended to establish a standard of care or to replace medical judgement but rather to provide guidance for weighing the relative risks and benefits of performing neuraxial procedures in obstetric patients with thrombocytopenia. A focused history and physical examination are recommended prior to initiation of neuraxial labor analgesia. The American Society of Anesthesiology (ASA) and SOAP practice guidelines for obstetric anesthesia endorse that a platelet count is not required prior to initiation of neuraxial blockade in low-risk obstetric patients [14] . Rather, the anesthesiologist's decision to order a platelet count should be individualized based on the patient's obstetric history (e.g., HELLP (hemolysis, elevated liver enzymes, and low platelet count syndrome)) a history of clinically concerning mucocutaneous bleeding or petechiae, or other comorbid states (e.g., severe thrombocytopenia without a known etiology). Severe thrombocytopenia and a change in platelet count from > 100,000 × 10 6 /L to < 100,000 × 10 6 /L was found to be extremely rare in a recent retrospective review of 984 patients with pre-eclampsia, except in the case of patients with HELLP syndrome [15•] . In general, obstetric patients are hypercoagulable due to an increase in most procoagulant factors and most commonly have normal hemostasis parameters. When evaluating patients for neuraxial labor analgesia, the PT and aPTT may be most clinically useful when patients are on exogenous anticoagulants and have DIC or other inherited or acquired coagulation defects. There are some reports of thromboelastography (TEG) and rotational thromboelastometry (ROTEM) being used to predict bleeding risk prior to initiation of neuraxial anesthesia and analgesia, including in thrombocytopenic obstetric patients. However, the correlation between TEG and ROTEM parameters and clinical bleeding may be poor, except at very low platelet counts [16] . As such, there is insufficient evidence to recommend the routine use of viscoelastic point of care testing in thrombocytopenic obstetric patients, including those with preeclampsia, to determine the safety of neuraxial techniques [2••]. Several advances in epidural catheter design and epidural drug delivery technology have improved the efficacy of neuraxial labor analgesia over the past decades. Epidural catheters are available in nylon blends with intermediate bending stiffness and in more flexible, wire-reinforced versions with either a nylon or polyurethane coating around the inner spring-wound coil. They also are made in single-end hole or multi-orifice designs. Some traditional nylon catheters have flexible tips to minimize the risks described below that have been associated with stiffer catheters. Several studies have demonstrated a lower incidence of epidural vein cannulation, paresthesia, and catheter migration with the use of flexible wire-reinforced catheters when compared with stiffer nylon versions [17] . However, wirereinforced catheters can be harder to thread, likely by virtue of the flexibility afforded by the stainless-steel wire coil and the material properties of the polyurethane or nylon blend coating surrounding the inner coil. If there is difficulty threading the catheter, then consider distending the epidural space with additional saline, retracting the epidural needle and re-identifying the epidural space, using a stiffer nylon catheter in lieu of a flexible catheter, or repeating the procedure at a different interspace [18] (Table 3) . Do not separately withdraw a catheter that has already exited the tip of the needle; take the needle and catheter out together to avoid catheter sheering. Wire-reinforced catheters have limited magnetic resonance imaging (MRI) compatibility, which may be important if obstetric patients require MRI peripartum neurologic evaluation; some are considered ""magnetic resonance-conditional,"" which allows for use under specific conditions [17] . There appears to be no difference in clinical outcomes (e.g., analgesic efficacy, episodes of breakthrough pain, occurrence of complications) between single-end hole catheters and multiorifice designs when wire-reinforced catheters are used [19] . We use single-end hole wire-reinforced polyurethane catheters with good effect. CSE kits are available with either a 25-, 26-, or 27-gauge long spinal needle. Alternatively, a 5-in. (12.7 cm) spinal needle that is compatible with the epidural needle provided in your epidural kit can be added to the sterile field. Epidural manufacturers and distributors in the USA provide different needle designs of a specific gauge in their non-custom CSE kits. Before performing a dural puncture technique, it is important to confirm that the spinal and epidural needle pair that you are using is compatible; the spinal needle must exit the epidural needle with sufficient length to reach the dura. Pump infusion technology has evolved over the past decades. Patient-controlled epidural anesthesia (PCEA) has largely replaced continuous epidural infusion (CEI) technology. With PCEA, a background continuous infusion is most commonly provided, with self-administered patient boluses at rotating the needle has been associated with an increased in accidental dural puncture) *Do not remove catheter separately from the needle, once it has been advanced at all through the tip of the epidural needle predetermined time intervals. PCEA with continuous background infusion may reduce the need for clinicianadministered boluses and improve maternal analgesia [20] , but may increase the incidence of motor block, depending on the concentration of local anesthetic (LA) [21] . Optimal PCEA settings have not been determined, but common patient bolus regimens with varying background infusions include 8-10 mL 0.0625% bupivacaine with 2 μ/mL fentanyl every 10 min; 6-8 mL 0.08-0.1% bupivacaine with 2 μ/mL fentanyl every 15 min; or 10 mL 0.1% ropivacaine with 2 μ/mL fentanyl every 10-15 min. In order to optimize maternal satisfaction, it is important to address patient expectations and training when using the PCEA technique. New PCEA technologies, including disposable devices and computer-integrated infusion pumps which modify the background infusion based on patient bolus requirements, are being developed. More recently, patient-controlled intermittent bolus (PIEB) techniques for the administration of epidural infusion medications have replaced continuous infusion and PCEA techniques at many institutions. With PIEB, the pump itself administers high-pressure boluses at predetermined intervals. Most PIEB technology permits patient-controlled boluses in addition to the programmed pump boluses. The PIEB technique has been shown in several studies to improve patient comfort and satisfaction, reduce total LA consumption, and reduce clinicianadministered boluses without increasing the incidence of motor block [22] . Rare cases of high block after inadvertent intrathecal catheter placement or unrecognized catheter migration have been reported in the literature. Determining the optimal dose and time interval for programmed boluses with epidural solutions of different concentrations has proven challenging. Bupivacaine 0.0625% with fentanyl 2 μ, 5 mL PCEA boluses, 6 mL PIEB q 30 min [23] and ropivacaine 0.1% with fentanyl 2 μ/mL, 5 mL PCEA boluses, 5 mL PIEB q 1 h [24] are among several options for a PIEB technique. Ultrasound identification of the intended lumbar interspace and estimation of the depth to epidural space is being used increasingly in patients with challenging anatomy (e.g., parturients with obesity or scoliosis), as well as for routine epidural placements. A recent systematic review (31 clinical trials and 1 meta-analysis) concluded that neuraxial ultrasound was more accurate than palpation in identifying a specific lumbar interspace and provided an excellent estimate of depth to the epidural or intrathecal space [25] . In addition, it showed that ultrasound increased the overall success and reduced the risk of traumatic procedures in a manner superior to palpation. Because becoming proficient in neuraxial use in patients with complex anatomy requires facility with the tool, we advocate the routine use of ultrasound, when feasible, prior to initiation of neuraxial blockade in obstetric patients. Neuraxial labor analgesia can be initiated with a single-shot spinal, standard epidural, CSE, or DPE technique, depending on patient-and provider-specific factors. A single-shot spinal technique may be appropriate if delivery is imminent or when a catheter-based technique is not feasible. Adding adjuvants (e.g., epinephrine) [26] to the spinal medication can prolong the effect. If labor is unexpectedly prolonged, an epidural catheter or repeat single-shot spinal procedure may be required. Unlike an epidural or CSE neuraxial analgesic technique, single-shot spinal labor analgesia does not provide an in situ catheter which can then be used to provide surgical anesthesia for unplanned cesarean delivery or for postpartum bleeding complications and procedures. The standard epidural technique is a common neuraxial procedure for labor analgesia as it is easy to perform and provides effective analgesia. After the epidural space is identified with the loss of resistance (LOR) technique and the epidural catheter is threaded no more than 6 cm into the space [27] , a loading dose of LA with or without an opioid (most commonly with) is administered in 3-5 mL aliquots over several minutes, with intermittent blood pressure (BP) and heart rate (HR) assessments. Other adjuvants can be added to the dilute LA solutions in current clinical use (bupivacaine 0.0625-0.1% or ropivacaine 0.08-0.1%) to improve analgesia, reduce LA consumption, and prolong the analgesic effect. The addition of dexmedetomidine (0.5 μg/mL), an α2-adrenoreceptor agonist, to LA epidural solutions has been shown to provide comparable or superior labor analgesia with fewer side effects (e.g., pruritus, nausea, and vomiting) when compared with opioid adjuvants (sufentanil) [28] . Studies are also evaluating the safety, efficacy, optimal dose, and mode of drug delivery (i.e., PIEB, PCEA) with clonidine, an α2-agonist, and/or neostigmine, an acetylcholinesterase inhibitor, adjuvants to the LA solution, with or without opioid [29] . The standard epidural technique requires larger doses of LA to initiate analgesia (i.e., the loading dose of up to 20 mL of dilute epidural solution). An epinephrine-containing test dose can be administered to help identify inadvertent intrathecal or intravenous catheter placement if used as the first part of the loading dose. If a misplaced catheter is not properly identified, a high or total neuraxial blockade or local anesthetic systemic toxicity (LAST) can ensue. The CSE technique is used increasingly by obstetric anesthesiologists and other providers with specialized training in the field. After the epidural space is identified via the loss of resistance technique, a 25-, 26-, or 27-gauge spinal needle is advanced via the epidural needle through the dura into the subarachnoid space. An opioid (e.g., fentanyl, 10-20 μ)), most commonly in combination with an LA (e.g., bupivacaine 1-2.5 mg), is administered into the subarachnoid space before the spinal needle is withdrawn and the epidural catheter is threaded into the epidural space. Dexmedetomidine (10 μg) can be administered, in lieu of an opioid, for the intrathecal component [30] . Intrathecal neostigmine (50 μg) and clonidine (75 μg) have also been used to augment analgesia in nonobstetric patients in one study with no adverse effects [31] . Potential benefits of the CSE technique, when compared with the standard epidural technique, include faster onset of analgesia (3-5 min versus 20 min), higher maternal satisfaction, lower incidence of unilateral or patchy block, reduced need for rescue analgesia, reduced total drug dosage, improved sacral coverage [32] , and increased maternal mobility [33] . The use of a CSE technique instead of a standard epidural is one of the several factors associated with the successful conversion of labor analgesia to cesarean delivery anesthesia. The CSE technique has also been associated with more rapid cervical dilation in one study when compared with epidural analgesia in nulliparous women in early labor [34] . The CSE technique is associated with an increased risk of fetal bradycardia [35] (not associated with an increased risk of cesarean delivery) and opioid-induced pruritus. Fetal bradycardia is postulated to result in part from opioid-induced uterine hypertonus and may be avoided by reducing the dose of intrathecal opioid [36] . Non-reassuring fetal heart rate after CSE labor analgesia can most often be treated with several commonly performed maneuvers, including a change in maternal position, fluid and terbutaline administration, and an intravenous dose of ephedrine or phenylephrine. Ultimately, there appears to be no difference in mode of delivery between CSE and standard epidural analgesic techniques [37] . Pruritus after CSE usually resolves after 45-60 min and can be minimized by decreasing the dose of fentanyl (to 10-15 mcg). A small intravenous dose (i.e., 5 mg) of the mixed agonist-antagonist nalbuphine successfully reverses pruritus without reversing analgesia. Concerns for increased risk of infection or PDPH related to the dural puncture have not been substantiated in the literature. Although the functionality of epidural catheters placed during a CSE technique is not known at the outset of the block, evidence suggests that these catheters are as or more reliable than epidural catheter placed during standard epidural procedures. The DPE technique for labor analgesia has become increasingly popular for labor analgesia. The procedure is similar to the CSE technique, except that no intrathecal medication is administered. Studies of DPE techniques are equivocal as to the extent to which the dural puncture epidural is superior to the traditional epidural technique. One recent systematic review concluded that there was a lack of clear benefit over traditional epidural techniques [38•] . A second noted that although the ""collective results remain ambiguous,"" 25-gauge (but not 26-or 27gauge) spinal needles have been reported to provide higher success rates than standard epidural techniques without dural puncture [39] . Two other studies agreed that dural puncture epidural results in fewer unilateral blocks and better sacral coverage than traditional epidural analgesia. When compared with the CSE technique, the DPE technique results in comparable analgesia in a somewhat comparable timeframe (median 2 min versus 11 min) with fewer maternal and fetal side effects (e.g., maternal pruritus, hypotension, fetal bradycardia) [40] . Both techniques can be used to aid with epidural space localization when LOR is equivocal or difficult to discern. CSF flow through the spinal needle provides supportive evidence that the tip of the epidural needle is close to the dura and therefore likely in the epidural space. Conflict of Interest The authors do not have any potential conflicts of interest to disclose@story_separate@Neuraxial labor analgesia continues to be the technique that provides optimal and versatile pain management during labor and the potential for conversion to an anesthetic for cesarean delivery if needed. Procedural techniques such as CSE and DPE have shown promise in improving sacral coverage and reducing the number of unilateral blocks. An increasing number of adjuncts to local anesthetics are being explored to augment the quality and duration of the block. Finally, drug delivery systems (e.g., PIEB) continue to be refined to capitalize on the advantage of bolus dosing to best pair patient need with drug delivery.","PURPOSE OF REVIEW: This article provides an update of recent practice trends in neuraxial labor analgesia. It reviews available evidence regarding management of labor pain in obstetric patients with COVID-19, serious adverse events in obstetric anesthesia to help inform risk/benefit decisions, and increasingly popular neuraxial labor analgesia techniques and adjuvants. State-of-the-art modes of epidural drug delivery are also discussed. RECENT FINDINGS: There has recently been a focus on several considerations specific to obstetric anesthesia, such as anesthetic management of obstetric patients with COVID-19, platelet thresholds for the safe performance of neuraxial analgesia in obstetric patients with thrombocytopenia, and drug delivery modes for initiation and maintenance of neuraxial labor analgesia. SUMMARY: Neuraxial labor analgesia (via standard epidural, dural puncture epidural, and combined spinal epidural techniques) is the most effective therapy to alleviate the pain of childbirth. SARS-CoV-2 infection is not, in and of itself, a contraindication to neuraxial labor analgesia or cesarean delivery anesthesia. Early initiation of neuraxial labor analgesia in patients with COVID-19 is recommended if not otherwise contraindicated, as it may reduce the need for general anesthesia should emergency cesarean delivery become necessary. Consensus regarding platelet thresholds for safe initiation of neuraxial procedures has historically been lacking. Recent studies have concluded that the risk of spinal epidural hematoma formation after neuraxial procedures is likely low at or above an imprecise range of platelet count of 70–75,000 × 10(6)/L. Thrombocytopenia has been reported in obstetric patients with COVID-19, but severe thrombocytopenia precluding initiation of neuraxial anesthesia is extremely rare. High neuraxial blockade has emerged as one of the most common serious complications of neuraxial analgesia and anesthesia in obstetric patients. Growing awareness of factors that contribute to failed conversion of epidural labor analgesia to cesarean delivery anesthesia may help avoid the risks associated with performance of repeat neuraxial techniques and induction of general anesthesia after failed epidural blockade. Dural puncture techniques to alleviate the pain of childbirth continue to become more popular, as do adjuvant drugs to enhance or prolong neuraxial analgesia. Novel techniques for epidural drug delivery have become more widely disseminated."
"Many of the one million older adults residing in assisted living/senior living communities across the United States, enter to maintain their social activities and physical independence as well as to receive care for medical ailments. 1 Direct care to residents is largely provided by nurse's aides (83%) and less often by licensed professional nurses (6.1%). 2 In some states, such as Pennsylvania, there are no requirements for supervision or provision of care by professional Registered Nurses (RN) to provide needed assessments and health monitoring and to supervise the provision of health promotion or restoration activities, such as exercise and physical activity 1 known to improve health, function, and overall well-being. 3, 4 Additionally, because the AL community is comprised mostly of the old-old, (i.e. persons over age 85) many of whom are diagnosed with multiple and chronic health conditions, ranging from dementia (41.9%), arthritis (42%); asthma (6.8%); and diabetes (18.1%) to depression (30.9%) 2 , continuous appraisals and monitoring of their health by qualified licensed health care providers is essential in chronic disease management. 5, 6 Yet, across the post-acute care continuum, national trends show care is being provided by staff who work with work force shortages of RNs, [7] [8] [9] and who lack specialization or certification in in geriatrics and geriatric nursing. 10 Owing to a lack of clinical expertise among the nursing staff and/or limited access to expert clinicians places front line healthcare staff in a vulnerable situation. Added to this situational context, is the emotional toll of compassion fatigue 11 , physical exhaustion 12 , and the psychological strain/stress associated with caregiving 13 . Further exacerbating this caregiver stress are the acute changes in older adult conditions resulting from contagious diseases such as COVID-19. This then jeopardizes front line staff's ability to provide comprehensive and timely assessment of residents' chronic health conditions. Older adults are at risk for not only acquiring additional co-morbidities and high risk conditions in the aftermath of COVID-19, but to also be impacted by the outcomes on cognition and mental health as a result of imposed social isolation. Social isolation stemming from social distancing has been shown to increase the older adult's risk of mortality, especially when the older adult is frail. 14 Due to the lack of social contact with family caregivers and significant others and restricted contact with other residents, older adults in AL are likely to experience: (1) dysphoria and major depressive episodes; (2) psychological stress/strain such as feelings of loneliness or anxiety; (3) reduced mobility/immobility; and (4) functional decline. Moreover, because physical mobility is impaired with social distancing, functional decline is likely, as there is less opportunity for physical activity and exercise. When functional decline occurs, so can deconditioning, further hindering restorative nursing efforts aimed at prevention of functional impairments and disability.@story_separate@The purpose of this paper is to present some practical solutions and best practice recommendations for care delivery which can be implemented by front line staff in their daily care of older residents in their rooms. These best practices are directed at the psycho-social, functional, and emotional needs of older adults. These practices are likely to also be beneficial to healthcare staff as they build on maintaining relationships with residents. The ultimate goal of care in the AL community is to preserve the resident's functional and social independence, quality of life, dignity, and autonomous decision making for self-determination. There is no shortage of information in the literature to support that ""humans by nature are social animals"" (Aristotle) with social needs, that when met, can, and will improve quality of life, protect against illness, and promote health and well-being. 15 All human beings, regardless of age, share the need to be loved, accepted by their peers, and to feel a sense of belonging to a community 15, in addition to the need for intimacy; but these social needs become more pronounced as people age. A systematic literature review of 14 studies conducted on the social needs of older people revealed four major themes: (1) diversity of needs; (2) proximity; (3) meaningful relationships; and (4) reciprocity. 16 Diversity of needs refers to the fact that everyone is different and what an individual needs as they age will depend on their personality, culture, and expectations at that particular time in their life. The social need for proximity, to develop a social support network of relationships with friends and family, is what contributes to ""older adults' feelings of safety, comfort, and connectivity"". [15] [16] Meaningful social relationships not only provide affection, but they give an individual a sense of purpose and respect that is needed in order to maintain one's independence. Finally, reciprocity is about that basic human need to not only receive but to give back, to feel useful by helping others. 15 17 Increased awareness of the impact of social distancing on social relationships provides an opportunity to examine the current strengths within AL communities and among its healthcare staff to foster an Age-Friendly community. This article highlights best practices as they relate to strengthening social and interpersonal relationships among older residents which can serve to strengthen their baseline mentation, emotional status, and mobility associated with social isolation from COVID-19. Within the health system or practice setting, the 4M Framework is instituted as a set of recommendations. Thus, at the beginning of every resident encounter, health care professionals ask the older adult: ""What Matters most, document it, and share What Matters across the care team, ensuring the care plan aligns with What Matters most"". 17 Framework around the prevention, identification, treatment and management of delirium 17 . Delirium is an acute change in mentation which is not only treatable, but more significantly, preventable. Delirium manifests as confusion and disorientation, along with changes in the level of alertness, and waxing and waning from states of hyperalertness to hypoalertness (i.e., lethargy). Because there is a relationship between mentation or cognition and emotional processes, it is important not only to assess the resident's cognition, but to screen for depression. 17 Delivering nursing care interventions directed at mentation involves acknowledgement of all of the factors known to influence mentation from chronic illnesses such as depression, Alzheimer's Disease, Parkinson's Disease, and acute illnesses such as dehydration, infection or myocardial infarction, to adverse reactions to medications and changes in environment. Nursing staff's knowledge of the resident's baseline mentation is critical as subtle changes in behavior (i.e. outbursts of anger and/or frustration, indicative of hyperarousal states), or increased isolation or function (refusal to engage in normal activities, inability to use commonly used objects which may signal confusion, apraxia, agnosia) can signal delirium. Change in mentation or delirium is a medical emergency warranting immediate assessment by a professional nurse, physician and/or referral to a local emergency department. Because COVID-19 is a highly contagious and infectious disease, any change in mentation among residents living in a high risk community, such as AL, must be assumed to be due to COVID-19 until diagnostically proven otherwise. Generalized nursing interventions need to promote social connectedness especially during a pandemic when residents are forced to physically distance from their relationships. Interventions to promote mental and emotional well-being include: maintenance of normal and familiar routines; ensuring care is delivered by the same nursing staff; authentic presence, such as sitting aside the resident; actively listening and reflecting with that resident; personalizing the residents' room with familiar items and promoting privacy; choice in participation in activities which can be accomplished in one's room (refer to Box 1). Additionally, staff need to communicate in writing using large print lettering on notepads, to use large print signage in the room (i.e., to accomodate visual impairment), and to facilitate identification of resident needs (signage could indicate identification of unmet needs such as thirst, hunger, pain). Mr. P is an independent retired professor residing in AL who became vocal about the conferment to stay in his room. He wrote a letter to state officials in the Governor's Office stating his strong opposition to the room confinement on the basis that it violated his resident's rights and civil liberties. Though his voice was heard by state officials, no action was taken because of the heightened risk for the spread of COVID-19 throughout the community. In light of the COVID-19 pandemic, there are now many more efforts made to effectively communicate the psycho-social and medical concerns and needs of AL residents so that their voices are heard beyond the AL community. An essential nurse's response is not only to promote and ensure a safe living and care environment for AL residents, but to advocate for their resident rights and civil liberties. Framework around the assurance that each older adult moves safely every day to maintain function and do What Matters. 17 Impaired mobility can be due to an acute or chronic illness, unwitnessed fall, and adverse effect(s) of a medication or due to a change in mentation which is potentially preventable. Impairment in mobility can quickly progress to a handicap or disability if not appropriately assessed and managed. Impaired mobility manifests as difficulty in ambulation or transferring from one position to another or the need for use of an assistive device suddenly. It is important for nursing staff to regularly assess/evaluate mobility by screening for mobility limitations and to monitor for lower extremity weakness or generalized weakness which can be indicative of the COVID-19 infection. Nursing staff's knowledge of the resident's baseline mobility is critical to early recognition of potentially modifiable causes. Interventions to promote safe mobility and to maintain motion and movement through activity and exercise are provided in Box 2. Ms. T is a spry 85 year old female resident of AL who has resided at the community for about one year. Her primary diagnosis on admission to AL is generalized osteoarthritis of her knees. Her baseline mental status is alert and oriented with no evidence of cognitive impairment. During the COVID-19 pandemic at the AL community she tested positive for COVID and was isolated as required. As she recovered however, Ms. T became so weak she was unable to stand without assistance and required three staff members to help her move from a sitting to a standing position. The AL community transferred Ms. T with her consent to a rehabilitation facility for a short term stay for rehabilitation. She then regained her strength and was able to return home to AL. Despite the fact that we are in a pandemic and the resident is COVID-19 positive, this case demonstrated that health promotion and restorative practices need to be prioritized by attending to and treating active medical problems such as reduced muscular strength, rather than maintaining the resident in isolation at the AL community. To say that 2020 will be memorable would be an understatement. Many of us, particularly those in healthcare and in positions of interacting with the public, may look back and describe this year of the COVID-19 pandemic as one riddled with despair, death, and dependency with the most devastating impact of this disease seen among the aging community, especially those confined to LTC and AL communities. The end of 2019 leading into 2020 began as something so un-imaginable, starting with worldwide social distancing and isolation; business, travel and life as we have known it coming to a halt; the loss of personal identity (covering our faces with masks and shields); the proscription of customary greetings (shaking hands, hugging, kissing), to the unthinkable legal mandate barring people from being with loved ones during illness, hospitalization and at the time of their dying and ultimate death (being unable to accompany loved ones, friends, pets to the physician's office or visit them while hospitalized, and ultimately not being able to share in communal burial rituals). However, a phoenix emerged from these losses to reveal a transformation from grief and disbelief to a revelation of awareness, awakening, and assessment. As a society, there was an awakening to the value of family, particularly the inclusion of older adults in our lives. Collectively, we became aware of the importance of a comprehensive hygiene and infection control protocol for everyone, not just for health care providers that is as simple, yet scientifically significant, as handwashing. It became readily apparent that we all need to assess how we go about life and business from an individual, but more importantly, population and public health consideration. From COVID-19, emerged what should have been obvious all along, the need for societal awareness, awakening and assessment of the social needs of all human beings and in particular, older adults, especially those who have moved into an AL community. According to the United States Census Bureau, in 2018 there were 52 million people 65 and older, making up 16% of the population. 18 10,000 baby boomers turn 65 every day. By 2030 all baby boomers will be 65 or older and by 2034 older adults will outnumber children under the age of 18. 18 These statistics demonstrate that as the population of older adults continues to expand, ensuring their social needs are met becomes paramount for them to remain healthy and ""community-dwelling"". 16 Though addressing the social needs of the older segment of society is the 'right thing to do' altruistically, the practical basis for this concern is grounded in economics, because the longer an individual can remain heathy in body, mind and spirit, the less of a burden they become financially and medically on themselves, family and society. The unprecedented death toll among older adults in our communities due to COVID brought much needed attention to a vulnerable group who, until this pandemic, has been ignored and kept in the dark, particularly those in post-acute care settings. Attention to the socialization needs and the impact of social distancing and isolation of residents in ALs and nursing homes throughout the country has been heightened. This nation's older adults' living in post-acute care settings, were not just separated from society due to their living arrangements, but now were confined to one room. Visitation and contact with other human beings was limited to direct care staff who were now covered with masks and face shields, gowns and gloves, who only entered the room to complete a task and left just as quickly as they entered if the resident tested positive to COVID-19. Even with COVID negative residents there was a reduction in social engagement between residents, staff, family and friends because of state official mandates disallowing gatherings during mealtimes and recreation activities, visitation and community trip that could lead to cognitive and physical deterioration leading to a failure to thrive and increased risk for co morbidities and mortality. Based on state guidelines many of the above mentioned activities were not permitted at all. But as noted previously, the pandemic brought awareness, awakening, and assessment to the healthcare community and to society as a whole, of what can occur to pre-frail and frail older adults who are confined to a post-acute care setting. That has led to the societal and healthcare community re-commitment to the promotion of health and wellness, prevention of disease and focus on healing that studies have shown result when social needs of older adults are met whether living at home, alone, or in AL.@story_separate@Older adults who move from their home of many years to reside in AL communities do so primarily for: assistance with medications, management of chronic illnesses and access to, and diversity of, social benefits that range from recreational activities and entertainment to dining experiences, excursions and most significantly the forging of new friendships. The health care component offered In AL communities, though important is often subsidiary to the socialization endeavors that are unique to AL communities. This socialization aspect, offered in AL communities, allows for an integration of the whole person 19 : body, mind, and spirit. Emotional attachment is often associated with places of socialization within ALs, (i.e. the dining room), that quickly became off limits with COVID. The psychosocial consequences that resulted from the need to initially quarantine and then require social distancing to prevent the spread of COVID, for many led to a sense of loss of place, social isolation, or loneliness which in turn could lead to low mood or dysphoria. Nurse's aides have been, and continue, to provide a quintessential role in ensuring that the social needs of older adults in AL facilities are met, in addition to their valuable contributions as members of the interprofessional healthcare team. Additionally, to hold onto and/or restore their sense of place, older adults in AL describe the authenticity of home that come from the caring interactions provided by an interprofessional health care staff. 20 In this article, the challenges within teams working in AL during a pandemic that restricted socialization via quarantine and social distancing was highlighted. A discussion on the 4M framework with its template that provides specific directives to promote a sense of belonging, familiarity with routines and rituals was offered as a guide to ensure the social needs of older adults living in AL facilities. Results of a literature review of 14 studies that had been conducted to determine the social needs of older adults and how meeting those needs can prevent disease and illness and promote and maintain health was presented. Census statistics on the number of baby boomers over 65 now and in the future was also presented to provide additional evidence and support of why society and healthcare providers must prioritize and ensure the social needs of the older adult population are met. Older adults in AL require interventions that promote cognitive function, decrease the risk for falls, maintain muscle tone, and provide engagement to limit the negative effects of social distancing and social isolation. Simple direct interventions that can be accomplished in light of limited time and money as well as reduced healthcare provider resources in AL can lead to optimum function despite the marked limitation of normal functioning during the COVID pandemic.","Human beings are social in nature and maintaining social interactions, relationships and intimacy are fundamental needs of older adults (OAs) living in assisted living (AL) communities. Yet, these very basic human needs have been impeded by quarantine mandates imposed by the COVID-19 pandemic. The socialization aspect offered in AL, allows for an integration of the whole person: body, mind, and spirit and is beneficial in mitigating the development of co-morbidities and negative patient outcomes. Additionally, the authenticity of home comes from the caring interactions provided by an interprofessional health care staff. Utilizing the 4 M Framework, created by The John A. Hartford Foundation and Institute of Healthcare Improvement, the authors describe simple direct bedside interventions of low cost, and high patient-centered value which front-line nursing and caregiver staff can employ to maintain social connections, interactions, mentation, function and mobility among residents they care for, and care about, in AL communities."
"derived the lending channel of monetary policy, which essentially predicted that low (policy) interest rate, e.g., the Federal Fund Rate, increases the bank supply of loans (i.e., increases credit). Bernanke and Blinder (1992) and Jimenze et al. (2012) are among others who provided empirical support for this theory. Goodfriend (2000) , however, was the first to argue that negative policy interest rate is a possible solution to the zero lower bound (i.e., the nominal interest rate reaches zero and monetary policy becomes ineffective in stimulating the economy). 1 The regression is Δbr t = � br t−1 − − r t−1 � + Δr t + ∑ T i=0 i Δbr t−i + t , where the dependent variable is either the bank lending rate or the deposit rate; and r t is the official policy rate. 2 He examined bank-level expectations about the impact of negative short-term interest rates on their profitability (net interest margins) using a confidential supervisory data called Comprehensive Capital Analysis and Review (CCAR). This is part of the Fed's stress testing procedure. His method of analysis is based on the fact that negative rates were introduced by the Federal Reserve as an explicit scenario design feature in the supervisory severely adverse scenario of the 2016 vintage of CCAR. This design feature allowed him to isolate how individual banks view their net interest margins as evolving in a negative rate environment, even after controlling for underlying macroeconomic developments and bank-specific characteristics. He uses three vintages of the CCAR data. Each vintage has five different scenarios including a baseline scenario. A panel regression equation of bank net interest margins is regressed on a constant term, lagged dependent variable, lagged three-month Treasury rate, lags of the spread between the 10-year bond rate and 3-month Treasury rate, real GDP growth, and a number of bank-specific factors. He uses the predicted values from this regression and compares them to the banks projections. These deviations are regressed on an indictor function that takes on the value of one if negative short-term interest rates are a qualitative feature of the given bank scenario-vintage in quarter t and zero otherwise. The RBNZ has already said that it is willing to reduce the OCR to negative if needed. Most observers expected the OCR to be negative early in 2021, March or April. This paper attempts to measure the effect of negative interest rate on lending rates, deposits rate, and bank profit in New Zealand. Banks in New Zealand hold reserves in the Settlement Cash Account at the Reserve Bank (RBNZ). The lending channel hypothesis predicts that a negative interest on this account (i.e., negative OCR) encourages banks not to hold more reserves with the RBNZ, hence increasing lending, and that would stimulate demand. A low and negative interest rate should also increase asset prices (e.g. Razzak and Moosa 2018) and reduce the cost of funds (e.g. reduce the interest rate on deposits). Together, these changes, depending on the relative magnitudes, affect bank income and profit. We are unaware of any other papers on this subject about New Zealand. Our paper is methodologically different from the studies cited earlier. We accomplish our objective by estimating the equilibrium lending rate and the deposit rate in New Zealand then making projections of the effect of a negative OCR in New Zealand on the future bank lending rate, deposit rate, and profit. We derive an equilibrium lending and deposit rates from a constrained bank-profit maximization problem, i.e., a partial equilibrium rather than the DSGE panel model used for the Euro Area. Then, we use an unrestricted VAR to summarize the dynamics of the equilibrium rates instead of single-equation regressions. Then the VAR model is solved using a dynamic and stochastic method, whereby the innovations are produced using bootstraps to produce baseline projections over the period from Sep 2020 to Dec 2024. Thus, we provide genuine out-of-sample baseline projections. We follow the same methodology to make projections under a counterfactual scenario, whereby the OCR is negative. Similarly, we produce baseline projection and out-of-sample projection under counterfactual scenario for the period from Jun 2020 to Dec 2024 under a negative OCR, bank interest income, non-interest income, interest cost, and non-interest cost, which allow us to analyze bank profit under baseline and under a negative OCR scenario. We found that both the equilibrium lending and deposit rates decline significantly when the OCR turns negative, and they both turn negative as the projection horizon increases. On average-over the projection horizon-however, the lending rate remained higher than the deposit rate. In addition, net interest income increased. We project that a negative OCR increases bank profit relative to baseline by about 19% on average over the period Sep 2020 to Dec 2024, which is consistent with Bernanke and Blinder (1988) . However, the trade-off is having more uncertainty. Interest income and costs, and non-interest income, among all the components of profit (i.e., income from derivatives, trade, fees etc.) becomes more volatile when the OCR turns negative. Next, we derive the equilibrium lending rate and the deposit rate from constrained profit maximization. In Sects. 3 and 4, we estimate the dynamic of the equilibrium lending and deposit rates using a VAR, and provide a dynamic stochastic baseline projection up to Dec 2024. Then we provide projections of the equilibrium lending and the deposit rates under scenarios of negative OCR. Section 5 is a similar analysis of the effect of the OCR on the bank profit. Section 6 is a conclusion.@story_separate@These equilibrium rates result from the interaction of supply and demand curves of loans and deposits. Let us assume a representative bank, which takes deposits D t from households, firms, and the government to make loans L t to firms and households. The interest paid on deposits is r d t and the lending rate is r l t . Banks receive interest r ocr t on the deposits D s t in the Settlement Cash account held at the RBNZ. r ocr t is the OCR. 3 Banks can invest in bonds B t or other financial products in the money and bond markets and obtain returns. We assume that the money and bond markets are one market for simplicity. The representative one-period bank maximizes profit, which is, total revenues less total cost. The profit function is: 4 Π t is bank profit. r l t is the lending rate. L t is the quantity of loans of the bank. D s t is the settlement cash balance at the RB, which is paid r ocr t , r b t is the interest rate on bonds. B t the RB bonds held by the bank and r d t is the deposit rate paid by the bank and D t is bank deposit. NP t is the bank net position of the bank in the money and bond market, whereby banks invest in these markets, and r n t is the market interest rate. c(.) is the bank managing cost; it is strictly convex and twice continuously differentiable. Assume that the net position of the bank is given by: We specify a simple quadratic cost function. The parameters 1 and 2 are positive marginal costs of deposits and loans. Substitute both (2) and (3) in (1). The bank maximizes Π t subject to a constraint. The constraint is on the capital/asset ratio. We write this constraint as The assets, A t = L t + x t , where L t is loans and x t is all the rest of the (3) c t = 1 2 1 D 2 t + 2 L 2 t . (4) bank assets. For convenience, we rewrite the constraint (L t − K t − x t ) , where is the one-period Lagrange multiplier. Solve for the first order conditions (FOC). So from (6), the OCR, r ocr t is equal to the risk-free money market rate r n t . From (5), We replace the risk-free market interest rate r n t with the OCR r ocr t and rewrite Eq. (9): Therefore, the optimal supply of loans is: We postulate the demand for loans to be negatively related to the lending rate and positively to demand. Equate the supply and the demand and solve for the lending rate. The optimal (equilibrium) lending rate is: Thus, The high positive correlations are tested using 2 (0.95,2) Confidence Ellipse. The cov r l t , r ocr t > 0 and cov r l t ,ỹ t > 0 , i.e., the lending rate is positively correlated with the OCR, and with income. Similarly, we could derive the equilibrium deposit rate as a positive function of r ocr t and a negative function of aggregate saving. From (7), And we postulate that the supply of deposits is a positive function of aggregate savings S t and the deposits rate r d t . The equilibrium deposit rate is: (17) D s t = S t + r d t . > 0 , and cov r d t , S t < 0, The increase in savings is associated with a lower deposit rate. Figures 3 and 4 are scatter plots of the actual deposit rate and the OCR, and the deposit rate and savings, which we them tested using a 2 095,2 Confidence Ellipse. Next, we estimate the dynamics of the lending and deposit rates. We analyze the equilibrium lending rate over the sample from Mar 1999 to Jun 2020. 5 We summarize the dynamics of OCR, lending rate, and a measure of household demand in order to make baseline dynamic stochastic projections and projections of a counterfactual scenario of a negative OCR out-of-sample covering the The data sources are in the data appendix Table 7. period up to 2024, which we chose arbitrarily. Therefore, we use a standard unrestricted VAR. 6 The VAR is given by the standard form where y t = y 1 , y 2t , ⋯ y kt � is a k × 1 vector of endogenous variables. There is also an exogenous constant term, And write the VAR is a compact form: Y is r ocr t ,ỹ t , r l t ; is 1t , 2t, 3t both are matrices of the endogenous variables are the innovations. The matrices B = A 1 , A 2, A 3, constant and Z = Z 1t , Z 2t , Z 3t are the matrix of coefficients and matrix of regressors, respectively. 7 The RBNZ reports two lending rates; a business lending rate and a housing lending rate. Here we report our analysis of the housing lending rate r l t as a measure of the lending rate. Because we use the house lending rate instead of the business lending rate, it seems more appropriate to use household disposable income gap than the output gap to measure demand, ỹ t . 8 Figure 5 plots the three variables of the VAR, the OCR, the disposable income gap, and the housing lending rate (we also plot the business lending rate to show how closely correlated it is to the housing lending rate). The VAR is estimated for Estimating an SVAR does not alter the results, therefore, we do not report the result. The results are available on request. The observed residuals e t have a covariance matrix where u t is a matrix of unobserved shocks, which we want to identify. This matrix has an identity covariance matrix ∑ � uu � � = I . Different methods can be used to identify shocks, but the orthogonality of the shocks implies that the identifying restrictions on A and B are of the form Since the matrices on both sides of the equality sign are symmetrical, we have k(k + 1)∕2 restrictions on the 2k 2 unknown elements in A and B . To identify A and B , additional 2k 2 − (k + 1)∕2 identifying restrictions are needed. We use short-run restrictions on B . These restrictions imply that the OCR is unaffected by the lending rate and disposable income and it is a function of its own past, disposable income is a function of its own past values and the OCR past values, and the lending rate depends on its own lags, disposable income lags, and OCR lags. 7 We tested a dummy variable that takes a value of 1 during the period Mar 2009 to Dec 2009 to account for the significant drop in interest rate. We found it to be statistically insignificant and only marginally significant in the income equation. 8 We also used the business lending rate and then the average of the business and the housing lending rates, and the real GDP output gap instead of disposable income gap. The results are qualitatively similar, but the statistics differ slightly. We do not report these results but they are available on request. The HP filter is used to de-trend the real disposable income. New Zealand using quarterly data from March 1999 to Jun 2020. 9 The VAR includes a constant term. We fit three lags. 10 Figure 6 plots the generalized impulse response functions, Pesaran and Yongcheol (1998) . 11 The standard errors of these impulse response functions are computed using a Monte Carlo with 1000 repetitions. The responses are consistent with the theory and Eq. (15). 12 The middle plot in the first The standard Dickey-Fuller test for unit root is a weak test against stationary alternative, however, we adjust the test for a break in the data, especially during the Global Financial Crisis, and we could easily reject the null hypothesis of a unit root in interest rates. For the disposable income gap is stationary by design. 10 The VAR satisfies the stability conditions with all roots are inside the unit circle. The joint Wald statistic for lag-exclusion test has p-values of 0.0000, 0.0001, and 0.0211 for lags 1-3. The AIC, SC, and HQ Information Criteria to determine the lag structure suggested three lags. The residuals are tested for serial correlation using the LM test. The null hypothesis that the residuals are serially uncorrelated at lag 1, 2, and 3 cannot be rejected. The P values are 0.0771, 0.0611, and 0.2939, respectively. When testing the null hypothesis of no serial correlation at lag 1-3, the P values of the Edgeworth expansion corrected likelihood ratio statistic are 0.0771, 0.2162, and 0.0501, respectively. The F statistics in Eqs. (1-3) are highly statistically significant. 11 The order of the variables does not seem to matter. We tested that and found that the standard Choleski impulse response functions to be the same. Further, Koop, Pesaran and Potter (1996) and Isakin and Ngo (2020) show that when models are linear, traditional IRFs and variance decomposition. 12 First, the actual data of the output gap, and the disposable income gap and the OCR are positively correlated over the sample from Mar 1999 to Mar 2020. There is no correlation if June 2020 data are included because output and income fell significantly after COVID-19. These positive correlations between the short-term nominal OCR and real output suggest that aggregate demand shocks dominate. Theoretically, take for example, a simple IS-LM, AD-AS model. If shocks were dominantly positive (negative) shocks in the goods market, the IS curve shifts up (down), and both the OCR (on the vertical axis) and output (on the horizontal axis) decline (i.e., move in the same direction). The AD would also shift in the same way. Second, since the RBNZ reacts to aggregate demand shocks, it responds by increasing the OCR when the output gap opens up. Thus, the OCR response to the output gap is positive as shown by the IRF (Fig. 6 , row 1, middle plot). row of Fig. 6 shows that the OCR responds positively to the disposable income gap. The first plot in the second row shows that the disposable income gap responds positively to the OCR. The third row shows that the lending rate is highly positively responsive to the OCR and income. These responses are reasonable. The next step is to produce a baseline dynamic stochastic projection of the lending rate for the period from Sep 2020 to Dec 2024. This end date is arbitrary. The model is solved and dynamic and stochastic projections are produced, whereby the innovations are generated using bootstrapping with 1000 iterations over the period Mar 1999 to June 2020. 13 Figure 7 plots the dynamics of the baseline projections. Response to Generalized One S.D. InnovaƟons ± 2 S.E. Response to generalized one S.D. innovations ± 2 S.E 13 Dynamic Stochastic solution of the model has been used before in the literature to deal with the Lucas critique. When solving, we use an approximated Jacobian to linearize the model. Then the approximation is updated each iteration by comparing the residuals, which result from the new trial value of the endogenous variables with the residuals of the linear equation. The method is not significantly different from Newton, but it runs faster. We generate the innovations to the stochastic equations by drawing a set of random shocks from a standard normal distribution each period. To match the variance-covariance system, we scale these draws by multiplying the vector by its standard deviation because the covariance matrix is diagonal. 14 The solution is described in the technical appendix Table 8 Counterfactual projections of the lending rate under a negative OCR The final step is to produce projections of the lending rate under a counterfactual scenario. We assume that the OCR was reduced in Mar 2020 to a negative 0.25 and it remained − 0.25 in Jun 2020. 15 We make no assumptions about the OCR after June 2020. Figure 8 displays the actual OCR and the negative OCR that we assumed for the counterfactual scenario. We re-estimate the VAR over the same sample from Mar 1999 to Jun 2020. The optimal number of lags is three. The residuals are serially uncorrelated. 16 Then the model is solved, and dynamic and stochastic projections for the period Sep 2020 to Dec 2024 are produced; the innovations were generated using 1000 Bootstraps. Figure 9 plots the projections under this counterfactual negative OCR scenario and the standard error bands. The housing lending rate declines more under a negative OCR scenario relative to the baseline projections. Figure 10 plots the actual rate, the baseline projections, the projections under the counterfactual scenario and the deviations of the counterfactual projections from the baseline, which clearly Fig. 8 Actual OCR and simulated counterfactual 15 The RBNZ announced that it could reduce the OCR to a negative rate in 2021 if more stimuli needed to deal with the downturn caused by COVID19. 16 We do not report the statistics to save space, but they are available on request. shows that the lending rate falls significantly under the counterfactual scenario of a negative OCR. Table 1 reports data of the actual housing lending rate, the baseline projections, the projections under the counterfactual scenario, and the deviations from the baseline. Under the counterfactual scenario of a negative OCR, the lending rate declines steadily from 3.35%, in Sep 2020, to 2.20%, in Dec 2024. On average over the projection horizon, the average of the house lending rate under the counterfactual scenario of a negative OCR is 2.39%. The average baseline projection of the lending rate is 4.15%. In addition, note that the projections of the lending rate under the counterfactual scenario of a negative OCR are significantly less volatile than the baseline projection. The standard deviations are 0.30 and 0.69 for counterfactual projections and the baseline projections, respectively. We examined the business lending rate and the average of the business lending rate and the housing lending rate with the real GDP output gap.  Lending rate 17 We do not report the results to save space, but they are available on request. Equation (18) and scatter plots (3) and (4) show that cov r d t , r ocr t > 0 , and cov r d t , S t < 0 . Figure 11 plots the annual time series of the deposit rate, aggregate national savings, and the OCR. We use annual data from 2000 to 2019 because the RBNZ reports annual savings only and the data are available to 2019. We use national savings because the savers include not only households, but also businesses, Figure 11 shows that the correlations are consistent with the model. We estimate a VAR for the OCR, aggregate savings, and the deposit rate using annual data from 2000 to 2019. The Information Criteria identifies three lags. 18 Figure 12 displays the generalized impulse response functions. The deposit rate responds positively to the OCR and negatively to aggregate savings as predicted by Eq. (18). Then, we solve the model and produce a dynamic stochastic baseline projection, where the innovations were generated using 1000 bootstraps exactly like what we did for the lending rate. We estimate the VAR under the counterfactual scenario using the same methods as before. The OCR is − 0.25 in 2019 and remained negative in 2020. The model is solved from 2021 to 2024 and the innovations were generated by 1000 bootstrapping. Table 2 reports the actual deposit rate, the mean dynamic stochastic baseline projection, and then the mean dynamic stochastic projections under the counterfactual scenario, followed by the deviations from the baseline. The projections of the deposit rate under the counterfactual scenario declined significantly, and turned negative in 2023 and 2024. Table 3 compares the average baseline projections of the lending and deposit rates, and the mean of the projection scenarios. Under the baseline projection, the lending rate (4.05%) is above the deposit rate (3.4%). Under the counterfactual scenario that the OCR is − 0.25, the averages of both the lending rate and the deposit rate over the projection's horizon fall to 2.39 and 2.02%, respectively. Figure 13 plots the deviations of the deposit rate projection under the counterfactual scenario from the baseline projection, which is a negative steady decline over time. The results of the above analysis of the housing lending rate and the deposit rate under a negative OCR indicate that both rates would fall. Over the projection horizon from 2020 to 2024, the lending rate falls by about 1.65% and the deposit rate by about 1.38%. On average and over the period 2020 to 2024, the deposit rate is projected to be lower than the lending rate by about 0.25%. However, it is unclear what would be the effect on bank profit because profit depends on interest and non-interest incomes and costs such as derivatives, trade, fees and commissions among more. Negative OCR is a monetary policy response to anticipated economic slowdown, which has adverse effects on equities, assets, derivatives, fees and commissions, etc. Next, we examine the bank profit data.  The RBNZ reports quarterly time series data on bank income, expenses, and profit from June 1991. Table 4 describes the data. The OCR affects interest and non-interest incomes and costs differently. Figure 14 plots bank profit (before tax); it had a negative spike during the Great Recession that followed the Global Financial Crisis (GFC) in June-September 2009. Bank profit declined sharply even though bank income was positive in these two quarters; it was most clearly related to a significant spike in the operating cost, which increased significantly by 54% and 37% in June and in Sep quarters, respectively. During that recession, the output gap fell significantly, − 2% and − 1.7%. The RBNZ slashed the OCR. It remained, relatively, low until 2020. The OCR dropped from an average of 6.25% to 2.35% over the subsamples from 1999 to 2008, and 2009 to 2020 respectively, as shown in Fig. 5 . The lending rate kept falling for more than two quarters before and after the recession; it fell by 1.8% and 0.37% in these two quarters. The deposit rate, however, fell significantly by 0.30% in 2009 and by 3.2% in 2010. Bank profit is the sum of interest and non-interest incomes less interest and noninterest costs. The final effect of negative OCR on bank profit depends on the magnitudes of the various costs and incomes. During the 2009 recession, bank total cost increased (interest and non-interest costs) substantially while income (interest and non-interest income) remained unchanged, which resulted in a sharp decline in bank profit in those two quarters. However, despite this downward spike, the overall trend of bank profit from 1999 to 2020 has been positive. The RBNZ reduced the OCR from 1% to 0.25% in Mar 2020 in response to COVID-19; and expected to make the OCR negative in Mar 2021. Figure 15 plots the total interest income, total interest cost (or expense), and the net interest income. Note that interest income and expenses grew significantly over time and peaked in Dec 2008, during the GFC, then fell sharply in March 2009. They are also highly correlated. After Dec 2008, interest income fluctuated slightly, but remained almost unchanged while interest expense declined a little and the difference between interest income and expense (the net interest income) increased over time. Table 5 compares the banking system outcomes for the period Mar 2009-Dec 2009, i.e., the recession that followed the GFC with Mar 2020-June 2020, i.e., the lockdown response to COVID-19. We show that the negative impact of the lockdown on bank profit has been very substantial compared with the effects of the recession in 2009. We report the average growth rates over the period Mar 2009 to Dec 2009 and over the first two quarters in 2020, March and June. The average growth rate of interest income fell sharply in the past two quarters compared to 2009, − 11% compared with -6%. The interest cost average growth rate fell more during the pandemic compared with 2009; − 17.7% compared with − 9.4%. Net interest-income growth rate declined significantly. The average growth rate of non-interest income is − 50.6% in 2020; it was + 9.6% in 2009. These are clearly significant differences and the decline in the growth rate reflects the lockdown of the economy. Essentially, total operating bank income growth rate is − 15.4% in 2020 compared with + 12.6% in 2009. Bank profit before tax growth rate in 2020 is − 13.7%; it was + 37% in 2009. Bank profit went down significantly. Would bank profit recover if the OCR were negative? Table 6 reports descriptive statistics of bank profit components, in-sample, and the out-of-sample projections. In sample, we report statistics over 2 sub -samples, 1999-2008 and 2009-2020 . The components of bank profit are (1) interest cost, (2) non-interest cost, (3) interest income, (4) non-interest income, (5) net interest income (income less cost), (6) net non-interest income (non-interest income less non-interest cost), impairment, and (7) profit (income less cost less impairment). Each column has two statistics, the average over the sample and the correlation of each of the profit components with the OCR. Note that banks were more profitable during the period from 2009 to 2020, when the OCR was relatively lower than the period from 1999 to 2008 when the OCR was high. As the OCR declined significantly over time, bank profit increased. Lower OCR implied lower interest cost, and more lending (volume)-credit expansion as in Bernanke-Blinder (1988) . More lending generated more income to banks; net interest income increased as a result. At the same time, lower OCR also led to higher asset prices. Non-interest income increased too but so did non-interest cost; however, the increase was not sufficient to offset the rise in income. Eventually profit increased from $920 million over the period 1999-2008 to 1463 million over the period 2009 to 2020. The correlation coefficient of each of the profit components and OCR also changed over the two sub-samples; they become smaller. Four of these profit components' correlations with OCR changed signs over the two sub-samples. The last three columns of Table 6 report the descriptive statistics of the baseline projections and those of the projections under a counterfactual scenario of a negative 0.25 OCR. We produce the projections using these same methodology used earlier by fitting a VAR with six variables, OCR, and the components of profit, which are the interest income, non-interest income, interest cost, non-interest cost, and impairment. The sample is Mar 1999 to Jun 2020. We do not report the details but they are available on request. 19 The baseline projections are from Sep 2020 to Dec 2024. Then we re-estimate the VAR under a counterfactual scenario, whereby the OCR was negative 0.25 in Mar 2020 and June 2020. Then we made dynamic stochastic projections from Sep 2020 to Dec 2024 under this counterfactual scenario. The baseline projection of bank profit shows declines then increases, but on average over the projection horizon, profit increases by 4.6% relative to actual profit (Mar 2009 to Jun 2020), from $1,463 million to $1,530. The projection under the counterfactual scenario of a negative 0.25 OCR increases to $1,816 million, which is 24% higher than actual on average. However, on average over the projection horizon from Sep 2020 to Dec 2024, the deviations of bank profit projections under the counterfactual scenario of a negative OCR of 0.25 from the baseline are + $286 million, a 19% increase. Most of the projected increase in bank profit under the counterfactual scenario of negative OCR comes from the projected increase in bank interest income; it increases by $784 million. Non-interest income projections also increase by $18 million. Costs also increase under the counterfactual scenario, but by less than the incomes. The interest cost increases by $445 million and the non-interest cost increases by $73 million. Impairments decline by $2 million. Therefore, total income projected to be $802 million and total costs $516 million. Figure 16 plots the actual profit, the baseline profit projections, and the projections under the negative OCR scenario. Bank profit is projected to increase under a negative OCR. However, there is a trade-off for this increase in bank profit. The increase in profit is associated with more uncertainty. For the period from 2009 to 2020, where the average OCR was relatively low, Bank profit, non-interest income, non-interest income, and impairment became more uncertain. 20 For the projection period 2020 to 2024, interest income, non-interest income, and interest costs projections under the counterfactual scenario of a negative OCR are more volatile compared with the baseline projections. 21 So, while banks may benefit from higher income from interest and noninterest operations their incomes become more uncertain under a negative OCR.@story_separate@We analyzed the lending and deposit rates and bank profit in New Zealand for the period from Mar 1999 to Jun 2020. An equilibrium lending and deposit rate was derived from a constrained profit maximization problem. The actual data show and the model predicts that the official Reserve Bank interest rate, the OCR, which is the rate paid nightly to the Settlement Cash Accounts at the Reserve Bank, is correlated positively with the lending, and deposit rates. We estimated an unrestricted VAR, produced baseline projections, and projections under a counterfactual scenario whereby the OCR is reduced to a negative 0.25 for two periods. The projections under the counterfactual scenario of both, the lending rate, and the deposit rate, over the period Sep 2020 to Dec 2024, declined on average. However, on average, the projected lending rate remained higher than the deposit rate. Bank profit has five components; the interest and non-interest incomes, the interest and non-interest costs, and impairment residuals. There is a break in the OCR data. The average OCR from Mar 1999 to Dec 2008 was 6.25%. The OCR was reduced during the recession in June and September 2009 that followed the Global Financial Crisis. The average OCR for the period Mar 2009 to June 2020 is 2.24%. 20 We test the hypothesis that variance for the sub-sample 2009-2020 is equal to the variance for the sub-sample 1999-2008 using the statistic F 40,45 = S 2 1 S 2 2 , where S 2 1 is the sample variance over the period 2009-2020, where the OCR was declining, and S 2 2 is the sample variance over the period 1999-2008 where the OCR was relatively higher. These ratios (P-value) are interest income 0.03615 (1), non-interest income 5.07 (0.0000), interest cost 0.07 (1), non-interest cost 3.6 (0.0000), impairment 6.8 (0.0000), and profit 6.0 (0.0000). The hypothesis that the variances are equal across the two samples is rejected except in the cases of interest income and interest cost. 21 We test the hypothesis that variance under the counterfactual scenario is equal to the variance under the baseline, against the alternative that it is larger by computing the statistic F = S 2 1 S 2 2 , where S 2 1 is the sample variance of each component under the counterfactual scenario of negative OCR, and S 2 2 is the sample variance under baseline. The F stats (P values) are 3.0 (0.01118), 2.8 (0.0174), 2.2 (0.0523), 0.65 (0.8101), 0.50 (0.9193), and 1.13 (0.3946) for interest income, non-interest income, interest cost, noninterest cost, impairment, and profit, respectively. There is evidence of increased volatility under the counterfactual scenario of negative OCR, especially in interest income, non-interest income, and interest cost. The components of bank profit also changed significantly after 2008, and the correlation with the OCR became relatively lower and changed signs. Bank profit increased steadily over the period of low OCR from 2009 to 2020. We also found that the OCR over the period from 2009 to 2020 to be less volatile than the period of high interest rate from 1999 to 2008, however, non-interest income, impairment, and bank profit were more volatile. On average, a counterfactual scenario of negative 0.25 OCR predicts an increase in bank profit by $286 million, about 19% relative to baseline projections, because interest and non-interest incomes increase by $802 million and interest and noninterest costs and impairment increase by $516 million. The growth rates of bank interest and non-interest incomes, costs, and profit during the period Mar to Jun 2020 are in a stark contrast to the growth rates during the period Mar to Dec 2009 after the GFC. Actual bank profit's growth rate was about 37.2% in 2009; so far in 2020, bank profit's growth rate is − 13.7%. Most of the decline in bank profit is due to − 50.6% growth rate of non-interest income. Noninterest income is investments, derivatives, trading, fees, and commissions, which have declined significantly due to the shutdown of the economy. New Zealand Banks benefit from looser monetary policy and benefit more from negative OCR because lending activity increases significantly with the lending rate higher than deposit rate, and net interest income increases. Non-interest income component of bank profit, which is the income from derivatives, trading, fees, commissions etc. also predicted to increase under negative OCR scenario, however, becomes more uncertain compared with the baseline projection. Therefore, there is a trade-off. Instability of bank income increases in the long run as OCR becomes more negative. We solve the VAR using Broyden's method, which is a modified Newton's method. It involves the use of an approximation, rather than the true Jacobian when linearizing the model. We update the approximation at every iteration of the 5000 iterations we used by comparing the residuals from the new trial values of the endogenous variables with the residuals predicted by the linear model based on the current Jacobian approximation. This method is faster than Newton. See, Dennis and Schnabel (1983) . We use analytic derivatives. The starting values are actual values. The model is solved both directions. We stop solving when we hit a missing value In a stochastic simulation, we solve the equations of the model such that the residuals match to randomly drawn errors, and the coefficients and exogenous variables of the model change randomly. The solution generates a distribution of outcomes for the endogenous variables in every period. We approximate the distribution by solving the model many times using different draws (1000) or the random components in the model then calculating statistics over all the different outcomes Only values of the endogenous variables from before the solution sample are used in the dynamic solution of the projections. Lagged endogenous variables are calculated using the solutions calculated in previous periods, i.e., not from actual historical values. A series for the mean is calculated. We consider one thousand repetitions reasonable to capture the true values; however, some random variation may be present between adjacent observations The 95% confidence intervals are computed using Jain and Chlamtac (1985) updating algorithm. This updating algorithm provides a reasonable estimate of the tails of the underlying distribution as long as the number of repetitions is not too small We use bootstrapped innovations; however, bootstrapped innovations drawn from a small sample provides a rough approximation to the true underlying distribution of the innovations. For the diagonal covariance matrix, the diagonal elements are set to zero. We do not scale the variances","We derive an equilibrium lending and deposit rates from a constrained profit optimization model, and estimated them over the period from 1999 to 2020. Then, dynamic stochastic baseline projections of these equilibrium rates and bank profit, and their projections under a counterfactual scenario of a negative interest rate, were produced for the period 2020–2024. The model predicts that a negative official cash rate (OCR) lowers the lending and deposit rates on average over the period Jun 2020 to Dec 2024; but the lending rate is higher than the deposit rate. It also increases the volatility of these rates relative to baseline projections. Negative OCR increases both incomes and costs; however, bank profit increases on average, by about 19% relative to baseline projections over the period Sep 2020 to Dec 2024. However, that increase of bank profit is associated with more uncertainty."
"The novel coronavirus virus, SARS-CoV-2, has affected the lives of people in almost every corner of the globe and imposed threats and several challenges on the healthcare front. Since the emergence of this novel virus in Wuhan, China in late November 2019, 3.02 million people have lost their lives worldwide as of April 20, 2021 (Johns Hopkins University and Medicine, 2020) . As the death toll continues to rise in the United States and all across the globe, clinicians and translational researchers are taking a closer look at the molecular cross-talk between sleep, circadian rhythms and immunity that may play an essential role during the COVID-19 pandemic. SARS-CoV-2-induced multiorgan failure affects both central and peripheral organs, causing increased mortality in the elderly (Garg et al., 2020) . However, whether differences in sleep, circadian rhythms and immunity between older and younger individuals contribute to the agerelated differences in systemic dysregulation of target organs observed in SARS-CoV-2 infection is still being investigated. Accumulating evidence from the literature demonstrates the emerging role of sleep, circadian rhythms and immunity in the development of chronic pulmonary diseases and respiratory infections in human and mouse models (Sundar et al., 2015c) . The exact mechanism underlying acute respiratory distress syndrome (ARDS) and other cardiopulmonary complications in elderly patients in combination with associated comorbidities remains unclear, yet understanding the nexus of sleep, circadian clock dysfunction in target organs and immune status of patients with SARS-CoV-2 may provide novel insights into possible therapies. The circadian timing system is a lot more complex process than a simple internal machinery that regulates our sleep-wake cycle. It has been established categorically that the circadian rhythms can regulate our innate and adaptive immune response, modulate viral replication within the host cells, coordinate physiological processes, and often determine the severity of many illnesses (Zhuang et al., 2017; Haspel et al., 2020) . Prior evidence indicates that if the circadian system is disrupted, the vulnerability of host cells toward influenza and herpes virus increases (Edgar et al., 2016; Mazzoccoli et al., 2020; Zhuang et al., 2021) . It may seem counterintuitive but the truth is occurrences of circadian disruption and disturbances in sleep had been more frequent during the pandemic despite social confinement (Morin et al., 2020) . For maintaining good physical health and ensuring optimum functionality of the immune system, adequate amount of good quality sleep is mandatory. Sleep promotes mental and emotional well-being which is key for driving away anxiety, depression and stress (Medic et al., 2017) . The COVID-19 pandemic has triggered anxiety, stress and fright among citizens of the United States and globally, which indirectly disrupts the circadian rhythms and increases the disease severity in COVID-19 patients . Chronotherapy is an emerging concept that is being increasingly discussed in sleep medicine. Through this, medication is administered to patients in sync with their circadian rhythms. Chronotherapy can potentially reduce the side effects of the administered drugs while optimizing the therapeutic impact (Selfridge et al., 2016; Cederroth et al., 2019; Ruben et al., 2019) . Additionally, it can also achieve the same efficacy at a lower dose (Matsuzawa et al., 2018) . COVID-19 patients might tremendously benefit from chronotherapy. For instance, a recent report indicated that statins significantly reduce COVID-19 severity by reducing cholesterol, which is used by the SARS-CoV-2 to infect the lung epithelial cells, making hyperlipidemia a risk factor for mortality among COVID-19 patients (Aung et al., 2020) . Strong evidence exists that cholesterol synthesis peaks during the nighttime hours between 12 and 6 am, and that administering the same dose of statin in the evening or nighttime markedly reduced cholesterol levels in patients when compared to the same dose given in the morning (Izquierdo-Palomares et al., 2016) . In this article, we will summarize the critical role of sleep and circadian rhythms in health and disease and suggest ways to reset the circadian timing system. Additionally, we discuss some major classes of drugs that can be repurposed for the treatment and management of COVID-19 in the light of chronotherapy and highlight some of the knowledge gaps in this domain for the scope of future research.@story_separate@• The dose of acyclovir needed to prevent HSV-2 infection during the active phase was four times more compared to the resting phase in mice. • Corticosteroids are anti-inflammatory drugs that suppress the activation of the immune system, thus prevents cytokine storm. • Meta-analysis of clinical trials showed that corticosteroids reduced the risk of mortality and the duration of mechanical ventilation in patients suffering from ARDS and COVID-19. • Elevated levels of pro-inflammatory cytokines throughout the nighttime. (Channappanavar and Perlman, 2017) (Mammen et al., 2020; Sterne et al., 2020 ) (De Silva et al., 1984 • Prevents the nighttime rise in pro-inflammatory cytokines, particularly IL-6, which may be beneficial for COVID-19, as reported in studies from rheumatoid arthritis patients. (De Silva et al., 1984; Buttgereit et al., 2008) Blood thinners (Anticoagulants or antiplatelet drugs) • Increased rate of blood clots among hospitalized COVID-19 patients. • Full dose anticoagulants when given to moderately ill hospitalized COVID-19 patients, the requirement for vital organ support such as ventilation and ICU was significantly reduced. • Prophylactic anticoagulation administration did not increase the risk of serious bleeding in COVID-19 patients. • Hypercoagulatory and hypofibrinolytic conditions are more frequent in the morning because of increased platelet activity and concentration of coagulation factors like Factor V, VII, and prothrombin fragment F1 + 2, and D-dimer. • Rivaroxaban and Aspirin both have been shown to exert better effects when taken in the evening compared to the morning. • The anti-arrhythmic drug, amiodarone prevents the fusion of the viral envelop with the endosomal membrane and accumulates in late endosomes/ lysosomes, and disrupts the viral endocytic pathway. • Amiodarone prevents entry of the Filovirus and methyldiethanolamine (metabolite) was able to inhibit Ebola virus entry. • Amiodarone increased the survival of mice infected with the Ebola virus. • Amiodarone also inhibits other viruses like the Arenavirus, the SARS-CoV-1, and Hepatitis C virus. • Arrhythmias are common among COVID-19 patients, and it is associated with higher morbidity and mortality. • Chlorpromazine exhibits antiviral activity against the Crimeancongo hemorrhagic fever virus, Adenovirus, Ebola virus, MERS-CoV, and SARS-CoV. • Chlorpromazine blocks the formation of clathrin-coated pits and thus prevents viral entry into the cells • Clomiphene blocks the Ebola virus entry by inhibiting the NPC1-dependent pathway, which has been hypothesized to increase cholesterol accumulation in the late endosomes and impair viral entry for SARS-CoV-2 • Prior studies suggest that cardiac arrhythmia peaks mostly between 6:00 am and 12:00 noon. Arrhythmogenesis appeared to be less frequent or suppressed during the nighttime. • A relatively lower dose of chlorpromazine administered at 1:30 was able to show the same sedative effect than when administered at 7:30 (on a 24-hour clock). (Nagayama et al., 1978) Janus-associated Kinase Inhibitor (Baricitinib) • JAK inhibitors can prevent the phosphorylation of proteins that are involved in the signal transduction cascade of the Jak-Stat pathway and thereby reduce cytokine-mediated inflammation and collateral damage in the vital organs. • Baricitinib significantly reduced the median number of days to recovery, from 18 to 10 days, in hospitalized COVID-19 patients requiring high-flow oxygen or non-invasive ventilation, when used together with remdesivir. • The need for ventilation support or death was reduced by > 50% (34.9% to 16.9%) using Baricitinib compared to the placebo/control group. • Baricitinib can successfully inhibit type-1 interferon response (exaggerated in COVID-19 patients) that increases ACE2 expression (in liver cells), the receptor that plays an essential role in the entry of SARS-CoV-2 in host cells to increase the viral load. • Baricitinib is a potent inhibitor of the Numb-associated kinase (NAK) family of proteins, particularly AAK1, that plays a pivotal role in clathrin-mediated endocytosis, which further prevents viral entry into the cells. • Studies have shown a peak in IL-6 levels during nighttime and early morning hours. • A better outcome was observed when Baricitinib was administered during the time (evening) when the cytokine production was at the highest. (Yaekura et al., 2020) Drugs that manage comorbidities and pleiotropic effects against SARS-CoV-2 Hyperlipidemia drug (Statins) • Anti-inflammatory effects can block the infectious potential of enveloped viruses (in vitro) and considerably reduce the mortality risk among COVID-19 patients. • Independently associated with lower ICU admission among COVID-19 patients. • Reduce hyperlipidemia and decrease cytokine levels with its pleiotropic effects under different non-infectious conditions. • Cholesterol synthesis peaks between 12:00 am and 6:00 am. • Administration of Statins in the evening is more effective than when taken in the morning. , 1991; Lund et al., 2002; Wallace et al., 2003; Ozaydin et al., 2006; Tharavanij et al., 2010) . • Acute lung damage can be reduced by renin-angiotensinaldosterone system inhibitors. • Case fatalities are much higher in COVID-19 patients due to pulmonary hypertension. Hypertensive drugs are associated with decreased mortality. • Blood pressure (BP) is higher during early mornings. Increased nighttime ambulatory BP is related to fatal and non-fatal cardiovascular events. ( Baral et al., 2020) (Surveillances, 2020) (Elliott, 1999)  Sleep is a normal physiological and behavioral state of the body, described as reversible unconsciousness with minimal physical movement and non-responsiveness to external stimuli. Evolutionarily, all animal species, including vertebrates (e.g., mammals, fish, and birds) and invertebrates (e.g., insects, roundworms, and jellyfish), exhibit some characteristics of sleep or sleep-like states (Keene and Duboue, 2018) . Sleep and circadian rhythms are complex and tightly interconnected mechanisms. In humans, the central pacemaker, or ""clock, "" resides in the suprachiasmatic nucleus (SCN) of the hypothalamus in the brain. The SCN controls the periodic release of melatonin, a hormone that induces sleep in humans. It primarily receives stimulus in the form of light via the optic nerve, which is responsible for relaying the stimulus from the eye to the brain. Melatonin secretion is inhibited by light while stimulated by darkness. At night when light is scarce, the SCN stimulates the release of melatonin from the pineal gland and thus inducing sleep. On the other hand, melatonin feeds back to the SCN to decrease the neuronal firing of the SCN and the cycle resets (Doghramji, 2007) . Additionally, light is also able to entrain the clock at the level of the SCN, which subsequently entrains the cell-autonomous clock that resides in the peripheral cells and tissues thus impart the circadian rhythms of gene expression in them (Haspel et al., 2020) . A comprehensive review and perspective on the basic concepts and molecular mechanism that involve the circadian clock in lung pathophysiology of chronic airway disease has been previously reviewed (Sundar et al., 2015c,b) . Disruption of circadian rhythms can occur due to changes in normal daily activities such as shift work, jet lag, unusual photoperiods (e.g., polar regions), or possibly due to sleep disorders (Blask, 2009; Mattis and Sehgal, 2016) . Additionally, aging plays a critical role in causing sleep-related disorders among the elderly population. It has been reported that the ability of the circadian clock to adapt to changes in the light-dark schedule, either due to jet lag, shift work, or other changes to the light-dark exposure regime reduces with increasing age (Monk et al., 2000) . Evidence from the literature suggests that aged individuals demonstrate fragmented sleep patterns compared to younger individuals and that the duration of slow-wave sleep (SWS: deep sleep) decreases with increasing age (Mattis and Sehgal, 2016) . Age-related neurodegenerative diseases such as Alzheimer's, Parkinson's, and Huntington's show signs of circadian disruption, which is indicative of older individuals having an increased susceptibility to circadian dysfunctionassociated clinical ailments including cancer risk (Blask, 2009; Abbott and Videnovic, 2016; Mattis and Sehgal, 2016) . Generally, sleep helps maintain normal physiological processes such as brain development, plasticity, memory, learning and immunity (Bollinger et al., 2010; Abel et al., 2013 ; Figure 1 ). Circadian disruptions have been documented to have detrimental effects on an individual. This is more prominent in shift workers who have to continuously change biological rhythms to work at different shifts, thus and have a great chance of developing metabolic diseases, viral infections, cardiovascular diseases, diabetes, obesity, cancer or fibrosis (Almeida and Malheiro, 2016; Kecklund and Axelsson, 2016) . Recent evidence also shows that shift workers have a greater likelihood of testing COVID-19 positive in hospital settings compared to that of non-shift workers, and this association remained increased regardless of the time of the shift (Maidstone et al., 2020) . Although exact reasons are not known, circadian misalignment has shed some light on why this might be the case as described below. Past studies have clearly demonstrated the relationship between sleep, immunity and aging (Dickstein and Moldofsky, 1999; Besedovsky et al., 2019) . During the early resting period, secretion of growth hormone (GH), melatonin, prolactin and leptin are increased, resulting in immune cell activation, proliferation, differentiation and production of pro-inflammatory cytokines (e.g., interleukins [IL-1 and IL-2], tumor necrosis factor-alpha [TNFα]) (Dickstein and Moldofsky, 1999; Esquifino et al., 2004; Hattori, 2009; Radogna et al., 2010) . As mentioned previously, as individuals age, they show a higher incidence of disrupted sleep patterns, which leads to a decrease in amplitude of GH, melatonin and other hormone secretion (Kern et al., 1996; Pandi-Perumal et al., 2006) . Prior report has demonstrated the effect of sleep deprivation on the immune response to influenza vaccination. Antibody titers measured 10 days postimmunization in sleep-deprived subjects were lower than half of those measured in subjects with normal sleep patterns (Spiegel et al., 2002) . Another report showed the role of Gα s -coupled receptor agonists (isoproterenol, epinephrine, norepinephrine, prostaglandin E2, prostaglandin D2, and adenosine) in blocking T cell receptor (TCR)-induced β 2 -integrin activation in human Cytomegalovirus (CMV)-and Epstein-Barr virus (EBV)-specific T cells in a dose-dependent manner (Dimitrov et al., 2019) . Interestingly, sleep suppresses Gα s -coupled receptor signaling, but increased β 2 -integrin activation in CMV-and EBV-specific CD8 + T cell subsets (specifically in the early and intermediate differentiation states, but not late) that occur early in the morning (between 2 and 6 am; dark phase) (Dimitrov et al., 2019) . These findings highlight the immune-enhancing effects of sleep, wherein it regulates normal circadian rhythms to provide an optimal immune response (e.g., via formation of immunological synapses by T cell adhesion to antigen-presenting cells or target cells) and resistance to infectious challenge. Ongoing and future clinical trials for COVID-19 vaccination and therapy in humans should consider these caveats, quality of sleep index and combining the use of specific Gα s -coupled receptors agonist such as catecholamines, PGs and adenosine that may constitute immune checkpoint inhibitors, when treating SARS-CoV-2 patients with comorbid chronic underlying pathologies. Additionally, the influx of circulating immune cells (neutrophils, natural killer [NK] cells, monocytes and B cells) occurs during prolonged wakefulness, which is then reduced by recovery sleep, eluding to the crucial role of sleep in regulating immune cell trafficking (Ingram et al., 2015; Besedovsky et al., 2019) . All the immune cell types including the lymphoid organs retain the functional clock (Labrecque and Cermakian, 2015; Scheiermann et al., 2018) . Documented evidence suggests that both humans and mice display daily oscillations in levels of circulating leukocytes, and that the circadian rhythms control leukocyte trafficking-mediated immune function (Scheiermann et al., 2012; Zhao et al., 2017; He et al., 2018) . Aging affects the normal physiology and hormonal responses, including circadian rhythms in SCN, the central player that coordinates sleep and sleep-associated age-related disorders. Hence, disruption of circadian rhythms can impose a weaker immune response because of decreased sleep duration or age-related reduction in the period of sleep. A self-reported survey by the National Health and Nutrition Examination Surveys (NHANES; 2005-2012) that enrolled 22,726 participants showed a direct association between sleep duration (short duration of sleep) and an increased incidence of respiratory viral infections (Prather and Leung, 2016) . Lack of sufficient human and animal studies that directly relate to the role of sleep, circadian rhythms and immunity in the context of aging makes it difficult to speculate upon the role of age-related sleep disruption in the increased prevalence of respiratory bacterial and viral infections in older populations. However, epidemiological studies including human and animal experimentation have directly or indirectly indicated that sleep deprivation alters specific immune processes (Ingram et al., 2015) . A few examples of this include elevated levels of systemic inflammatory mediators, increased susceptibility to viral infections, and altered adaptive immune response to influenza vaccination. Additionally, there is evidence of chronic sleep deprivation in animals causing death due to splenic atrophy and polymicrobial bacteremia possibly due to immune dysregulation (Rechtschaffen et al., 1983; Everson, 1993;  FIGURE 1 | Homeostasis between sleep, circadian rhythms and immunity: a bidirectional relationship. Sleep and circadian rhythms are recognized as important intertwined regulators of the immune system. Disruption of either of the processes can lead to altered immune response, functional immunocompromise and inflammation. The lack of sleep, shorter sleep duration, or sleep disturbances greatly increases our susceptibility to infections as a result of an altered immune response. The reduced mitogenic proliferation of lymphocytes decreased HLA-DR expression and variations among the T lymphocytes, have been observed in sleep-deprived individuals. Conversely, the immune response has also been shown to alter sleep patterns. Immune response to a pathogen is accompanied by the release of cytokines and interleukins, which depending on the magnitude, can either facilitate or cause sleep disruption. Sleep enhancement is usually assumed to be a defense mechanism for the host. Similarly, sleep and circadian rhythms are coordinated as well. Light plays the most important role in providing an environmental cue to the circadian rhythms. When light hits the retinal ganglion cells, the SCN switches off the melatonin secretion and, in turn, diminishes sleep. At nighttime, when lights are off, the SCN trigger the release of melatonin, which facilitates sleep. Melatonin levels decline as people age, the old generation experience a shorter duration of slow-wave sleep (SWS) that interferes with the function of circadian rhythms as shown in the schematic. Chronic exposure to light at nighttime, consuming caffeine, alcohol disrupt the sleep schedule and thus affect the circadian clock. Given, the wide range of physiological functions that are under circadian control, care must be taken to maintain the normal homeostasis. This schematic was prepared from SMART (Servier Medical Art), licensed under a Creative Common Attribution 3.0 Generic License. http://smart.servier.com/. Toapanta and Ross, 2009 ). Preclinical studies from our lab and others have shown how deletion of the circadian clock gene Bmal1 (Bmal1 KO) alters viral burden, the chance of survival, and the degree of lung inflammation and remodeling following influenza A virus (IAV) infection, albeit with altered circadian phase and amplitude (Sundar et al., 2015a; Sengupta et al., 2019) . Constitutively low expression of Bmal1 results in enhanced susceptibility to Herpes simplex virus 1 and IAV infection. Additionally, genetic disruption of clock molecule Bmal1 augments viral replication in mice and cell culture models thereby demonstrating the dynamics of circadian clockdependent host-virus interaction (Edgar et al., 2016) . In a recent study, observed time of day-dependent alterations in IAVinduced lung inflammation and fatality in mice demonstrate the importance of circadian clock control in modulating immune response (Sengupta et al., 2019) . WT mice infected with IAV showed temporal gating [mice infected at zeitgeber time 11 (ZT11) showed greater mortality, weight loss, clinical severity scores, and respiratory distress compared to mice infected at ZT23] while inducible Bmal1 KO mice infected with IAV showed loss of this temporal gating response (Sengupta et al., 2019) . In IAV-infected mice, a higher survival rate was associated with a time-dependent increase in NK and NKT cells and a reduction of inflammatory monocytes in the lungs, indicative of the underlying circadian clock-mediated gating response (Sengupta et al., 2019) . In a recent report, circadian rhythms of druggable host factors that interact with SARS-CoV-2 proteins were analyzed using the Circadian Expression Profiles Database (CircaDB 1 ). The majority of these human and mouse orthologous host proteins that are targeted by FDA-approved drugs showed a greater degree of protein-protein interaction with SARS-CoV-2 proteins, exhibit 24-h oscillation under constant conditions (Ray and Reddy, 2020) . Previous report shows that glucocorticoid receptor signaling affects the circadian clock bidirectionally (Caratti et al., 2018) . Studies demonstrated that therapeutic administration of glucocorticoids has been linked with increased incidence of reactivation of Hepatitis B virus (HBV) and caused poor clinical outcomes during influenza-associated ARDS (Hatano et al., 2019; Tsai et al., 2020) . Melatonin on the other hand reduces inflammation by blocking NLRP3 inflammasome thereby indirectly regulating circadian rhythms and viral infection (Borrmann et al., 2021) . The putative role of circadian clock role in the pathophysiology of SARS-CoV-2 infection based on prior evidence from available literature was summarized recently (Meira et al., 2020) . Circadian changes in the expression of angiotensin-converting enzyme 2 (ACE2) may exist in the lungs and other peripheral organs which is not yet reported. We know that angiotensin II affects the rhythmic expression of Per2 in the SCN and heart of rats in vivo (Herichova et al., 2013) and circadian clock genes (Per2, Dbp, and Bmal1) in vascular smooth muscle cells in vitro (Nonaka et al., 2001) . Hence, we speculate that altered ACE2 expression in the lung may have circadian changes that indirectly affect the renin-angiotensin system during SARS-CoV-2 infection-induced ARDS. Since circadian clock genes directly regulate functional and physiological outcomes in the lungs and cardiovascular system, it is more likely that clockdependent immune dysregulation may be the cause for adverse pathobiology observed in SARS-CoV-2 infection which needs to be further explored. The importance of understanding the nexus of circadian rhythms, sleep and immunity remains one of the highest priorities for translational biomedical research, which was highlighted recently in the National Institute of Health (NIH)-sponsored workshop ""Sleep insufficiency, circadian misalignment and the immune response"" summary report (Haspel et al., 2020) . Overall, there is ample evidence for the role of the circadian clock and sleep in maintaining immune homeostasis (i.e., innate and adaptive immune response) (Pick et al., 2019; Haspel et al., 2020) . Prior observations elegantly support the occurrence of time-of-day-dependent changes in leukocyte trafficking and mobilization, cytokine-mediated chemotaxis and T cell differentiation (Pick et al., 2019; Haspel et al., 2020) . Understanding the role of circadian rhythms-sleepimmunity crosstalk during SARS-CoV-2-induced respiratory infection and multi-organ failure may enable us to identify novel drug targets for the treatment and management of coronavirus-induced infectious disease (Figure 1 ). Light is the most powerful environmental cue for the human circadian timing system. Besides the classical rod and cones photoreceptors, there is another subpopulation of retinal ganglion cells known as the intrinsically photosensitive retinal ganglion cells (ipRGCs) that express the photopigment melanopsin. These ipRGCs in the eye detect the light signal and relay the message to the SCN, the master regulator (hypothalamus) of the brain (Shuboni and Yan, 2010; Fernandez et al., 2018) . The sensitivity of the photopigments on these retinal ganglion cells are more for shorter wavelengths of light like blue and green, and less for red-shifted lighting. When diurnal animals are exposed to lower intensities of blue light during the nighttime, they experience trouble falling asleep due to inhibition of sleep-inducing neurons, and subsequently, there is a significant reduction of the natural sleep hormone, melatonin. Light also stimulates the activation of the sympathetic nervous system making us more alert and conscious (Ramsey et al., 2013) . Thus, sleep quality deteriorates and alertness level on a subsequent day reduces. From an evolutionary point of view, humans are diurnal animals. Chronic exposure to light at night, when humans are supposed to be asleep, has been shown to have detrimental effects on the immune system, particularly on the inflammatory response of the immune system during infection (Bedrosian et al., 2011) . Many studies have found a profound association between chronic light exposure at night and the increased risk for developing certain medical conditions such as breast cancer, anhedonia (loss of interest in activities that once used to be pleasurable), depression, and elevated body mass index. The discussion of how chronic light exposure at night increases the chances of acquiring these medical conditions is beyond the scope of this review and has been summarized previously . In circadian biology, the synchronizing effect of light on the circadian timing system is characterized graphically on a phase-response curve (PRC). Light-induced phase shifts can be visually observed in the PRCs. In general, light stimuli late in the day or early at night generates a phase delay on the PRC while light stimuli late at night or early in the day generate a phase advance (Czeisler et al., 2005) . These findings are important as it highlights how light acts as a powerful modulator for the circadian timing system and how phototherapy can be used to treat sleep phase disorders in humans. While sometimes it is hard to have a strict sleep schedule, studies have shown that maintaining a strict sleep schedule can help strengthen the circadian rhythms and facilitate better sleep quality. Frequent changes in the sleep schedule can affect the quality of sleep, decrease cognitive performance, and may increase the risk of cardiovascular events (James et al., 2017) . Additionally, studies have found that irregular bedtime often leads to shorter sleep durations which can negatively impact cognition, alertness, memory and mood (Kang and Chen, 2009 ). Partial sleep deprivation leads to the impairment of lymphocyte proliferation, decreased HLA-DR expression, upregulation of CD14 + , and variations among key T lymphocytes like CD4 + and CD8 + , which in turn increases our susceptibility to diseases (Wilder-Smith et al., 2013) . Hence, chronic exposure to light at nighttime must be avoided, which will in turn help reinforce a proper sleep schedule. Skipping late-day naps and practicing pre-bedtime relaxation exercises may also help ensure proper bedtime. While it is true that short naps, ideally 10 to 15 min, can feel refreshing especially when a person is sleep-deprived, longer and late-day naps can actually impact the quality of nighttime sleep (Monk et al., 2001) . The sleep cycle of a healthy adult is subdivided into four stages. The first two stages consisting of light, nonrapid eye movement (NREM) sleep, during which the heart and breathing rate slows down and brain activity gradually declines. The third stage comprises deep NREM sleep where brain activity, breathing and heart rates would be the lowest. The final stage is the rapid eye movement (REM) sleep where there is a surge in brain activity with a rapid movement of the eye. Typically, it is easier to wake up during the first two stages of the NREM sleep but it is difficult during the third stage. Arousal from sleep during the third stage is accompanied by grogginess and feeling of confusion. Naps later during the day usually comprise of deep sleep when there is a natural decline of our energy, and this, in turn, affects our ability to fall asleep during the night (Brooks and Lack, 2006) . Surveys conducted in the United States revealed that approximately 30 to 50% of Americans have some form of insomnia (Ancoli-Israel and Roth, 1999) . Stress and anxiety may be the direct or indirect consequence of this medical condition. During periods of high stress, our sympathetic nervous system is most active, increasing our heart rate and stress hormone levels. This, in turn, makes us feel more awake and alert (Valentino and Foote, 1988; Han et al., 2012) . Thus, trying to fall asleep during this stress response can be challenging. Fortunately, research has shown that this stress response can be mitigated by calming the mind and relaxing the body, which can facilitate sleep naturally. This relaxation response can be activated consciously. Relaxation exercise like breathing exercises, where we can consciously control the speed and depth of our breathing; meditation, where we can consciously focus on scanning parts of our own body while letting go of any stress-inducing thoughts, light yoga, and progressive muscle relaxation, where specific muscles groups of the body can be consciously contracted and relaxed periodically (Norelli and Krepps, 2018) . These simple relaxation techniques have been shown to lower the heart rate and blood pressure, create a sense of calm and reduce stress, and thus help us fall asleep faster. Circadian rhythms can also be disrupted by an imbalance in metabolic functions, and prior studies have indicated that this can happen due to food intake at an inappropriate period or irregular eating habits (Wehrens et al., 2017 ). The circadian system impacts metabolic homeostasis by regulating daily physiological processes like gastrointestinal functions, absorption of nutrients, gastric acid secretion, colonic motility, appetite, and secretion of pancreatic insulin (Hoogerwerf, 2006) . SCN's neural and humoral signals coordinate the functions for driving and modulating the feedback loop of molecules in metabolic tissues such as the pancreas and liver. The circadian function can be altered by the food's nutritional constitution, intake volume, frequency and timing. In mice, for instance, restriction to accessing food during unsuitable periods in the day like normal sleep time results in shifting the molecular clock's phase in fat cells, adrenals, liver, and other peripheral tissues while unaltering the SCN rhythm (Damiola et al., 2000) . This disrupts the coordination between the SCN and the peripheral tissues. Additionally, high-fat diets have also been shown to alter the mammalian circadian clock leading to obesity. In mice, a high-fat diet alters the circadian pattern of feeding and leads to excess calory intake at incorrect circadian time (Bass, 2012) . Good health can be maintained by adhering to a fixed feeding schedule that will help preserve and strengthen clock gene expression. Additionally, studies show that evening intake of alcohol, caffeine and nicotine taken as early as 4 h before bedtime is associated with fragmented sleep at night, and therefore should be avoided (Spadola et al., 2019) . Short sleep or chronic sleep deprivation leads to disruptions in energy balance and are now considered independent risk factors for metabolic abnormalities like insulin resistance and hyperglycemia (Arble et al., 2015) . These studies further highlight the possibility that dietary modifications in circadian rhythms may have therapeutic benefits in the long term. The ability of the circadian timing system to receive feedback related to alteration of its functioning from exercise or stimulated activity is also well documented. Although light is considered the most important zeitgeber in humans, recent evidence has also highlighted the role of exercise as a chronobiological tool to correct circadian misalignment (Youngstedt et al., 2019) . In rodents, stimulated activity leads to a change in SCN properties, neuron firing, and the expression of clock genes (Reebs and Mrosovsky, 1989) . The human circadian timing system is responsive to exercise as well. This is evident through the changes in the PRC in humans which describes the relationship between different zeitgeber like light or exercise (the stimulus) and the shift of the circadian rhythms (the response). Recently, the timing of the morning phase advance and the night phase delay regions of the exercise PRC was shown to be comparable to that of bright light PRC (Youngstedt et al., 2019) , further emphasizing the pleiotropic role of exercise in correcting circadian misalignment. Exercising causes circadian system timing delay as evidenced by shifts in the timing of melatonin rhythms (Buxton et al., 2003) . These responses are linked to exercise intensity and duration which can hasten the circadian rhythms re-entrainment and schedule readjusting while working in shift (Buxton et al., 1997) . In geriatric patients, midday long-term fitness training positively consolidates the sleep-wake cycle (Van Someren et al., 1997) . Older people with insomnia experience improvement in sleep quality, self-reported, when they exercised during the early evening or afternoon (Reid et al., 2010) . Thermoregulation in our body is another key mechanism that determines the quality of sleep (Gilbert et al., 2004) . The principal way the body regulates core body temperature is through a process known as vasodilation and vasoconstriction. Vasodilation increases the blood flow to the extremities which facilitates heat loss when the core temperature is high (Kräuchi et al., 2000) . Recent reviews have revealed that heat exposures during sleep can increase wakefulness while drastically reducing REM and slowwave sleep (Okamoto-Mizuno and Mizuno, 2012) . Additionally, humid heat is often associated with an increase in thermal load during the different stages of sleep, which in turn interferes with thermoregulation and affects the quality of sleep (Tsuzuki et al., 2004) . While on the other hand, cold exposures don't seem to affect the sleep stages and therefore don't affect the quality of sleep even though the cardiac autonomic response may be affected (Okamoto-Mizuno and Mizuno, 2012) . Thus, there seems to be an ambient temperature that facilitates good quality sleep. Studies have concluded the ideal bedroom temperature to be approximately 13 • C to 23 • C when no difference in sleep stages was observed (Muzet et al., 1984) . Thus, we can see that the environment where we sleep matters. Noisy environments should also be avoided since it negatively impacts sleep quality (Simons et al., 2018) . Melatonin is a natural hormone that is released by the pineal gland that induces sleep in humans. During the nighttime, there is a surge in the melatonin levels as a result of the inputs received from the SCN. Polymorphisms in melatonin receptors are associated with an increased risk of diabetes, cardiovascular diseases and depression (Lyssenko et al., 2009; Gałecki et al., 2010; Samimi-Fard et al., 2011) . Melatonin supplements are readily available and can be used to realign the circadian rhythms with the external environment. Melatonin release from the pineal gland gradually declines with age and is related to lowered sleep efficacy. In older individuals, melatonin supplements have been shown to improve sleep, alertness in the morning and increased cognitive performance (Lemoine and Zisapel, 2012) . Melatonin's role during this pandemic is further highlighted in the repurposing of drugs section described below. Circadian rhythms play an essential role in patterning and regulating many physiological functions and gene expression networks within our body. Thus, it is not surprising that the circadian rhythms are sensitive and can be influenced easily by many factors. Studies reveal that 40-70% of the elderly population in the United States experience some form of sleep disturbances (Van Someren, 2000) . This is alarming, given the wide implication of a dysregulated circadian rhythms to overall health. Due to the COVID-19 pandemic, it has now become more essential than any time ever that we re-establish alignments to help facilitate proper functioning and robust circadian rhythms. Resetting the circadian rhythms can improve the quality of life, and most importantly help prevent a more serious illness (Figure 2 ). Despite the advancement in research and collaborative effort worldwide, a viable treatment specific against the SARS-CoV-2 just seems to be out of reach amidst this pandemic. To date, the treatment for patients with COVID-19 mainly incorporates symptoms management only. Researchers across the world are trying to develop new therapeutic drugs to combat the COVID-19. However, designing new drugs is an expensive and timeconsuming process. It might take several months to years more to test the efficacy of these drugs, starting in vitro, followed by in vivo animal models (preclinical testing), and finally testing them in human subjects through controlled clinical trials. Thus, repurposing old drugs for the treatment and management of COVID-19 has become an attractive option. Currently, drugs that are being repurposed fall under one of the two categories: drugs that are able to inhibit/block various stages of the SARS-CoV-2 life cycle in humans, like remdesivir, and those that can counteract the insidious effects caused by the virus. Additionally, some drugs do not fall under any of those two categories but can effectively manage other comorbidities like hyperlipidemia, cardiovascular diseases, pulmonary hypertension, depression, and anxiety in hospitalized COVID-19 patients that are known to increase mortality rates Richardson et al., 2020; Surveillances, 2020) . Some of these classes of drugs may also impart its pleiotropic effect to reduce the severity of symptoms associated with COVID-19. Chronotherapy is an emerging focus area that utilizes the approach of delivering drugs at specific times of the day to either maximize treatment efficacy or minimize negative effects (Zaki et al., 2019) . Recent report has demonstrated that circadian clock targets Bmal1 and Rev-erbα both influence the life cycles and replication of Hepatitis C virus and other related Flavivirus (e.g., Dengue and Zika virus) by modulating viral receptors in a human hepatoma cell line (Huh-7) (Zhuang et al., 2019) . Additionally, researchers showed that genetic ablation of Bmal1 and overexpression or over activation of Rev-erbα using synthetic ligands such as GSK2667 and SR9009 blocks HCV viral replication through perturbation of lipid signaling pathways (Zhuang et al., 2019) . Understanding the novel role of circadian clock components such as Bmal1 and Rev-erbα, and repurposing novel chronotherapeutic drugs (e.g., Rev-erbα agonists) for the treatment of SARS-CoV-2 and other respiratory viral infections will help reduce the exacerbation events that occur in patients with chronic lung diseases and thereby reduce the number of deaths caused by epidemic/pandemic respiratory viral infections worldwide (Figure 3) . Previously, a circadian gene expression atlas in mammals revealed the importance of circadian biology in medicine. This study showed that 56 of the top 100 best-selling drugs, directly or indirectly, target a product of a circadian gene (Zhang et al., 2014) . Approximately half of these drugs show a very short half-life (<6 h), suggesting the importance of timedadministration and its impact on mode of action (Zhang et al., 2014) . Here, we will examine the chronotherapeutic advantages of some major classes of drugs that can be repurposed for the treatment and management of COVID-19 and highlight some of the knowledge gaps in this domain for the scope of future research. Statins are used widely for the prevention of cardiovascular events by reducing in vivo cholesterol synthesis (Ziaeian and Fonarow, 2017) . These classes of drugs can potentially ward off uncontrolled systemic inflammatory response triggered by SARS-CoV-2 in COVID-19 patients. The anti-inflammatory effects of statins can block the infectious potential of enveloped viruses, as in vitro studies have shown, and therefore statins are now used by clinicians as a part of their treatment protocol (Schönbeck and Libby, 2004; Massachusetts General Hospital, 2020) . If lipophilic statins reach the site where the virus accumulates and unleashes damage, it can considerably lessen the mortality risk among COVID-19 patients (Rossi et al., 2020) . Additionally, statins have been independently associated with lower ICU admission among hospitalized COVID-19 patients (Tan et al., 2020 ; Figure 3) . Statins reduce hyperlipidemia and bring about a reduction in cytokines with its pleiotropic effects under different noninfective conditions (Wassmann et al., 2003; Fang et al., 2005) . In a randomized controlled trial, atorvastatin was evaluated for treating influenza virus infection and the results showed remarkable potential in lowering inflammatory cytokines level [NCT02056340] , which often causes damage to vital organs during hyperinflammation. Cholesterol biosynthesis follows a strict circadian pattern, which typically peaks at night between 12:00 and 6:00 am, during which the 3-hydroxy-3-methyl-glutaryl-coenzyme A reductase (HMG-CoA reductase) the rate-limiting enzyme of cholesterol FIGURE 2 | Ways to optimize your circadian rhythms. Light is one of the most important regulators of the circadian rhythms. Chronic exposure to light, particularly to shorter wavelengths (like blue and green), at nighttime inhibits sleep-inducing neurons and decrease melatonin secretion. Naps later during the day usually comprise deep sleep (slow-wave), when there is a natural decline of our energy, and this interferes with nighttime sleep. Hot and humid temperatures increase wakefulness, reduced REM and slow-wave sleep. However, cooler temperatures between 13 to 23 • C do not interfere with any of the sleep stages. Noisy environments can also interfere with deep sleep thus a quiet environment is preferred. Food intake at an inappropriate time of the day disrupts the coordinate function of circadian rhythms between the central and the peripheral organs resulting in metabolic imbalances. Alcohol, caffeine, or nicotine should be avoided at least 4 h before bedtime as it is associated with a fragmented sleep pattern. Relaxation exercises before bedtime can help lower heart rate, blood pressure, and reduce sympathetic nervous system activation, which facilitates sleep. Stimulating activities or exercise leads to alteration of the circadian functioning by changing the expression of clock genes and creates shifts in the timing of melatonin rhythms in humans. Melatonin supplements can also help fall asleep, especially during insomnia or jet lag. Adhering to a strict sleep schedule is also important that can help strengthen the circadian rhythms. This schematic is prepared using http://biorender.com. biosynthesis peaks (Izquierdo-Palomares et al., 2016) . When hypolipemic therapy is adjusted and aligned with biological rhythms, the efficacy has been determined to be better in several studies. Particularly, the administration of statins in the evening is more effective than when it is taken in the morning (Saito et al., 1991; Lund et al., 2002; Wallace et al., 2003; Ozaydin et al., 2006; Tharavanij et al., 2010) . This makes sense when we realize that cholesterol synthesis peaks during midnight. Additionally, when using statins, the half-life of the drug should also be carefully considered. For instance, simvastatin has a very short half-life of 2-3 h, compared to Atorvastatin or rosuvastatin, which has a half-life of 14 and 30 h, respectively. Therefore, the time of administration becomes even more important for simvastatin compared to others. However, studies have also reported better outcomes in atorvastatin with evening administration (Ozaydin et al., 2006) . Thus, when one dose of statins is administered daily in the evening, its inhibitory activity seems to be expressed maximally since cholesterol biosynthesis happens optimally at that nighttime. Clinicians should therefore take the timing of statin administration into account, which can subsequently lower drug toxicity in COVID-19 patients. Future research will focus to determine the efficacy of timed-administration of statins in COVID-19 patients. In the early phase of the pandemic, clinicians observed an increased rate of blood clots among the hospitalized COVID-19 patients (Malas et al., 2020) . This helped explain the increased FIGURE 3 | Schematic showing key classes of drugs that are currently repurposed and suggested drugs that could be repurposed for SARS-CoV-2. The drugs proposed for repurposing have been classified under 3 categories: Drugs that inhibit SARS-CoV-2 lifecycle, counteract the side effects post-infection or can efficiently manage comorbidities and other risk factors. It is important to note that the representation above is purely hypothetical, and based solely on the limited literature that was available on SARS-CoV-2 or the beneficial effects that were observed with other viruses from both in vitro or in vivo studies. Antiviral drugs like remdesivir are effective against the SARS-CoV-2. Statins have also been shown to inhibit the SARS-CoV-2 life cycle. Cationic amphiphilic drugs (CADs), like amiodarone, chlorpromazine, and clomiphene have been shown to inhibit several virus families including the SARS-CoV-1 and MERS-CoV. Therefore, CAD's potential against the SARS-CoV-2 should be further evaluated. Janus-associated kinase inhibitors, baricitinib, have been shown to decrease mortality among COVID-19 patients when used together with remdesivir. However, the beneficial effect of common corticosteroids like prednisone or methylprednisolone has not been determined but several clinical trials are evaluating the same. A higher incidence of blood clots is observed among COVID-19 patients. The massive inflammatory response during SARS-CoV-2 infection can trigger blood clots which can eventually cause strokes or heart attacks. Thus, blood thinners, which have been associated with decreased mortality among COVID-19 patients can be beneficial. We have additionally proposed a novel chronotherapeutic agent, SR9009, which has immense potential to inhibit several virus families, reduce lung inflammation and injury by interfering with the production of inflammatory cytokines. Additionally, REV-ERB agonists (e.g., SR9009 and GSK2667) can also be shown to boost endurance, reduce cholesterol, body weight and prevent heart disease. This schematic was prepared using SMART (Servier Medical Art), licensed under a Creative Common Attribution 3.0 Generic License. http://smart.servier.com/ and http://biorender.com. incidence of critical complications such as lung failure, stroke, and myocardial infractions among the COVID-19 patients (Morrone and Morrone, 2018) . Although the exact cause of this increased incidence of blood clots among COVID-19 patients remains unclear, evidence suggests that severe inflammatory response, similar to that of COVID-19 patients with a hyperactive immune system, can trigger coagulation, decrease the activity of natural anticoagulants and also impair the fibrinolytic system (Esmon, 2005) . However, whether it was safe to administer anticoagulants without any consequences were not determined earlier. In a recent large clinical study, comprising of patients from more than 300 hospitals worldwide, it was established that full dose anticoagulants (blood thinner) when given to moderately ill hospitalized COVID-19 patients, the requirement for vital organ support such as ventilation was significantly reduced. In addition to being safe, a full dose of blood thinners gave superior results in preventing ICU admits when compared to the dose normally given to patients to prevent blood clots (National Institutes of Health, 2021). Additionally, prophylactic anticoagulation administration did not increase the risk of serious bleeding in COVID-19 patients (Rentsch et al., 2021) . Translational researchers are still trying to further optimize the administration of these drugs to improve the quality of care provided to patients suffering from COVID-19 (Figure 3) . One such optimization technique can consider the timing of the drug administration and the half-life of these drugs. Diurnal variation in coagulation and fibrinolysis have been observed in humans in the past. Hypercoagulatory and hypofibrinolytic conditions are more frequent in the morning than any other time during the day because of increased platelet activity. Between 8 to 10 am, the concentration of coagulation factors such as Factor V, VII, prothrombin fragment F 1 +2 , and D-dimer peaks (Kapiotis et al., 1997) . Additionally, the activity of the plasminogen activator, which facilitates fibrinolysis, is highest only in the afternoon (Angleton et al., 1989) . This partly explains the increased incidence of thromboembolic events during the early hours of the day. Rivaroxaban (Xarelto in the United States) is a commonly prescribed blood thinner that is an inhibitor of activated coagulation factor X and has a very short half-life of 5-9 h and does not accumulate even with multiple doses (Kubitza et al., 2005; Mueck et al., 2014) . It is administered once a day and used for multiple purposes like lowering the risk of strokes, deep vein thrombosis, pulmonary embolism, and other conditions. A study observed that rivaroxaban concentration was significantly higher 12 h after evening intake than it was when taken in the morning (53.3 ng/mL vs. 23.3 mg/mL). Additionally, the evening regimen was able to significantly reduce the prothrombin fragment F 1 +2 concentration better in the morning when compared to the morning regimen (85 ± 25 nmol/L vs. 106 ± 34 nmol/L), and also evening intake had a longer-lasting effect (Brunner-Ziegler et al., 2016) . Similar effects have been observed with another antiplatelet drug, Aspirin. Van Dieman et al. examined the effect of morning vs. evening administration of aspirin on the platelet activity in patients with stable cardiovascular disease and found that evening regimen resulted in higher platelet inhibition and a significant reduction in reticulated platelets compared to the morning regimen (van Diemen et al., 2020) . Another study showed that bedtime administration of aspirin significantly reduced platelet reactivity (compared to morning administration) during the morning high-risk hours but did not change the blood pressure in patients using aspirin for cardiovascular diseases (Bonten et al., 2015) . While a plethora of literature is not available for all types of blood thinner, there is sufficient evidence that proves circadian variation in coagulation and fibrinolysis exists, and certain anticoagulatory or antiplatelet drugs work best only when administered at a specific time of the day. Therefore, timedadministration of these drugs can tremendously prove to be a cost-effective way to improve patient care during these times with the added benefit of reducing drug toxicity while maximizing the benefit in critically ill COVID-19 patients. Future studies should therefore aim to further evaluate the effect of timedadministration of full doses of blood thinners, once a day, on COVID-19 patients. Earlier, links were seen between hypertension and COVID-19 fatalities . This was ascribed to the relationship between SARS-CoV-2 and angiotensin-converting enzyme (ACE2). ACE2 was purportedly serving as an entry port for the SARS-CoV-2 ). Yet some evidence indicated that COVID-19 induced acute lung damage can be reduced by renin-angiotensin-aldosterone system inhibitors and can be beneficial in hypertensive hospitalized COVID-19 patients (Baral et al., 2020) . At the European Society of Cardiology Congress 2020, the BRACE CORONA trial's data showed zero difference in results among patients who continued consuming ARBs (angiotensin receptor blockers) or ACE inhibitors and those who didn't consume them for a month after being diagnosed with COVID-19 (European Society of Cardiology, 2020). The first randomized trial suggests that heart patients who take ACE and ARBs can safely consume them even after contracting COVID-19 (Figure 3) . Blood pressure (BP) control laxity in United States adults is increasing hypertension, an important modifiable risk factor for strokes and heart attacks (Fuchs and Whelton, 2020) . Patients with severe heart problems and hypertension have enhanced risk if they contract SARS-CoV-2. A large study from China revealed that overall case fatality from COVID-19 was 2.3%, but it increased to 6.0% for hypertensive patients (Surveillances, 2020) . Studies have documented diurnal changes in blood pressure are higher during the early morning and lower in the late evening or sleeping hours. This pattern is more prominent in dippers, those who experience a 10 to 20% reduction in their nocturnal BP, and a slightly lesser extent on non-dippers, those that experience a blunted decline in their BP at night (Di Raimondo et al., 2020) . Increased night-time ambulatory blood pressure, as studies suggest, is related to fatal and non-fatal cardiovascular events like myocardial infarction, cardiac arrest, etc. (Elliott, 1999) . Therefore, lessening BP at night is important from the perspective of bringing down cardiovascular events particularly in hypertensive, hospitalized COVID-19 patients. Improvement in 24 h BP was observed along with a dip in BP profile when a minimum of one antihypertensive drug was administered in the evening (Bowles et al., 2018) . Hermida et al. studied the chronotherapeutic effect of morning vs. evening administration of antihypertensive drugs and found that bedtime administration, as opposed to morning, significantly reduced BP throughout the night and in the morning hours, and markedly reduced the likelihood of a major cardiovascular event (Hermida et al., 2020) . Other antihypertensive drugs, like ramipril, telmisartan, amlodipine with hydrochlorothiazide, amlodipine with olmesartan, and valsartan have also been shown to have better effects when taken in the evening or before bedtime Hermida and Ayala, 2009; Hermida et al., 2010; Hoshino et al., 2010; Zeng et al., 2011) . However, antihypertensive drugs like amlodipine, which has a long halflife of 35-50 h, made no difference between evening or morning administration (Mengden et al., 1993; Nold et al., 1998) . Overall, the results from several human studies have shown that ARBs and ACE inhibitors are completely safe for consumption in hypertensive patients. Additionally, hypertensive medications have also been associated with decreased mortality among COVID-19 patients. Thus, these drugs can be potentially effective remedies for the treatment and management of hypertensive COVID-19 patients. We additionally advocate taking into consideration the time of administration of these hypertensive drugs for maximum benefit. In October 2020, 7 months since the declaration of the COVID-19 pandemic, the United States FDA authorized the antiviral drug, remdesivir to treat COVID-19. Earlier, remdesivir was shown to be effective against viruses in other coronavirus families like the Middle East Respiratory Syndrome coronavirus (MERS-CoV) and severe acute respiratory syndrome coronavirus (SARS-CoV) (de Wit et al., 2020; Malin et al., 2020) . Additionally, studies have also observed remdesivir to have a superior effect over placebo in shortening the recovery time in hospitalized COVID-19 patients (Madsen, 2020) . This facilitated the use of remdesivir by clinicians among hospitalized COVID-19 patients. However, a recent WHO solidarity trial published, taking 11,330 adult COVID-19 patients from 30 countries worldwide, showed that antiviral drugs like remdesivir and lopinavir have little to no effect on the outcome such as overall mortality, the need for ventilation support, or hospital recovery time (WHO Solidarity Trial Consortium, 2020; Figure 3 and Supplementary Table 1 ). Yet, remaining optimistic amidst this deadly pandemic and tweaking strategies in antiviral drug administration may help with better outcomes in vulnerable patients. Viruses are obligate parasites that rely totally on hosts to survive, replicate and disseminate. The viral life cycle starts with the viral entry into the cell through binding to host receptors or factors manifest at the cell surface. Once the virus enters the cell and capsid disassembly happens, the release of DNA or RNA genomes occur. The translational and transcriptional pathways in a host are exploited for initiating viral replication (Ryu, 2017) . About 80% of protein-coding genes in different tissues, as shown in a study on primates, exhibit rhythmic expression on a daily basis (Mure et al., 2018) . Host clock components help viruses replicate in a direct or indirect manner. A study found that when wild-type mice, living in a controlled environment with 12 h of light-dark cycles, were infected with herpes virus at the start of the day (at sunrise, when nocturnal animals are transitioning into their resting phase), the viral load was 10 times greater than those infected 10 h after sunrise (around the time when nocturnal animals are starting their active phase). On the other hand, when the researcher knocked out Bmal1, a core circadian clock gene, the level of viral replication remained the same (Edgar et al., 2016) . This study further reiterated that circadian variation in immune response or viral activity exists. However, timed-administration of antivirals in patients has rarely been taken into consideration. In a recent retrospective study, the effect of administering antiviral drugs in the morning and evening was evaluated on COVID-19 patients in Italy. The result indicated that CRP values reduced significantly for the morning regimen although measurable differences were not evident in other evaluated parameters (De Giorgi et al., 2020) . In another study, mice dosed with acyclovir to prevent HSV-2 infection during the active phase were four times more compared to the resting phase (Matsuzawa et al., 2018) . The half-life of a drug is linked to dosing time. Hence, drugs having short half-life of 6 h or less exhibit higher sensitivity to the time during the day when they are administered (Sahin and Benet, 2008) . Timed-administration of drugs is a promising approach that needs to be investigated to optimize the available antiviral drugs, like remdesivir and other antiviral drugs that can be repurposed for the treatment and management of COVID-19 patients (Supplementary Table 1) . Recommendations regarding the use of corticosteroids for the treatment and management of COVID-19 largely stemmed from a clinical trial conducted in the United Kingdom, where dexamethasone, a corticosteroid, was observed to reduce mortality at 28 days in hospitalized COVID-19 patients (RECOVERY Collaborative Group, 2020a) . Corticosteroids are anti-inflammatory drugs that work by suppressing the activation of the immune system, thus preventing the so-called ""cytokine storm"" in patients with a hyperactive immune response (Channappanavar and Perlman, 2017) . Cytokine storms are responsible for the damage to vital organs like the lungs, which is mainly responsible for COVID-19-related deaths (Ruan et al., 2020) . Dexamethasone, prednisone, and methylprednisolone are potent anti-inflammatory drugs that could help prevent these deleterious effects. A recent meta-analysis of the clinical trials involving the use of corticosteroids revealed that compared to the control (placebo), treatment with corticosteroids significantly reduced the risk for mortality (Sterne et al., 2020 ; Figure 3 and Supplementary Table 1) . Nevertheless, the risks and benefits of using corticosteroids in COVID-19 patients have to be further evaluated carefully before its implementation on a larger scale. Particularly, the use of prednisone or methylprednisolone cannot be blindly implemented since dexamethasone has shown some positive outcomes and have similar anti-inflammatory effects. It is important to note that routine administration of corticosteroids can exacerbate COVID-19-induced lung injury in some instances (Russell et al., 2020) . Corticosteroids are only beneficial in cases of hyperinflammation. Earlier, prednisone has been shown to have both beneficial and detrimental outcomes in patients with critical pulmonary infections. For instance, while patients suffering from Pneumocystis jirovecii Pneumonia benefited from using prednisone (with a reduced death rate), patients with MERS-CoV infection experienced a delay in their viral clearance (Bozzette et al., 1990; Arabi et al., 2018) . Similarly, in influenza virus-induced severe cases of pneumonia, the use of corticosteroids has also been reported to have worse outcomes (Rodrigo et al., 2016) . Indeed, treatment with corticosteroids in patients suffering from ARDS has shown some conflicting results. Yet, a metaanalysis of several clinical trials has shown that overall corticosteroids reduce the risk of mortality and the duration of mechanical ventilation in patients suffering from ARDS (Mammen et al., 2020) . There are still controversies regarding the use of corticosteroids for COVID-19 treatment, particularly because of the absence of the larger randomized clinical trials. Corticosteroids like dexamethasone, prednisone, or methylprednisolone are used in clinical medicine because they possess immunosuppressive effects. Thus, these drugs may have adverse effects in COVID-19 patients if used in full doses (Russell et al., 2020) . Therefore, timed-administration of the drugs should be carefully considered in this realm. In patients with rheumatoid arthritis (RA), the chronotherapeutic use of prednisone has been prominently highlighted. Diurnal variation in pain, stiffness and functional disability has been observed in patients suffering from RA, with a peak around the morning hours as a result of an elevated level of pro-inflammatory cytokines throughout the nighttime (De Silva et al., 1984) . Prevention of this nighttime rise in proinflammatory cytokines, particularly, IL-6, by evening doses of prednisone, is hypothesized to be better for RA patients than managing morning symptoms when it becomes too late to curb the inflammation. Delayedrelease prednisone tablets, which are modified to be released slowly in the blood and many hours after the intake, have been developed so that patients do not have to get up at night to take their medications. These modified drugs were significantly more beneficial to RA patients (Buttgereit et al., 2008) . Studies have repeatedly shown that evening administration of prednisone was more beneficial in reducing RA symptoms in patients when compared to the same dose taken in the morning (De Silva et al., 1984) . Thus, it remains vital to consider timed-administration of corticosteroids as a potential variable when evaluating the efficacy of corticosteroids against COVID-19 (Supplementary Table 1 ). To date, there are no published clinical studies that have evaluated the efficacy of prednisone or methylprednisolone against the SARS-CoV-2, and therefore future studies should aim to address this knowledge gap. Almost all living organisms from bacteria to humans possess an internal clock that oscillates rhythmically to help anticipate environmental changes (Patke et al., 2020) . In mammals, this is possible through a translational-transcriptional feedback loop that regulates a gene expression pattern. The dimerization of two transcription factors, CLOCK and BMAL1, followed by the binding of the CLOCK-BMAL1 complex at the E-box motif in the promoter region of the core clock genes, facilitate the transcription of Period genes (Per1, Per2, and Per3) and Cryptochrome genes (Cry1 and Cry2). In turn, PER-CRY protein complex inhibits the transcription of its own genes by directly inhibiting CLOCK:BMAL1 activity. In addition to this, there are two nuclear receptors involved in this feedback loop known as the REV-ERBα and RORα. RORα work against REV-ERBα to facilitate the transcription of Bmal1 by binding to the promoter region (Sundar et al., 2015b,c) . Thus, both REV-ERBα and RORα plays a crucial role to maintain the circadian clock machinery. SR9009, a REV-ERBα agonist, is a synthetic drug that was originally designed to better understand circadian rhythms in vitro and in vivo. In mice, this drug was shown to boost endurance by increasing mitochondria count in skeletal muscles (Woldt et al., 2013) , significantly reduced cholesterol and body weight (Solt et al., 2012) , and also lower anxiety to a level as effective as using benzodiazepine (Banerjee et al., 2014) . SR9009 was effective in reducing inflammation by interfering with the production of inflammatory cytokines like TNFα, CCL2, and MMP-9 in a rat model (in vivo) and rat C6 astroglial cells (in vitro) (Li et al., 2014; Morioka et al., 2016) . Additionally, some studies have also observed some great benefits of using SR9009 treatment for heart disease. For instance, when genetically modified mice prone to blood vessel lesions and hardening of arteries were administered with SR9009, there was a significant reduction in observed lesions while other parameters mostly remained unchanged (Sitaula et al., 2015) . Circadian rhythms have been shown to regulate both the innate and adaptive immune systems in multiple ways. Although the exact mechanisms are not known, it has been determined that changes in core clock genes and the nuclear factors can increase the host's susceptibility to infectious agents (Scheiermann et al., 2013) . For instance, the genetic ablation of the Bmal1 and the stimulated activation of REV-ERBα with synthetic agonist (like SR9009) was successful in inhibiting the positive-strand RNA viruses of the Flaviviridae family like the Hepatitis C, Dengue, and the Zika virus by interfering with the lipid signaling pathway (Zhuang et al., 2019) . Additionally, REV-ERBα agonists were also shown to regulate HIV-1 replication by inhibiting promoter activity in CD4 T cells and macrophages, while antagonism of REV-ERBα lead to increase promoter activity (Borrmann et al., 2020) . Yet another evidence showed that SR9009 was successfully able to inhibit replication of alphaviruses like chikungunya and o'nyong'nyong virus by suppressing the synthesis of the structural proteins (Hwang et al., 2018) . Besides the drug's ability to inhibit the aforementioned virus, COVID-19 patients might benefit the most from its potential to selectively regulate proinflammatory cytokines like IL-6, which has been considered as a prognostic marker for mortality among the same (Gibbs et al., 2012; Liu et al., 2020) . Considering the beneficial effects of REV-ERBα agonists, like SR9009, which was originally developed to study circadian rhythms, can therefore be implemented as a unique therapeutic drug against the SARS-CoV-2 during these unprecedented times. However, the chronotherapeutic drugs such as REV-ERBα agonists should be initially tested using in vitro and in vivo models of SARS-CoV-2 infection before they can be repurposed to the treatment of COVID-19 (Figure 3 and Supplementary Table 1 ). Cationic amphiphilic drugs (CADs) belong to a wide group of compounds that share several structural properties like a hydrophobic ring or ring system on the molecule and possess a hydrophilic side chain with an ionizable cationic amine group. CADs may include drugs used for the treatment of depression, psychosis, bacterial infections, arrhythmias, malaria, and lower cholesterol (Halliwell, 1997) . Some of these drugs are effective against several infectious agents including viruses. For instance, the antimalarial drug Hydroxychloroquine and Chloroquine were found to be successful in preventing the SARS-COV-2 virus replication in vitro. For this reason, clinicians were hopeful that the in vitro results would translate in vivo and started using those antimalarials for the treatment of COVID-19 worldwide (Giri et al., 2020) . However, larger clinical trials had clearly shown that these drugs were not effective against the SARS-COV-2 in humans (RECOVERY Collaborative Group, 2020b). Fortunately, other FDA-approved CADs might be useful (Figure 3 and Supplementary Table 1) . Anti-arrhythmic drug, amiodarone is an ion channel blocker used for the treatment of ventricular arrhythmias and atrial fibrillation. This drug can prevent the fusion of the viral envelop with the endosomal membrane and also accumulate in the late endosomes/lysosomes and disrupts the viral endocytic pathway (Salata et al., 2015) . Earlier amiodarone was shown to prevent the entry of the Filovirus at concentrations that are found in the patient's serum routinely using anti-arrhythmic drugs (Gehring et al., 2014) . A similar result was also observed with the Ebola virus using methyldiethanolamine, a metabolite product of amiodarone (Salata et al., 2015) . Amiodarone, dosed at 60 mg/kg, significantly increased survival in mice infected with the ebola virus (Madrid et al., 2015) . However, amiodarone wasn't effective against the Ebola virus in humans in the Western African population (Wolf et al., 2015) . Amiodarone was also shown to inhibit other viruses like the arenavirus, the SARS-CoV-1 and Hepatitis C virus (Stadler et al., 2008; Cheng et al., 2013; Gehring et al., 2014) . Arrhythmias are common among COVID-19 patients, and it is associated with higher morbidity and mortality (Babapoor-Farrokhran et al., 2020) . Amiodarone might be particularly beneficial for COVID-19 patients showing signs of ventricular arrhythmias, although the pros and cons of these drugs need to be studied in detail before using for the treatment of COVID-19 patients. The time of the day when the occurrence of cardiac arrhythmias is more frequent has not been studied extensively and multiple studies investigating the same have failed to take potential confounders like alcohol or caffeine consumption into consideration. Thus, interpreting circadian patterns in arrhythmias have been difficult. However, Portaluppi and Hermida summarized the available literature, and most studies found a peak time occurrence of cardiac arrhythmias (Ventricular pre-mature beats, ventricular tachycardia, or fibrillation) mostly between 6 am and noon. Based on their findings, they concluded that arrhythmogenesis appeared to be less frequent or suppressed during the nighttime (Portaluppi and Hermida, 2007) . This information can be useful for clinicians who are treating COVID-19 patients with cardiac arrhythmias. Future research is needed in this domain to evaluate the consequence of timed-administration of antiarrhythmic drugs. Another class of CADs is the antipsychotics, particularly chlorpromazine, which has been shown to have antiviral properties against the Crimean-Congo hemorrhagic fever virus, the adenovirus, Ebola, MERS-CoV and SARS-CoV (Bhattacharyya et al., 2010; Diaconu et al., 2010; Dyall et al., 2014; Ferraris et al., 2015) . During the first stage, the endocytosis of the virus, clathrin-coated pits are internalized along with the virus and forms the clathrin-coated vesicles (Bayati et al., 2021) . Chlorpromazine blocks the formation of clathrin-coated pits and thus prevents viral entry into the cells (Daniel et al., 2015) . Several other antipsychotic drugs have also been reported to have antiviral properties. For instance, antidepressant drug desipramine has been shown to prevent parvovirus entry into cells by introducing disorder in the cholesterol-rich lipid rafts (Pakkanen et al., 2009) , and sertraline has been reported to inhibit the Zika virus (Barrows et al., 2016) . Nagayama et al., studied the effect of timed-administration of chlorpromazine in rats and found that the sedation period of the drug was both time-and dose-dependent. In rats, housed in a controlled environment with a 12-h light-dark cycle (19:30 -lights on, 7:30 -lights off), at 1:30 a significantly smaller dose of chlorpromazine was able to show the same sedative effect than when administered at 7:30, and the response increased with increasing dose. The sedation period variation was attributed to the sensitivity of the catecholamine receptors but the exact reasons are not known (Nagayama et al., 1978; Supplementary Table 1 ). The purpose of using chlorpromazine for COVID-19 is a different purpose altogether. However, this study might suggest that there may be a specific window of the day when the drug might be best utilized. Ongoing and future studies can attempt to see if there is any variation in time-dependent response. Another class of drugs belonging to the CADs is the selective estrogen receptor modulators (SERMs). SERMs can exert both agonist and antagonist effects depending on the location of the estrogen receptor. In the breast, estrogen receptors are different from that of the other parts of the body such as the bone, liver, or uterine cells. SERMs block estrogen function in the breast while activates it on the other parts of the body (Riggs and Hartmann, 2003) . SERMs have earlier been shown to have protective functions against MERS-CoV, Ebola virus, HSV-1, and HCV (Johansen et al., 2013; Murakami et al., 2013; Zheng et al., 2014) . SERM, clomiphene, possesses antiviral activity against the Ebola virus by interfering with the late-stage fusion of the viral envelope with the endosomal membrane (Nelson et al., 2016) . Similar to the Ebola virus, the SARS-CoV-2 is a lipidenveloped virus that encounters the host late endosome/lysosome during its replication cycle. Additionally, clomiphene has also been shown to inhibit the Ebola virus entry by inhibiting the NPC1-dependent pathway, which has been hypothesized to increase cholesterol accumulation in the late endosomes and impair viral entry for SARS-CoV-2 (Ghasemnejad-Berenji et al., 2020) . Overall, CADs are effective against many viruses in vitro. CAD's ability to interfere at different stages of the viral life cycle may be exploited to see if any benefit exists against the novel SARS-CoV-2. In the severe cases of COVID-19, the overactivation of the immune system, followed by a cytokine storm results in profound damage to vital organs, especially the lungs. This causes the development of ARDS, the leading cause of death among COVID-19 patients Zhou et al., 2020) . Proinflammatory cytokines, particularly IL-6 through the activation of the Jak-Stat pathway, have been identified as a prognostic indicator for mortality among COVID-19 patients (Del Valle et al., 2020; Liu et al., 2020) . Therefore, known drugs capable of inhibiting the Jak-Stat pathway can be beneficial in patients suffering from COVID-19. JAK inhibitors can prevent the phosphorylation of proteins that are involved in the signal transduction cascade of the Jak-Stat pathway and thus are able to reduce cytokine-mediated inflammation and collateral damage to vital organs (Lin et al., 2020) . Baricitinib is a JAK inhibitor used for the treatment of rheumatoid arthritis (Lin et al., 2020) . Recently, this drug has received some attention after a study showed that baricitinib significantly reduced the median number of days to recovery, from 18 to 10 days, in hospitalized COVID-19 patients requiring high-flow oxygen or non-invasive ventilation, when used together with remdesivir (Kalil et al., 2020) . Baricitinib was very well tolerated among COVID-19 patients and had a significant reduction in inflammation as measured by the levels of inflammatory cytokines. Another study showed that the need for ventilation support or death was also reduced by more than 50% (34.9 to 16.9%) using baricitinib compared to the control group (Stebbing et al., 2021) . At clinically relevant concentration, Baricitinib can successfully inhibit type-1 interferon response (exaggerated in COVID-19 patients) which is known to increases ACE2 expression (in liver cells), the same cell receptor used by the SARS-CoV-2 to enter host cells, increasing the viral load (Stebbing et al., 2021) . Additionally, baricitinib is a potent inhibitor of the Numb-associated kinase (NAK) family of proteins, particularly AAK1, that plays a pivotal role in clathrinmediated endocytosis, which further prevents viral entry into the cells (Conner and Schmid, 2002; Sorrell et al., 2016) . Despite promising findings in smaller studies, sufficient evidence is still lacking that demonstrates the beneficial effects of baricitinib particularly against the SARS-CoV-2. We, therefore, advocate for larger clinical trials and in vivo animal studies that will help to determine the optimal dose and efficacy of these drugs. Other JAK-inhibitors like fedratinib and ruxolitinib can also be explored at the same time for their efficacy against the SARS-CoV-2 (Figure 3 and Supplementary Table 1) . Timed-administration should also be taken into consideration when using Baricitinib. This is because this drug is metabolized rapidly in the body and has a very short half-life in humans . In collagen-induced arthritis mice models, the diurnal variation in the expression of inflammatory cytokines was observed. Subsequently, the superior effect of baricitinib was observed when the drug was administered in a period (evening) where the cytokine production was the highest (Yaekura et al., 2020) . Similarly, diurnal variation of the pro-inflammatory cytokine, IL-6, has been observed in humans (Gudewill et al., 1992) . In healthy volunteers, IL-6 levels correspond with the sleep-wake cycle (Bauer et al., 1994) . Many studies have observed a peak of IL-6 levels during nighttime and early morning hours (Vgontzas et al., 2005) . However, how these pro-inflammatory cytokine levels change during SARS-CoV-2 infection is not well known. Future studies should aim to determine the diurnal changes of proinflammatory cytokines, if possible, in COVID-19 patients, at different time points during the day. These studies can further determine if the timedadministration and slow release of these anti-inflammatory drugs, like Baricitinib, can lead to even better outcomes in COVID-19 patients. Melatonin is the hormone produced in the pineal gland during the dark (night), and its synthesis is directly controlled by the central clock (SCN) (Blask, 2009) . Melatonin exerts its effects by binding to one of two melatonergic receptors, namely MT1 and MT2, both of which belong to the G protein-coupled receptor family. Both receptors have the same affinity for melatonin, but the MT2 receptor has double the dissociation half-time of MT1 (Legros et al., 2014) . Expression of MT1 in retinas, kidneys and the SCN implicates the role of both melatonin and MT1 in the regulation of circadian rhythms and reproductive cycles. Expression of the MT2 subtype is somewhat similar to that of MT1 in areas such as the retina and brain, but MT2 is completely absent in the SCN. The MT2 melatonin receptor plays a major role in the regulation of body temperature (Pandi-Perumal et al., 2008; Bjorvatn and Pallesen, 2009 ). Circadian regulation of melatonin signaling is one of the ways by which it influences sleep timing. Studies that utilize exogenous melatonin for the treatment of age-related sleep disorders demonstrate how melatonin can have profound effects on sleep and circadian rhythms. Prior report has shown that administration of a physiologically relevant dose of melatonin helps improve sleep quality in older individuals who generally demonstrate poor sleep quality (Luboshizsky and Lavie, 1998; Zhdanova et al., 2001) . Using an ex-vivo approach, melatonin treatment in brain SCN slices from WT mice resulted in inhibition of neuronal firing in a concentration-dependent manner. Alternatively, this process was inhibited in brain SCN slices from MT1 knockout mice treated with melatonin, indicating that MT1 has a profound effect on the central clock (Dubocovich, 2007) . Thus, melatonin may serve as one of the key modulators of sleep, circadian rhythms, and immunity in the elderly. The previous report showed rhythmic changes in circulating immune cells and leukocyte migration from the blood to various organs. Bmal1 deletion in endothelial cells resulted in an abolished time of day-dependent differences in expression of VCAM-1 and ICAM-1, and failure to maintain rhythmic homing of immune cells into peripheral tissues (He et al., 2018) . In another report, the role of melatonin controlling circadian clock target Bmal1 occurs via activation of PI3K/AKT signaling that is critical for cell survival (Beker et al., 2019) . Melatonin (10 mg/kg/d, s.c for 7 days) treated aged male Wistar rats (28 months old) when given 4 × 10 8 sheep erythrocytes via i.p. evoked augmented humoral immune response (IgG1 and IgM levels) compared to aged controls. This finding suggests that exogenous melatonin treatment showed attenuation of humoral immune responses in aged rats (Akbulut et al., 2001) . Therefore, it is vital to unwrap the novel relationship between endogenous or exogenous melatonin signaling and circadian rhythms that may be directly contributing to outcomes of SARS-CoV-2 infection. Future and ongoing studies will address the potential role of the host circadian clock-agingimmunity axis on the pathogenesis of SARS-CoV-2-induced viral infection. Studies also indicate that melatonin levels gradually decrease with age, causing disruptions of many circadian controlled physiological functions (Karasek, 2004) . Therefore, melatonin supplements can be useful that have the potential to reverse a dysfunctional circadian rhythms. Additionally, melatonin is a potent antioxidant and possesses immunomodulatory, antiviral, and anti-inflammatory properties. A detailed explanation of the different properties are beyond the scope of this review but can be found here (Malhotra et al., 2004; Anderson and Reiter, 2020) . In the elderly population, the lower level of melatonin therefore partially explains why certain age groups are more likely to suffer from more severe symptoms of the COVID-19 than the younger generation. The use of melatonin supplements has been shown to reduce severity in other viruses and may be useful for COVID-19 patients as well (Figure 3) . To date, there is no randomized control clinical trial conducted using melatonin for the treatment of COVID-19 patients is available. Thus, we advocate for future clinical studies that will aim to address the benefits of using melatonin supplements in elderly COVID-19 patients. As of February 20, 2021, approximately 12.6 percent of the United States population has received at least one dose of either the Moderna or the Pfizer mRNA vaccine, and about 4.9 percent are fully vaccinated. Currently, the United States is administering about 1.9 million doses per day (Johns Hopkins University and Medicine, 2021). While it is true that mass vaccination could be the only answer to eradicate the SARS-CoV-2 from the globe, as we have seen in the past with smallpox and the poliovirus in the United States, it might take longer time than we imagine, especially with the slow progress and the newly emerging variant of SARS-CoV-2 strains. Meanwhile, this time can be efficiently utilized for conducting both preclinical and clinical studies to understand how antibody response changes to the COVID-19 vaccine, if there exist any, when they are given at a specific time of the day. The circadian clock in CD8 + T cells plays an important role in response modulation that occurs during vaccination. In mice, this leads to the activation of higher numbers of T cells when vaccination with dendritic cells (loaded with ovalbumin peptides) is administered during the day compared to the night (Nobis et al., 2019) . Attenuated encephalomyelitis vaccine when administered at 8:00 am was shown to reach the peak antibody titer 4 days earlier than those that were vaccinated at 8:00 pm (Feigin et al., 1967) . Influenza vaccination administered in the morning between 9:00 and 11:00 am was shown to have a better antibody response against different influenza strains in humans compared to administration in the afternoon between 3:00 and 5:00 pm (Long et al., 2016) . Timing thus seems to modulate antibody response and better outcomes are not always associated with morning administration. Hepatitis B vaccine, for instance, when administered between 1:00 and 3:00 pm resulted in significantly higher antibody response when compared with those who received the vaccine between 7:30 and 9:00 am (Pollman and Pollman, 1988) . Additionally, antibody responses can be triggered better for protection against different viruses if the month of vaccine administration is strategically chosen in nations experiencing seasonal variations. For instance, B-cell maturation factor expression exhibits seasonal variation (Dopico et al., 2015) , and these expressions are linked to responses conducive to trivalent influenza vaccine (Nakaya et al., 2011) . Another study found a considerable association between the response of antibodies to rabies vaccine and vaccination month (Moore et al., 2006) . Pathogen recognition receptors (PPR) also exhibit seasonal variability with an enhanced expression during winter, modulating vaccine response quality and providing enhanced defense against the virus causing yellow fever (Querec et al., 2009; Dopico et al., 2015) . Overall, all these studies cumulatively suggest that timing might play a key role to further determine the efficacy of the COVID-19 vaccine. In these challenging times when the SARS-CoV-2 virus is posing a significant threat to vulnerable people, the findings of the above-mentioned studies on timed vaccine administration can be effectively leveraged to offer comprehensive protection against COVID-19. AG, AS, and IS conceived the ideas, collected the appropriate literature, drafted the outline, contributed to the writing of this review, updated the figures/schematics, and checked, edited, and approved the final version of the review. All authors contributed to the article and approved the submitted version.  The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fnins. 2021.674204/full#supplementary-material@story_separate@As the coronavirus pandemic continues to rage on and to counter its insidious effect, researchers are taking a closer look at sleep, an important contributor to optimal health. Throughout this pandemic, sleep has been negatively impacted due to anxiety induced by illnesses, financial insecurity, housing issues, or working as a frontline worker, social media news, lesser exposure to sunlight inside homes, and so on. Low-quality sleep has a disruptive effect on the immune system at the molecular level, greatly increasing the susceptibility to diseases. Proper sleep also makes recovery from sickness faster. Poor sleep also triggers heart ailments (e.g., blood pressure), kidney problems, diabetes, cancer, obesity, and hypertension. Amidst this pandemic, it is very essential to sleep well at night to render the immune system stronger. Quality sleep should complement nutritional food intake at proper times and physical activities. As daily routines have been thrown haywire, it is important to maintain a strict sleep schedule and wake up at the same time each day. Before going to sleep, relaxation-inducing activities like meditation, bath, book reading, or enjoying soul-soothing music can calm down the sympathetic nervous system, which helps facilitate sleep. Sleep interference can occur if we gaze at computers or tablets that do not have a blue light filter in place before bedtime, consume nicotine, alcohol, or caffeine, or take late-day naps. Unhealthy practices should be left behind, and conscious efforts need to be made to adopt a healthier lifestyle. Embracing healthier sleeping habits, as we have discussed in this review, will help strengthen and normalize the circadian rhythms naturally. The outbreak of the new contagious virus, SARS-CoV-2, imposed unique challenges on the healthcare front, particularly in the selection of the appropriate therapeutics against the new infectious virus. Currently, there is still no specific treatment against the novel COVID-19. Since the beginning of the pandemic, there has been an urgent need for effective therapeutic agents against the SARS-CoV-2. To combat the COVID-19 pandemic, it is imperative that we collaboratively adopt novel interventions across various medical specialties. Here, we introduce a fundamental concept that is frequently discussed in sleep medicine, chronotherapy, and integrate it with the discussion of the several major classes of drugs that can be repurposed for the treatment and management of COVID-19. Accumulation evidence from the literature suggests that nearly half of all physiological functions, including the body's pathogenic response, are controlled tightly by the circadian clock. Chronotherapy exploits this rhythmic pattern to its advantage to improve the outcome of different medical interventions as discussed throughout this review. This manuscript highlights that diurnal variation exists in the functioning of our immune system, in vivo cholesterol synthesis, coagulation, fibrinolysis and blood pressure, and that considering the time of drug administration might significantly improve the treatment and management of the novel coronavirus disease-19.","The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic has affected nearly 28 million people in the United States and has caused more than five hundred thousand deaths as of February 21, 2021. As the novel coronavirus continues to take its toll in the United States and all across the globe, particularly among the elderly (>65 years), clinicians and translational researchers are taking a closer look at the nexus of sleep, circadian rhythms and immunity that may contribute toward a more severe coronavirus disease-19 (COVID-19). SARS-CoV-2-induced multi-organ failure affects both central and peripheral organs, causing increased mortality in the elderly. However, whether differences in sleep, circadian rhythms, and immunity between older and younger individuals contribute to the age-related differences in systemic dysregulation of target organs observed in SARS-CoV-2 infection remain largely unknown. Current literature demonstrates the emerging role of sleep, circadian rhythms, and immunity in the development of chronic pulmonary diseases and respiratory infections in human and mouse models. The exact mechanism underlying acute respiratory distress syndrome (ARDS) and other cardiopulmonary complications in elderly patients in combination with associated comorbidities remain unclear. Nevertheless, understanding the critical role of sleep, circadian clock dysfunction in target organs, and immune status of patients with SARS-CoV-2 may provide novel insights into possible therapies. Chronotherapy is an emerging concept that is gaining attention in sleep medicine. Accumulating evidence suggests that nearly half of all physiological functions follow a strict daily rhythm. However, healthcare professionals rarely take implementing timed-administration of drugs into consideration. In this review, we summarize recent findings directly relating to the contributing roles of sleep, circadian rhythms and immune response in modulating infectious disease processes, and integrate chronotherapy in the discussion of the potential drugs that can be repurposed to improve the treatment and management of COVID-19."
"SARS-CoV 2 , a novel, human-infecting beta-coronavirus, provisionally named 2019 novel coronavirus (2019-nCoV), was identified using of next-generation sequencing. This virus is now pandemic [1] . Most of the infected patients have a high fever and some have dyspnea, with chest radiographs revealing invasive lesions in both lungs, accompanied by hyperinflammation, respiratory distress syndrome and death [2] . SARS-CoV 2 infection (COVID-19) is characterized by high mortality especially in older and fragile individuals. This virus induces not only a systemic inflammatory response but also a vigorous lung inflammation that may lead to a difficult-to-treat RDS and death [3] . Human bronchial epithelial cells and pneumocytes express ACE2, the receptor used by the virus to enter the cells and perpetuate its viral life cycle [4] . The infection of such cells ignites molecular mechanisms of defense (cell death and apoptosis) along with the recruitment of the immune system. Cell damage and repair mechanisms contribute to tissue restructuration. Furthermore, other anatomical, physiological and functional barriers (i.e. respiratory cilia) also contribute to viral clearance and tissue regeneration. Knowing the mechanism involved in the pathogenesis of coronavirus induced respiratory and systemic damage is essential to target therapeutic intervention. In the present study, we investigated the effect of SARS-CoV 2 on human bronchial epithelial cells (HBEC). We found that the upregulation and down-regulation of CSF3 and DNAH7, along with other genes overlapping the same paths, are the most modulated genes in NHBE cells infected with COVID-19. We hypothesized that respiratory cilia are impaired by SARS-CoV 2 infection of epithelial cells, while apoptosis, matrix destructuration and collagen deposition do occur, potentially leading to respiratory distress syndrome (RDS). These pathogenetic factors could be critical to dissect future clinical and therapeutic interventions. SARS-CoV 2 , a novel, human-infecting beta-coronavirus, provisionally named 2019 novel coronavirus (2019-nCoV), was identified using of next-generation sequencing. This virus is now pandemic [1] . Most of the infected patients have a high fever and some have dyspnea, with chest radiographs revealing invasive lesions in both lungs, accompanied by hyperinflammation, respiratory distress syndrome and death [2] . -19) is characterized by high mortality especially in older and fragile individuals. This virus induces not only a systemic inflammatory response but also a vigorous lung inflammation that may lead to a difficult-to-treat RDS and death [3] . Human bronchial epithelial cells and pneumocytes express ACE2, the receptor used by the virus to enter the cells and perpetuate its viral life cycle [4] . The infection of such cells ignites molecular mechanisms of defense (cell death and apoptosis) along with the recruitment of the immune system.@story_separate@The aim of our study was to investigate the effect of SARS-CoV 2 infection on the bronchial parenchyma. We hypothesized that SARS-CoV 2 could modulate bronchial cells of COVID-19 patients at multiple anatomical and physiological levels and regulate the cytoskeletal structures. Furthermore, our hypothesis predicted that these changes were specific to COVID-19 infection and not common to other pandemic virus of airways such as SARS-CoV, MERS-CoV, and H1N1. In order to test our hypothesis we have collected and analyzed several microarray datasets available on NCBI Gene Expression Omnibus (GEO) database (http://www.ncbi.nlm.nih.gov/geo/) [5] [6] [7] . Mesh terms ""coronavirus"", ""Human"", and ""airway epithelial cells"", were used to identify human potential datasets of interest. Three datasets were selected (GSE147507, GSE47962, GSE81909) ( Table 1 ). The GSE147507 dataset [8] was composed of Normal Human Bronchial Epithelial cells (NHBE) and transformed lung alveolar (A549) cells treated with MOCK or infected with SARS-CoV 2 (USA-WA1 / 2020) at different MOI (NHBE: 2, A549: 0.2) for 24hrs. The authors declared that cDNA libraries were sequenced using an Illumina NextSeq 500 platform (Illumina, CA), following differential expression analysis using DESeq2. As for our investigation, we focused only on the effect of SARS-CoV 2 on NHBE cells. The submitter-supplied pre-preprocessed and normalized expression matrix was used for this re-analysis. The GSE147507 RawReadCounts7, subsequently was used for the identification of Differentially Expressed Genes (DEGs). From GSE47962 we downloaded the data of Human bronchial airway epithelium (HAE) cells, seeded in 6-well plates (1 x 10e6 cells/well) two days prior to infection and then inoculated with wild type infectious clone derived SARS-CoV viruses (icSARS) (MOI = 2) or H1N1 (MOI = 1) [9] . Mock-infected controls were inoculated with culture medium only. For our analysis, we sorted only the data of 24hrs treatment. From GSE81909 we selected the transcriptome of primary human airway epithelial cells infected with a multiplicity of infection of 5 PFU per cell of wild type MERS-coronavirus (MERS-CoV) (icMERS). Furthermore, we sorted only the transcriptome corresponding to 24hrs of treatment. Time-matched mocks were collected in parallel with infected samples. (Table 1) . In order to process and identify Significantly Different Expressed Genes (SDEG) in all selected datasets, we used the MultiExperiment Viewer (MeV) software (The Institute for Genomic Research (TIGR), J. Craig Venter Institute, USA). In cases where multiple genes probes insisted on the same GeneID, we used those with the highest variance. The significance threshold level for all data sets was p<0.05. Statistically significant genes were selected for further analysis. For GSE47962 and GSE81909 datasets we performed a statistical analysis with GEO2R, applying a Benjamini & Hochberg FDR (False discovery rate) to adjust P values for multiple comparisons [10, 11] . For the analysis of GSE147507, we used a conservative approach. We have set a filtering cutoffs of gene-level read counts >= 10, for differential gene expression analysis, in order to reduce artificial variance that results in differential expression or splicing calls, and then standardized the data with z-score transformation [12] . For the identification of the Differentially Expressed Genes (DEGs) in the HNBE cells, the LIMMA (Linear models for microarray data) (MeV) parametric test was used. An adjusted p-value < 0.05 was considered to indicate a statistically significant difference. The genes Ontology analysis was performed using the web utility GeneMANIA (http://genemania.org/, http://genemania.org/) [13] and the GATHER (Gene Annotation Tool to Help Explain Relationships) (http://changlab.uth.tmc.edu/gather/) [14] . The GeneMania was also used to built the weighted gene networks commonly modulated. The database assembles all available interaction data in the dataset by creating large networks, which captures the current knowledge on the functional modularity and interconnectivity of genes in a cell. Gene's annotation was obtained by STRING software (https://string-db.org/). The STRING-combined score was based on data from neighborhood in the genome, gene fusions, co-occurrence across genomes, co-expression, experimental/ biochemical data, and association in curated databases [15] . The genes overlapping was represented with Venn diagram using a public online tool (http://bioinformatics.psb.ugent.be/webtools/Venn/) and the images readapted for our data with CorelDraw. The Gene set enrichment analysis (GSE) and gene ontology (GO), were expressed in weighted percentage and graphically rendered in a circular diagram format using freely available CIRCOS software (http://circos.ca/) [7, 16, 17] . Ribbon size encodes gene number and FDR associated row/column segments with high significance. CIRCOS can be applied to the exploration of data sets involving complex relationships between large numbers of factors. The FDR has been transformed for graphic representation purposes into 2^ (-log 10 FDR). In order to identify potential novel pharmacological strategies for the treatment of SARS-CoV 2 infection, we used The L1000fwd, Large Scale Visualization of Drug Induced Transcriptomic Signatures web-based utility [18] . L1000fwd calculates the similarity between an input gene expression signature and the LINCS-L1000 data, in order to rank drugs potentially able to reverse the transcriptional signature [18] . The compounds, over a range of concentrations and time [18] . An adjusted p-value (q-value) of 0.05 has been considered as threshold for statistical significance. Combined score is calculated by multiplying the logarithm of the p-value from the Fisher exact test and the Z-score as a composite index: =z•log 10 ( ) For statistical analysis, Prism 8.0.2 software (GraphPad Software, USA) was used. Based on Shapiro-Wilk test, almost all data were normal, so parametric tests were used. Significant differences between groups were assessed using the Ordinary one-way ANOVA test, and Tukey's multiple comparisons test was performed to compare data between all groups. Correlations were determined using Pearson correlation. All tests were two-sided and significance was determined at P < 0.05. All MD selected were transformed for the analysis in Z-score intensity signal. Z score is constructed by taking the ratio of weighted mean difference and combined standard deviation according to Box and Tiao (1992) [19] . The application of a classical method of data normalization, z-score transformation, provides a way of standardizing data across a wide range of experiments and allows the comparison of microarray data independent of the original hybridization intensities. The z-score it is considered a reliable procedure for this type of analysis and can be considered a state-of-the-art methods, as demonstrated by the numerous bibliography [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] . Differential expression analysis was performed using the MeV 4.9 TM4 software, which used R v.2.11.1 and LIMMA v3.4.5. Principal Component Analysis (PCA) was performed to evaluate the segregation of the genes according to the cells treatment. The PCA was performed with PAST-4 a free software for scientific data analysis (https://folk.uio.no/ohammer/past/), with functions for data manipulation, plotting, univariate and multivariate statistics, ecological analysis, time series and spatial analysis, morphometric and stratigraphy. The efficiency of each biomarker was assessed by the receiver operating characteristic (ROC) curve analyses. The area under the ROC curve (AUC) and its 95% confidence interval (95% CI) indicate diagnostic efficiency. The accuracy of the test with the percent error is reported [32] . In order to identify a specific gene signature characterizing bronchial epithelial cells (NHBE) infected with SARS-CoV 2 for 24hrs, we first interrogated the GSE147507 dataset. We identified 105 DEGs in NHBE cells infected with SARS-CoV 2 as compared to MOCK controls (40 upregulated and 56 downregulated genes) (Table S1 ). By carrying out a restrictive analysis (gene-level read counts >= 10, pvalue<0.01), we highlighted 13 upregulated and 18 downregulated genes ( Figure 1a ) (Table S1 ). Among these genes, we have excluded for future analysis the c17orf67, ANKAR, LOC401109, and DBIL5P genes, currently without a characterized function. GO analysis revealed a partial overlapping of enriched biological processes among the upregulated DEGs in NHBE cells infected with SARS-CoV 2 , that included, among the first six significant one, the ""collagen metabolic process"", ""multicellular organismal macromolecule metabolic process"", ""extracellular matrix disassembly"", ""multicellular organismal metabolic process"", ""collagen catabolic process"", and ""multicellular organismal catabolic process"" (Figure 1b/c/d) (Table S2) . As regard the downregulated DEGs in NHBE cells infected with SARS-CoV 2 , we highlighted several biological overlapped processes, among which the first six significant were ""extracellular matrix organization"", ""actin cytoskeleton"", ""muscle contraction"", ""contractile fiber part"", ""muscle system process"", and ""extracellular matrix disassembly"" (Figure 1e/f/g) (Table S2) . CSF3 upregulation and DNAH7 down-regulation may induce granulocytes, the monocytes-macrophages differentiation, and the reduction of function of respiratory cilia, respectively. The MeV Performed SDEGs analysis showed 12 genes high modulated in NHBE cells infected with SARS-CoV 2 (RNA count >10 reads, p<0.01). Among these genes, granulocyte colony-stimulating factor (CSF3) was the top up modulated. Its expression is linked to the production, differentiation, and function of two related white cell populations of the blood, the granulocytes, and the monocytes-macrophages. As regards the 17 most downregulated genes, dynein heavy chain 7, axonemal (DNAH7) was the top down-modulated. This gene produces force towards the minus ends of microtubules, and consequently generate the force of respiratory cilia. The 12 genes significantly upregulated in NHBE infected cells, has been segregated according to the gene functional characteristics, returned by STRING and GeneMania (Table S2 ). The groups obtained were heterogeneous. Several mechanisms appear to be related to the infection of SARS-CoV 2 virus on NHBE cells. In particular, we have targeted our investigation on the cluster that included CSF3. This gene cluster has been identified as ""innate immunity recruitment"" group. Three genes out of 12 belong to this group, and were: the CSF3 (Figure 2a) , the transcriptional and immune response regulator (c8orf4) (Figure 2b ), and the carcinoembryonic antigen-related cell adhesion molecule 7 (CEACAM7) (Figure 2c ). When we segregated the 15-downregulated genes highlighted during NHBE cell infection, mechanisms were consistent with the GO analysis previously carried out. We selected a genes cluster composed by the Dynein heavy chain 7 (DNAH7) (Figure 3a) , the P21 (RAC1) activated kinase 5 (PAK7) (Figure 3b ), the thrombospondin type-1 domain-containing protein 7A (THSD7A) (Figure 3c) , and the RCSD domain containing 1 (RCSD1) genes (Figure 3d ), belonging to the ""mechanisms of cytoskeletal organization"" ( Table S2 ). The 27 SDEG identified were used to perform a PCA analysis on samples of NHBE cell, MOCK treated or infected with SARS-CoV 2 , obtained from the GSE147507 dataset. As shown in Figure 3a and b, strong separation of samples from MOCK and infected SARS-CoV 2 was observed. We highlighted in red CEACM7, CSF3, DNAH7 and c8orf4, which were strongly modulated in our analysis. In order to evaluate the potential diagnostic ability of these genes to discriminate against the SARS-CoV 2 virus infection, we  In order to identify possible common strategies among the main pandemic viruses, such as SARS-CoV 2 , SARS-CoV1, MERS-CoV, and H1N1, we carried out a Venn analysis of the main genes transcribed in the cells of the respiratory tract exposed to these viruses. We have downloaded two further datasets from GEO, GSE47962 composed of the transcriptome of Human bronchial airway epithelium cells (HAE) exposed to SARS-CoV (MOI 1 for 24hrs) or H1N1 (MOI 1 for 24hrs), and the GSE81909 composed of HAE infected cells with the MERS-CoV virus (5 PFU x cells for 24hrs) ( Table 1 ). The statistical analysis with GEO2R of GSE47962 highlighted 393 upregulated genes and 329 downregulated genes in HAE cells exposed to the SARS-CoV virus, and 5216 upregulated genes and 7258 downregulated genes in HAE cells infected with H1N1 (Table 1 ) (Table S3 ) (Figure 4a and b) . As regards the effect of MERS-CoV on HAE cells transcriptome, we showed 7039 genes upregulated and 6838 genes downregulated (Table 1 ) (Table S3 ) (Figure 4a with that of MERS-CoV, and 2.1% (p=0.241, RF=0.5) with that of SARS-CoV (Table 2) . Furthermore, 11 genes were in common between COVID-19, H1N1, and MERS-CoV, among which we highlighted CSF3 (Figure 4a and d) (Table S3) . DNAH7 together with other 13 genes, was in common between SARS-CoV 2 , SARS-CoV, and H1N1-induced transcriptomes (Figure 4a and e) . Instead, CEACAM7 was one of the 62 genes in common between SARS-CoV, H1N1, and MERS-CoV-induced transcriptomes (Figure 4b and f) (Table S3 ).  Anti-signature perturbation analysis was performed using the DEGs (GSE147507) identified for the HNBE cells infected with SARS-CoV 2 (Figure 5a , b) (Table S3) . Among the significantly predicted drugs, we only highlighted those that are already in clinical use (Launched) ( Table 3) . We chose to list in Table 3 the potential six anti-SARS-CoV 2 drugs identified by the L1000FWD analysis using the HNBE cell model infection. The complete list can be retrieved in Table S3 . Desoximetasone, a glucocorticoid receptor agonist, used in corticosteroid-responsive dermatoses (Table S3) (Table 3 ) (Figure 5b ). The human respiratory epithelium is the primary target of SARS-CoV 2 , SARS-CoV, MERS-Cov, and influenza viruses H1N1. The direct and indirect effects of SARS-CoV 2 on the respiratory epithelium, and the severity of such process may lead to detrimental anatomical and structural damages, respiratory distress syndrome and death. Viral replication induces a variety of transcriptional events, which pathophysiological consequences need to be dissected. The integrity of the bronchial epithelium is fundamental for the maintenance of respiratory functions, as well as for limiting the traffic of the immune system locally. Alterations of the bronchial parenchyma cause devastating effects on respiratory functions, [33] . In our investigation, we focused our attention on two main processes, the immuno-cells recruitment and structure and function perturbation of muscles and bronchial cilia. We found that SARS-CoV 2 induces the modulation of CSF3, DNAH7, CEACAM7 and c8orf4. CSF3 is involved in granulocytes and the monocytes-macrophages differentiation and recruitment to the damaged area, while the other genes contribute to the rearrangement of the extracellular matrix and collagen metabolism. Moreover, actin cytoskeleton and muscle contraction are highly down regulated by SARS-CoV 2 infection. The immunological, structural and physiological perturbations induced by SARS-CoV 2 critically contribute to the respiratory distress that characterizes COVID-19. Indeed, tissue destruction, cell recruitment and the inflammatory response, the altered processes of matrix and collagen deposition, and tissue regeneration along with cilia impairment impede the primary function of the respiratory system, namely, to take in oxygen and eliminate carbon dioxide. We highlighted that three genes, belonging to the processes of immune cells recruitment (CSF3, c8orf4, and CEACAM7), were significantly highly expressed in NHBE cells infected by SARS-CoV 2 . Furthermore, we showed that four genes that play a fundamental role in the structuring of bronchial cilia, and therefore in the processes of muco-ciliary movement, were significantly downregulated by the effect of viral infection. In addition, these genes were able to discriminate the viral infection compared to In endothelial cells, the c8orf4 expression (also known as TCIM) gene, it would seem to enhance key inflammatory mediators and inflammatory response through the modulation of NF-kappaB transcription [36] . In addition, the TCIM protein enhances inflammatory parameters such as monocyte-endothelial adhesion and endothelial monolayer permeability, mechanisms present in virus airway infection and most likely in the respiratory tract of COVID-19 patients [37] . These key features in cellular recruitment processes could explain why its expression is the only one in common among the four pandemic viral infections analyzed. As regards to CEACAM7, this gene belongs to the immunoglobulin superfamily and in epithelial cells plays a role with innate immunity [38] . Noteworthy, the MERS-CoV virus uses the CEACAM5 (belonging to the same family of CEACAM 7) protein to infect the cells of the bronchial epithelium [39] . The bronchial epithelium cells mechanisms of cytoskeletal rearrangement could represent a fundamental element in SARS-CoV 2 infection. In particular, cytoskeletal changes could significantly undermine the ability of these cells to activate ciliary movement. To confirm this hypothesis, our analysis revealed that the expression levels of the DNAH7, PAK7, THSD7A, and RCSD1 genes were significantly downregulated in the NHBE cells treated with the SARS-CoV 2 , compared to the MOCK control. These genes could be linked to the ""mechanism of respiratory Cilia regulation"". In particular, as stated above the DNAH7 gene plays important role in the movement of bronchial cilia [40] , and its reduction is linked to ciliary dysmotility [41] . Interesting to note that in respiratory syncytial virus infection, a major cause of respiratory disease, there is increased ciliary dyskinesia combined with ciliary loss and epithelial damage, likely to result in reduced mucociliary clearance [42] . This condition resembles that caused by SARS-CoV 2 [43] . It is interesting to note that DNAH7A expression levels were significantly downregulated in both NHBE cells infected with SARS-COV-2 and in those infected with MERS-CoV and H1N1. Indeed, it has been shown that MERS-CoV, H1N1, but not SARS-CoV, rapidly induces apoptosis of Human Bronchial Epithelial Cells [44, 45] . As regards PAK7, it is known that it is involved in a variety of different signaling pathways including cytoskeleton regulation by microtubule protein phosphorylation, cell migration, proliferation or cell survival [46] . The repression of PAK7 expression triggers the apoptotic cascade that leads to apoptosis through the inhibition of BAD phosphorylation [47] . The expression of THSD7A gene, promotes endothelial cell migration and filopodia formation [48] . It is plausible, given its role, that the reduction of THSD7A is associated with a reduction of Bronchial Epithelial Cell Migration Dynamics. As regards to RCSD1, it may regulate the ability of F-actin-capping protein to remodel actin filament assembly (Figure 3e ) [49] . The importance of actin filaments for the integrity of the epithelial barrier has long been recognized [50] . Our analysis also showed that drugs currently used to treat other diseases could modulate the gene counter the infection. Interestingly, in our study, the thalidomide and lenalidomide (two immunomodulatory drugs) were predicted to be a potential anti-SARS-CoV 2 drug, when using in NHBE cells. This is in line with recent evidence [55, 56] showing that thalidomide and lenalidomide could reduce the immuno-activation. The flunisolide is a corticosteroid prescribed for the treatment of allergic rhinitis. The flunisolide's main action mechanism is to activate glucocorticoid receptors, and as a consequence, an anti-inflammatory action. Furthermore, the inhibitory effect of inhaled flunisolide on inflammatory functions of alveolar macrophages has already been demonstrated [57] . Interestingly, the HFA-flunisolide effectively suppress eosinophilic inflammation in peripheral and central airways, and these changes are accompanied by improvement in lung function [58] . In addition, our analysis identified salmeterol, xylazine, and desoximetasone, as potential anti-SARS-CoV 2 drugs. Salmeterol is used by inhalation to reduce bronchospasm in some pathological conditions such as asthma and other obstructive respiratory diseases [59] . Furthermore, an anti-inflammatory action has been observed at the level of the bronchial epithelium [60] . As regard to desoximetasone, this is a synthetic glucocorticoid receptor agonist with metabolic, anti-inflammatory and immunosuppressive activity. It is used in the treatment of many conditions, including rheumatic problems, severe allergies, asthma, chronic obstructive lung disease, and along with antibiotics in tuberculosis [61] . Among the predictions was xylazine. Most likely, this has been called into question by prediction analysis for its muscle relaxant effect, against bronchospasms caused by respiratory infection. Its choice is extremely controversial, by virtue of the side effects it could cause, including respiratory depression [62] . to total ineffectiveness of the respiratory system ( Figure 6 ). A weakness of our study is that it is based on an experiment in vitro performed in bronchial cell epithelial obtained from 1 subject (female, 79 years old). The age of this subject is critical because age could dramatically influence response to the virus. It is also true that our study represents a ""picture"" of a currently evolving event in which all information can make a difference in the idea of a therapeutic strategy. The strength of our manuscript is to be the first to identify the potential genes responsible for muco-ciliary atheration and the potential drugs that can reverse this process. Cell damage and repair mechanisms contribute to tissue restructuration. Furthermore, other anatomical, physiological and functional barriers (i. e. respiratory cilia) also contribute to viral clearance and tissue regeneration. Knowing the mechanism involved in the pathogenesis of coronavirus induced respiratory and systemic damage is essential to target therapeutic intervention. In the present study, we investigated the effect of SARS-CoV 2 on human bronchial epithelial cells (HBEC). We found that the upregulation and down-regulation of CSF3 and DNAH7, along with other genes overlapping the same paths, are the most modulated genes in NHBE cells infected with COVID-19. We hypothesized that respiratory cilia are impaired by SARS-CoV 2 infection of epithelial cells, while apoptosis, matrix destructuration and collagen deposition do occur, potentially leading to respiratory distress syndrome (RDS). These pathogenetic factors could be critical to dissect future clinical and therapeutic interventions. The aim of our study was to investigate the effect of SARS-CoV 2 infection on the bronchial parenchyma. We hypothesized that SARS-CoV 2 could modulate bronchial cells of COVID-19 patients at multiple anatomical and physiological levels and regulate the cytoskeletal structures. Furthermore, our hypothesis predicted that these changes were specific to COVID-19 infection and not common to other pandemic virus of airways such as SARS-CoV, MERS-CoV, and H1N1. In order to test our hypothesis we have collected and analyzed several microarray datasets available on NCBI Gene Expression Omnibus (GEO) database (http://www.ncbi.nlm.nih.gov/geo/) [5] [6] [7] . Mesh terms ""coronavirus"", ""Human"", and ""airway epithelial cells"", were used to identify human potential datasets of interest. Three datasets were selected (GSE147507, GSE47962, GSE81909) ( Table 1) . The GSE147507 dataset [8] was composed of Normal Human Bronchial Epithelial cells (NHBE) and transformed lung alveolar (A549) cells treated with MOCK or infected with SARS-CoV 2 (USA-WA1/2020) at different MOI (NHBE: 2, A549: 0.2) for 24 h. The authors declared that cDNA libraries were sequenced using an Illumina NextSeq 500 platform (Illumina, CA), following differential expression analysis using DESeq2. As for our investigation, we focused only on the effect of SARS-CoV 2 on NHBE cells. The submitter-supplied pre-preprocessed and normalized expression matrix was used for this re-analysis. The GSE147507 RawReadCounts7, subsequently was used for the identification of Differentially Expressed Genes (DEGs). From GSE47962 we downloaded the data of Human bronchial airway epithelium (HAE) cells, seeded in 6-well plates (1 � 10e6 cells/ well) two days prior to infection and then inoculated with wild type infectious clone derived SARS-CoV viruses (icSARS) (MOI ¼ 2) or H1N1 (MOI ¼ 1) [9] . Mock-infected controls were inoculated with culture medium only. For our analysis, we sorted only the data of 24 h treatment. From GSE81909 we selected the transcriptome of primary human airway epithelial cells infected with a multiplicity of infection of 5 PFU per cell of wild type MERS-coronavirus (MERS-CoV) (icMERS). Furthermore, we sorted only the transcriptome corresponding to 24 h of treatment. Time-matched mocks were collected in parallel with infected samples. Complete experimental details are available in referenced publications or in the GEODatabase correspondent webpages (Table 1) . In order to process and identify Significantly Different Expressed Genes (SDEG) in all selected datasets, we used the MultiExperiment Viewer (MeV) software (The Institute for Genomic Research (TIGR), J. Craig Venter Institute, USA). In cases where multiple genes probes insisted on the same GeneID, we used those with the highest variance. The significance threshold level for all data sets was p < 0.05. Statistically significant genes were selected for further analysis. For GSE47962 and GSE81909 datasets we performed a statistical analysis with GEO2R, applying a Benjamini & Hochberg FDR (False discovery rate) to adjust P values for multiple comparisons [10, 11] . For the analysis of GSE147507, we used a conservative approach. We have set a filtering cutoffs of gene-level read counts �10, for differential gene expression analysis, in order to reduce artificial variance that results in differential expression or splicing calls, and then standardized the data with z-score transformation [12] . For the identification of the Differentially Expressed Genes (DEGs) in the HNBE cells, the LIMMA (Linear models for microarray data) (MeV) parametric test was used. An adjusted p-value < 0.05 was considered to indicate a statistically significant difference. The genes Ontology analysis was performed using the web utility GeneMANIA (http://genemania.org/, http://genemania.org/) [13] and the GATHER (Gene Annotation Tool to Help Explain Relationships) (http://changlab.uth.tmc.edu/gather/) [14] . The GeneMania was also used to built the weighted gene networks commonly modulated. The database assembles all available interaction data in the dataset by creating large networks, which captures the current knowledge on the functional modularity and interconnectivity of genes in a cell. Gene's annotation was obtained by STRING software (https://string-db.org/). The STRING-combined score was based on data from neighborhood in the genome, gene fusions, co-occurrence across genomes, co-expression, experimental/biochemical data, and association in curated databases [15] . The genes overlapping was represented with Venn diagram using a public online tool (http://bioinformatics.psb.ugent.be/webtools/Venn /) and the images readapted for our data with CorelDraw. The Gene set enrichment analysis (GSE) and gene ontology (GO), were expressed in weighted percentage and graphically rendered in a circular diagram format using freely available CIRCOS software (http://circos.ca/) [7, 16, 17] . Ribbon size encodes gene number and FDR associated row/column segments with high significance. CIRCOS can be applied to the exploration of data sets involving complex relationships between large numbers of factors. The FDR has been transformed for graphic representation purposes into 2^ (-log 10 FDR). In order to identify potential novel pharmacological strategies for the treatment of SARS-CoV 2 infection, we used The L1000fwd, Large Scale Visualization of Drug Induced Transcriptomic Signatures web-based utility [18] . L1000fwd calculates the similarity between an input gene expression signature and the LINCS-L1000 data, in order to rank drugs potentially able to reverse the transcriptional signature [18] . The L1000 transcriptomic database belonging to the Library of Integrated Network-based Cellular Signatures (LINCS) project, a NIH Common Fund program, that extended the Connectivity Map project and includes the large transcriptional profiles of approximately 50 human cell lines upon exposure to about 20,000 compounds, over a range of concentrations and time [18] . An adjusted p-value (q-value) of 0.05 has been considered as threshold for statistical significance. Combined score is calculated by multiplying the logarithm of the p-value from the Fisher exact test and the Z-score as a composite index: C ¼ z⋅log 10ðpÞ For statistical analysis, Prism 8.0.2 software (GraphPad Software, USA) was used. Based on Shapiro-Wilk test, almost all data were normal, so parametric tests were used. Significant differences between groups were assessed using the Ordinary one-way ANOVA test, and Tukey's multiple comparisons test was performed to compare data between all groups. Correlations were determined using Pearson correlation. All tests were two-sided and significance was determined at P < 0.05. All MD selected were transformed for the analysis in Z-score intensity signal. Z score is constructed by taking the ratio of weighted mean difference and combined standard deviation according to Box and Tiao (1992) [19] . The application of a classical method of data normalization, z-score transformation, provides a way of standardizing data across a wide range of experiments and allows the comparison of microarray data independent of the original hybridization intensities. The z-score it is considered a reliable procedure for this type of analysis and can be considered a state-of-the-art methods, as demonstrated by the numerous bibliography [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] . Differential expression analysis was performed using the MeV 4.9 TM4 software, which used R v.2.11.1 and LIMMA v3.4.5. Principal Component Analysis (PCA) was performed to evaluate the segregation of the genes according to the cells treatment. The PCA was performed with PAST-4 a free software for scientific data analysis (http s://folk.uio.no/ohammer/past/), with functions for data manipulation, plotting, univariate and multivariate statistics, ecological analysis, time series and spatial analysis, morphometric and stratigraphy. The efficiency of each biomarker was assessed by the receiver operating characteristic (ROC) curve analyses. The area under the ROC curve (AUC) and its 95% confidence interval (95% CI) indicate diagnostic efficiency. The accuracy of the test with the percent error is reported [32] . In order to identify a specific gene signature characterizing bronchial epithelial cells (NHBE) infected with SARS-CoV 2 for 24 h, we first interrogated the GSE147507 dataset. We identified 105 DEGs in NHBE cells infected with SARS-CoV 2 as compared to MOCK controls (40 upregulated and 56 downregulated genes) (Table S1 ). By carrying out a restrictive analysis (gene-level read counts �10, pvalue<0.01), we highlighted 13 upregulated and 18 downregulated genes ( Fig. 1a ) (Table S1 ). Among these genes, we have excluded for future analysis the c17orf67, ANKAR, LOC401109, and DBIL5P genes, currently without a characterized function. GO analysis revealed a partial overlapping of enriched biological processes among the upregulated DEGs in NHBE cells infected with SARS-CoV 2 , that included, among the first six significant one, the ""collagen metabolic process"", ""multicellular organismal macromolecule metabolic process"", ""extracellular matrix disassembly"", ""multicellular organismal metabolic process"", ""collagen catabolic process"", and ""multicellular organismal catabolic process"" (Fig. 1b /c/d) (Table S2) . As regard the downregulated DEGs in NHBE cells infected with SARS-CoV 2 , we highlighted several biological overlapped processes, among which the first six significant were ""extracellular matrix organization"", ""actin cytoskeleton"", ""muscle contraction"", ""contractile fiber part"", ""muscle system process"", and ""extracellular matrix disassembly"" (Fig. 1e/f/g) (Table S2) . The MeV Performed SDEGs analysis showed 12 genes high modulated in NHBE cells infected with SARS-CoV 2 (RNA count >10 reads, p < 0.01). Among these genes, granulocyte colony-stimulating factor (CSF3) was the top up modulated. Its expression is linked to the production, differentiation, and function of two related white cell populations of the blood, the granulocytes, and the monocytesmacrophages. As regards the 17 most downregulated genes, dynein heavy chain 7, axonemal (DNAH7) was the top down-modulated. This gene produces force towards the minus ends of microtubules, and consequently generate the force of respiratory cilia. The 12 genes significantly upregulated in NHBE infected cells, has been segregated according to the gene functional characteristics, returned by STRING and GeneMania (Table S2 ). The groups obtained were heterogeneous. Several mechanisms appear to be related to the infection of SARS-CoV 2 virus on NHBE cells. In particular, we have targeted our investigation on the cluster that included CSF3. This gene cluster has been identified as ""innate immunity recruitment"" group. Three genes out of 12 belong to this group, and were: the CSF3 (Fig. 2a) , the transcriptional and immune response regulator (c8orf4) (Fig. 2b) , and the carcinoembryonic antigen-related cell adhesion molecule 7 (CEACAM7) (Fig. 2c) . When we segregated the 15-downregulated genes highlighted during NHBE cell infection, mechanisms were consistent with the GO analysis previously carried out. We selected a genes cluster composed by the Dynein heavy chain 7 (DNAH7) (Fig. 3a) , the P21 (RAC1) activated kinase 5 (PAK7) (Fig. 3b) , the thrombospondin type-1 domain-containing protein 7A (THSD7A) (Fig. 3c) , and the RCSD domain containing 1 (RCSD1) genes (Fig. 3d) , belonging to the ""mechanisms of cytoskeletal organization"" (Table S2) . The 27 SDEG identified were used to perform a PCA analysis on samples of NHBE cell, MOCK treated or infected with SARS-CoV 2 , obtained from the GSE147507 dataset. As shown in Fig. 3a and b, strong separation of samples from MOCK and infected SARS-CoV 2 was observed. We highlighted in red CEACM7, CSF3, DNAH7 and c8orf4, which were strongly modulated in our analysis. In order to evaluate the potential diagnostic ability of these genes to discriminate against the SARS-CoV 2 virus infection, we performed a Receiver operating characteristic (ROC) analysis. We confirmed the diagnostic ability of CSF3 (p ¼ 0.04) (Fig. 3c) , DNAH7 (p ¼ 0.04) (Fig. 3d) , CEACAM7 (p ¼ 0.04) (Fig. 3e) , and c8orf4 (p ¼ 0.04) (Fig. 3f) to discriminate the NHBE cells infected by SARS-CoV 2 from MOCK treated. In order to identify possible common strategies among the main pandemic viruses, such as SARS-CoV 2 , SARS-CoV1, MERS-CoV, and H1N1, we carried out a Venn analysis of the main genes transcribed in the cells of the respiratory tract exposed to these viruses. We have downloaded two further datasets from GEO, GSE47962 composed of the transcriptome of Human bronchial airway epithelium cells (HAE) exposed to SARS-CoV (MOI 1 for 24 h) or H1N1 (MOI 1 for 24 h), and the GSE81909 composed of HAE infected cells with the MERS-CoV virus (5 PFU x cells for 24 h) ( Table 1 ). The statistical analysis with GEO2R of GSE47962 highlighted 393 upregulated genes and 329 downregulated genes in HAE cells exposed to the SARS-CoV virus, and 5216 upregulated genes and 7258 downregulated genes in HAE cells infected with H1N1 (Table 1) (Table S3 ) ( Fig. 4a and b) . As regards the effect of MERS-CoV on HAE cells transcriptome, we showed 7039 genes upregulated and 6838 genes downregulated (Table 1 ) (Table S3 ) ( Fig. 4a and b) . The Venn diagrams (Fig. 4a/b) showed that the upregulated transcriptome of the cells infected with SARS-CoV 2 uniquely overlapped to that of MERS-CoV (10%, 4 genes), H1N1 (15%, 6 genes), and SARS-CoV (2.5%, one gene). Only one gene, c8orf4, was in common between SARS-CoV, H1N1, MERS-CoV, and SARS-CoV 2 (Fig. 4a and c) (Table 2) . Furthermore, 11 genes were in common between COVID-19, H1N1, and MERS-CoV, among which we highlighted CSF3 (Fig. 4a and d) (Table S3) . DNAH7 together with other 13 genes, was in common between SARS-CoV 2 , SARS-CoV, and H1N1-induced transcriptomes (Fig. 4a and e) . Instead, CEACAM7 was one of the 62 genes in common between SARS-CoV, H1N1, and MERS-CoV-induced transcriptomes ( Fig. 4b and f) (Table S3) .  Anti-signature perturbation analysis was performed using the DEGs (GSE147507) identified for the HNBE cells infected with SARS-CoV 2 ( Fig. 5a and b) (Table S3) . Among the significantly predicted drugs, we only highlighted those that are already in clinical use (Launched) ( Table 3) . We chose to list in Table 3 the potential six anti-SARS-CoV 2 drugs identified by the L1000FWD analysis using the HNBE cell model infection. The complete list can be retrieved in Table S3 . Among them: Flunisolide, a cytochrome P450 inhibitor, used for the allergic rhinitis; Xylazine, a α2 class of adrenergic receptor agonist, used as an anesthetic; Salmeterol, an adrenergic receptor agonist, used for asthma, chronic obstructive pulmonary disease (COPD), bronchospasm; Thalidomide, a tumor necrosis factor production inhibitor, used as an immunomodulatory agent; Lenalidomide, an anticancer agent, used in multiple myeloma, in mantle cell lymphoma (MCL), and in myelodysplastic diseases (MDS); Desoximetasone, a glucocorticoid receptor agonist, used in corticosteroid-responsive dermatoses (Table S3 ) ( Table 3 ) (Fig. 5b) . The human respiratory epithelium is the primary target of SARS-CoV 2 , SARS-CoV, MERS-Cov, and influenza viruses H1N1. The direct and indirect effects of SARS-CoV 2 on the respiratory epithelium, and the severity of such process may lead to detrimental anatomical and structural damages, respiratory distress syndrome and death. Viral replication induces a variety of transcriptional events, which pathophysiological consequences need to be dissected. The integrity of the bronchial epithelium is fundamental for the maintenance of respiratory functions, as well as for limiting the traffic of the immune system locally. Alterations of the bronchial parenchyma cause devastating effects on respiratory functions, [33] . In our investigation, we focused our attention on two main processes, the immuno-cells recruitment and structure and function perturbation of muscles and bronchial cilia. We found that SARS-CoV 2 induces the modulation of CSF3, DNAH7, CEACAM7 and c8orf4. CSF3 is involved in granulocytes and the monocytes-macrophages differentiation and recruitment to the damaged area, while the other genes contribute to the rearrangement of the extracellular matrix and collagen metabolism. Moreover, actin cytoskeleton and muscle contraction are highly down regulated by SARS-CoV 2 infection. The immunological, structural and physiological perturbations induced by SARS-CoV 2 critically contribute to the respiratory distress that characterizes COVID-19. Indeed, tissue destruction, cell recruitment and the inflammatory response, the altered processes of matrix and collagen deposition, and tissue regeneration along with cilia impairment impede the primary function of the respiratory system, namely, to take in oxygen and eliminate carbon dioxide. We highlighted that three genes, belonging to the processes of immune cells recruitment (CSF3, c8orf4, and CEACAM7), were significantly highly expressed in NHBE cells infected by SARS-CoV 2 . Furthermore, we showed that four genes that play a fundamental role in the structuring of bronchial cilia, and therefore in the processes of mucociliary movement, were significantly downregulated by the effect of viral infection. In addition, these genes were able to discriminate the viral infection compared to controls. The transcriptome overlapping analysis induced on bronchial epithelium cells infected with SARS-CoV 2 , SARS-CoV, MERS-CoV, and H1N1 has shown that the c8orf4 gene represents a common point of the virus diseases induced in our analysis and that CEACAM7 gene expression is exclusive of SARS-CoV 2 infection. Using the new prediction systems L1000FWD has allowed us to highlight that several drugs, already in clinical use (flunisolide, xylazine, salmeterol, thalidomide, lenalidomide, and desoximetasone) for the treatment of other diseases, may have an effect on SARS-CoV 2 viral infection. Paroxysmal immune activation triggered during SARS-CoV 2 infection is one of the causes of mortality of this disease. The dysregulation of the immune response allows the development of viral hyperinflammation. The uncontrolled immuno-activation, in particular the one triggered by innate immunity, mainly represented by the neutrophils and alveolar macrophages, is supported by the genes with chemotactic function. In this light, genes such as CSF3, CEACAM7 and c8orf4 represent a hot spot in this process. In particular, CSF3 is a modulator of neutrophil activation during the antiviral responses to human respiratory syncytial virus (hRSV), a major human pathogen that primarily targets the respiratory epithelium [34] . It is very interesting to note that in a cohort of 452 patients with COVID-19, 286 were diagnosed as severe infections, these patients tended to have lower lymphocytes (a and c) . Eleven genes, including CSF3, were commonly modulated by SARS-CoV 2 , MERS-CoV, and H1N1 (a and d). As regards the overlap of downregulated genes, we showed that no genes were modulated by the four viruses. Fourteen genes were shared between SARS-CoV 2 and H1N1, including DNAH7 (b and e). 62 genes were identified in common between H1N1, MERS-CoV, and SARS-CoV, including CEACAM7 (b and f). Data are expressed as RNA Read count and presented as violin plots. P values < 0.05 were considered to be statistically significant (*p < 0.05). counts, higher leukocytes count, and neutrophil-lymphocyte-ratio (NLR). This data could indicate that the recruitment of Neutrophils during SARS-CoV 2 infection could be due to production of CSF3 from the cells of the bronchial epithelium [35] . In our investigation, it emerged that MERS-CoV and H1N1 could use CSF3 for the recruitment of the immune system. This data is in agreement with the percentages of genomic overlap that we found in our analysis. SARS-CoV did not result in modulation of CSF3. These results surprised us. We would have expected a common line of SARS-CoV 2 , with MERS-CoV and SARS-CoV and not with H1N1. This high transcriptional overlap with H1N1 (almost 40) and with MERS-CoV (33.3%), represents an important point to explore in the future. In endothelial cells, the c8orf4 expression (also known as TCIM) gene, it would seem to enhance key inflammatory mediators and inflammatory response through the modulation of NF-kappaB transcription [36] . In addition, the TCIM protein enhances inflammatory parameters such as monocyte-endothelial adhesion and endothelial monolayer permeability, mechanisms present in virus airway infection and most likely in the respiratory tract of COVID-19 patients [37] . These key features in cellular recruitment processes could explain why its expression is the only one in common among the four pandemic viral infections analyzed. As regards to CEACAM7, this gene belongs to the immunoglobulin superfamily and in epithelial cells plays a role with innate immunity [38] . Noteworthy, the MERS-CoV virus uses the CEACAM5 (belonging to the same family of CEACAM 7) protein to infect the cells of the bronchial epithelium [39] . The entry of MERS-CoV is increased when CEACAM5 is overexpressed in target cells, suggesting that CEACAM5 could facilitate MERS-CoV entry in conjunction with DPP4 despite not being able to support MERS-CoV entry, independently. This result would suggest that CEACAM7 could play the same role in SARS-CoV 2 infection. Furthermore, the fact that in the infections caused by SARS-CoV, MERS-CoV, and H1N1 the expression of CEACAM7 was downregulated, would suggest that the action of CEACAM7 is exclusive of the infection caused by SARS-CoV 2 . The bronchial epithelium cells mechanisms of cytoskeletal rearrangement could represent a fundamental element in SARS-CoV 2 infection. In particular, cytoskeletal changes could significantly undermine the ability of these cells to activate ciliary movement. To confirm this hypothesis, our analysis revealed that the expression levels of the DNAH7, PAK7, THSD7A, and RCSD1 genes were significantly downregulated in the NHBE cells treated with the SARS-CoV 2 , compared to the MOCK control. These genes could be linked to the ""mechanism of respiratory Cilia regulation"". In particular, as stated above the DNAH7 drug (a) . The drugs with a high significance pvalue (qvalue) and a high combined score (flunisolide, xylazine, thalidomide, lenalidomide, desoximetasone, and salmeterol) were selected (b). Table 3 Potential anti-AD drugs identified by the L1000FWD analysis. gene plays important role in the movement of bronchial cilia [40] , and its reduction is linked to ciliary dysmotility [41] . Interesting to note that in respiratory syncytial virus infection, a major cause of respiratory disease, there is increased ciliary dyskinesia combined with ciliary loss and epithelial damage, likely to result in reduced mucociliary clearance [42] . This condition resembles that caused by SARS-CoV 2 [43] . It is interesting to note that DNAH7A expression levels were significantly downregulated in both NHBE cells infected with SARS-COV-2 and in those infected with MERS-CoV and H1N1. Indeed, it has been shown that MERS-CoV, H1N1, but not SARS-CoV, rapidly induces apoptosis of Human Bronchial Epithelial Cells [44, 45] . As regards PAK7, it is known that it is involved in a variety of different signaling pathways including cytoskeleton regulation by microtubule protein phosphorylation, cell migration, proliferation or cell survival [46] . The repression of PAK7 expression triggers the apoptotic cascade that leads to apoptosis through the inhibition of BAD phosphorylation [47] . The expression of THSD7A gene, promotes endothelial cell migration and filopodia formation [48] . It is plausible, given its role, that the reduction of THSD7A is associated with a reduction of Bronchial Epithelial Cell Migration Dynamics. As regards to RCSD1, it may regulate the ability of F-actin-capping protein to remodel actin filament assembly (Fig. 3e) [49] . The importance of actin filaments for the integrity of the epithelial barrier has long been recognized [50] . Our analysis also showed that drugs currently used to treat other diseases could modulate the gene transcription mechanisms activated by the SARS-CoV 2 virus on NHBE cells. The finding of novel indications for already approved drugs allows their quick use due to the broad knowledge of toxicity, pharmacokinetic and pharmacodynamics. Recently, the use of computational studies has made it possible to investigate the pharmacological response to a determined disease in a faster way and reducing experimental laboratory costs [24] . Platforms such as L1000FWD are based on the evaluation of the anti-similarity between drugs and disease. Several studies have been carried out based on these procedures [51] [52] [53] [54] . Unfortunately, this type of approach has several limitations. The variation of the transcriptome alone cannot accurately predict the effects of the drug on the disease and vice versa. It is also true that the pandemic viral infection we are affected by has prompted us to speed up the investigation time. Even if full of limitations, computational analysis can show new strategies in order to counter the infection. Interestingly, in our study, the thalidomide and lenalidomide (two immunomodulatory drugs) were predicted to be a potential anti-SARS-CoV 2 drug, when using in NHBE cells. This is in line with recent evidence [55, 56] showing that thalidomide and lenalidomide could reduce the immuno-activation. The flunisolide is a corticosteroid prescribed for the treatment of allergic rhinitis. The flunisolide's main action mechanism is to activate glucocorticoid receptors, and as a consequence, an anti-inflammatory action. Furthermore, the inhibitory effect of inhaled flunisolide on inflammatory functions of alveolar macrophages has already been demonstrated [57] . Interestingly, the HFA-flunisolide effectively suppress eosinophilic inflammation in peripheral and central airways, and these changes are accompanied by improvement in lung function [58] . In addition, our analysis identified salmeterol, xylazine, and desoximetasone, as potential anti-SARS-CoV 2 drugs. Salmeterol is used by inhalation to reduce bronchospasm in some pathological conditions such as asthma and other obstructive respiratory diseases [59] . Furthermore, an anti-inflammatory action has been observed at the level of the bronchial epithelium [60] . As regard to desoximetasone, this is a synthetic glucocorticoid receptor agonist with metabolic, anti-inflammatory and immunosuppressive activity. It is used in the treatment of many conditions, including rheumatic problems, severe allergies, asthma, chronic obstructive lung disease, and along with antibiotics in tuberculosis [61] . Among the predictions was xylazine. Most likely, this has been called into question by prediction analysis for its muscle relaxant effect, against bronchospasms caused by respiratory infection. Its choice is extremely controversial, by virtue of the side effects it could cause, including respiratory depression [62] . It is conceivable that, in line with the results obtained in our investigation, during the course of SARS-CoV 2 infection, the virus descends the respiratory tract and interacts with the cells of the bronchial epithelium through a surface receptor, which could be couple with CEACAM7 as a co-receptor. Once entered, the virus triggers an inflammatory response that passes through several viral containment mechanisms. A series of genes are transcribed that deal with the rearrangement of the extracellular environment, and the recall of innate immunity cells (CSF3 and c8orf4) . Alveolar macrophages and neutrophils are recalled locally and activate the paroxysmal response, which results parenchymal destruction and edema. The bronchial epithelium changes its structure. The cytoskeleton of epithelial cells is deconstructed, due to the downregulation of genes such as DNAH7, PAK7, TSHD7A, and RCSD1. The ciliary movement of the bronchial epithelium is lost. Mechanisms of apoptosis are triggered which lead to total ineffectiveness of the respiratory system (Fig. 6) . A weakness of our study is that it is based on an experiment in vitro performed in bronchial cell epithelial obtained from 1 subject (female, 79 years old). The age of this subject is critical because age could dramatically influence response to the virus. It is also true that our study represents a ""picture"" of a currently evolving event in which all information can make a difference in the idea of a therapeutic strategy. The strength of our manuscript is to be the first to identify the potential genes responsible for muco-ciliary atheration and the potential drugs that can reverse this process. Not applicable. This article does not contain any studies with human participants or animals performed by any of the authors. The datasets analyzed during the current study are available in the GSE147507, GSE47962, GSE47962, and GSE81909 repository, https ://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc¼GSE17507. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. We would like to show our gratitude to the authors of microarray datasets made available online, for consultation and re-analysis. In addition, I would like to express my gratitude to Oliver Di Rosa, an inspiration in my life. Funding This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors. Supplementary data to this article can be found online at https://doi. org/10.1016/j.yexcr.2020.112204. Fig. 6 . Graphical representation of the main process predicted in NHBE cells infected with SARS-CoV 2 . Panel A and B show our hypothesis of key events in SARS-CoV 2 pathogenesis, which is based on extremely limited observations on an in vitro model of NHBE cell infected with SARS-CoV 2 at MOI 2 for 24 h. After the inoculation of SARS-CoV 2 , the NHBE cells were infected. We hypothesize that viral entry could be facilitated by another cell-surface protein, carcinoembryonic antigen-related cell-adhesion molecule 7 (CEACAM7), which is also expressed in gastrointestinal tract. This mechanism could be similar to that used by the MERS-CoV virus. In that case, the virus uses the CEACAM5 protein to infect the cells of the bronchial epithelium. Inflammatory signaling molecules that are released by infected cells (CSF3, c8orf4) , recruits.@story_separate@In our analysis, we provided the basis for the identification of potential gene targets for the (a and c) . Eleven genes, including CSF3, were commonly modulated by SARS-CoV 2 , MERS-CoV, and H1N1 (a and d). As regards the overlap of downregulated genes, we showed that no genes were modulated by the four viruses. Fourteen genes were shared between SARS-CoV 2 and H1N1, including DNAH7 (b and e). 62 genes were identified in common between H1N1, MERS-CoV, and SARS-CoV, including CEACAM7 (b and f). Data are expressed as RNA Read count and presented as violin plots. P values <0.05 were considered to be statistically significant (*p<0.05). The drugs with a high significance pvalue (qvalue) and a high combined score (flunisolide, xylazine, thalidomide, lenalidomide, desoximetasone, and salmeterol) were selected (b). Figure 6 . Graphical representation of the main process predicted in NHBE cells infected with SARS-CoV 2 . Panel A and B show our hypothesis of key events in SARS-CoV 2 pathogenesis, which is based on extremely limited observations on an in vitro model of NHBE cell infected with SARS-CoV 2 at MOI 2 for 24hrs. After the inoculation of SARS-CoV 2 , the NHBE cells were infected. We hypothesize that viral entry could be facilitated by another cell-surface protein, carcinoembryonic antigen-related cell-adhesion molecule 7 (CEACAM7), which is also expressed in gastrointestinal tract. This mechanism could be similar to that used by the MERS-CoV virus. In that case, the virus uses the CEACAM5 protein to infect the cells of the bronchial epithelium. Inflammatory signaling molecules that are released by infected cells (CSF3, c8orf4), recruits  In our analysis, we provided the basis for the identification of potential gene targets for the SARS-CoV 2 infection. Both immune recruitment and the destruction of the ciliary parenchyma could represent the two key points of the disease triggered by the SARS-CoV 2 infection. Future studies are now warranted in order to further confirm our results in a clinical setting. Furthermore, single and combined administration of potential anti-SARS-CoV 2 drugs deserves to be taken into consideration for more in-depth investigations.","BACKGROUND: SARS-CoV2, the agent responsible for the current pandemic, is also causing respiratory distress syndrome (RDS), hyperinflammation and high mortality. It is critical to dissect the pathogenetic mechanisms in order to reach a targeted therapeutic approach. METHODS: In the present investigation, we evaluated the effects of SARS-CoV(2) on human bronchial epithelial cells (HBEC). We used RNA-seq datasets available online for identifying SARS-CoV(2) potential genes target on human bronchial epithelial cells. RNA expression levels and potential cellular gene pathways have been analyzed. In order to identify possible common strategies among the main pandemic viruses, such as SARS-CoV(2), SARS-CoV1, MERS-CoV, and H1N1, we carried out a hypergeometric test of the main genes transcribed in the cells of the respiratory tract exposed to these viruses. RESULTS: The analysis showed that two mechanisms are highly regulated in HBEC: the innate immunity recruitment and the disassembly of cilia and cytoskeletal structure. The granulocyte colony-stimulating factor (CSF3) and dynein heavy chain 7, axonemal (DNAH7) represented respectively the most upregulated and downregulated genes belonging to the two mechanisms highlighted above. Furthermore, the carcinoembryonic antigen-related cell adhesion molecule 7 (CEACAM7) that codifies for a surface protein is highly specific of SARS-CoV(2) and not for SARS-CoV1, MERS-CoV, and H1N1, suggesting a potential role in viral entry. In order to identify potential new drugs, using a machine learning approach, we highlighted Flunisolide, Thalidomide, Lenalidomide, Desoximetasone, xylazine, and salmeterol as potential drugs against SARS-CoV(2) infection. CONCLUSIONS: Overall, lung involvement and RDS could be generated by the activation and down regulation of diverse gene pathway involving respiratory cilia and muscle contraction, apoptotic phenomena, matrix destructuration, collagen deposition, neutrophil and macrophages recruitment."
"Streptococcus intermedius belongs to the S. anginosus group (SAG) that also includes S. constellatus and S. anginosus [1] . It is part of the normal oral cavity and upper respiratory tract floras, as well as those of the gastrointestinal and female urogenital tracts [2] [3] [4] [5] . This bacterium was first described by Guthof in 1956 after being isolated from dental abscesses [6] . S. intermedius may also cause human infections, usually monomicrobial, including purulent abscesses of the liver, lungs, psoas, spine and/or central nervous system, and infective endocarditis [7] . Over the years, the role of S. intermedius in human infections has increasingly been reported. Patients with invasive S. intermedius infections were described to cause significantly longer hospital stays and higher mortality than patients with other S. anginosus group infections, suggesting that identifying this species might be important for the management of patients [8] . Various putative virulence factors have been described for Streptococcus intermedius, among which the ability to form biofilms to protect itself from antibiotics and the host immune system [9] , the production of hydrolytic enzymes, including both glycosaminoglycan-degrading enzymes, such as hyaluronidase and chrondroitin sulphate depolymerase, and glycosidases, such as α-N-acetylneuramidase (sialidase), β-D-galactosidase, N-acetyl-β-D-glucosaminidase and N-acetyl-β-D-galactosaminidase, which allow S. intermedius to grow on macromolecules found in host tissue [10] ; a cytotoxin, intermedilysin (ILY), that can directly damage host tissues and immune defense cells and participate in bacterial pathogenicity; and the surface protein antigens I/II that are involved in adhesion to fibronectin and laminin, which is an important step in the pathogenesis of endocarditis and abscess formation [11] . The development of high throughput nucleic acid sequencing technologies has enabled observing variations of the genetic repertoire among strains of a given bacterial species. Our present study analysis aimed at describing the genetic diversity and pathogenesis substratum of S. intermedius. Twenty-seven genomic sequences from S. intermedius strains, including 13 newly sequenced from our laboratory and 14 from public databases, were used for pan-genomic analysis. Predicted genes were compared among strains to determine the size of the core and dispensable gene pools, the pangenome, the gain/loss of putative virulence determinants, and to identify genomic islands.@story_separate@The 13 genome sequences determined in this study were deposited in GenBank and their accession numbers are listed in Table 1 . The genomic DNA (gDNA) of each studied S. intermedius strain was extracted in two steps: a mechanical treatment was first performed using acid-washed glass beads (G4649-500 g Sigma) and a FastPrep BIO 101 instrument (Qbiogene, Strasbourg, France) at maximum speed (6.5) for 90 s. following a 2-hour lysozyme incubation at 37°C, DNA was extracted using an EZ1 biorobot and the EZ1 DNA Tissue kit (Qiagen, Hilden, Germany). The elution volume was 50µL. Genomic DNA was quantified using the Qubit assay (Life technologies, Carlsbad, CA, USA). The gDNAs were sequenced using a MiSeq sequencer with the Paired-End strategy and the Nextera XT library kit (Illumina, Inc, San Diego, CA, USA). The Paired-End library was prepared using input solutions of 1 ng gDNAs. The gDNAs were fragmented at the tagmentation step. Then, limited cycle PCR amplification (12 cycles) completed the tag adapters and introduced dualindex barcodes. After purification on AMPure beads (Life technologies, Carlsbad, CA, USA), the libraries were normalized according to the Nextera XT protocol (Illumina). Normalized libraries were pooled for sequencing on a MiSeq sequencer (Illumina). Automated cluster generation and paired-end sequencing with dual index reads was performed in a single 39-hour run in a 2 × 250 bp format. The numbers of paired-end reads were summarized in Table 2 . The paired-end reads were trimmed and filtered according to the read qualities. After sequencing, the obtained reads were assembled using the A5 software [12] with default parameters and then contigs were compared to NCBI using BLASTn to remove contaminations. Then, the online tool Fasta dataset joiner (http://users-birc.au.dk/biopv/php/fabox/ fasta_joiner.php) was used to merge sequences into a single molecule. The Mauve software was used for multiple genomic sequence alignment [13] . Genes were annotated using the Prokka software with default parameters [14] in which the similarity e-value cut-off is 0.000001 and the minimum contig size is 200 bp. This pipeline also includes several other tools like Aragorn for tmRNA detection, Barnap to count rRNAs and Prodigal to identify coding sequences. To estimate the mean level of sequence similarity at the genome level among studied strains, we used the OrthoANI [15] and Genome-to-Genome Distance Calculator (GGDC) [16] softwares, with the following respective threshold values of 95-96 and 70 %. A 16 S rRNA-based phylogenetic analysis of the 27 studied S. intermedius strains was performed using the [17] . For constructing the phylogenetic tree, the following options were used: Maximum Likelihood method; Kimura 2-parameter model for substitution model; uniform rates among sites; partial deletion option for gaps/missing data; 1000 bootstrap replicates. Using genomic sequences and the Roary program [18] , a clustered heatmap of core genes was drawn on the basis of the presence/absence approach [18] . We also detected SNPs with the snp-sites program [19] from the core genome alignment and drew a phylogenetic tree with CGEwebface [20] . Virulence-associated genes were detected by comparing studied genomic sequences with the virulence factor database (VFDB) [21] and sequences described in recent publications [22] . The BLASTp search was performed using the threshold scores reported by Olson et al.: 35 % identity and highest scoring pair length of 50 % [22] . Additionally, we reviewed the literature to identify the proteins involved in interactions with the host [10, 23, 24] . A principal component analysis was performed using the XLSTAT program (Data Analysis and Statistical Solution for Microsoft Excel, Addinsoft, Paris, France 2017) in which the Fisher's least significant difference (LSD, α = 0.005) and Pearson's correlation coefficients were used, to detect any association of virulence-associated genes with specific clinical conditions. Get_homologue [25] was used to reveal orthologous genes among S. intermedius strains, using the following parameters: minimal coverage (-C) 40 %, minimum identity (-S) 50 %, minimum e-value (-E) 1e-05. Sequence similarity searches and clustering of coding sequence (CDS) from the 27 genomes were performed using pairwise BLASTp and OrthoMCL algorithms [26] . Sequential inclusion of all possible combinations of up to 27 strains were simulated and fitted by regression analysis [27] of the amount of conserved genes and of strainspecific genes. This allowed to estimate and extrapolate the sizes of core-and pan-genomes. Roary [18] was also used, with default parameters, to confirm the reliability of the obtained pan-genome analysis results (identity percent ≥ 70 %, coverage ≥ 70 %) and to generate the core genome alignment. The Clusters of Orthologous Groups (COGs) database was used to identify gene functions [28] using BLASTP (E-value 1e − 03 , coverage 0.7 and identity percent 30 %). A circular comparison of genomes was obtained using the online GView Server (https://server.gview.ca/) with S. intermedius strain ATCC 27,335 as reference genome [29] . ResFinder and the ARG-ANNOT database were used to search antibiotic resistance-related markers [30, 31] . The presence of CRISPR repeats and prophages was predicted using the CRISPRFinder [32] and PHAS TER softwares, respectively [33] . The 27 studied S. intermedius strains originated from China, Canada, South Korea, US, Japan and France. The patients' data was not available for some strains. The 13 French strains (G1552-G1557 and G1562-G1568, Tables 1 and 2) were isolated in our laboratory from patients with various infections (Table 2) , from August 2014 to November 2016, on 5 % sheep blood-enriched Columbia agar (BioMérieux) at 37°C in anaerobic atmosphere. Their identification was confirmed by the high scores (> 2) obtained using MALDI-TOF MS. In addition, 14 S. intermedius genome sequences were retrieved from GenBank. The 27 strains were divided into 8 groups according to their isolation source ( Table 2 ). The genome sizes and gene numbers among S. intermedius strains were relatively similar, consisting for each strain in a single chromosome but no plasmid was identified in any strains and ranging in size from 1.85Mbp to 2.05Mbp ( Table 2) . A schematic view of all 27 studied genomes is provided in (Fig. 1) , showing an overall high degree of conservation. The general features of S. intermedius genomes are summarized in Table 2 The G + C content of S. intermedius ranged from 37.3 to 37.8 % (avg 37.641 %, n = 27). All 13 in-house sequenced S. intermedius  The 16 S rRNA-based phylogenetic analysis (Fig. 2) , widely used as a gene marker to differentiate Streptococcus species [34] , demonstrated that all S. intermedius strains were grouped in a single cluster that was closelyrelated to S. anginosus and S. constellatus within the S. anginosus group [22] (Fig. 2 ). In the topology S. intermedius, S. constellatus and S. anginosus strains clustered together with their sub-species. However, the heatmap obtained using Roary [18] , based on the core genome, was more discriminatory within the species than the 16 S rRNA-based analysis and identified 3 clusters that were independent from the strain source (Fig. 3) . The three clusters are as follows: strains G1557, G1556, LC4, G1562, SK54, AJKN01, ATCC27335 and JTH08 constituted the first group, strains 30,309, G1563, G1564, 631SC0N and G1554 clustered in the second group while the remaining strains clustered in a third group. There was neither evidence of correlation between strain clusters and their clinical forms, nor between genomic types and the geographical origin of isolates. To measure the divergence between all studied strains at a deeper level, we also analyzed their phylogenetic relationships on the basis of core genome SNPs, which demonstrated that strains G1562, G1566 and FO413 diverged from other strains and exhibited a higher tendency of recombination. However, again no diseasespecific clustering was observed (Fig. 4) . Digital DNA-DNA hybridization (dDDH) values ranged from 80.5 to 99.3 % between all 27 strains, thus confirming their classification within a single species. This was also cross-validated by the OrthoANI program, which produced pairwise values ranging from 97.78 to 100 % which is well above the consensus 95-96 % threshold for prokaryotic species demarcation [35] . This corresponded to 100 % 16 S rDNA sequence identity across all studied isolates. The above data correlate with a strong degree of genome conservation and synteny. The overall distribution of S. intermedius proteins in COG categories was quite similar in all 27 studied strains (Fig. 5) . Previous studies of other Streptococcus species also suggested that, within a given species, the majority of strains had a similar COG profile [36] [37] [38] . Approximately 79.72 % of all proteins predicted in all strains were identified in COG superfamilies. The The alignment gaps tend to coincide with the regions of low G + C contents. The rings, from the inside out, display the size in kbp; GC skew; G + C content; followed by genomes as listed in the left legend proportion of each category fluctuated within a very small range, showing almost similar percentages of distribution in all strains. The most abundant subcategories were related to carbohydrate transport and metabolism (G) and translation, ribosomal structure and biogenesis (J) like their distribution in core genes. Less than half of strain-specific genes, but more than 90 % of core genes, had a match in the COGs database. The most abundant functions in core genes were associated with metabolism (Fig. 5a) . The overall proportion of metabolic functions in core genes was 32.47 %, whereas that in strain-specific genes was 9.58 %. More specifically, energy production and conversion (C), amino acid transport and metabolism (E), nucleotide transport and metabolism (F), carbohydrate transport and metabolism (G) and coenzyme transport and metabolism (H) were noticeably more abundant in core genes (p-value < 0.01) (Fig. 5b) . No mobilome-related functions were detected in S. intermedius. The functional category of information storage and processing showed highly different proportions in sub-categories (Fig. 5b) . The functions of translation, ribosomal structure and biogenesis (J) were significantly enhanced (p-value < 0.0001) in core genes, whereas the functions of replication, recombination and repair (L) were significantly enhanced (p-value < 0.01) in strain-specific genes. This trend was also observed in other bacteria [35] . In the cellular processing and signaling category, the function of defense mechanisms (V) was more abundant in strain-specific (p-value < 0.05) than in core genes (Fig. 5c) . The average number of new genes added by a novel genome was 40 when the 27th genome was added (Fig. 6) . The exponential decay model shown in Fig. 7a suggests that the number of conserved core genes approached an asymptote with the comparison of 27 genomes. A total of 1,355 core genes were identified in S. intermedius. The average proportion and sequence identity of core genes per strain were 72 and 97.79 %, respectively, indicating that core genes in S. intermedius are highly conserved and reflecting a low degree of intraspecies genomic variability too. Examination of the functional annotation of these core genes suggests, as expected, that they encode mostly core metabolic processes. A total of 1,054 strain-specific genes were identified in S. intermedius and the average number of strain-specific genes was 39 (Fig. 7b) . Among strain-specific genes, 148 genes were found in strain G1562, 107 in strain TYG1620, 105 in strain BA1, 96 in strain 32,811, 82 in strain G1557, 73 in strain C270, 69 in strain 631_SCON, 61 in strain G1555, 41 in strain G1554, 38 each in strains G1565 and F0413, 33 each in strains G1564 and LC4, 27 each in strains G1556 and ATCC27335, 20 in strain 30,309, 16 in strain G1553, 13 in strain G1552, 11 in strain B196, 6 in strain KCOM1545, 5 in strain FDAA RGOS_233 and 1 in strains G1563, G1566, G1567, JTH08, SK54AJKN01, respectively. The size of the pangenome increased steadily without reaching any plateau. The pangenome trend depicted in (Fig. 7b) shows a gradual expansion by addition of new genomes and thus the pangenome of S. intermedius may be considered as open and indicates a homogenous pattern of genome evolution with similar rates of gene gain/ loss process across the whole population. In addition, a total of 1,611 accessory genes that were shared by two or more strains were identified. Overall, we identified a S. intermedius pangenome of 4,020 genes including 1,355 core genes, 1, 054 strain-specific genes and 1,611 accessory genes. In the S. intermedius pangenome, 252 virulence factors were identified in total. Of these, 70 core virulence factors were shared by all strains and 78 unique virulence factors were present in one strain each (Table 3) . Virulence-associated genes present in all studied genomes included homologous virulence genes that contribute to bacterial avoidance of the immune system, such as ily which encodes an intermedilysin, the lmb, pspA, pavB/pfbB, fss3 genes coding surface proteins, the genes coding the polysaccharide capsule (cps4A, cps4B, cps4C, cps4D, cps8D), the auto-inducer LuxS (luxS), the binding proteins (pavA, hitC, fbpC, psaA, mntA, clpC, fss3), neuraminidase (nanA), hyaluronidase (hysA), and heat shock protein B (htpB), genes from the sil locus known to play a role in quorum-sensing and virulence in S. pyogenes (silA, silD, silE, salX), genes associated with secretion systems (lem11, lem15, sdeC, ceg32, esxA, essC, lpg2372, lirB), and genes associated with Mg2 + transport proteins (mgtB, mgtC); the response regulator CsrR betahemolysin gene (cylG), lamanin-binding surface protein like Pac and invasion protein inlA were also present in all strains. Among these core virulence genes, the surface protein antigen I/II that was demonstrated to play a potential role in S. intermedius pathogenesis [39] , and human fibronectin and laminin that are supposed to bind to this antigenic protein induce IL-18 release from monocytes [39] ; genes from the streptococcal invasion locus (sil) are related to enhanced virulence in the SAG group and may contribute to the invasive behavior of S. intermedius strains; the internalin (inlA), likely acquired from Listeria monocytogenes, increases the virulence of S. intermedius by playing a key role in attachment to host cells [40] ; the hyaluronidase (hysA) acts in the liquification of tissues and is also involved in biofilm formation, which protects bacteria from host defenses and antibiotics, and plays a role in infection [9] ; the ily-coded intermedilysin can directly damage host tissues and immune defense cells, causing human cell death by membrane bleb formation [23] . It has been also reported that intermedilysin helps in invasion and adhesion of bacteria to human liver cells, and in cytotoxicity [41] ; the galE gene codes galactose which plays a role in biofilm formation and its key residues are essential for epimerase activity [42] ; the [21] laminin-binding surface protein, homologous to that in Streptococcus agalactiae is coded by the Pac gene and is essential in binding and invasion of different host surfaces, and is present in almost all group B Streptococcus strains causing pneumonia, septicemia and meningitis [43, 44] ; psaA codes a surface lipoprotein that plays a role in Streptococcus pneumoniae systemic infections by interacting with monocytes [45] ; we also identified the heat shock protein-coding gene htbB that is known in Legionella pneumophila to act in adhesion to host fibronectin [46] ; the clpC gene codes a heat shock protein involved in the invasion of hepatocytes in Listeria monocytogenes and has an ATPase activity [47] ; ATPase proteins were shown to play a role in the survival and virulence in Salmonella typhimurium and S. aureus [48] ; clpP codes an ATP-dependent caseinolytic protease that was proven in Streptococcus suis to play a role in colonization and bacterial adaptation to various environmental stresses [49] , pavB codes a fibronectin-binding protein that mediates bacterial attachment to human epithelial and endothelial cells and also promotes transfer of bacteria to the bloodstream [50, 51] ; and nanA codes a highly conserved neuraminidase that also possesses a sialidase activity to catalyze the cleavage of terminal sialic acid residues from glycoconjugates. In S. pneumoniae, it promotes biofilm formation and contributes significantly to broncho-pulmonary colonization [52] . Although most of the strains exhibited one to eight unique virulence genes, strains G1562 and BA1 possessed 14 and 10 specific virulence genes, respectively. Eight strains (G1563, G1566, G1567, G1568, BA1, KCOM1545, JTH08, SK54AJKN01) had no strainspecific virulence factor (Table 3) . Among unique virulence genes, sdcA, ybtE,lpbA, SalR, salK, VopT are secretory system-associated genes that are involved in iron-mediated transport across cellular membranes. Some of these genes are linked with bacterial growth and act as important anti-inflammatory effectors [42, [53] [54] [55] [56] [57] [58] . Among other unique genes, the pilC gene is suspected to be essential for secretion and assembly of transcription factor P, important in pilus formation [59] while pilT helps in polymerization and depolymerization of pilin [60] . The brkA gene inhibits bactericidal activity and protects the bacterium from complement activation products [61] . Other unique genes are linked with bacterial adherence and colonizationm such as hopH, toxB, mpn 372 and stcE which contribute significantly to actin organization and bacterial attachment to human surfactant proteins [62] [63] [64] [65] . The iraAB gene utilizes iron-loaded peptides that promote iron assimilation [66] while lepA plays a role in bacterial growth and induces inflammatory response. This gene also plays a key role in pathogenicity in Psudomonas aeruginosa [67] . The fcrA gene codes a protein containing receptor domains for immunoglobulins similar to those M-related proteins [68] . Another immunoglobulin-related gene, aga, plays a barrier function for mucosal antibodies by cleaving IgA1 [69] . IpsA controls transcriptional biogenesis of the cell wall in inositol-derived lipid formation in Corynebacterium and Mycobacterium species [70] . The vasL gene is considered to be component of vas genes, associated with the membrane type VI secretion system [71] , and ravL is presumably activated at low oxygen level and regulates virulence gene expression via clp gene [72] . The lpg0365 codes a lypophosphoglycan that together with other membrane polypeptides, is necessary for Leishmania pathogenesis [73] . The pvdJ gene is involved in the production of cyclodipeptides that may regulate the production of biofilm [74] . In addition, pvdL is associated to biosynthesis or uptake of the siderphores pyoverdine and pyochelin that act in the transport of heme and ferrous ions [75] , while pvdD is involved in the biosynthesis of pyoverdine in Pseudomonas aeruginosa [76] . IpaJ codes a plasmid antigen involved in demyristoylation of proteins by inducing golgi fragmentation and inhibiting hormone trafficking [77] . AliA is associated with nasopharyngeal colonization in Streptococcus pneumoniae [78] . The espN gene is reported in Mycobacterium tuberculosis to play a role in adding an acetyl group to the N-terminus of the esaT-6 virulence factor [79] . Flagella-related unique genes found in different studied strains include flgG, flgI, flgJ and flgk which play a major role in virulence, adhesion and motility. They are mostly involved in flagellum formation and also act as interface with other flagellar proteins [80] [81] [82] [83] . The lnlK gene was reported in Listeria monocytogenes to help avoid autophagy while virB8 localizes to the inner membrane and is related to the export of alkaline phosphatase to the periplasm [84] . Finally, sigA codes a sigma factor linked with galactosidase activity [85] . Using principal component analysis of differentially distributed virulence genes, three distinct clusters were visualized (Fig. 8) . A clear separation of virulence genes associated with brain or broncho-pulmonary abscesses (cps4E, sda and lap) from those associated with liver or abdominal abscesses (cpsB, fbp54 and cap8D) was observed. The first component which has maximum coverage and represents the largest variation showed that brain abscess-causing strains were associated with genes coding ATP-dependent proteolytic enzymes, which indicates their potential role in abscess formation. Other virulence genes clustered independently, excluding any association with the previous two disease categories. Among virulence genes associated to brain and bronchopulmonary infections, sda codes an histidine kinase that regulates sporulation initiation in Bacillus subtilis and mediates the expression of virulence-associated factors [86] ; lap codes the Listeria adhesion protein (LAP) that is a host stress response protein responsible for adhesion and promotion of translocation across monolayers [87] ; and cps4E codes the capsular polysaccharide biosynthesis protein that was demonstrated in S. pneumoniae to prevent phagocytosis by forming an inert shield essential for encapsulation [88, 89] . In S. pyogenes, fbp54 codes a fibronectin-binding protein that acts as an immunogen in humans. The amino acid sequence of fbp54 in S. intermedius is similar to that of S. pneumoniae. cap8D codes a dehydratase that is essential for the synthesis of the capsule precursor involved in adhesion. It has also been targeted as component for vaccine development [85, 90] ; cpsB code capsular polysaccharide biosynthesis proteins that are essential for encapsulation in S. pneumoniae and are involved in the interaction of bacteria with their environment, notably their host organism [91] ; In contrast to the above-mentioned genes, some were not found to be disease-specific. These included glf, cpsE, cpsI, cpsA, cps4C, cps8P and hasC. The glf gene is involved in the biosynthesis of unusual monosaccharide galactofuranose [92] ; cpsE codes a glycosyl transferase responsible for the addition of activated sugars to the lipid carriers in the bacterial membrane and are essential for encapsulation in S. pneumoniae [93] ; cpsI is essential for the production of high molecular weight capsular polysaccharides [94] ; cpsA and cps8P are necessary for normal cell wall integrity and composition [95] ; cps4C codes a polysaccharide tyrosine kinase adaptor protein that plays a key role in the regulation of capsule biosynthesis [96] ; finally, hasC is involved in biosynthesis of hyaluronic acid capsule biosynthesis encodes glucose-1phosphate uridylyltransferase [97] . The tetracycline resistance gene tetM was identified in strains G1552, C270, KCOM1545, G1555, LC4, 30,309 and 32,811 whereas tet32 was identified in strain 631_ SCON ( Table 4 ). The macrolide resistance gene ermB was detected in strains G1552, C270, G1555 and 30,309. In other strains, no antibiotic resistance gene was identified. (Table 1 ). In addition, four prophage-like elements were detected in strain BA1, three in strain TYG1620, and two in strains G1562, G1564, G1553, G1557, F0413 and G1555. The major difference in the genome size between all 27 studied strains of S. intermedius resided in the phage numbers and this presence of phages also denotes contribution of horizontal gene transfer in the emergence of this species [98] . The search for CRISPR elements showed that 14 of the 27 studied genomes contained CRISPRs. Three of these 14 strains (G1564, G1565, 631_SCON) had more than one CRISPR, for a total of 17 CRISPR modules identified in studied strains. The direct repeat (DR) length in identified CRISPRs ranged from 24 to 36 bp while there was variation in the number of spacers present within each CRISPR. CRISPRs also differed among strains but the DR regions were similar for a given CRISPR element subtype. Based on the type of cas proteins, the CRISPRs of strains G1562, G1563, G1564, G1556, G1554, 631_ SCON, 30,309 were subtype I-C CRISPRs; those of strains FDAARGOS_233 and KCOM1545 were subtype II-A CRISPRs; finally, the CRISPRs of strains G1565, G1552, B196, G1555 and 32811were subtype II-C CRIS PRs [93] (Table 5) .@story_separate@In the present study, we reported 13 new clinical isolates of S. intermedius and, based upon a combined approach of pangenomics, core-genomics and virulence profiling of 27 strains, attempted identification of disease-specific genetic profiles. The comprehensive analysis revealed a genomic variability across strains within the species, although synteny of the core genome was preserved. Our results highlight the importance of surface proteins like pavB, pspA and cps4 (polysaccharide-coding proteins) and the binding proteins psaA, pavA, which are present in all studied strains, in pathogenesis. PCA results suggests two distinct categories of virulence genes, ATP dependent proteolytic virulence genes cps4E, sda and lap that are associated with brain and broncho pulmonary abscess while capsular polysaccharides protein coding genes cpsB and cps8D are linked with liver and abdominal abscess formation. The fibronectin binding protein coded by fbp54 is also showing its connection with liver and abdominal abscess formation. A recent study also attempted to determine the pangenome of S. intermedius. [99] The SNP-based phylogenetic tree as well as core gene-based tree showed no clustering related to any disease entity in S. intermedius strains. The whole study provides a key genetic framework for assessing and understanding the molecular events contributing to S. intermedius pathogenesis. However, due to the limited number of studied strains, validation of the role of these virulence factors will require experimental confirmations.","BACKGROUND: Streptococcus intermedius, a member of the S. anginosus group, is a commensal bacterium present in the normal microbiota of human mucosal surfaces of the oral, gastrointestinal, and urogenital tracts. However, it has been associated with various infections such as liver and brain abscesses, bacteremia, osteo-articular infections, and endocarditis. Since 2005, high throughput genome sequencing methods enabled understanding the genetic landscape and diversity of bacteria as well as their pathogenic role. Here, in order to determine whether specific virulence genes could be related to specific clinical manifestations, we compared the genomes from 27 S. intermedius strains isolated from patients with various types of infections, including 13 that were sequenced in our institute and 14 available in GenBank. RESULTS: We estimated the theoretical pangenome size to be of 4,020 genes, including 1,355 core genes, 1,054 strain-specific genes and 1,611 accessory genes shared by 2 or more strains. The pangenome analysis demonstrated that the genomic diversity of S. intermedius represents an “open” pangenome model. We identified a core virulome of 70 genes and 78 unique virulence markers. The phylogenetic clusters based upon core-genome sequences and SNPs were independent from disease types and sample sources. However, using Principal Component analysis based on presence/ absence of virulence genes, we identified the sda histidine kinase, adhesion protein LAP and capsular polysaccharide biosynthesis protein cps4E as being associated to brain abscess or broncho-pulmonary infection. In contrast, liver and abdominal abscess were associated to presence of the fibronectin binding protein fbp54 and capsular polysaccharide biosynthesis protein cap8D and cpsB. CONCLUSIONS: Based on the virulence gene content of 27 S. intermedius strains causing various diseases, we identified putative disease-specific genetic profiles discriminating those causing brain abscess or broncho-pulmonary infection from those causing liver and abdominal abscess. These results provide an insight into S. intermedius pathogenesis and highlights putative targets in a diagnostic perspective."
"Survival to hospital discharge following pediatric in-hospital cardiac arrests ranges from 16% to 48%. [1] [2] [3] [4] [5] Hundreds of in-hospital cardiac arrests occur suboptimally outside of the pediatric intensive care unit (ICU) and are considered preventable harm by the Children's Hospital Association. 6, 7 Nationally, many efforts have attempted to address this distinct population with a demonstrable reduction in in-hospital cardiac arrests outside of the ICU, most notably the implementation of rapid response teams. 3, [8] [9] [10] [11] However, unplanned transfers to the ICU persist and carry an associated but avoidable increased mortality. [12] [13] [14] Institutions looking to prevent these events have begun to diversify their deterioration detection efforts. One such focus is creating an optimal Pediatric Early Warning Score (PEWS) system, [15] [16] [17] [18] [19] [20] [21] which is typically composed of rules applied to objective and subjective patient data and an associated actionable mitigation plan. To date, PEWS systems have been unable to demonstrate a significant decrease in hospital mortality but have favorably affected the rate of other clinical deterioration events. 22, 23 Therefore, institutions have sought to broaden their deterioration detection efforts by utilizing scoring systems and increasing situational awareness while continuing to search for the optimal predictive model. 14 Widespread adoption of electronic health records (EHR) has created large repositories of patient data, making the development and validation of predictive analytics models against large datasets feasible. Progress in this realm has led to a prediction model of ICU transfer within 24 hours of admission, which outperformed a modified PEWS 24 and completely automated EHR-based warning systems. 25 Many of these scoring systems are still dependent on subjective assessments, such as mental status or capillary refill time, and rely on real-time documentation. These limitations can increase the documentation burden and reduce the efficacy of such systems when data entry is delayed. 26 Initiatives have begun to investigate using objective variables available in real-time within the EHR to identify patients at high-risk for clinical deterioration. 24 Our objective was to develop and validate an automated and objective pediatric early warning score utilizing predictive analytics modeling techniques that would perform as well as, or better than, our institution's current standard, a modified version of Monaghan's PEWS (see Supplemental Digital Content at http://links.lww.com/ PQ9/A170 for Table 1 ). 20, 27 Such a model would be devoid of the variability currently present with subjective PEWS components (behavioral assessment, capillary refill time, respiratory distress, persistent vomiting). This model, the Vitals Risk Index (VRI), is composed of entirely objective component inputs measured during routine inpatient clinical care with automated risk score calculation. Predicted outcomes of interest were code blue activations outside of the ICU 6 and emergent transfers to the ICU. 14@story_separate@The study population included children hospitalized from July 1, 2011, to December 31, 2017, at Nationwide Children's Hospital, a freestanding, quaternary care academic children's hospital. As this work was not human subject research, but rather a quality improvement study, review and approval by the Nationwide Children's Hospital Institutional Review Board was not required per policy. Exclusion criteria for both cases and controls included patients who are 19 years of age or older, neonatal ICU hospitalizations, length of stay <12 hours, and absence of vital sign measurements recorded in the EHR (Epic Systems Corporation, Verona, Wisc.). The study population was also limited to patients with at least 1 PEWS assessment recorded in the EHR. Case hospitalizations were those with either a code blue event outside of the ICU 6 or an emergent transfer to the ICU (see Supplemental Digital Content at http://links. lww.com/PQ9/A171, for full case definitions). 14 Code blue events required emergency assisted ventilation, chest compressions, or electric shock. We excluded code blue events triggered by a seizure (an internal practice) as they are often considered not predictable. Emergent transfers required 1 or more of the following interventions 1 hour before or after transfer: intubation, initiation of vasoactive medications, 60 mL/kg of fluid boluses, or cardiopulmonary resuscitation. We defined control hospitalizations as those without a clinical deterioration event type, as defined above. During the study period, there were 102 emergent transfers and 56 code blue events outside the ICU for a total of 158 case hospitalizations and 135,597 control hospitalizations. We built the study dataset from data collected on patients outside the ICU within our inpatient EHR system. Unless otherwise ordered, vital signs and PEWS evaluations were nominally recorded every 4 hours. If a patient received a PEWS score of 3 or 4, vital signs and PEWS assessment frequency increased to every 2 hours, and a PEWS score of 5 or 6 resulted in hourly vital signs and PEWS evaluations. Vital sign measurements and supplemental oxygen flowsheet row data were considered valid for up to 24 hours or until a new measurement was recorded; if the most-recent value recorded in the EHR was over 24 hours old, the field was coded as missing. Missing data points were replaced with age-specific nominally normal values that were ultimately coded to zeros in the final scoring algorithm. Similarly, if no supplementary oxygen data were available, we assumed the patient was not receiving supplementary oxygen. Handling missing data in this manner assumes that patients without a recent assessment are not at an increased risk of deterioration. Missing data imputation procedures were not considered because the implementation of such procedures would not be possible in real-time with the EHR system. To prevent bias toward hospitalizations with multiple events and to avoid unknown effects of event-driven interventions on the detection of subsequent events, only the first event of each hospitalization was employed. PEWS is recorded in the EHR and was included in the extracted data. Because a PEWS score of 5 or greater (PEWS-5) is used by the study institution as a trigger for evaluation, mitigation, and escalated response, PEWS-5 is the baseline against which we compare VRI performance. Comparisons against PEWS-4 are also provided, as this alternative allows for higher sensitivity when compared with PEWS-5, but lower specificity. Five patient vital sign measurements and a proxy for supplemental oxygen requirements were used to develop the VRI algorithm. Vital signs included heart rate, respiratory rate, temperature, percutaneous oxygen saturation, and systolic blood pressure. We captured supplemental oxygen as either flow rate (liters per minute) or fraction of inspired oxygen delivered when the flow rate was not recorded. We trained a multivariate logistic regression model to predict the occurrence of an inpatient deterioration event within the subsequent 24-hour period. All vital signs and supplemental oxygen variables were discretized into bins (eg, ""very low,"" ""low,"" '""normal,"" ""high,"" and ""very high""), by age group when appropriate, following the work of Duncan et al (2006) 15 and Fleming et al (2011) . 28 Note that this previous work was only referenced to threshold-continuous values by age group for algorithm development and is unrelated to Monaghan's PEWS scoring and thresholds. High and very high supplemental oxygen thresholds were determined by observing change points in the receiver operating characteristic (ROC) curves for these flowsheet rows, predicting emergent transfers and cardiopulmonary failure events. For each case hospitalization, we limited data used in model training to the 6 hours leading up to the deterioration event, to allow the model to be more specifically trained to data very near an event. For case events occurring <6 hours after admission, we included all data before the event. For controls, we used data from the middle 24 hours of hospitalization if the hospitalization was >24 hours; otherwise, all data were included. This middle 24-hour period is intended to represent a period of relative clinical stability among controls. In the final model, for future EHR implementation, statistically insignificant predictors (P-value ≥ 0.05) with negative coefficients were manually dropped. The remaining coefficients were reweighted so that the smallest and largest possible VRI values would be 0 and 100, respectively. For performance evaluation and comparisons, we included case data for the 24 hours before the first deterioration event and control data for the middle 24 hours of hospitalization. Comparisons were performed using all data up until the time of the event as well as removing data recorded within 1, 2, and 3 hours of the event. This strategy helps address the challenge of identifying at-risk patients with sufficient lead time to either prevent the event from occurring or limit its severity. To allow for the discussion of the VRI as an early warning tool, the primary model results chosen for discussion hereafter are those representing the removal of case data within 2 hours of the deterioration event. VRI performance results were generated by performing 10-fold cross-validation (dividing the data into 10 folds, then using 9 folds to produce predicted values for the remaining fold). The 10-fold cross-validation process was repeated 10 times to reduce the variability in the reported performance results. For graphical and statistical comparisons, PEWS performance is compared with an averaged VRI ROC curve, constructed by taking the mean predicted response at the patient level across the 10 repetitions of cross-validation. We did not use a train-test split in performance assessment due to the small number of available cases. Confidence intervals for PEWS and VRI ROC curves and sensitivities were constructed using non-parametric stratified bootstrapping, 29 via the ""pROC"" package in R. 30 All figures were created using the ""ggplot2"" package in R. 31 The final model coefficients are presented in Table 1 . ""High"" and ""low"" respiratory rate categories as well as ""low"" and ""very low"" temperature ranges were found not to be predictive of deterioration and were collapsed into the ""normal"" ranges. The VRI is the sum of the applicable model coefficients for the 6 model components. ROC curves for Monaghan's PEWS and the VRI, based on at least 2 hours of lead time, are presented in Figure 1 . There was no significant difference in the area under the ROC curve (AUC) of the VRI 0.76 (95% CI, 0.72-0.80) compared with PEWS 0.73 (0.69-0.78) (P = 0.16; Fig. 1 ). Additionally, there was no significant difference in the areas under the ROC curve between VRI and PEWS at false-positive rates ≤ 10% (pAUC 10 ), a threshold chosen to compare the 2 approaches under clinically tolerable false positive rates (pAUC 10 of 0.065 and 0.064, respectively; P = 0.74). The pAUC 10 metric can take on values from 0 to 0.1 with a random algorithm taking on the value 0.005. The VRI is a continuous score, ranging from 0 to 100, so a threshold must be set along this scale to calculate metrics like sensitivity and specificity. Testing VRI thresholds that match the false alarm rate of PEWS-4 (5%) or PEWS-5 (1%) allows for fair comparisons of the sensitivities of these 2 approaches, controlling for the ""cost"" of false-positive alarms. When the threshold for the VRI is set to match the specificity of PEWS-4 (0.95), the VRI and PEWS have very similar sensitivity levels from the time of the event to 3 hours before an event ( Fig. 2A) . Matching on the very high specificity of PEWS-5 (0.99), the sensitivity of the VRI (0.25, 95% CI, 0.19-0.32) is significantly lower than that of PEWS-5 (0.46, 95% CI, 0.38-0.54) at the time of the event (Fig. 2B) . However, the advantage of PEWS-5 over the VRI disappears if a successful alarm is required to occur 2 or more hours before the deterioration event. Both approaches perform similarly when requiring this much lead time at such high specificity. The goal of this study was to develop a fully automated and objective early warning predictive analytics tool-the VRI-that would perform at least as well as PEWS in the accurate and timely identification of hospitalized pediatric patients at risk for clinical deterioration outside of the ICU setting. We chose the specific and defined outcomes of code blue events outside of the ICU and emergent transfers to the ICU, which represent late or unrecognized inpatient clinical deterioration, because of their associated increased morbidity and mortality. 1, 32 To eliminate these preventable harm events, we have focused on earlier detection, mitigation, and potential escalation or transfer to a higher level of care before decompensation. Currently, we have been unable to sustain zero emergent transfers and continue to have patients with insidious deterioration undetected by Monaghan's PEWS and the healthcare team. Specifically, there have been concerns regarding the predictive ability of PEWS due to its subjective aspects (eg, behavior and mental status) and non-automated nature. Thus, the emphasis is on developing an automated, objective predictive tool that would augment or ultimately replace PEWS. The VRI did not include any subjective components that exist in Monaghan's PEWS. Despite this advantage, the VRI still requires accurate and timely documentation of vital signs and supplemental oxygen requirements. Fortunately, this behavior is ""hard-wired"" and embedded in our current workflow. In this study, we intentionally chose a relatively simple modeling approach of logistic regression, because the published coefficients can be easily implemented into any EHR system with built-in clinical decision support. We believe the VRI model is simple, purely objective, automated, real-time, and easily reproducible in other pediatric institutions. Further, we anticipate VRI will complement rather than replace PEWS and add a layer of decision support for clinicians to consider. An automated alert may particularly be beneficial during the high census and high acuity winter months when resources are stretched thin. Zhai et al 33 reported a machine learning-based algorithm predicting ICU transfer within 24 hours of initial admission. This model used 36 measurements and 155 variables, including vital signs and nursing assessments, with an AUC of 0.91 when identifying patients with at least 2 hours of lead time. Rubin et al 24 developed a predictive model that used objective vital signs as well as automatically calculated pulse pressure, mean arterial pressure, and shock index. Their case encounters were defined as any ICU transfer during the hospitalization, rather than just ICU transfers within 24 hours of admission. Their predictive model performed better than modified PEWS at both study institutions tested, with a false positive rate of 26%-27%. Finally, Rothman et al 19 published the development of a pediatric Rothman Index (pRI) using vital signs, nursing assessments, laboratory tests, and cardiac rhythms. AUC for 24-hour mortality was reported over 3 hospitals to be 0.93, 0.93, and 0.95. Due to the relative rarity of pediatric mortality compared with adults, the authors proposed using their model for prediction of ""unplanned transfer to the ICU"" as an outcome metric. The authors noted a ""trend in physiologic deterioration before and after unplanned transfer to the ICU further validates the pRI."" 19 However, no sensitivity, specificity, or AUC data were provided to support this conclusion. Our work confirms that EHR-derived data can be used to successfully develop a predictive model that performs similar to PEWS in detecting pediatric inpatient deterioration outside of the ICU. Compared with the Zhai et al 33 and Rothman et al 19 models, the VRI has added benefits of simplicity and objectivity using vital signs and level of supplemental oxygen while not relying on potentially subjective or delayed nursing assessments. The VRI adds, to prior prediction models, the ability to detect a set of focused, and, in our opinion, meaningful clinical deterioration events that have been demonstrably associated with increased morbidity and mortality. We avoided targeting the set of all ICU transfers because it is very heterogeneous and has not been demonstrably associated with negative healthcare outcomes. Identification and detection of impending clinical deterioration need to be done in advance of the actual event to mitigate successfully and, if necessary, escalate or transfer to a higher level of care and intensity of resources. Neither VRI nor Monaghan's PEWS perform well (sensitivities <15%) when removing data within the 2 hours before the event under strict specificity levels (eg, ≥0.99). The drop in sensitivity for PEWS-5 from 0 to 2 hours before the event may provide some insight as to why PEWS has not been associated with measurable decreases in mortality and morbidity. The sensitivity of 45% at event time fades to <15% at 2hours before the event when there is still time to intervene. There is a balance of ""alert fatigue"" and the ""needle in the haystack"" phenomena when trying to predict clinical deterioration. Equally important as predicting and identifying at-risk patients is the evaluation and mitigation response. These responses-including potential physician assessments, rapid response and code teams, increased monitoring, and increased EHR documentation-require personnel, time, and resources. Excessive false positives can lead to alert fatigue and undermine the utility of a predictive analytic tool and the accompanying response. An important question to address moving forward is to determine the targeted or acceptable false-positive rate in predicting clinical deterioration and adverse patient outcomes. To achieve zero-preventable events such as these, we will likely need to tolerate an increase in the false-positive rates associated with pediatric early warning systems and predictive analytic tools. There are multiple potential limitations to our study. This work represented patients from a single, albeit large, freestanding academic children's hospital. While the control population was robust with over 135,000 hospitalizations, there were only 158 case hospitalizations despite including 7 years' worth of data. Also worth mentioning is the heterogeneity in EHR systems utilized by institutions, potentially limiting the generalizability of the described model. In developing the VRI, while the calculation of the individual components is objective, the categories used to A B Fig. 2 . Sensitivity of the VRI when matching the specificity of (A) PEWS-4 and (B) PEWS-5. A, The VRI has a similar sensitivity to PEWS-4 from 0 to 3 hours before an event (with both approaches operating at a specificity of 0.95). B, PEWS-5 (specificity = 0.99) has a higher sensitivity than VRI at event time, but the 2 approaches perform similarly at 2 and 3 hours before an event. discretize vital sign measurements were based on expert opinion and could potentially be optimized by a fully datadriven approach. The need to collapse some categories after initial model fitting may be a result of this limitation. Additionally, our approach utilized logistic regression to allow final model coefficients to be implemented in the study institution's EHR, but we acknowledge that deep learning approaches, such as recurrent neural networks, may present an opportunity for improved predictive performance. 34, 35  The next steps include validating model performance prospectively in the patient care environment as well as determining how to integrate VRI into our current workflow, selecting a false positive rate that allows for accurate patient identification but not at the expense of alert fatigue or exhaustion of available resources. Specific patient populations whose baseline vital signs are abnormal for age (eg, oncologic, single ventricle, or ventilator-dependent patients) may require a different algorithm for triggering an alert. Finally, we may incorporate other objective or electronically captured patient characteristics that would improve accuracy and identify at-risk patients, such as past medical history, technology dependence, or prior recent critical event (ie, ICU transfer, rapid response team). Future parameter considerations will balance improved performance with simplicity, need for automation, and objectivity. The authors have no financial interest to declare in relation to the content of this article. This study was internally funded by Nationwide Children's Hospital.@story_separate@In this study, we developed a novel pediatric early warning systems-the VRI-based solely on objective vital sign measurements and supplemental oxygen demand. The VRI was shown to be as sensitive as Monaghan's PEWS as implemented at Nationwide Children's Hospital, when predicting patient deterioration outside of the ICU 2 to 3 hours before an event. In settings where an early warning system has not been implemented, the VRI may serve as an important clinical decision-support tool utilizing clinical workflows that are likely already incorporated into the EHR.","INTRODUCTION: Pediatric in-hospital cardiac arrests and emergent transfers to the pediatric intensive care unit (ICU) represent a serious patient safety concern with associated increased morbidity and mortality. Some institutions have turned to the electronic health record and predictive analytics in search of earlier and more accurate detection of patients at risk for decompensation. METHODS: Objective electronic health record data from 2011 to 2017 was utilized to develop an automated early warning system score aimed at identifying hospitalized children at risk of clinical deterioration. Five vital sign measurements and supplemental oxygen requirement data were used to build the Vitals Risk Index (VRI) model, using multivariate logistic regression. We compared the VRI to the hospital’s existing early warning system, an adaptation of Monaghan’s Pediatric Early Warning Score system (PEWS). The patient population included hospitalized children 18 years of age and younger while being cared for outside of the ICU. This dataset included 158 case hospitalizations (102 emergent transfers to the ICU and 56 “code blue” events) and 135,597 control hospitalizations. RESULTS: When identifying deteriorating patients 2 hours before an event, there was no significant difference between Pediatric Early Warning Score and VRI’s areas under the receiver operating characteristic curve at false-positive rates ≤ 10% (pAUC(10) of 0.065 and 0.064, respectively; P = 0.74), a threshold chosen to compare the 2 approaches under clinically tolerable false-positive rates. CONCLUSIONS: The VRI represents an objective, simple, and automated predictive analytics tool for identifying hospitalized pediatric patients at risk of deteriorating outside of the ICU setting."
"Cardiovascular diseases (CVDs) remain one of the leading causes of death worldwide, highlighted by the statement from the World Heart Federation that the number of deaths due to CVDs is at the moment around 17.3 million per year. It is anticipated that the number of deaths due to CVDs, in particular due to its main clinical outcomes myocardial infarction (MI) and stroke, will exceed 23.3 million by 2030 [1] . Besides this enormous impact on health, CVDs also have a high economic burden due to diagnostic and treatment costs, which are still rising every year. This underlines the importance of further research into and development of novel therapeutic approaches.@story_separate@The main underlying cause of CVDs is atherosclerosis, a lipid-driven chronic inflammatory disease of the arterial wall [2] . The pathophysiological cascade is mainly initiated by hemodynamic shear stress leading to endothelial damage [3] . This results CVD management mainly revolves around the stabilization of blood lipid levels via statins and the reduction of thrombotic factors via for example aspirin, which result in slowing down disease progression. Statins are the current gold standard of CVDtherapy. They inhibit HMG-CoA reductase, consequently decreasing the production of cholesterol [12] . A meta-analysis on clinical statin trials unveiled that this medication can indeed reduce the LDL levels in plasma by 50-55% thereby also significantly decreasing the risk of further cardiovascular events [13] . However, treatment with statins also has a lot of off-target effects. For example, a study by Preiss et al. [14] revealed that treatment with statins increased the risk for the development of diabetes by a striking 9%. This discovery led to a discussion about the use of statins in the clinic and especially stimulated the development of alternative treatment options. One of these new strategies is the use of monoclonal antibodies against proprotein convertase subtilisin/kexin type 9 (PCSK9). Its physiological function is to stimulate the degradation of the LDL receptor via direct interaction in the liver [15] . Additionally, PCSK9 prevents LDL receptor recycling to the membrane. In this way, inhibiting PCSK9 will avert LDL receptor degradation and hence lead to increased surface expression of LDL receptors that can bind and internalize LDL particles, thereby lowering plasma LDL levels. Interestingly, PCSK9 inhibition can reduce plasma LDL levels by a striking 60%, even on top of statin-induced LDL decrease, without any signs of serious side effects [16] [17] [18] . Although PCSK9 inhibition is a very promising therapeutic option, the production costs of these antibodies remain at the moment too high in order for it to be used on a large scale. Besides these lipid-focused CVD-therapies, novel methods based on immunomodulation have emerged in the last decades. The immune system protects the body from infections. In large parts, this is based on the recognition of ""self"" and ""non-self"". Immunity is vital to protect the body, but exaggerated inflammation and elevated white blood cell counts are a risk factor for CVD [19] as described before. A study by Ridker and Luscher [20] showed the beneficial role of interleukin-1β (IL-1β), tumor necrosis factor (TNF) and interleukin-6 inhibition on CVD outcome, which are all a part of a common pathway. Initially, an inactive precursor of IL-1β is produced which requires proteolytic cleavage by the nucleotide-binding leucine-rich repeat-containing pyrin receptor 3 (NLRP3) inflammasome [21] . By using the monoclonal antibody canakinumab, IL-1β can be inhibited. Notably, this inhibition results additionally in a reduction of plasma IL-6 and high-sensitivity C-reactive protein (hsCRP) levels [22] . In the Canakinumab Anti-inflammatory Thrombosis Outcome Study (CANTOS) trial the effects of targeting IL-1β on the risk of cardiovascular events were evaluated. CANTOS was a double blind, randomized, placebo-controlled trial which involved stable patients with previous MI. This trial showed that inhibition of IL-1β by canakinumab was indeed effective in lowering plasma hsCRP levels and preventing further adverse cardiac events [23] . This study showed the great potential of the immunomodulatory drug canakinumab; however, the use of this drug was associated with increased fatal infections and sepsis. This occurred despite the exclusion of participants with chronic or recurrent infections. Additionally, targeting TNF has also been studied in several settings. While anti-TNF treatment has proven success in several inflammatory diseases such as rheumatoid arthritis, it has not been directly investigated in CVD patients [24] . However, 1 year of anti-TNF-α therapy showed beneficial effects on vascular function in rheumatoid arthritis and ankylosing spondylitis patients [25] . Although targeting of these cytokines seems a very promising therapeutic approach, there remains a high unmet need for interventions in CVD that aim at controlling the immune system. As described above, CVD still puts the highest burden on both health and economy worldwide while there is still no treatment regimen to completely prevent or revert disease progression. A lot of the novel treatment options have either serious side effects, are too expensive to produce for large-scale use or have yet to be proven effective for CVDs. Recently, nanomedicine has gained interest as it could alleviate at least some of the disadvantages of the conventional medication. Nanoscience is the science behind the production, manipulation, design and application of materials smaller than 1000 nm. It is a field that is gaining interest in many areas, such as biotechnology, medical technology, biology and medicine. In the field of medicine, nanotechnology has provided many innovative methods for diagnostics, imaging, analytics and treatment procedures, for example gene delivery systems, targeted drug delivery and tissue engineering scaffolds [26] . The application of nanotechnology in medicine is referred to as nanomedicine, which enables the generation of drugs that interact with cells on the level of their receptors, and to modify and direct cellular processes after entering these cells. Nanomedicine is devoted to repairing, controlling and monitoring biological and physiological systems via nanoparticles (NPs), which can act as carriers for drugs, proteins and nucleic acids [27] . While the definition includes particles sizing up to 999 nm, most therapeutic NPs are below 100 nm to avoid clearance by the cells by the reticuloendothelial system (RES), which includes ECs, monocytes and macrophages. However, large quantities of NPs are unavoidably internalized by the liver. Generally, one can distinguish inorganic and organic NPs. Inorganic NPs consist of metals, like silica, gold and silver, which allows imaging systems to trace them, while organic NPs are amongst others dendrimers, carbon nanotubes, liposomes and polymeric micelles [28, 29] . Of the inorganic NPs, the gold NPs have been investigated the most. We have demonstrated that particularly macrophages can ingest large quantities of gold NPs [30] . Among the metallic nanoparticles, usage of iron oxide potentially enables an application of magnetic fields to guide the particles in the body [31] . However, the inorganic NPs suffer from the drawback of not being biodegradable, and thus organic NPs are suited best as carrier material for nanomedicine. Particularly lipid capsules have demonstrated to have a great usability due to their similarity with the natural cell membrane that is also composed of phospholipids [32, 33] . Therefore, lipid-based drugs represent the most successful nanomedicine at the moment. However, it is important to realize that the usage of lipids for drug encapsulation strongly effects the pharmacokinetics of drugs. For instance, the chemotherapeutic Doxil, which is encapsulated doxorubicin, greatly enhances the circulation period of the drug [34] . Recently, lipid-based nanoparticles (LNPs) to encapsulate mRNA have also proven to be a huge success as a novel class of vaccines for severe acute respiratory syndrome coronavirus type 2 (SARS-CoV-2) [35] . Additionally, the clinically approved LNPs that are most famous at the moment are based on lipid encapsulation, i.e., the COVID-19 vaccines from BioNTech/Pfizer and Moderna, are lipid nanoparticles [35] . LNPs enter the cells by binding to the LDL-receptor aided by apolipoprotein E (ApoE), which stimulates its uptake significantly [36] . As the LDL-receptor is not only expressed on hepatocytes, but also on other cells like antigen-presenting cells, LNPs can target a variety of cell-types to exert its effects and modulate lipid and/or inflammatory mechanisms ( Figure 1 ). Another lipoprotein that has been encapsulated into LNPs for receptor-mediated uptake is ApoA1, which has been extensively discussed in another review [37] and which will therefore only be briefly highlighted here. ApoA1 is the main protein that is present in high density lipoprotein (HDL) and is taken up in the liver via the scavenger receptor class B type I (SR-B1). Conveniently, NP uptake via SR-B1 can be achieved by engineering reconstituted HDL (rHDL) NPs containing various structures of ApoA1, which can alter the biological behavior of the NP [38] . One approach involves the use of ApoA1 mimetic peptides that contain amphipathic helix motifs comparable with those of ApoA1 or ApoE [39] . Some of these mimetic peptides have been shown to improve reverse cholesterol transport via ATP binding cassette transporter A1 (ABCA1) [40, 41] . Besides targeting lipid metabolism, rHDL particles can also be used to influence immune check-points. For example, Seijkens et al. used rHDL NPs to deliver small molecule inhibitors blocking the interaction between tumor necrosis factor receptor-associated factor (TRAF) 6 and CD40 (TRAF-STOPs) to macrophages and hereby successfully treated atherosclerotic ApoE −/− mice [42] . Lipoproteins and lipid nanoparticles show a high similarity regarding their physical, chemical and compositional properties. One main advantage of lipid-based nanoparticles is the storage ability, as LNPs can be stored in the range of 2 • C to −20 • C for more than 50 days and in lyophilized state even up to 11 months without the loss of gene silencing efficacy, compared to freshly prepared ones [43] . Depending on the physicochemical properties and surface modification, LNPs can remain more than 4 h in circulation in vivo before being cleared [44] . the particles in the body [31] . However, the inorganic NPs suffer from the drawback of not being biodegradable, and thus organic NPs are suited best as carrier material for nanomedicine. Particularly lipid capsules have demonstrated to have a great usability due to their similarity with the natural cell membrane that is also composed of phospholipids [32, 33] . Therefore, lipid-based drugs represent the most successful nanomedicine at the moment. However, it is important to realize that the usage of lipids for drug encapsulation strongly effects the pharmacokinetics of drugs. For instance, the chemotherapeutic Doxil, which is encapsulated doxorubicin, greatly enhances the circulation period of the drug [34] . Recently, lipid-based nanoparticles (LNPs) to encapsulate mRNA have also proven to be a huge success as a novel class of vaccines for severe acute respiratory syndrome coronavirus type 2 (SARS-CoV-2) [35] . Additionally, the clinically approved LNPs that are most famous at the moment are based on lipid encapsulation, i.e., the COVID-19 vaccines from BioNTech/Pfizer and Moderna, are lipid nanoparticles [35] . LNPs enter the cells by binding to the LDL-receptor aided by apolipoprotein E (ApoE), which stimulates its uptake significantly [36] . As the LDL-receptor is not only expressed on hepatocytes, but also on other cells like antigen-presenting cells, LNPs can target a variety of cell-types to exert its effects and modulate lipid and/or inflammatory mechanisms ( Figure 1 ). Another lipoprotein that has been encapsulated into LNPs for receptor-mediated uptake is ApoA1, which has been extensively discussed in another review [37] and which will therefore only be briefly highlighted here. ApoA1 is the main protein that is present in high density lipoprotein (HDL) and is taken up in the liver via the scavenger receptor class B type I (SR-B1). Conveniently, NP uptake via SR-B1 can be achieved by engineering reconstituted HDL (rHDL) NPs containing various structures of ApoA1, which can alter the biological behavior of the NP [38] . One approach involves the use of ApoA1 mimetic peptides that contain amphipathic helix motifs comparable with those of ApoA1 or ApoE [39] . Some of these mimetic peptides have been shown to improve reverse cholesterol transport via ATP binding cassette transporter A1 (ABCA1) [40, 41] . Besides targeting lipid metabolism, rHDL particles can also be used to influence immune check-points. For example, Seijkens et al. used rHDL NPs to deliver small molecule inhibitors blocking the interaction between tumor necrosis factor receptor-associated factor (TRAF) 6 and CD40 (TRAF-STOPs) to macrophages and hereby successfully treated atherosclerotic ApoE −/− mice [42] . Lipoproteins and lipid nanoparticles show a high similarity regarding their physical, chemical and compositional properties. One main advantage of lipid-based nanoparticles is the storage ability, as LNPs can be stored in the range of 2 °C to −20 °C for more than 50 days and in lyophilized state even up to 11 months without the loss of gene silencing efficacy, compared to freshly prepared ones [43] . Depending on the physicochemical properties and surface modification, LNPs can remain more than 4 h in circulation in vivo before being cleared [44] . Overall, the use of lipid encapsulation for treatment strategies in various settings further demonstrates the potential of nanomedicine in the clinic. However, as briefly highlighted in the next section there are several advantages and limitations that should be kept in mind during the development and use of NPs. In medicine, NPs have numerous advantages over traditional and modern therapies. These advantages make them ideally suited to be used in disease diagnosis and treatment. One technological advantage of NPs is their high stability, as it provides for a long shelf life. Additionally, NPs have a high carrier capacity, meaning that many drug molecules can be incorporated into the NP enabling the delivery of relatively high amounts of drug to the cells. Another advantage is the possibility to incorporate both hydrophilic and hydrophobic substances into the NP, making it a highly dynamic and flexible delivery method. Lastly, NP-based treatment has higher practicality in terms of administration as different routes, such as oral ingestion or inhalation, can be used to administer NP-based drugs [45] . Furthermore, NPs can be produced in a well-controlled manner regulating size and allowing further modifications which improve the overall drug delivery. In particular, size is a crucial factor that determines delivery efficiency. On the one hand small NPs will be excreted faster through the kidneys, while on the other hand larger particles are often trapped in the liver and spleen and therefore unable to reach their target cell/organ. One of the main routes for NP take-up is endocytosis, where the optimal size for NPs to induce potent endocytosis ranges from 30 to 60 nm. NPs smaller than 30 nm fail to induce endocytosis while the ones larger than 60 nm cause receptor overconsumption [46] . Furthermore, these particles can be processed in a way that allows additional conjugations of chemical molecules, peptides or electrical charges, which all improve targeting, water solubility, cellular uptake and structural stability [47, 48] . For example, hybrid NPs that consist of two types of NPs, such as polymers and liposomes, further improve the benefits of each particle for drug delivery and imaging. Advantages of NPs do not only apply to drug delivery, but also to other medical treatment options. For instance, by embedding NPs into biomaterials a hybrid biomaterial system is created. Biomaterials like hydrogels are for example being used during surgery to stop bleeding and improve wound healing. A hydrogel can be combined with a 3D polymer structure network to better control the kinetics of both the NPs itself that are released from the hydrogel and of the drugs released from the NPs. Additionally, NP clearance is accelerated by the biodegradable property of the hydrogel for detoxification [49] . Besides the many advantages of the use of NPs, there are also some downsides. One of the downsides is the potential ""nano-toxicity"" of these particles. NPs could accumulate in the blood and organs which could be harmful. For example, a study by Miller et al. showed that gold NPs were preserved in the urine and blood of healthy subjects for three months after administration via inhalation [50] . Although the gold particles did not show an immediate adverse effect on health, the accumulation of these particles in organs could be problematic for long term treatments. Moreover, NPs could potentially be contaminated with small amounts of bacterial lipopolysaccharide (LPS) or endotoxin during the particle preparation. Consequently, the LPS could induce inflammasome activation in immune cells by binding to receptors expressed in these cells such as Toll-like receptor 4 (TLR4) [51] . However, contamination of the NPs is extremely sparse when working carefully so the chance that these off-target effects occur is very low. Overall, the benefits of NPs seem to outweigh their disadvantages and NPs definitely have several advantages compared to traditional and other modern medical strategies. To overcome any safety issues for NP applications, NPs must be designed carefully followed by a series of experimental tests to confirm their functionality. As atherosclerosis is an inflammatory disease, the reaction of immune cells to NPs and vice versa should also be taken into consideration in their design, as discussed below. As described before, the immune system functions mostly by recognition of the body's own particles and foreign bodies. Therapeutic NPs will in any case have unintended interactions with the RES as they can be recognized by the immune system as foreign [52] . Several immune cells, such as monocytes, macrophages, dendritic cells (DCs) and polymorphonuclear leukocytes will, upon recognition, phagocytose these NPs for clearance. Once degraded, NPs have been shown to be able to reprogram macrophage polarization by inducing or suppressing gene expression [53, 54] . Furthermore, these particles stimulate inflammatory cytokine production by immune cells and elicit complement activation in the blood [55] [56] [57] [58] . In contrast, exposure to silver or cationic lipid NPs could lead to neutrophil elimination as it triggers neutrophil extracellular trap formation, release and cell death [59, 60] . Thus, NPs can trigger various immune responses on their own, independent from whether they carry drugs or not. There have been various strategies to avoid interactions of NPs with the RES. For example, reduction of unspecific binding to cells and proteins is achieved by PEGylation, which is the process of attaching strands of the polymer polyethylene glycol (PEG) [61] . Although this method is used frequently, many researchers oversee the fact that PEG will accumulate in the body and that an immune response in the form of antibody production takes place against PEG. This is most likely responsible for the more rapid clearance of liposomes from the blood upon repeated NP administration [62] . Another method to reduce unintended interactions with the RES is the use of coatings that decrease the acute immune reaction, such as poly(N-isopropylacrylamide) hydrogel particles crosslinked with PEG diacrylate. In this way the immune response is optimized, avoiding unwanted triggering of the RES [63] . With regard to leukocytes, it is known that monocytes, macrophages and DCs recognize pathogens, internalize them and present the antigens of novel pathogens via the major histocompatibility complex II (MHC-II) to T helper cells [64] . Subsequently, by secretion of cytokines the T helper cells then recruit other immune cells such as cytotoxic T cells and B cells, which kill infected cells and produce antibodies against antigens, respectively. It has been shown that surface chemistry, material composition and peptide modifications of NPs can affect DC maturation, thereby possibly also affecting T cell instruction by DCs [64] . Besides their direct effect on monocytes, macrophages and DCs, NPs could also interact with the MHC-II complex on these cells to influence the immune response as shown by Clemente-Casares et al. [65] . It is important to note that several studies have highlighted that there is a discrepancy in NP uptake comparing in vitro and in vivo experiments. For example, Bartneck et al. [64] showed that when ECs were cultured as a monolayer they took up large amounts of NPs, while another study showed that in vivo the ECs did not internalize NPs in the liver, as these particles were only found in hepatic macrophages [30] . This highlights the need for further in vivo studies to evaluate the potential of nanomedicine-based strategies for immunomodulation as a potential CVD-therapy. The interactions of different LNP formulations with leukocytes in the context of liver fibrosis were recently elaborately reviewed elsewhere [66] . As mentioned before, current CVD-therapies are far from perfect. Nanomedicine can provide interesting, and perhaps better, alternatives to the traditional lipid-or immunomodulatory drugs. Nanotechnological drugs could work especially well for the treatment of atherosclerosis as NPs are being phagocytosed by macrophages, the most abundant cell type in atherosclerotic plaques. This, together with their phagocytotic capacity opens up a new road to deliver drugs to macrophages inside atherosclerotic plaques [67] . In the following sections we will discuss potential approaches to treat CVDs with nanomedicine. One can distinguish two main levels on which leukocytes can be targeted with drugs: the first can occur on the level of their receptors, and the second can be exerted from modifying biological processes inside the white blood cells. The first option can aim at leukocyte receptors that guide them through the vascular wall: the circulating leukocytes are ""called in"" by any injured organ through the secretion of specific cytokines or chemokines which attract the leukocytes. The process of endothelial transmigration is based on the expression of various adhesion molecules such as selectins and integrins [68] . Integrins are membrane glycoproteins that can bind to various components in the extracellular matrix surrounding them. Blocking selectins and integrins was shown to be a successful strategy in limiting the infiltration of additional immune cells which otherwise would further amplify inflammation and lead to organ damage. Besides transmigration, the integrins like ανβ3 integrins also play roles in other atherosclerosis-related processes, such as angiogenesis. Integrin ανβ3 is expressed on ECs and its expression is upregulated by pro-angiogenic factors [69] . This integrin can then be used to deliver NPs into atherosclerotic plaques by coating the NPs with cRGDfK peptides. These cRGDfK peptides specifically bind to ανβ3, thereby allowing for entrance into the cell [70] . Another receptor that can be targeted with NPs is stabilin-2 (STAB2). This is a transmembrane receptor that is involved in angiogenesis and cell adhesion. STAB2 can bind and clear multiple ligands such as heparin and hyaluronic acid (HA) [71] . In atherosclerotic plaques, macrophages and ECs have abundant surface expression of STAB2 [72] . HA, which is highly biocompatible and biodegradable, was incorporated into NPs and used to visualize atherosclerotic plaques by binding to macrophages and diseased endothelium [73] . HA is also a ligand for other receptors such as CD44 [74] . This receptor is involved in the activation of immune and vascular cells and in cell adhesion of leukocytes to the endothelium and is known to be pro-atherogenic by stimulating the production of pro-inflammatory mediators [75, 76] . Furthermore, inhibition of the receptor led to reduced leukocyte recruitment and vSMC activation [77] . Thus, targeting this receptor with HA NPs could be an interesting strategy to target atherosclerotic lesions and therefore usable for the treatment of CVDs. The second level of immune cell manipulation can be considered by intervening with cellular processes. One of the key processes in atherosclerosis is vSMC proliferation and death, which ultimately leads to the development of the necrotic core, thinning of the fibrotic cap and calcification [78] . To target these processes, various caspase inhibitors have been tested such as rapamycin. Rapamycin was for example loaded into gel-like NPs. These NPs showed a 20% reduction in human vSMC proliferation in vitro compared to regular, free rapamycin [79] . The same study also tested these NPs in rats in vivo by infusion into the left common carotid artery after vascular injury. Interestingly, the concentration of rapamycin in the carotid artery remained stable for 2 weeks and was not detectable in the opposite carotid, indicating a localized delivery. The treated rats showed a significant decrease of hyperplasia and increased reendothelialization of the injured artery. Overall, this study thus shows the potential of NPs in treating CVDs by targeting cellular processes. Another important process in the pathophysiology of CVDs is inflammation. By targeting the process of inflammation, other processes initiated by the increased inflammatory state could also be reduced indirectly. For example, Meneghini et al. injected chemotherapy drug docetaxel (DTX)-loaded NPs that are lipid-based (LDE) intravenously into a rabbit model of atherosclerosis [80] . Notably, the levels of pro-inflammatory markers NF-κB, IL-6, IL-1β and TNF-α were all significantly decreased by LDE-DTX. Possibly as a result of the decreased inflammatory state, treatment with LDE-DTX resulted in a striking 80% reduction of atherosclerotic plaque area compared to controls. Additionally, expression of chemokine (C-C motif) ligand 2 (CCL2), which is involved in monocyte infiltration, and the macrophage marker CD68 were significantly lowered by LDE-DTX treatment, strongly suggesting that leukocyte infiltration was inhibited, possibly via the lowered inflammatory state. We have demonstrated before that encapsulated dexamethasone is applicable to treat liver diseases, both in acute and chronic settings [33] . More specifically, targeting immune cells with small interfering RNA (siRNA) for transcription factors was demonstrated to be efficient to circumvent inflammatory activation [81] . Additionally, a recent study by Tao et al. [82] showed that the Ca2+/calmodulin-dependent protein kinase γ (CaMKIIγ) gene can be targeted with siRNA NPs in Ldlr −/− mice. CaMKIIγ is activated in plaque macrophages and stimulates necrosis, which is also a key cellular process in atherosclerosis development. Inhibition of this gene via siRNA NPs indeed resulted in decreased necrotic area in the plaques of these mice and an overall increased plaque stability. Combined, these results clearly demonstrate the great potential that NPs have to target cellular processes in light of atherosclerosis and CVD. Besides the targeting of cellular receptors and processes, one can also target other key characteristics of CVD such as the increased LDL plasma levels. As described before, a popular target for the reduction of plasma LDL is PCSK9. In the more classical strategies, PCSK9 is inhibited via monoclonal antibodies leading to increased LDL receptor expression and consequently lower levels of LDL in the plasma. Another way of interfering with this process is by inhibiting the synthesis of PCSK9 via siRNAs. As their stability is limited in vivo, a nanocarrier is required for the cellular delivery of siRNAs. In a study by Fitzgerald et al. [83] , the efficacy of ALN-PCS, a PCSK9 expression inhibitor, was tested in healthy volunteers with increased cholesterol level who were not using any lipid-lowering drugs. ALN-PCS is a siRNA that inhibits PCSK9 synthesis and was delivered through lipid NPs. The treatment-emergent adverse events in the group of volunteers who received ALN-PCS were not different compared to the group who received the placebo, strongly suggesting that ALN-PCS is safe to use. Interestingly, ALN-PCS treatment reduced PCSK9 levels by 70% and LDL cholesterol levels by a striking 40% compared to placebo treatment. Overall, this study shows the great potential of nano-engineered drugs that target lipid levels to treat CVDs. Although, it should be kept in mind that these effects have been observed in healthy volunteers and therefore the additive effects on top of for example statin treatment remains to be determined. Interestingly, a nanomedical drug was approved in December 2020 in the EU for use in adults with primary hypercholesterolemia (heterozygous familial and non-familial) or mixed dyslipidemia, as an adjunct to diet: Inclisiran (Leqvio ® ; Novartis, Basel, Switserland) is a first-in-class, siRNA conjugated to triantennary N-acetylgalactosamine carbohydrates (GalNAc). Inclisiran targets hepatocytes in the liver by binding to the asialoglycoprotein receptor aided by p-aminophenyl δ-D-galactopyranoside ( Figure 1 ). It functions, similar to the statins, by lowering cholesterol levels. The advantage is that it is administered subcutaneously and only twice yearly [84] . LDL itself is also a promising candidate for NP-based medicine as it accumulates inside atherosclerotic plaques. Sobot et al. suggested to target LDL-accumulating cancer cells by using the cholesterol pre-cursor squalene conjugated with drugs [85, 86] . This strategy could also be applied to target LDL-accumulating macrophages in atherosclerotic plaques, but for now only imaging methods using florescent markers have been established [87] . Although further research is necessary in this area, targeting LDL with NPs seems to be a promising strategy to treat atherosclerosis and further CVDs. One of the promising aspects of nanomedicine is the possibility to revive the clinical potential of certain therapeutics via their reformulation. An example of this is the nanomodulation of the abandoned drug wortmannin (Wtmn), an inhibitor of phosphatidylinositol 3 kinases (PI3Ks) and phosphatidylinositol 3 kinase-related kinases (PIKKs). The drug was shown to be an effective therapeutic in the pre-clinical stages, but failed to be translated into the clinic because of low stability, poor solubility and high toxicity. A polymeric NP formulation of Wtmn was shown to overcome these negative aspects [88] , thus proving the additional value of nanomedicine for translating medicine successfully into the clinic. In order to use NPs in the clinic, their safety and optimal dosage needs to be tested in clinical trials first. Besides the clinical study by Fitzgerald et al. [83] on the effects of ALN-PCS, other clinical trials have been performed testing drugs based on nanomedicine. Table 1 shows an overview of the completed clinical trials that include nano-engineered drugs for the treatment of CVDs. Although many clinical trials involving nanomedicine have been performed in various fields, only few of them specifically aim to target CVDs. However, these trials show promising results which highlight exactly how nanomedicine can improve current treatments. One of these studies is for example the clinical trial by Van der Valk et al. [89] in which liposomal encapsulation of the drug prednisolone phosphate improved its half-life in vivo. Unfortunately, no favorable effects of the drug were seen regarding inflammation, but the study still shows the benefits of adding nanotechnology to existing medicines. Another promising clinical trial focused on liposomal prostaglandin E 1 (lipo-PGE 1 ) as additional therapy to surgery in patients with acute lower limb ischemia (ALLI) [90] . The therapy was shown to be beneficial as the overall incidence of adverse clinical events was significantly lower in patients receiving lipo-PGE 1 as compared to the control group (8.2% vs. 20.8%, respectively) [90] . Moreover, in a phase I clinical trial by Margolis et al. [91] , the drug paclitaxel was bound to albumin and intravenously injected into patients with multivessel disease. In this manner the safety and optimal dosage was determined and results suggested that dosages below 70 mg/m 2 were optimal in these patients. Tables 2 and 3 provide an overview of the clinical trials that are still active or have been terminated, respectively. The ongoing clinical trials ( Table 2 ) are in more advanced stages (i.e., phase II/III) than the completed trials (Table 1) , thereby hopefully bringing nanomedicine one step closer to being used in actual clinical settings. An important fact to stress is that the terminated trials (Table 3 ) have been terminated based on reasons other than safety concerns, further stressing the safety and potential of nanomedicine as clinical approach. Albumin-bound Abraxane/Paclitaxel Phase I The safety and optimal dose of Paclitaxel was tested in patients with multivessel disease. Dosages below 70 mg/m 2 were tolerated by the patients and no adverse events were noted. [91] Silica gold and silica gold iron-bearing -Not applicable Patients with atherosclerotic lesions received either silica gold NPs in an on-artery patch (Nano); silica gold iron-bearing NPs with targeted microbubbles and stem cells via a magnetic navigation system (Ferro); or a stent (Control). In both experimental groups the total atheroma volume was reduced up to 60 mm 3 with a high level of safety. A five-year follow-up showed a higher safety and better mortality rate in the Nano group compared to the Ferro and Control. [92, 93] Iron oxide-bearing -Not applicable After acute MI, NPs of iron oxide were injected intravenously into patients and detected via magnetic resonance imaging. These NPs were taken up in the infarcted and remote myocardium, thus highlighting the method's potential to be used to assess cellular myocardial inflammation and left ventricular remodeling. [94] @story_separate@To summarize, CVD remains a major contributor to both the global economic and health burden. Many diagnostic and treatment options are available, but these unfortunately still have some major drawbacks. Nanomedicine has the potential to overcome some of the hurdles that conventional medicine faces. Some of these advantages lie in their high stability, high carrier capacity, and the various ways of administrating nano-engineered drugs. One of the few limitations of the use of NPs is their potential toxicity. However, as proven by the various clinical trials performed with NPs, the use of these type of drugs is considered rather safe. Some issues regarding their application in CVD also remain for now. A key issue in targeting the vessels with nanomedicine will probably always be the superior clearance by the liver. However, when accessing monocytes in the blood stream as precursors for macrophages, or of neutrophils that might serve as Trojan horses, and further directing these cells into the vascular environment might help to inhibit the development of atherosclerotic lesions. Furthermore, the leukocyte recruitment cascades and the vascular and leukocyte receptors involved in this process are a potential interesting point of intervention using nanomedicines that should be investigated in greater detail in the future (Figure 2) . Overall, nanomedicine can be applied in various ways to aid current treatment options and further exploration of the use of NPs in CVDs will pave the way for nanomedicine in reducing the global burden of CVDs. Overall, nanomedicine can be applied in various ways to aid current treatment options and further exploration of the use of NPs in CVDs will pave the way for nanomedicine in reducing the global burden of CVDs. Figure 2 . Future potential points of intervention for nanomedicine. The figure shows a schematic representation of leukocyte recruitment into the vessel wall and subsequent macrophage differentiation, the main process that drives atherosclerosis. Interesting potential points of intervention are highlighted that in the future might be promising processes at which nanomedicine can intervene as therapeutic approach.","Atherosclerosis is the main underlying cause of cardiovascular diseases (CVDs), which remain the number one contributor to mortality worldwide. Although current therapies can slow down disease progression, no treatment is available that can fully cure or reverse atherosclerosis. Nanomedicine, which is the application of nanotechnology in medicine, is an emerging field in the treatment of many pathologies, including CVDs. It enables the production of drugs that interact with cellular receptors, and allows for controlling cellular processes after entering these cells. Nanomedicine aims to repair, control and monitor biological and physiological systems via nanoparticles (NPs), which have been shown to be efficient drug carriers. In this review we will, after a general introduction, highlight the advantages and limitations of the use of such nano-based medicine, the potential applications and targeting strategies via NPs. For example, we will provide a detailed discussion on NPs that can target relevant cellular receptors, such as integrins, or cellular processes related to atherogenesis, such as vascular smooth muscle cell proliferation. Furthermore, we will underline the (ongoing) clinical trials focusing on NPs in CVDs, which might bring new insights into this research field."
"The coronavirus disease-2019 (COVID-19) pandemic has taken a heavy toll on the healthcare system across the world. With a global incidence of over six million, the mortality rate so far has been approximately 6.5% [1] . Though a few countries have succeeded in containing the spread of the virus, the overall incidence of fresh infections is still on the rise. Moreover, with no vaccine or definitive treatment in sight [2] , the health care systems across the world are strained and are staring at an uncertain future and a long-term coexistence of severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) with the human host has been anticipated. From the perspective of a rheumatologist, there are concerns regarding compelling actions and future preparedness for facing COVID-19 and its after effects. We have always considered rheumatic musculoskeletal diseases (RMDs) as conditions that are precipitated when a genetically susceptible host encounters specific environmental triggers. Our firsthand knowledge of the host-environment interaction in RMD until now was limited to epidemiological studies related to small infectious outbreaks like the Chikungunya epidemic which has given us insights into various post-infectious RMD manifestations. The COVID-19 pandemic, in contrast to these relatively small occurrences, is a mammoth event in the history of humankind. The infection has spread across all large countries affecting people from all age groups. Such an infection is likely to have long-term consequences, and RMDs could be a major fallout given the quantum of immune dysregulation the virus is known to cause [3] . There are already reports of post-COVID-19 autoimmunity with an increase in the incidence of Kawasaki disease [4] and Guillain-Barré syndrome [5, 6] . In a global attempt to curb this pandemic, a variety of drugs with varying mechanisms of action are being tried. While some of these drugs are known to cause adverse effects mimicking RMDs and other autoimmune diseases, there is a lack of information for the majority of them. There are also concerns regarding the right approach to treat existing rheumatic diseases and about the outcome of infection among those on immunosuppressive agents. This and many more challenges faced by patients with RMDs and their care providers during the COVID-19 pandemic are being addressed by the Global Alliance Registry and the COVID-19 database of the European League Against Rheumatism (EULAR); mining these data will serve as a guide in the future for the management of patients with RMDs [7] . The COVID-19 pandemic has undoubtedly uncovered the strengths of global collaborations, effective communication, and dissemination of information. Unless we prepare and foresee the likely new set of RMDs and the possible mimics of RMDs [8] due to this pandemic, we are going to fail in delivering care to our patients. We have recently learned the costs of being underprepared when challenged with the rheumatological manifestation [9] resulting from the use of immune checkpoint inhibitors in cancer treatment. The initial dilemma on the management plan and prognosis of these conditions set the rheumatology community on a backfoot [10] . Keeping this context in mind, it is time to pool all our existing knowledge and resources to prepare for any rheumatological fallout of COVID-19 and its treatment, which may have left behind its footprints even in the post-COVID-19 era.@story_separate@Our objective was to narrate various autoimmune and rheumatic manifestations that are associated with COVID-19 and the drugs used in its treatment. We conducted a literature search for articles since inception and published until 20 May 2020 in the English language using the Medline database. The search terms used in various combinations were: ""coronavirus disease-19"", ""COVID-19"", ""SARS-CoV-2"", ""rheumatic"", ""autoimmune"", ""musculoskeletal"", ""clinical"", ""manifestations"", ""antivirals"", and ""vaccines"". Additionally, we performed a focused literature search in the same database for articles on drugs used in COVID-19. We included original research articles, reviews, viewpoints, opinions, commentaries, case series, and case reports as relevant to our objective. The search strategy was used to obtain the titles and the abstracts of the relevant studies which were initially screened, and if necessary, the full text was retrieved to determine the suitability. The references and related citations for the resulting articles were also reviewed for pertinence. Finally, the information was synthesized in a logical sequence with expert inputs from the senior authors. SARS-CoV-2 enters the cells through angiotensin-converting enzyme 2 (ACE2) receptors located on the nasopharyngeal and bronchial mucosa, and on alveolar pneumocytes in the lungs [11] . The smooth muscles of the arterial walls and endothelial cells of the veins in every organ of the body express ACE2 receptors explaining the ubiquitous distribution, and the possibility of multisystem involvement by the SARS-CoV-2 virus [12] . The spectrum of presentation of COVID-19 ranges from asymptomatic or mild symptoms to severe-critical illness which is categorized as mild, severe, and critical illness [13] . Patients with mild presentation have symptoms of fever, cough, nasal congestion, sore throat, malaise, headache, and myalgia without dyspnea. The severe presentation includes patients with respiratory distress as indicated by a respiratory rate of ≥ 30/min, SpO 2 (blood oxygen saturation) of ≤ 93%, PaO 2 (partial pressure of oxygen)/FiO 2 (fraction of inspired oxygen) ratio of < 300, or presence of infiltrates in more than half of the lung fields in a radiograph or CT scan. Critical illness encompasses respiratory failure, septic shock, disseminated intravascular coagulation (DIC), and multiorgan dysfunction syndrome [13] . Besides respiratory symptoms, multiple cohorts have reported anosmia and dysgeusia, ranging from 34-87% of the patients [14, 15] . Gastrointestinal manifestations, including nausea, vomiting, and diarrhea have been reported in 11-24% of the cases and as presenting symptoms in a few reports of COVID-19 [16, 17] . In a case series from China, 32% of the patients had ophthalmological manifestations, mainly conjunctival redness and edema with increased eye discharge [18] . The other musculoskeletal, dermatological, neurological, and cardiovascular manifestations have been discussed in detail further in this review. The risk factors for severe disease and mortality include old age, immunocompromised status, and comorbidities, such as hypertension, diabetes mellitus, cardiovascular disease, chronic respiratory disorders, chronic kidney disease, liver diseases, cancer, and severe obesity [19] . Initial data from the COVID-19 Global Rheumatology Alliance provider registry suggests that the features and prognosis in patients with RMDs on immunosuppressive agents contracting SARS-CoV-2 are similar to those in the general population [20] . With scientific literature pouring in from different parts of the world on the clinical presentations and complications of SARS-CoV-2 infections, atypical clinical and laboratory manifestations mimicking RMDs have been reported (Table 1) . Musculoskeletal manifestations reported with COVID-19 include arthralgia, myalgia, and proximal weakness with elevated creatine kinase (CK) level [21, 22] . Some of these manifestations preceded respiratory symptoms of COVID-19. For instance, a patient who presented with proximal weakness with muscle edema on magnetic resonance imaging (MRI), turned negative for myositis-specific and associated antibodies but progressed to develop the symptoms of COVID-19 [22] . The spectrum of dermatological manifestations mimicking RMDs includes COVID toes/pseudo-chilblain, transient urticarial or maculopapular rash, livedoid/necrotic lesions, punctiform or diffuse purpura, and erythema elevatum diutinum-like rash [23] . COVID toes/pseudo-chilblain is reported predominantly in children and young adults and seems to be a relatively late feature of COVID-19. On the other hand, livedoid/necrotic lesions are seen in elderly patients with severe COVID-19 with no association with the duration of infection. Cases of young stroke involving large vessels, commonly evaluated in rheumatology clinics, have also been reported Case reports Likely to occur within 7 days of symptoms Circumferential pericardial effusion, global hypokinesia, low ejection fraction and normal cardiac valves on echocardiography Normal coronary angiography Cardiac MRI: myocardial edema and pattern of late gadolinium-enhancement fulfilling Lake Louis criteria of acute myocarditis Improved with supportive care, hydroxychloroquine, lopinavir/ritonavir, and intravenous methylprednisolone [25, 26] Multisystem autoinflammatory syndrome Cytokine storm/Secondary Hemophagocytic lymphohistiocytosis (sHLH) After 8-9 days of the symptom onset Unremitting fever, cytopenia, and hyperferritinemia Acute respiratory distress syndrome and multiple organ failure Interplay of Interferons, interleukins, chemokines, colony-stimulating factors, and TNF-alpha Hyperferritinemia and elevated serum IL-6, associated with mortality H-score of > 169, 93% sensitivity and 86% specificity for the diagnosis of the sHLH Report on improvement with IL-1 and IL-6 inhibitor [27] with COVID-19 [24] . A couple of cases of Guillain-Barré syndrome (GBS) has been described as a consequence of post-viral autoimmune phenomenon. Out of the two reported cases, one succumbed to respiratory complications, and the other recovered with intravenous immunoglobulin (IVIg)/ plasmapheresis [5, 6] . Myocarditis related to COVID-19 in young patients without any prior cardiac morbidity has been reported with early onset of cardiac symptoms. They present with intact coronaries in cardiac echo and MRI showing the classical features of myocarditis. Improvement of myocarditis was noticed following treatment with hydroxychloroquine, lopinavir/ritonavir, and intravenous methylprednisolone [25, 26] . An increase in the incidence of Kawasaki disease has been reported from Italy as a post-SARS-CoV-2 phenomenon. As compared to the classical Kawasaki disease, these children were older (mean age, 7.5 years) and reported to have more cardiac involvement, shock syndrome, and macrophage activation syndrome (MAS) [4] . Cytokine storm or secondary hemophagocytic lymphohistiocytosis (sHLH), yet another entity encountered in rheumatology practice, is reported with COVID-19. The clinical manifestations start 8-9 days after the onset of respiratory symptoms and range from unremitting fever, cytopenia, to hyperferritinemia ultimately resulting in multiorgan failure. Hyperferritinemia and elevated serum interleukin (IL)-6 are often associated with mortality in these patients [27] . There are reports of improvement with therapy targeting IL-1 and IL-6, and there are multiple ongoing clinical trials to evaluate the role of immunosuppressive therapy for this cytokine storm reported with COVID-19. In addition to these clinical manifestations mimicking RMDs, laboratory reports of positive antinuclear antibodies (ANA) [28] , antiphospholipid antibodies, lupus anti-coagulant assay [29, 30] and increased level of D-dimer [31] have been reported with COVID-19. All these reports of COVID-19 mimicking or precipitating RMDs points towards a possibility of persisting intermediate to long-term immune dysregulation. Based on evidence from in-vitro studies and experience from other viral infections, several antiviral therapies are currently in trial/practice in different parts of the world [32] . There are reports of rheumatic musculoskeletal adverse reactions following the use of these drugs. Table 2 depicts the list of the major anti-SARS-CoV-2 drugs with their mechanisms of action and the important rheumatological adverse events. Of these, some adverse events Post-viral autoimmunity Guillain-Barré syndrome (GBS) Case reports The interval from COVID symptoms to GBS symptoms was 5-10 days Axonal or demyelinating variant Negative PCR for SARS-CoV-2 from CSF One of the patients succumbed to respiratory complications, and the other recovered with IVIg/plasmapheresis [5, 6] Kawasaki-like disease 30-fold increased incidence as compared to the pre-COVID time in Italy Higher mean age (7.5 years) More cardiac involvement, shock syndrome, and macrophage activation syndrome as compared to pre-COVID-19 Kawasaki disease [4] Laboratory findings Positive Antinuclear antibodies (ANA) Reported in 35% of the patients Single-center report No impact on outcome with positive ANA Reported in 4.4% of the patients Antiphospholipid antibodies Case series (n = 56) LAC positive (n = 25) Anticardiolipin or anti-β2-glycoprotein I antibodies IgG/IgM (n = 5) Case reports (n = 3) Anticardiolipin or anti-β2-glycoprotein I antibodies IgA Epiphenomenon rather than autoimmunity Expert opinion favoring to start heparin in patients with antiphospholipid test positivity [29, 30] Increased d-Dimer without DIC > 0.5 mg/L in 46% of the patients Higher chance for ICU admission > 1 mg/L on admission has 18-times increased mortality (95% CI, 2·6-128·6; p = 0·0033) [31] COVID-19 coronavirus disease-2019, CSF cerebrospinal fluid, DIC disseminated intravascular coagulation, IL interleukin, LAC lupus anti-coagulant, MAA myositis-associated autoantibodies, MSA myositis-specific autoantibodies, RA rheumatoid arthritis, PCR polymerase chain reaction, SARS-CoV-2 severe acute respiratory syndrome coronavirus-2 deserve special mention. Myopathy and neuromyopathy can rarely occur following long-term treatment with chloroquine and hydroxychloroquine [33] . Favipiravir can lead to hyperuricemia [34] . Lopinavir-ritonavir-related rheumatic adverse events include arthralgia, back pain, osteonecrosis, and vasculitis [35] . Ribavirin can cause arthralgia, back pain, myositis, and exacerbation of sarcoidosis [36] . Musculoskeletal pain and myalgia have been reported in up to half of the patients on interferon therapy. Additionally, in rare cases, interferon therapy can lead to drug-induced RMDS, such as rheumatoid arthritis, lupus, Sjogren syndrome, myositis, sarcoidosis, and vasculitis [37] . With the increased use of these drugs, there is a possibility of a rise in these adverse drug reactions necessitating active pharmacovigilance. Moreover, it must be noted that some of the frontrunner drugs like remdesivir have limited clinical data making it even more important to be vigilant. One of the factors leading to mortality in COVID-19 patients is the presence of coagulopathy [38] . The autopsy findings of COVID-19 death from a majority of the cases show the presence of coagulopathy either in the form of deep venous thrombosis, pulmonary embolism, or multiple pulmonary thrombi coexisting with acute respiratory distress syndrome (ARDS) changes in the lungs [39] . The laboratory markers of COVID-19 coagulopathy include increased D-dimer Convalescent plasma Chance of transfusion-related adverse events: urticaria, anaphylaxis, transfusion-related acute lung injury Latent risk of hyperimmune attacks: Possibly via antibody-dependent enhancement of tissue damage and blunting of endogenous immunity to the virus level, borderline thrombocytopenia, and prolonged prothrombin time [40] . Though the exact mechanism underlying coagulopathy is unclear, a possibility of local lung-TMA as a coronavirus-associated hemostatic lung abnormality (CAHA) is proposed, linking inflammation and endothelial cell activation [41] . Endothelial cell activation results in the production of von Willebrand factor (vWF) in excess to the clearance capacity of ADAMTS-13. This leads to local thrombotic microangiopathy and perpetuates lung damage [42] . The association of netosis has also been proposed with coagulopathy in COVID-19 patients requiring further therapeutic exploration [43] . Moreover, the presence of lupus anti-coagulant and antiphospholipid antibodies may point towards the autoimmune contribution rather than just an epiphenomenon. Thus, linking inflammation to coagulopathy which is not a common occurrence in other viral infections suggests the role of immune activation at the vessel wall. This phenomenon warrants serious consideration of the anecdotal evidence supporting the role of immunosuppressants in addition to anticoagulants in severe COVID-19 cases [44] . The interplay of various genetic, hormonal, immunological, and environmental factors constitutes the mosaic of autoimmunity [45] . Viral infections play a substantial role in the development of several autoimmune diseases in individuals with underlying immune dysregulation [46] . Follow up data from survivors of viral outbreaks like influenza, Zika, Ebola, and Chikungunya have shown development of autoimmune phenomenon within weeks to months after recovery. While GBS, fulminant type1 diabetes, IgA vasculitis, APS have been observed after a previous outbreak of influenza [47] , transverse myelitis, arthralgia, myalgia, and arthritis were reported following Zika [48] , Chikungunya [49] and Ebola infections [50] . Besides overt clinical manifestations, longterm persistence of autoreactive cells and autoantibodies (against antiphospholipid with influenza and ds-DNA and heat shock protein-60 with Ebolavirus) have also been demonstrated with some of these infections [47, 49] . The mechanisms by which viruses disrupt self-tolerance include molecular mimicry, epitope spreading, bystander activation, persistence of the latent virus, and poly/oligoclonal immune activation in the background of autoimmunity mosaic [51] . Table 3 enlists known viral infections and associated autoimmune diseases with the possible mechanisms as elaborately described by Smatti et al. [52] . Similar mechanisms may lead to autoimmunity following SARS-CoV-2 infection. We herein describe some of the possible mechanism of autoimmunity following SARS-CoV-2 infection as follows: Molecular mimicry, as a result of the cross-reacting epitope between the virus and the host, leads to both humoral and cellular autoreactivity (Fig. 1a) [53] . This mechanism plays a vital role in the pathogenesis of prototype systemic rheumatological diseases such as rheumatoid arthritis (RA), systemic lupus erythematosus (SLE), systemic sclerosis, and Sjogren syndrome [54] . SARS-CoV-2 proteins displayed at least one match with the human protein on a comparative peptidome analysis comprising of 37 viral proteins [55] . These similarities potentially can lead to loss of tolerance to self-peptides and result in autoimmunity. Recently, an epitope mapping analysis has identified immunogenic linear epitopes, 2′-O-ribose methyltransferase, RNA-dependent RNA polymerase and 3′-to-5′ exonuclease proteins from autoimmune dermatomyositis patients matching with the SARS-CoV-2 peptides [56] . In addition to the presence of cross-reactive epitopes, activation of antigen-presenting cells (APCs) either by an adjuvant or infectious stimulus is essential to increase the expression of co-receptors [51] . Viral RNA of the SARS-CoV-2 activates dendritic cells through cytosolic RIG like receptors and endosomal TLRs, as well as by the release of interferon α/β and γ enhancing antigen-presenting capacity (Fig. 1b) [57] . Such activation by the virus can result in precipitation of inflammatory cascade in the presence of an already existing cross-reactive antigen. The clinical phenomenon of initial asymptomatic to mild symptoms followed by severe autoinflammatory syndrome in COVID-19 is proposed to be due to a similar autoreactive adaptive response with SARS-CoV-2 infection [3] . The role of genetic susceptibility should also be considered while evaluating the role of cross-reactive epitopes in precipitating autoimmunity. Human leukocyte antigen (HLA) susceptibility map for SARS-CoV-2 has shown that HLA-B*15:03 efficiently presents highly conserved SARS-CoV-2 peptides that are shared among the common human coronaviruses [58] . The association of HLA-B*15 with primary Sjogren's syndrome [59] and Bechet's diseases [60] is well documented, and the consequence of its association with SARS-CoV-2 peptide presentation needs to be carefully followed up. Following molecular mimicry with the dominant epitope, diversification in epitope specificity commences resulting in the neo-epitopes presentation. Immune response to these neo-epitopes differs from that to the dominant epitope and involves newer targets for autoimmunity [61] . The sequential appearance of various autoantibodies in RA and SLE follows the theory of epitope spreading [62, 63] . In SLE, before clinical disease onset, autoantibodies are targeted against Ro, La, and phospholipid antigens. In contrast, the clinical manifestations commence with the appearance of autoantibodies to ds-DNA, Smith (Sm), and ribonuclear protein (RNP) antigens [64] . Although with SARS-CoV-2 infection, epitope diversification is not reported, it would be worthwhile to closely follow those patients with positive autoantibodies. Pre-clinical studies of diabetes and experimental autoimmune encephalomyelitis suggest the role of bystander activation as one of the mechanisms predisposing to autoimmunity [65, 66] . The bystander damage starts with the virus-specific CD8 + T cells migrating to the infected target tissues and exerting perforin and granzyme-mediated cytotoxicity. The target cell death in the inflammatory milieu activates the surrounding macrophages to release reactive oxygen species and nitric oxide resulting in bystander killing of surrounding uninfected cells [67] . The CD4 + T cells contribute to this bystander damage through the release of proinflammatory cytokines and enhancing phagocytic activities of macrophage [68] . Ineffective clearance of these killed cells exposes autoantigen to antigen-presenting cells, resulting in the generation of autoreactive cells (Fig. 2) [51] . The association of hepatitis C virus and Sjogren syndrome serves a classical example among RMDs explaining the bystander activation and damage theory [69] . Hepatitis C, primarily a hepatotropic virus, also exhibits sialotropism and lymphotropism leading to bystander activation of the salivary gland epithelium and lymphocytes, respectively [70] . The activation of salivary epithelial cells and resultant antiviral defense in the salivary gland result in bystander damage manifesting as sicca symptoms and parotidomegaly. Bystander activation and dysregulated proliferation of the lymphocytes result in cryoglobulinemic vasculitis, generation of rheumatoid factor, autoantibodies (ANA, anti-SSA, anti-SSB), and low complements. Lymphopenia due to reduced CD4 + T cells and CD8 + T cells is well documented in severe cases of SARS-CoV-2. The reduction is possibly secondary to functional exhaustion and preferential accumulation of the primed lymphocytes at the site of viral infection [71] . These preferentially accumulated lymphocytes exhibit increased activity as confirmed by the expression of HLA-DR, CD69, CD38, and CD44. Activated lymphocytes, along with activated macrophages at the target tissue lead to bystander killing of adjacent, non-infected healthy cells via proinflammatory cytokine and reactive oxygen species [72] . With SARS-CoV-2 infection, the multisystem inflammatory syndrome appears after the peak of viral load. This suggests a buildup of inflammatory cytokine and bystander activation of the macrophages by SARS-CoV-2 through pattern recognition receptors (PRR). It also suggests bystander killing of the cells devoid of ACE2 receptors by free oxygen radicals released in the inflammatory milieu [73] . Bystander damage can be one of the mechanisms responsible for the manifestations like ARDS, myocarditis, and neurological involvement reported with SARS-CoV-2 infection. Autopsy reports of COVID-19 patients show diffuse infiltration of lymphocytes in the lungs and focal infiltration in the heart, kidney, liver, pancreas, and adrenal gland, suggesting bystander damage by cytotoxic CD8 + T cells [74] . Besides, in patients surviving SARS-CoV-2 infection, bystander activation may lead to sequestered autoantigen presentation leading to the emergence of neoepitope and autoimmune manifestations, as seen in SLE [75] . The third theory of persistent viral infection and oligo/polyclonal activation can be explained by the constant presence of viral antigens driving immune-proliferation resulting from the ineffective clearance of viruses [76] . Such mechanisms are commonly encountered with the Epstein-Barr virus which resides in autoreactive B cells imparting immortality to the cells. These long-living autoreactive B cells cause lymphoproliferation and polyclonal activation culminating in chronic autoimmunity [77] . As the possibility of the persistence of the SARS-CoV-2 virus cannot be completely ruled out, this may add to an additional mechanism of autoimmunity. On a brighter side, complete clearance is reported in cases of other members of the coronavirus family following the development of specific antibodies [78] , making the possibility of persistent immune activation due to SARS-CoV-2 chronicity less likely. Knight et al. noted the formation of neutrophil extracellular traps (NETs) by demonstrating elevated serum levels of cell-free DNA, myeloperoxidase-DNA complexes, and citrullinated histone H3 among patients with COVID-19 compared to healthy controls [79] . NETs consist of extracellular webs of nuclear chromatin materials and supporting histones along with antibacterial proteins and oxidant enzymes from neutrophilic granules. The principal role of this transient phenomenon is to trap the microorganisms to resolve the infection. However, sustained netosis may beget inflammation and thrombosis [80] . In the past decade, researchers have established the role of netosis in autoimmune disease including but not limited to, SLE, RA, anti-neutrophil cytoplasmic antibody-associated vasculitis (AAV), antiphospholipid antibody syndrome (APS), and an autoinflammatory syndrome, deficiency of adenosine deaminase-2 (DADA-2) [81] . In COVID-19, a rise in neutrophil counts during early infection is a poor prognostic factor. The migration of these neutrophils towards virally infected sites follows the IL-8 gradient [82] . The production of free oxygen radicals and IL-1β by macrophages and pyroptosis of virally infected cells stimulate and maintain netosis of the migrated neutrophils [83] . Exposure of chromatin, histones, and neutrophil granules with sustained netosis, serve as a source of autoantigens. This may lead to the recognition of self-peptides by antigen-presenting cells and result in the expansion of autoreactive cells (Fig. 3) . In addition to the interaction of virus and host, the environmental factors play an important role in susceptibility and protection to autoimmune disease [45] . A paradigm shift has taken place in the human behavior with advocacy of social distancing, handwashing, use of mask and gloves, and movement restrictions in an attempt to contain the spread of COVID-19 [21] . This shift is a double-edged sword in the mosaic of autoimmunity. The positive side is reflected by a reduction in airborne infectious disease with a documented shortening of the influenza season in the northern hemisphere by about six weeks [84] . This may reduce the occurrence of subsequent effects of influenza infection on RMDs. Diseases like acute rheumatic fever, where overcrowding plays an essential role in familial predisposition may also reduce with these hygienic measures [85] . At the negative spectrum of these hygienic measures, an increase in autoimmune disease should be born in mind as proposed by the proponents of the hygiene hypothesis. During the second half of the twentieth century, with the improvement in lifestyle especially in the developed world, the burden of infectious disease reduced with a parallel increase in autoimmune diseases like type1 diabetes mellitus, inflammatory bowel disease, and multiple sclerosis [86] . One of the key influencers to hygiene hypothesis is the microbiome of an individual which plays a protective role against autoimmune diseases. The mechanisms conferring protections by microbiome include antigenic competition, the role of lymphocyte homeostasis against pathogens, and the effect on immune regulatory pathway favoring the anti-inflammatory milieu with IL-10 and TGF-β secretion from the regulatory cells and TLR mediated receptor desensitization [87] . Disturbance in this microbiome is evident in the pathogenesis of autoimmune disorders including RA, SLE, and inflammatory bowel disease [88] . Over practice of distancing and antimicrobial sanitizers may lead to dysbiosis triggering autoimmunity. Additionally, the psychological stress caused by isolation, the anxiety of contracting the infection, and the economic burden along with its numerous adverse short-and long-term socio-economic consequences can turn as a trigger of various autoimmune diseases [89] . In summary, understanding the immune consequences of SARS-Cov-2 interaction with the host along with the environmental changes may explain the basis of rheumatic musculoskeletal manifestations. Currently, a multitude of vaccines for the prevention of SARS-CoV-2 infection is under investigation [90] . Except for the nucleocapsid, all immunogenic epitopes have been reported to have at least one match with human proteins [55] . This homology between the human and viral proteins is an established factor in vaccine-induced autoimmunity with the mechanism of molecular mimicry, as discussed above. Furthermore, there is a theoretical possibility of the involvement of pathogenic priming in re-infection by COVID-19, triggering the release of proinflammatory cytokines leading to cytokine storm [55] . Similar instances have been experienced with H1N1 influenza [91] , MERS [92] , and SARS [93] . Thus, while developing vaccines, these epitopes should be carefully excluded to minimize unintended autoimmunity due to the risk of pathogenic priming. The clinical outcome of the patients relies on the fate of interaction between the SARS-CoV-2 virus and immune cells of the host [94] . Modulation of this virus-host cell interaction and its aftereffects can be possibly achieved with immunomodulatory therapies repurposed from drugs used in autoimmune diseases. The potential drug targets for COVID-19 has been reviewed extensively by Misra et al. [8] . In the early asymptomatic or mild symptomatic stage, antiviral therapy is likely to have maximum efficacy, and the addition of interferon therapy at this stage may theoretically benefit from augmenting the innate antiviral response. In the next stage of pulmonary and systemic hyper-inflammation, early initiation of immunosuppressant, including IVIg, corticosteroids, IL-6, or IL-1 inhibitors may help to halt the immune-mediated damage [95] . We have summarized the plausible role of targeted immunosuppressive therapy in Fig. 4 . The role of IL-1 and IL-6 in particular is worth noticing as they are significant drivers of proinflammation in the cytokine release syndrome (CRS) of COVID-19. The IL-1 receptor antagonist, anakinra, has proven its beneficial effects in the MAS in Anti IFN γ-> emapalumab. (9) JAK inhibitor-> baricitinib, ruxolitinib, tofacitinib (multi-cytokine targeted therapy). CCL2 chemokine (C-C motif) ligand 2, CXCL8 C-X-C motif chemokine ligand 8, ACE2 angiotensin-converting enzyme 2, GM-CSF granulocyte-macrophage colony-stimulating factor, IFN interferon, IL interleukin, IRAK4 interleukin-1 receptor-associated kinase 4, IRF interferon regulatory transcription factor, ISRE interferon-stimulated response element, JAK Janus kinase, MDA-5 melanoma differentiation-associated protein-5, MVAS mitochondrial antiviral-signaling protein, MYD88 myeloid differentiation primary response-88, RIG-1 retinoic acidinducible gene-I, NF-κB nuclear factor kappa-light-chain-enhancer of activated B cells, STAT signal transducer and activator of transcription, TLR toll-like receptor, TNF tumor necrosis factor rheumatic diseases, as well as in chimeric antigen receptor T cell (CAR-T)-mediated severe CRS [96, 97] . Data from uncontrolled or historically controlled case series are encouraging for anakinra showing its safety and benefit on mortality, especially in critically ill patients with COVID-19 [98] . The high level of IL-6 in COVID-19 patients results from the secretion through viral infected respiratory cells, as well as from the infiltrating lymphocytes and monocytes [99] . The anti-IL-6 treatment strategy has shown efficacy in a similar CRS-like phenomenon observed with MAS in rheumatic diseases and has become an attractive target for COVID-19 critical cases [100] . To date, the supporting evidence for the beneficial effects of tocilizumab (a monoclonal antibody against the IL-6 receptor) is limited to observational studies [101] . The results of ongoing randomized controlled trials with IL-1 and IL-6 targeted therapy may clarify the role of tocilizumab in COVID-19. The downstream effects of IL-6 and its receptor are mediated via cytosolic Janus kinase-signal transducer and activator of transcription (JAK-STAT) signaling, a common pathway for multiple proinflammatory cytokines. Trials with baricitinib, which targets JAK-STAT signaling, are also ongoing in critically ill patients with COVID-19 [102] . The authors declare that they have no conflict of interest.@story_separate@In this review, we primarily aimed at understanding the potential pathways and manifestations leading to autoimmunity and other RMD-like illnesses that could be triggered due to COVID-19 or the treatment for the same. This information may help the rheumatology community to tackle the threat of novel RMDs, RMD mimics, and other manifestations, including cytokine storm, Kawasaki disease, and coagulopathy. The immune consequences of SARS-CoV-2-host interaction along with the environmental changes, explain the basis of rheumatic musculoskeletal manifestations of COVID-19. There is a need for preparedness for a possible surge in diverse autoimmune diseases following the pandemic. The rheumatology community is already joining hands to treat, support, and inform our existing patient partners during this pandemic with the advantage of accessibility to cutting edge technology at our disposal. With a likely long-term coexistence of SARS-CoV-2 and the human host and the use of numerous therapeutic strategies, this preparedness may help in the effective management of the rheumatic manifestations of SARS-CoV-2 infection in the post-COVID-19 era. Author contributions SS, VSN, and DD have initially conceptualized the review; SS, CKG, SD, and AMB were involved in drafting and critically revising the review; SS prepared all the figures with inputs from CKG and VSN; VSN and DD have provided expert inputs and updated the final review. All authors have provided substantial contributions to the conception and design of the work along with the interpretation. All authors have substantially contributed in drafting the manuscript and revising it critically for important intellectual content. All authors have approved the final version of the manuscript. All authors agree to be accountable for all aspects of the work related to accuracy and integrity. Funding No funding or support was received for the work.","The coronavirus disease-2019 (COVID-19) pandemic is likely to pose new challenges to the rheumatology community in the near and distant future. Some of the challenges, like the severity of COVID-19 among patients on immunosuppressive agents, are predictable and are being evaluated with great care and effort across the globe. A few others, such as atypical manifestations of COVID-19 mimicking rheumatic musculoskeletal diseases (RMDs) are being reported. Like in many other viral infections, severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) infection can potentially lead to an array of rheumatological and autoimmune manifestations by molecular mimicry (cross-reacting epitope between the virus and the host), bystander killing (virus-specific CD8 + T cells migrating to the target tissues and exerting cytotoxicity), epitope spreading, viral persistence (polyclonal activation due to the constant presence of viral antigens driving immune-mediated injury) and formation of neutrophil extracellular traps. In addition, the myriad of antiviral drugs presently being tried in the treatment of COVID-19 can result in several rheumatic musculoskeletal adverse effects. In this review, we have addressed the possible spectrum and mechanisms of various autoimmune and rheumatic musculoskeletal manifestations that can be precipitated by COVID-19 infection, its therapy, and the preventive strategies to contain the infection."
"Corona Virus Disease 2019 was reported in Hubei, China on 31 December 2019 and the WHO declared a global pandemic disease after one month. The infection was spreading at an alarming rate both domestically and internationally. 1 According to the WHO, more than 25 million confirmed cases of COVID-19 and 800,000 deaths have been reported globally as of 31 August 2020. 2 On March 13, 2020, the Ethiopian Federal Ministry of Health has confirmed a coronavirus disease case in Addis Ababa, Ethiopia. Consequently, the government of Ethiopia suspended schools and public gatherings. The total confirmed cases increased to 51,910 and the reported death rate of 815 as of 31 August 2020. 3 People infected with COVID-19 may have little or no symptoms and the symptoms ranged from mild symptoms to severe illnesses, and the incubation period of COVID-19 may last 2 weeks or longer. The disease may still be infectious during the latent period of infection and the virus can spread through respiratory droplets and close contact from person to person. 4 In the fight against the pandemic, it is crucial to be able to identify the rate at which the epidemic spreads. Awareness at the level of spread at any given time has the ability to help governments plan and develop public health policies to deal with the consequences of the pandemic. The way to be aware of the magnitude of the spread, and thus the timing of its peak, is to be able to accurately predict the number of active cases at any given time. 5 Epidemic mathematical models are best possible technique in analyzing the control and spread of infectious diseases. Time-series analysis is a tool to extrapolate forecasts, in which the mathematical model is established based on to the regularity and trend of the historical values observed over time and has been commonly used in predicting the spread of COVID-19. Modeling the disease and providing future forecasts of the possible number of cases per day may help the health care system to prepare for new patients. The statistical prediction models are therefore useful both in predicting and monitoring the global threat of pandemic. Therefore, it is extremely important to create models that are both computationally competent and practical in order to help policy makers and medical staff. 6, 7 Auto Regressive Integrated Moving Average (ARIMA) models are the most commonly used methods. 8, 9 The ARIMA model has been successfully applied in the field of medical research due to its simple structure, fast implementation and ability to explain the data set. 10 The use of ARIMA to forecast time series is important with uncertainty as it assumes no knowledge of any underlying model or relationship as in some other methods. Generally, ARIMA depends on past series values as well as earlier forecast error terms. However, in relation to short-run forecasting, the ARIMA models are comparatively more robust and efficient than more complex structural models. 11, 12 The ARIMA methodology is a statistical approach used to evaluate and create a forecasting model that best represents a time series by modeling the correlations in the data. Many of the advantages of the ARIMA model have been found in empirical research and support the ARIMA as an effective way in particularly short-term time series prediction. A major advantage of the ARIMA approach is that it makes no assumptions about the number of terms or the relative weights to be applied to the terms. 13, 14 The advantage of the ARIMA model is its versatility to reflect with simplicity, numerous time series varieties, as well as the related Box-Jenkins methodology for optimum model construction operation. 8, 15 In addition, ARIMA model gives weight to past values and error values to correct model prediction more reliable than other basic regression and exponential methods. Generally, ARIMA models frequently outshine more complex structural models in terms of short-term predictive capabilities. 16 A number of studies were conducted to evaluate the global forecasts for COVID-19. A study in Iran showed that the ARIMA model predicts that Iran can easily show an increase in daily COVID-19 total confirmed cases and total deaths, while the daily total confirmed new cases, total new deaths. The study predicts that Iran will be able to control COVID-19 in the near future. 17 A study conducted in Nigeria to develop an appropriate predictive model could be used as a decision-making tool for the health interventions and to minimize the spread of Covid-19 infection. Data on the daily spread were collected for the development of the autoregressive integrated moving average (ARIMA) model and the result showed a sharp increased trend of COVID-19 spread in Nigeria within the specified the time frame. 18 A study conducted in Italy used the ARIMA model to forecast reported and recovered case of the COVID-19 outbreak. The projections for confirmed cases may exceed 182,757, and the recovered cases could be reported 81,635 at the end of May. The final findings suggest that there will be a decrease of about 35% in confirmed cases and an increase of 66% in recovered cases. 19 To our knowledge, there is no study conducted on the trend analysis and forecasting of COVID-19 in Ethiopia. Thus, the main objective of the study was therefore to analyze trends in the spread of COVID-19 using ARIMA models and to find the best predictive model and apply it to the possible predictive occurrence of COVID-19 cases in Ethiopia. Therefore, this study will help policy makers and the public to adopt new strategies and strengthen existing preventive measures against the COVID-19 pandemic and can help predict the health infrastructure needs in the near future. The contributions of this paper can be summarized as follows: The first contribution is to find the best empirical http://doi.org/10.2147/IJGM.S306250@story_separate@International Journal of General Medicine 2021:14 model that has been established for the prediction of newly reported and recovered cases of COVID-19, the precision of which helps governors in decision-making to handle the pandemic and health system strategies; Second contribution, we can highlight the trend of reported and recovered cases of COVID-19 in Ethiopia. In addition, this paper explores a sample forecasting approach 60 days ahead. This forecast result enables us to check the efficacy of the forecasting models in various situations, helping in the battle against COVID-19 in Ethiopia in future strategy. The rest of the article is organized as follows: Dataset Description includes a description of the dataset used for this study. The forecasting models used in this study are described in Auto-Regressive Integrated Moving Average (ARIMA) Models to Parameter Estimation and Model Validation for details of the procedures used in the research methodology. Results obtained, related discussions and conclusions on the performance forecasting models are given in Result, Discussion and Conclusion. Regular updates of officially confirmed cases of COVID-19 were collected from the official website of the Ethiopian Public Health Institute (EPHI). A total 172 observations of laboratory-confirmed, recovered and fatal cases of COVID-19 were included in the study from 13 March to 31 August 2020. 3  Auto-Regressive Integrated Moving Average (ARIMA) Models The ARIMA model forecasting approach differs from other approaches because it does not consider specific trend in the historical data of the sequence to be predicted. It uses an interactive approach to identify a possible model from a general model class. The chosen model is then tested against historical data to see if the sequence is correctly represented. This model uses past errors as a dependent variable. 20 Let u t t ¼ 1; 2; 3; . . . ð Þ be a white noise process, a sequence of random variables independently and identically distributed (iid) E u t ð Þ ¼ 0 and Var u t ð Þ ¼ σ 2 ; then the q th order MA model is given as: This model is described in terms of past errors and thus, we estimate the coefficients θ j ; j ¼ 1; . . . q. Therefore, only q errors will affect the existing y t level, but higher order errors do not affect y t . This indicates that it is a short memory model. According to an autoregressive model of order p, an AR (p) can be expressed as; The model is described in terms of past values and therefore we would like to estimate the coefficients a j ; j ¼ 1; . . . p, and use the model for forecasting. All previous values will have cumulative effects on the existing y t level, which is a long-run memory model. 21 ARIMA modeling methods were used in this study based on a common method available for modeling and forecasting the time series data. ARIMA is the most common class of time series models which can be made ""stationary"" by differencing (if necessary), possibly in combination with non-linear transformations such as logging or deflating (if necessary) ARIMA (p, d, q) is the general non-seasonal ARIMA model: where p is the number of autoregressive terms, d is the number of differences and q is the number of moving average terms. A white noise model is classified as ARIMA (0, 0, 0) since there is no AR part because y t does not depend on y tÀ 1 , there is no differencing involved and also there is no MA part since y t does not rely on e tÀ 1 . For instance, if y t is non-stationary, we take a first-difference of y t so that Δy t becomes stationary. Δy t ¼ y t À y tÀ 1 (d = 1 implies one-time differencing) is an ARIMA (p, 1, q) model. A random walk model is classified as ARIMA (0, 1, 0) because there is no AR and MA part involved and only one difference exists. 22 The data required should be stationary for the development of time series models. If non-stationary data are used in a model, the results can show a relationship that is misleading. Therefore, time series data must be checked for stationary before the model is defined.  Generally, a time series is stationary if it is described by constant mean and variance, and an autocovariance that does not depend on time. If any of these requirements are not fulfilled, the data shall be considered nonstationary. The autocorrelation function (ACF) will be used to define this problem, and if the ACF plot is positive and shows a very slow linear decay pattern, the data are non-stationary. The issue of non-stationarity can be resolved by appropriate data differencing if it is caused by mean or model transformation caused by variance. Partial autocorrelation (PACF) is characterized as a linear correlation between Y t and Y (t-k), which controls the possible effects of linear relationships between intermediate lag values. The next is to determine the initial values for seasonality and nonseasonality orders (P and q). 23  After identifying the appropriate ARIMA order (p, d, q), we tried to find precise estimates of the model parameters using the least squares as described by Box and Jenkins. The parameters are obtained by the maximum probability for the time series, which is asymptotically accurate. For Gaussian distributions estimators are generally adequate, efficient and consistent and are asymptotically normal and efficient for non-Gaussian distributions. In this study, STATA v. 15 and SPSS version 25 softwares were used to develop the ARIMA model. The statistical significance level was set at 0.05. Models chosen in the last stage were validated using methods which include Root mean squared error (RMSE), mean absolute percentage error (MAPE) and normalize Bayesian information criteria (BIC). 23, 24 Result The overall data on the distribution of COVID-19 were collected and analyzed from 13 March 2020 to 31 August 2020. A total of 51,910 COVID-19 cases were observed from March 13, 2020 to 31 August 2020, and the incidence showed a rising trend day by day, with a high rate of increase after mid-August 2020. Total recovered and death rates as of 31 August 2020 were 37.2% and 1.57% of the totals, respectively, for the highest incidence and recovery ratio since the COVID-19 index in Ethiopia. The average total number of confirmed, recovered and reported cases per day from 13 March 2020 to 31 August 2020 was 301.8, 112.2 and 4.74, respectively ( Table 1) . The descriptive analysis of the overall data showed that the new daily COVID-19 confirmed cases and recovered cases significantly increased after the 154th and 143th days, respectively, since the outbreak of the epidemic. It displayed a progressively upward trend, suggesting a possible un-stabilized epidemic and a steady upward trend. From 21 June to 21 July, the number confirmed and recovered cases was almost constant. However, the number of confirmed and recovered cases increased by almost double as of August 2020 compared to July 2020 reports. However, the number of deaths remained stable between 13 March to 30 August, 2020 with minor changes. In Ethiopia, the trend of COVID-19 has been increased progressively in the upward direction for six months starting from the first reported case on 13 March 2020 (Figure 1 ). In the identification of the model, the ACF and PACF were applied in COVID-19 confirmed cases to check if the data were stationary. A very slow linear decay pattern can be corrected by first degree order of differentiation. After applying autocorrelation, the moderately large negative spike at the second lag followed by correlations that bounce around between being positive and negative and all of which are either not statistically significant or just barely cross the threshold of statistical significance. The steady decline in the partial correlations towards zero. Finally, the first difference of COVID-19 confirmed cases was best characterized as the following a second-or thirdorder moving average process. This indicates that the first variation in COVID-19 recovered cases is better described as following the first-order moving average process (Figures 2-7) .  The stationary test was conducted using the Augmented Dickey-Fuller Test (ADF). In order to apply the ARIMA modeling technique effectively, the series must be stationary and free from any sort of trend. Thus, to confirm the status of the daily confirmed and recovered cases of COVID-19 in Ethiopia, the ADF test was used to validate the stationarity observed from the series transformation (ADF test: t ¼ À 13:902 and P<0:05; t ¼ À 15:970 and P<0:05 for confirmed and recovered cases, respectively, indicating there is no unit root that means the series are stationary at first lag). However, the time series was not found to be stationary, which is the natural form of the data, and then we transformed into stationary by making the first difference (Table 2 ).  The order of the model was determined on the basis of ACF and PACF after a common difference. The following candidate models were developed based on the spikes seen in the ACF and PACF graphs. The candidate model with the lowest value of RMSE, MAPE and Normalize BIC was identified as the best model to match the daily spread of the COVID-19 in Ethiopia. The p and q parameters of the ARIMA models were predicted and the projected models were then compared to the RMSE, MAPE and BIC values. This suggests the estimation of ARIMA (0, 1, 5) and ARIMA (2, 1, 3) models for the forecasting of daily spread and the recovery cases of COVID-19 in Ethiopia, respectively. The guess models below were compared to different ARIMA models using model selection criteria such as RMSE, MAPE and BIC, but the model suggested proved to be relatively robust compared to other competing models using SPSS V25 software. Considering the RMSE and BIC values, it is clear that the ARIMA (0, 1, 5) model has the lowest RMSE, MAPE and BIC values, making it the most effective modeling and forecasting of the spread of COVID-19 in Ethiopia. The same is true for the recovered cases, we were able to measure the aforementioned candidate models and also to use the above model selection criterion, finally we have detected that the daily recovered cases used ARIMA (2, 1, 3) as the best model with the lowest RMSE, MAPE and BIC values. The performance of the various ARIMA models with different orders of Autoregressive and Moving Average were checked and verified using statistics such as RMSE, MAPE and BIC. The results show that the proposed model performed well, both in-sample and out-of-sample (Table 3) . The best candidate models for confirmed and recovered cases were ARIMA (0, 1, 5) and ARIMA (2, 1, 3) respectively, based on the RMSE, MAPE and BIC criterion. The model was then estimated with its forecasting parameter for the daily confirmed and recovered series of COVID-19 in Ethiopia (Tables 4 and 5 ). The best suited models can be re-written based on the findings and evaluation of the different ARIMA model described as presented in Tables 4 and 5 respectively. ΔY t ¼ u t À 0:88u tÀ 1 þ 0:343u tÀ 2 À 0:587u tÀ 3 þ 0:161u tÀ 4 þ 0:249u tÀ 5 (4) Where; Y t represents the value of daily confirmed cases, u t : represents the error terms Where; Y t represents the value of daily recovered cases, u t : represents the error terms The daily spread data from 13 March to August 31, 2020, were predicted using the ARIMA (0,1,5) model and the daily recovered were predicted using the ARIMA ( forecast for the next 2 months. The daily forecast was the point forecast with the 95% confidence limit of the upper and lower boundary values. The model's forecasting power is very high as demonstrated by the slight gap between real and fitted values (Table 6) . We can clearly conclude that the model selected can be used for modeling and forecasting the spread of COVID-19 in Ethiopia. Therefore, the forecasts showed that the spread of COVID-19 confirmed and recovered cases in Ethiopia would increase daily for the next sixty days (Figures 8 and 9 ). The study presented current trends of COVID-19 outbreak from March 13, 2020 to 31 August, 2020 as visualized in the EPHI official website report. Since then, COVID-19 cases showed an uptrend. Total recovery and death rates as of 31 August, 2020 were 37.2% and 1.57%, respectively, which reflected the peak incidence and recovery ratio since the outbreak of COVID-19 in Ethiopia. And, the number of confirmed, recovered and death rates were increased significantly. Based on the findings of the study, the spread of COVID-19 in Ethiopia was expected to move in an upward trend. Having developed an appropriate model, Ethiopia can apply this model to forecast the trend of COVID-19. In Ethiopia, starting with the first reported case, the COVID-19 trend showed a progressive upward direction for six months, which was consistent with the Nigerian study. 25 However, the trend of confirmed COVID-19 cases in Ethiopia has shown that it is better than the US and European countries, though they had comparatively higher testing capacities. Having significant level of inadequate preventive practice measures in Ethiopia, 26, 27 thus there is important to comprehend the trend of COVID-19 and to generalize the implications of the strategies used by the government to mitigate the spread of the disease. The candidate models were obtained using the autocorrelation function (ACF) and the partial autocorrelation function (PACF). The models were designed based on the peaks found in the ACF and PACF charts. Both ARIMA (0, 1, 5) and ARIMA (2, 1, 3) were found to be the optimal model for confirmed and recovered COVID-19 cases, respectively, based on the lowest RMSE, MAPE and BIC values. This model was then used to study the trend of COVID-19 and the estimated increase in the number of confirmed and recovered cases. The finding of the study was consistent with the study conducted in Nigeria, which showed an upward trend in the spread of COVID-19 within the selected timeframe. 18 The ARIMA model has been widely used in the infectious disease outbreak modelling. ARIMA, time series coupled with corrective gradual changes successfully predict a linear trend, but fails to forecast a series with turning points. 28 The current study used the complete periodic data to establish the ARIMA models and to forecast epidemic in the next 60 days. The ARIMA model fit well and is more suitable for short-term prediction. The ARIMA model was recently used to predict the dynamics of COVID19 disease with acceptable accuracy in a study conducted in Iran, Saudi Arabia, and a study conducted in the 15 most affected countries. 17, 29, 30 The optimal predictive ARIMA model was validated for confirmed and recovered COVID-19 cases based on lowest RMSE, MAPE and BIC value. It was estimated that the less out-of-sample forecast error and the lowest value are preferable, and which may contribute to the future forecast in Ethiopia. In the current study, wide confidence intervals help to address any unforeseen changes in the forecast of dynamic COVID-19 cases. The prediction interval allows users to determine future uncertainty and to prepare different strategies for the range of possible outcomes. In addition, the wider prediction interval resulting from the non-stationary process was more practical in allowing for higher uncertainty and helps to illustrate the special significance of model identification, especially in evaluating whether or not the data is stationary. 31 Furthermore, it is very important to discuss all the studies conducted on the basis of different techniques applied to COVID-19 prediction using statistical, mathematical/analytical and machine learning/data science models to control the spread of COVID-19 globally and for a specific country and to evaluate its impact, to create COVID-19 vulnerability index [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] . According to the model prediction, we need to be more aware of the tendency of COVID-19 spreading more than currently observed. In addition, based on the study findings, the trend towards the spread of COVID-19 in Ethiopia is expected to move upward. As a result, rapid control of infections in healthcare settings and in the community is mandatory in order to achieve success with COVID-19 prevention. It can also be used as a decisionmaking tool to allocate health interventions and mitigate the spread of Covid-19. This tool can also be used to more reliably forecast short-term disease transmission indicators, to provide response control at all levels of the departments and to provide short-term emergency prevention programs for policy makers. Having established an appropriate model, Ethiopia can apply this model to predict the trend of COVID-19 in the country. ARIMA model forecasts are stable in all variables in the near future, which may be useful in prevention of the COVID-19 pandemic. The ARIMA model can provide rapid assistance in forecasting cases and developing a better preparedness plan in Iran. 17 The ARIMA model is one of the most commonly used time series forecasting methods due to its simplicity and systematic structure and appropriate forecasting performance. 32 Based on the findings of the study, it was predicted that the spread of COVID-19 in Ethiopia would move upward and the model could be used to predict the COVID-19 trend in the country. ARIMA models were used to predict the progression of infectious diseases in order to identify the possible outcomes of an outbreak. However, artificial intelligence (AI) has the potential to help in all the stages of healthcare, from surveillance through to rapid diagnostic tests, and faster drug development. AI may also help to decide which patients should be prioritized for treatment and quickly learn which factors predict a higher risk of mortality, as well interventions and population-level controls, have led to reduced harm. 33, 34 As the number of COVID-19 cases increased nationally in Ethiopia and different studies showed the majority of the community had poor practice on preventive measures, 26, 27 there should be a need to focus on further measures to minimize the spread of COVID-19. All daily series of open-source data that support the findings of this study are also available from regular updates by the Ethiopian Public Health Institute: https://www.ephi. gov.et/[accessed on 10/01/2020].@story_separate@The current study showed that the spread of COVID-19 in Ethiopia is expected to move upward. Both ARIMA (0, 1, 5) and ARIMA (2, 1, 3) were found as the best model for confirmed and recovered COVID-19 cases, respectively, on the basis of the lowest RMSE, MAPE and normalized BIC values. Forecasts have shown that spread of COVID-19 confirmed and recovered cases in Ethiopia will progressively increase on a daily basis for the next 60 days. The study developed an appropriate statistical model which can be used as a decision-supporting method to implement health interventions and mitigate the spread of Covid-19 infection. While the accuracy of the proposed ARIMA models can be considered good, valid and satisfactory, and despite the fact that the projected values are classified as reliable forecasts. The study indicated that the ARIMA model was an easy-to-use modeling method for rapid forecasting the spread of COVID-19 in Ethiopia. In addition, we recommend to use other forecasting methods such as exponential smoothing and compare the results to our best selected ARIMA models as a baseline for new and recovered cases in Ethiopia. The limitation of the study was no risk factor was evaluated and analyzed, including demographic details of patients, their social network and travels due to the lack of individual-level data. Abbreviations ACF, autocorrelation function; ANFIS, adaptive neurofuzzy inference system; ADF, augmented Dickey-Fuller test; ARIMA, autoregressive integrated moving average; BIC, Bayesian information criteria; PACF, partial autocorrelation function; CDC, communicable disease control; CI, confidence interval; CMC, composite Monte-Carlo; CUBIST, cubist regression; COVID-19, corona virus disease 2019; EPHI, Ethiopia Public Health Institute; MAPE, mean absolute percentage error; RF, random forest; RMSE, root mean squared error; SPSS, Statistical Package for Social Science; VMD, variational mode decomposition; WHO, World Health Organization.","INTRODUCTION: COVID-19, which causes severe acute respiratory syndrome, is spreading rapidly across the world, and the severity of this pandemic is rising in Ethiopia. The main objective of the study was to analyze the trend and forecast the spread of COVID-19 and to develop an appropriate statistical forecast model. METHODOLOGY: Data on the daily spread between 13 March, 2020 and 31 August 2020 were collected for the development of the autoregressive integrated moving average (ARIMA) model. Stationarity testing, parameter testing and model diagnosis were performed. In addition, candidate models were obtained using autocorrelation function (ACF) and partial autocorrelation functions (PACF). Finally, the fitting, selection and prediction accuracy of the ARIMA models was evaluated using the RMSE and MAPE model selection criteria. RESULTS: A total of 51,910 confirmed COVID-19 cases were reported from 13 March to 31 August 2020. The total recovered and death rates as of 31 August 2020 were 37.2% and 1.57%, respectively, with a high level of increase after the mid of August, 2020. In this study, ARIMA (0, 1, 5) and ARIMA (2, 1, 3) were finally confirmed as the optimal model for confirmed and recovered COVID-19 cases, respectively, based on lowest RMSE, MAPE and BIC values. The ARIMA model was also used to identify the COVID-19 trend and showed an increasing pattern on a daily basis in the number of confirmed and recovered cases. In addition, the 60-day forecast showed a steep upward trend in confirmed cases and recovered cases of COVID-19 in Ethiopia. CONCLUSION: Forecasts show that confirmed and recovered COVID-19 cases in Ethiopia will increase on a daily basis for the next 60 days. The findings can be used as a decision-making tool to implement health interventions and reduce the spread of COVID-19 infection."
"In 2013, China launched the regional cooperation initiative ""the Silk Road Economic Belt and the 21st-Century Maritime Silk Road"" (i.e., ""The Belt and Road"" or ""The B&R"" Initiative), intending to promote the orderly and free flow of economic factors. ""The B&R"" Initiative is the greatest economic community in scale, covering 64% of the world's total population (Huang, 2016) , one-third of global GDP, 75% of the total energy reserves, and a quarter of global cross-border trade in goods and services. ""The B&R"" Initiative has facilitated policy coordination, unobstructed trade, financial integration, and people-to-people contacts. On the one hand, ""The B&R"" Initiative has enhanced regional economic development (Wong, 2017) , and accelerated partnership establishment in the process of RMB internationalization (Zhang et al., 2017) . On the other hand, the dominant position of dollar in trade settlement seems to stuck the process of renminbi internationalization in ""The B&R"" Initiative (Yan, 2020) . And the possibility of systemic risk derived from the participant countries' heterogeneous economic development status-had brought concern to merchants, investors, and policymakers. The exchange rate, as determined by money supply, price level, national income, interest rate, output, and other relative economic variables (Stcokman, 1980; Frankel, 1992; Bacchetta and Van, 2006) , fluctuated when changes happen in the country's economic openness, the flexibility of the exchange-rate regime, new information disturbance, etc. (Stanèík, 2007) . Specifically in the system that we are exploring in the article-""The B&R"" Initiative, research shows that exchange rates of participant countries fluctuated more widely and frequently in recent years (Lai and Guo, 2017) . As globalization goes deeper, the spillover effect not only exists in domestic financial assets but also occurs across markets and countries. There are three main paths for the spillover of exchange rate fluctuations. The first is associated with trade integration-countries in one geographical region tend to form regional currency bloc (Subramanian and Kessler, 2013) . The second correlates to the active global financial markets where geographically decentralized countries link to each other (Shu et al., 2015) . The third is among countries that have pegged exchange regime correlation. To better investigate the risk involved in the cross-border trading activities, some studies focused more specifically on spillover effects between currency markets, adopting the approach of constructing spillover index (e.g. McMillan and Speight, 2010; Bubák et al., 2011) and analyzing the dynamic spillover structure (i.e., risk transmission path) of the exchange rate fluctuation (Liao et al., 2019) . To fill the gap of the former research, this paper constructed the spillover index in a nascent currency market system (""The B&R"") and analyzed the time-varying dynamic effects of RMB and other participant currencies since ""The B&R"" Initiative was launched. More specifically, we portrayed the overall intensity of currency interdependency and individual currency stability under the strike of the COVID-19 pandemic. This paper is organized as follows: Section 1 introduces the research background. Section 2 presents the research methodology and data. Section 3 shows empirical results and discussions both in long-term view and in the COVID-19 period. Section 4 draws the conclusions.@story_separate@• • Highlights • • Volatility spillover effects are examined in ""The B&R"" currency market. • • ""The B&R"" system spillover index reflects some sudden regional crises. • • COVID-19 has affected systemic stability and the influence of RMB. The proposal of ""The Belt and Road"" (The B&R) Initiative has promoted regional economic cooperation and financial integration. It is crucial to measure the volatility spillover effects among ""The B&R"" currency market. Results from the time-varying spillover model show that ""The B&R"" system spillover index reflects some sudden regional crises. Likewise, the spillover The measurement of exchange rate spillover effects is based on generalized vector autoregressive (GVAR) models and forecast error variances decomposition (FEVD) developed by Diebold and Yilmaz (2009; 2012 (1), whose entries are the share of H-step-ahead error variance in forecasting i x due to shocks to j x . where  is the variance matrix for the error vector  , jj  is the standard deviation of the error term for the jth equation, and i e is the selection vector, with one as the ith element and zeros otherwise. The matrix H ~shows the spillover effects among the N variables. Using the spillovers derived from GVAR and FEVD, the total Spillover Index is defined as Equation (3), which measures the spillover effect of the whole model system: The Spillover Index of the system presents the average contribution of spillovers from shocks to the forecast error variance.internal spillover interaction among the exchange market system. Regarding the spillover effects of a certain currency, the aggregated spillover value of one currency to others ( To Index) can be represented as Equation (4). While the aggregated spillover value of one currency from others ( From Index) shown in Moreover, the Net spillover value of the currencies given by Equation (6) shows the direction of spillover risks. A positive Net suggests that the currency is a net risk transmitter, while a negative Net suggests that the currency is a net risk receiver. The above spillover indexes are static and simple, to further present the dynamic spillovers, a time-varying series of spillover index is necessary. The full-sample time window is divided into w-day rolling windows, and scrolls forward by s-day each time. That is, for the ith rolling window starting from day t to day t+w, the (i+1)th rolling window is started in day t+s. In each rolling window, the spillover indexes mentioned hereinabove is given. This paper uses a 100-day rolling window with an 8-step forecast horizon scrolling by 1-day. To capture system risk contagion structure in ""The B&R"" Initiative system, the top 14 ""The B&R"" Initiative countries' currencies were firstly selected according to the scale of total import and export trade with China, such as Indian Rupee, Korean Won, Chilean Peso. Further, to analyze the impact of global mainstream currencies, the top 20 countries' currencies in the world for foreign direct investment (FDI) and the top 20 countries' currencies in the Chinn-Ito Index (KAOPEN Index) (Chinn MD, Ito H., 2008) , including Kuwaiti Dinar, Australian dollar, Canadian dollar, and so on, were added. After excluding the currencies pegged to the U.S dollar or the Euro with a fixed exchange rate as well as those with a large number of missing exchange rate data such as UAE Dirham, 21 currencies were finally used as shown in Table 1 . The exchange rate data was from the IMF database. The data set contains 1802 observations from January 1, 2013, to March 20, 2020, which is exhaustive and can present the development path of ""The B&R"" Initiative since it was proposed in 2013. Note: Coefficient of Variation was calculated by dividing the standard deviation by the mean. It is a statistical measure of the dispersion of data points around the mean, which is commonly used to compare the data dispersion between distinct series of data. Apparently, as shown in Table 1 , VND, AOA have relatively high coefficients of variation, suggesting that these currencies' exchange rates fluctuated violently during the period because of the impacts of some political or economic events. Fig.1 shows how exchange rate fluctuates similarly when countries are geographically close (left part) or have cohesive trading relationships (right part). Fig.2 shows the appreciation or depreciation level of currencies in two stages during COVID-19. The results illustrate that the exchange fluctuation ranges of most currencies in the second stage are much greater than those in the first stage, and the emerging economics (such as Brazil and Russia) are a lot more sensitive in this fluctuation, which suggests that the pandemic brought serious uncertainty to currencies on different levels. Fig.2 shows currency performance in two stages. The first stage was mainly during the outbreak period in China. The second stage showed currency performance after WHO announced that the COVID-19 had become a global pandemic. In this section, we characterize spillover effects in ""The B&R"" currency markets throughout the relatively long term and during the short term of COVID-19. As defined above, To Index and From Index indicates risk transmission direction between specific currency pairs. Further, to have a holistic insight, we use the Spillover Index to characterize the intensity of interdependence between currency in an internal system. Here, an upward trend of Spillover Index together with currency depreciation should cause concern from independent investors and contravention from the central bank. (Liu, 2019) Because in this case, individual currency volatility could lead to a broader currency co-movement and cause wider risk (e.g. settlement risk) in trading activities. After deleting the missing value, data are pre-processed with the methods of logarithm and first-order difference to eliminate the possible seasonality and heteroscedasticity of the time series data. The results of unit root tests ensured that all the series are stationary. Fig.3 shows the thermodynamic diagram of the static full-sample spillover table. In the whole currency system, the Spillover Index within the 21-currency system reaches 20.34%, which indicates a significant level of internal interaction in the world's currency market. When it comes to CNY's influence in the world, results show that CNY ranks 6th in To CNY , 3rd from last in From CNY and 5th in Net CNY with a positive value respectively. CNY, therefore, as a net risk transmitter, has a strong international influence in the world, and it is less likely to be affected by international exchange rate fluctuations. Fig.4 illustrates the time-varying Spillover Index in ""The B&R"" exchange market system. Starting with 67.6%, the Spillover Index increases considerably after China issued ""The B&R"" Initiative. The Spillover Index in ""The B&R"" Initiative system is larger than 60%, indicating that there is a substantial internal spillover effect in ""The B&R"" exchange market. There were also some enormous increases in Spillover Index when crisis events happened in some participant countries of ""The B&R"" Initiative. Moreover, the exchange rate spillovers during crisis periods probably convert to risk contagion, further increasing the instability within the system. S. dollar in the interbank spot foreign exchange market from 1% to 2%. Next, later on August 11, 2015, PBC reformed the RMB exchange rate central parity rate quotation mechanism. What's more, on December 11, 2015, PBC released the RMB exchange rate index and introduced a basket of currencies and countercyclical factors to enable RMB's stability and marketization. During the periods after these events, the spillover effects of CNY fluctuated considerably. All these events caused policy shocks on CNY's spillover fluctuation pattern. Fig.6 shows the dynamic Spillover Index of the two systems during the COVID-19 outbreak. After the outbreak of the pandemic, the Spillover Index in both the two systems started to fluctuate frequently, which suggests that the shocks of the COVID-19 pandemic destabilized the global and ""The B&R"" Initiative exchange market. Yet compared to the range of bursts in other period (eg. Foreign Relations Crisis of Qatar in 2018 in Fig.4 ), our empirical results show the spillover severity inside the system is limited. This situation started to change since March 8, 2020, with the global spread of the COVID-19, U.S. stocks triggered circuit breakers which halted trading for four times in 10 days, the same triggering of circuit breakers also appeared in many countries in the world. Meanwhile, Chinese policymakers stepped in early to safeguard financial stability, taking actions like backstopping interbank markets, providing financial support to firms under pressure, and letting the RMB adjust to external pressures. As the pandemic eased and the economy started to recover in China, the influence of CNY measured by To CNY started to pick up. Meanwhile, From CNY , which implies the risk of CNY to be affected by other currencies, has decreased slightly. As shown in Fig.8 , the coronavirus also led to significant changes in the spillover indexes of other currencies. In late February and early March, COVID-19 began to spread worldwide, and Europe was becoming the center of the pandemic: the confirmed cases in Italy, Spain, France, and other European countries continued to increase. After March 3, there appeared a sharp decrease in To EUR . In ""The B&R"" Initiative system, the epidemic situation is synchronized with the degree of geographical dispersion. In Southeast Asia, due to their geographical proximity to China, the pandemic had begun to spread by the end of February. Singapore, for example, has long depended on international trade as its major economic growth impetus. SGD, as a special offshore financial center's currency and the net risk receiver, is highly susceptible to fluctuations in exchange rates of other currencies. To SGD has been decreasing since late February. To test the robustness of our results, we compared different hyperparameters: rolling window width (i.e., w = 90/100/110 days) and forecast horizons (i.e., H = 6/8/10 days). Results presented in Fig.9 verify that To CNY follows a similar fluctuation pattern for each value of w and H. This means that our results are robust and consistent. @story_separate@This paper implemented the VAR-based spillover index approach to explore the spillover effects during the COVID-19 outbreak among the global exchange market and ""The B&R"" Initiative exchange market. Our main findings can be summarized as follow. Firstly, the internal interaction is strong among the global foreign exchange markets and among ""The B&R"" Initiative participant countries. Second, spillovers of RMB are inevitably influenced by modifying exchange rate regimes and global economic and political events, which were reflected in the dynamic change of the spillover index of RMB, including To CNY and From CNY . Third, the results of the time-varying Spillover Index indicated that exchange rate risk contagion among the countries along ""The B&R"" Initiative is significantly related to their economic and trade relations as well as some sudden crises that happened in these countries. Regarding the recent COVID-19 events, the outbreak in China and its global spread has driven the global and ""The B&R"" Initiative foreign exchange markets increasingly risky and unstable. Specifically, the pandemics has made great shocks in RMB, impacting its spillover effects on the global foreign exchange market, and also led to considerable changes in the spillovers of other currencies in ""The B&R"" Initiative system.","The proposal of ""The Belt and Road"" (The B&R) Initiative has promoted regional economic cooperation and financial integration. It is crucial to measure the volatility spillover effects among ""The B&R"" currency market. Results from the time-varying spillover model show that ""The B&R"" system spillover index reflects some sudden regional crises. Likewise, the spillover of RMB exchange rate is affected by internal financial reforms as well as external economic shocks. Further, the recent outbreak of the COVID-19 has disrupted this system and the influence of RMB."
"Antimicrobial de-escalation (ADE) is a strategy to decrease the spectrum of the empirical antimicrobial regimen a few days into the treatment [1] . Multiple definitions have been used in the past but there appears to be consensus that ADE refers to stopping one or more components of combination therapy, changing an antimicrobial for another molecule with a narrower spectrum or a combination thereof (Fig. 1) [2] . Table 1 provides an overview of the terminology commonly used in this context. ADE was introduced in the intensive care unit (ICU) at the beginning of the century with the rationale that it may prevent the harm from (extremely) broad-spectrum empirical regimens [3] . Those were becoming increasingly necessary due to the emerging and mounting phenomenon of antimicrobial resistance (AMR) [4] . Many studies have looked at ADE in the ICU. Most were observational and published from centers with a particular interest in antimicrobial stewardship programs (ASP). ADE appears to be safe, but while improved outcomes are frequently reported, selection bias is prevalent: ADE is more frequently used in patients who are clinically improving [5] . We should be careful not to infer causation between ADE and improved clinical outcomes [1] . Regardless of the definition used or the intervention studied, it should be very clear that overall antimicrobial consumption is linked to AMR, irrespective of the class of antibiotics. Decreasing antibiotic exposure should, therefore, be the priority of any ASP [6] . In this manuscript, we aim to highlight recent insights into ADE, its value in ASPs and the practical application as well as discuss the controversies and Fig. 1 Schematic overview of the timeline of antimicrobial therapy including antimicrobial de-escalation, with the pivotal and companion antimicrobial components of the empirical regimen and most common changes within a short antibiotic course for critically ill patients with an infection. 'Antifungals' refer to antimicrobials targeting fungal pathogens, 'anti-MRSA' to antimicrobials targeting methicillin-resistant Staphylococcus aureus, 'anti-difficult to treat pathogens' to antimicrobials targeting resistance in Gram-negative pathogens, 'atypical/intracellular targeted' refers to a second antibiotic commonly prescribed for community-acquired pneumonia, 'antitoxin effect' to antimicrobials administered for the suppression of toxin and cytokine production, and 'synergistic effect' to most commonly an aminoglycoside given as combination therapy in patients with septic shock Table 1 Definition of terms@story_separate@Adequate antimicrobial therapy Antimicrobial therapy active against the pathogen responsible for infection, administered at the dose, route and mode in accordance to best current practices Broad-spectrum therapy Antimicrobial therapy aimed at covering all relevant pathogens potentially causing the infectious episode Narrow-spectrum therapy Antibiotic with activity exclusively against one specific pathogen or a more limited group of pathogens Combination therapy Two or more antibiotics aimed at 1. covering the identified or suspected pathogen(s) with more than one antibiotic to hasten pathogen clearance using antimicrobials with different mechanisms of action or 2. broadening antimicrobial spectrum Collateral effects of the antimicrobials administered to the patient, including downstream effects on the patient's microbiota favouring the acquisition, selection and overgrowth of multidrug-resistant bacteria Pivotal antibiotic Antibiotic that is central to the regimen, usually a beta-lactam antibiotic for Gram-negative severe infections Companion antibiotics Antibiotics added to the regimen to broaden the spectrum to pathogens not covered by the pivotal agent. Commonly glycopeptides and/or aminoglycosides, which are interrupted most of time after a short exposure (3 days) pitfalls related to the topic. For an overview of recent studies, we refer to the ESICM/ESGCIP position statement on this topic [2] . ADE aims to reduce broad-spectrum antimicrobial exposure, and as a result decrease the emergence of AMR, without impairing patient outcomes [5] . A randomised clinical trial comparing continuation or de-escalation of the pivotal or main antimicrobial found a decrease in broad-spectrum antimicrobial use in the de-escalation group, while the mortality rate was similar in both groups [7] . Reducing antimicrobial exposure is essential in any ASP, as antimicrobial use has an important impact on the gut where overgrowth of organisms resistant to antimicrobials significantly impacts the intestinal microbiome. Inadequate empirical therapy has been associated with an increased mortality rate in septic shock [8] . ADE indirectly legitimises the use of broad-spectrum empirical therapy, as it suggests that-once the causative pathogen has been identified and the susceptibility is known-therapy can be scaled down. Therefore, ADE would limit any further harm to the microbiome, inflicted by broad-spectrum agents and would thus allow for a broad-spectrum empirical safety net as well as for the application of antimicrobial stewardship principles. Observational studies and meta-analyses have suggested improved outcomes associated with ADE [9, 10] , but as mentioned before, any causal effect is not likely to be present. Finally, as discussed elsewhere, ADE may be associated with cost saving, since it allows reducing the use of expensive antimicrobials for short durations and using older and less expensive drugs for the continuation of treatment [5] . The dark side of de-escalation ADE was welcomed as a remedy to mitigate the effects of empirical broad-spectrum agents with the assumption that short courses of those agents have little impact on the development of AMR. However, this assumption has given us an unwarranted sense of safety that ADE would prevent the ecological consequences of extremely broad-spectrum empirical antimicrobial treatment regimens. These regimens are often considered lifesaving and necessary in patients with severe infections especially in the setting of high prevalence of AMR. Recent research, however, has clearly shown that AMR appears earlier than expected in the course of treatment, probably within the first few days [11] . Thus, ADE should not be used as an excuse for the indiscriminate prescription of broadspectrum antimicrobial regimens. When analysing the influence of this sense of safety that ADE has on our prescribing behaviour, we need to consider two other issues. First, although none of the involved studies was designed to assess its effect on total duration of therapy, ADE has been associated with an increase in the total duration of antimicrobial therapy [1] . There may be multiple possible explanations for this finding, including potential ""errors in counting total days of therapy"" and the perception that narrow-spectrum antimicrobials are harmless and can be continued for longer periods of time [12] . Second, the risk of using ADE as an excuse to continue antimicrobials in the absence of infection is likely to cause more harm than stopping all antimicrobials alltogether. On one hand, narrower agents will still cause the emergence of AMR, and on the other, continuing antimicrobials in the absence of infection may decrease the quality of diagnostic decision-making [13] . Finally, the broad-and narrower spectrum antimicrobials may differ in their pharmacokinetics resulting in insufficient concentrations at the site of infection and PK/PD target attainment, often with a disadvantage for narrowspectrum antibiotics [14] . ADE is often presented as an effective strategy to reduce AMR, but no direct associations were found between ADE and ecological impact in ICU patients. In an observational comparative study, De Bus et al. did not find associations between de-escalation and emergence of multidrug-resistant (MDR) pathogens [15] . Similar findings were reported in a randomised clinical trial comparing ADE and continuation of the pivotal antimicrobial [7] . Small but significant differences in carbapenemresistant Acinetobacter spp. colonisation were observed after carbapenem de-escalation [16] . Large numbers of patients are probably required to find a difference in terms of AMR, suggesting a limited overall ecological impact. In brief, the level of evidence showing that ADE reduces AMR is low. ADE, at least for the pivotal agent, is defined by the switch from a broad-spectrum antimicrobial to a narrower spectrum antimicrobial. However, ""grading"" of antimicrobials according to spectrum is not an easy task. A French group proposed a six-rank consensual classification of beta-lactam antibiotics. Despite several Delphi rounds, no consensus was reached to differentiate piperacillin/tazobactam, ticarcillin/clavulanic acid, fourth-generation cephalosporin and antipseudomonal third-generation cephalosporin. The group could not find an agreement on the delay within which ADE should be performed and on whether or not the shortening of antimicrobial therapy duration should be included in ADE definition [17] . In parallel, a group of experts from the US developed a numerical score to measure the spectrum of antimicrobial regimens [18] . The classification that was obtained using a Delphi consensus procedure based on clinical scenario's differed from the one reported by the French group. Piperacillin-tazobactam was the worst ecological antimicrobial for the US group, whereas imipenem was selected by the French group. This discrepancy underlines how difficult it is to assess the ecological impact of antimicrobials, and thereby to define ADE. ADE implicitly involves the use of more than one antimicrobial: either the number of antimicrobials is reduced in patients who receive combination antimicrobial therapy initially, or patients are administered two different antimicrobials sequentially. Although we generally assume that ADE is beneficial, there may also be downsides to the use of multiple antimicrobials, even for short periods of time. First, when one antimicrobial is replaced by another with a narrower spectrum, it should be considered that two antimicrobials may cause more harm than one. For example, when empirical treatment with meropenem is switched to levofloxacin, this may be considered as narrowing of the spectrum, but that patient is exposed to two courses of short duration antimicrobial therapy with a different -and potentially-cumulative damaging effect on the microbiome. Short exposure to broad-spectrum antimicrobials already results in early disruption of intestinal microbiome [19] . It has been demonstrated that as little as 1 day of exposure to imipenem is enough to result in AMR [11] . For each day of additional exposure to cefepime or piperacillin/tazobactam, the risk of MDR emergence increases with 8% [20] . Furthermore, antibiotics have been found to persist for up to 48 h at low concentrations after discontinuation [21] and these low concentrations are at high risk for the emergence of resistance. Second, combining antimicrobials in empirical therapy aims at broadening the spectrum of therapy, reducing AMR or creating synergy between drugs; although this was documented in experimental studies, the latter two effects were never confirmed in vivo. While the reduced number of antimicrobials after ADE may appear advantageous, one should question the true need for multiple antimicrobials in the first place [22] . Better risk stratification, the use of rapid diagnostic techniques and the use of surveillance cultures are all strategies that could avoid the use of multiple antimicrobials empirically [23] . Finally, the impact of combining different antibiotic classes on the intestinal flora is largely unknown [24] . Recent studies have shown differential effects according to the antimicrobial activity against anaerobes, with a four times higher risk of gut colonisation with ceftriaxone resistant Gram-negative bacteria after being exposed to anti-anaerobe antimicrobials [25] . A better insight into the effect of different antimicrobials is needed to understand the dynamics that are relevant in ADE. In most practice guidelines for ASP, ADE appears as a recommended stewardship objective [26] . In the US, a survey showed that prior authorisation for selected antimicrobials, antimicrobial reviews with prospective audit and feedback, and guideline development were common strategies in ASPs [27] , while ADE was not explicitly reported as a major component. In a French survey, reassessment of antimicrobial prescriptions, but not specifically ADE, appears as a major element of ASP for most respondents [28] . In nine Dutch hospitals, ADE was not yet included in ASP, although responders disclosed that the intervention was required in the future program [29] . Of the two most evidence-based ASP interventions (post-prescriptional review and prior authorisation [26] ), post-prescriptional review (which may include ADE) gained some advantage over the latter because of its larger effect on reducing antimicrobial use [30] . In the light of pitfalls of ADE mentioned before, post-prescriptional review by an expert remains essential for good antimicrobial practice. However, reviewing antimicrobial use only after prescription may equally stimulate unnecessary initial broad-spectrum empirical treatment. It is becoming increasingly clear that duration of therapy can be reduced to 5-7 days for most infections in ICU patients [31] , with specific exceptions such as some pathogens (e.g. S. aureus), patient conditions (immunosuppression) or inadequacy of source control. This development certainly questions the indication for ADE. If cultures become available 48-72 h after the start of therapy, what is the expected benefit of changing wellinstituted therapy for 2 more days? Apart from the considerations discussed earlier, getting ADE done properly in daily practice (collect cultures, correctly interpret cultures, instigate change of therapy in concordance with prescribers, prevent a time gap without effective antimicrobial therapy, adapting dose due to different PK properties of new antimicrobial, etc.) can be a challenge with little apparent benefit. However, this does not mean in any way that cultures should not be taken. For many reasons other than ADE (such as potentially inappropriate therapy, duration of therapy, MIC determination, complications, follow-up, epidemiology), appropriate sampling remains pivotal to ASPs in ICU. Rapid diagnostics (different molecular technology sepsis panels, metagenomics), another component of ASP, undoubtedly will change the way we will use antimicrobials in the future. For example, an observational retrospective study suggested that the use of a rapid test detecting MRSA within the first hour after bronchial sampling was associated with a reduction of empirical vancomycin or linezolid [23] . An ongoing multicentre randomised clinical trial evaluates the use of a rapid diagnostic test detecting early the presence of ESBL in patients with suspected infections to Enterobacteriaceae [32] . Rapid diagnostics would also give more opportunities for watchful waiting and not start broad-spectrum antimicrobial therapy, thereby eradicating ADE practice in subgroups of patients [33] . Especially in patients without shock, this could probably be done safely. Unfortunately, there are still a lot of uncertainties regarding the use of these tests to allow their routine use in septic ICU patients. Individualisation of antimicrobial treatment based on risk assessment and rapid diagnostics may be a less appealing strategy in institutions with high resistance rates. It is certainly easier to avoid carbapenems in hospitals where AMR rates are low. Moreover, rapid diagnostic techniques are often unavailable in low-and middle-income countries where an already higher resistance burden leads to a vicious circle of increasing AMR and indiscriminate broad-spectrum empirical treatment. In these settings where control of AMR is most urgently needed [34] -in spite of its limitations-ADE may still be felt as one of the few options to decrease broad-spectrum antimicrobial use. Counterintuitively, promoting ADE may cause an increase in broad-spectrum antibiotic use as an unexpected side effect in these settings [35] . ADE has been proposed repeatedly as an important objective for ASP in hospitals. It has been selected in a RAND-modified Delphi procedure among experts as 1 of 14 key quality indicators (QI) to measure and improve appropriate use in hospital [36] and clinimetric properties have been tested in a large group of hospitalised patients [37, 38] . Furthermore, it has been associated with reduced mortality, hospital length of stay and cost reduction in a systematic review, although the lack of a clear definition hampers interpretation of this association [39] . As a result, ADE has become an essential quality metric to evaluate the success of an ASP. For measurement purposes, ""appropriate ADE"" has been operationalised as the number of patients in whom empirical antimicrobials have been changed to a less broad-spectrum regimen (numerator) divided by all patients who were started on empirical therapy on admission (denominator). The score of the indicator is expressed as a percentage. In routine hospital practice, performance on key QIs is regarded as increasingly important as hospitals are often publicly and financially punished by healthcare authorities or health insurance companies if they do not meet expectations. As ASPs are now considered essential for the quality and safety of hospital care, it is likely that QIs related to ASP will come under increasing scrutiny in the following years. A higher percentage of ""appropriate ADE"" will be regarded as one of the elements of a more successful implementation of an ASP. However, due to the ongoing discussion about the definition of appropriate ADE, it remains extremely difficult to judge and compare hospitals on this specific QI: when is ADE actually considered appropriate, which are definitions and cut-off points? Here are two examples of how misinterpretation and unwanted effects of mandatory ADE reporting could play out: 1. If broad-spectrum antimicrobial therapy is prescribed for a patient with nosocomial pneumonia, e.g. with meropenem and vancomycin and it is changed by simply stopping vancomycin on day 2, this will count as ADE, even if the pivotal antimicrobial (meropenem) remains unchanged. This (undesirable) course of events would still be considered ADE and add to a higher percentage of hospital-wide ""appropriate ADE"". 2. On the other side, patients who are prescribed (relatively) narrow-spectrum antimicrobial therapy (e.g. starting with flucloxacillin for a suspected S. aureusrelated skin and soft tissue infection) and need not be changed once cultures become available, are 'punished' as no ADE has taken place while best medical practice has been followed. In summary, it becomes easy to achieve good QI results while performing poor antimicrobial stewardship. Even worse, starting narrower spectrum therapy is discouraged and broad-spectrum empirical is encouraged (as this will increase ADE performance metrics). A high performance on a QI for ADE may only reflect an overuse of empirical broad-spectrum antimicrobials. Clearly, the opportunities to de-escalate are largely determined by empirical therapy and, therefore, using ADE as an isolated quality measure should be discouraged. Identifying the pathogen responsible for infection is critical for ADE, which relies on an accurate interpretation of microbiological results in the context of clinical presentation of infection. A crucial necessity is to obtain cultures from relevant sites before antibiotics are administered, as the absence of cultures or negative cultures has been associated with non-ADE [1] . The challenge and complexity of this process in routine practice are often underestimated in recommendations and guidelines. First, all samples are not equal: samples obtained from sterile sites have a different role compared to samples obtained from superficial sites: e.g. positive blood cultures are more relevant than samples collected from skin wound or through a drain. In this context, respiratory samples are the most challenging to interpret in the absence of quantitative cultures or other diagnostic approaches that allow discrimination between infection and colonisation. Clearly, defining ventilator-associated pneumonia as well as hospitalacquired pneumonia remains difficult and may be an obstacle to ADE. Second, infective pathogens should be discriminated from colonising pathogens, in the absence of accurate biomarkers. Third, all pathogens are not equal: identification of S. aureus is more significant than that of coagulase-negative Staphylococci, although this should be modulated by the clinical context. In brief, there are some situations in which the confidence in a sample and its clinical relevance are higher compared to some other situations. Here, the resulting strategy will rely strongly on the microbiological result. To resolve this issue, an excellent interaction between intensivists, surgeons, radiologists, microbiologists and infectious diseases physicians is required. While ADE is sometimes conducted in patients in whom no microbiological samples were available [1] , obtaining samples before initiating any antimicrobial treatment should be a general rule, and the quality and relevance of these samples are critical. Our purpose is not to exclude ADE as a part of ASP's, and we remain convinced that ADE has great value. As discussed above, until the validation and large-scale use of rapid diagnostic techniques is a reality, ADE-or rather streamlining antimicrobial therapy-will remain essential. Based on the microbiological results available and clinical course of the patient, we recommend a clinical strategy that integrates ADE while acknowledging the inherent limitations of this approach. The planned duration of antimicrobial therapy is also to be considered when antimicrobial therapy is reevaluated (Fig. 2) . For short courses of antimicrobial therapy (5 days or less), continuing the empirical treatment, if appropriate, can avoid sequential use of different antimicrobials and thus multiple impacts on the microbiome. Another option, which should always be considered is stopping the antimicrobial treatment. Indeed, in patients who are improving, e.g. in whom the SOFA score decreased during the first 48-72 h, the need for continuing treatment beyond day 3 should be debated for a significant number of infections. A seminal randomised clinical trial suggested that, in patients with non-severe  No growth 3 Re-consider diagnosƟcs Fig. 2 Antimicrobial management strategies integrating antimicrobial de-escalation in clinical practice pulmonary infiltrate, a course of 3 days of ciprofloxacin was as efficient as a prolonged treatment [40] . The feasibility of ultra-short course of antimicrobial treatments has been suggested in an observational study comparing 259 patients with ventilator-associated pneumonia treated for 1-3 days and 1031 treated for > 3 days, the outcomes of two groups being similar [41] . For longer courses of antimicrobial therapy (7 days or more), ADE should probably be a recommended strategy, particularly if high-quality and clinically relevant samples are available. Its presumed effects on AMR and cost are relevant in these conditions. For intermediate-duration antimicrobial therapy (5-7 days), decisions should be tailored according to institutional ASP recommendations. Ranking antibiotics according to the local epidemiology and available drugs probably is more important than trying to obtain an international consensus on how antibiotics should be classified. In patients with confirmed infection and who are deteriorating, a single integrated recommendation is impossible. In our opinion, the first step is to rule out other-infectious and non-infectious-causes of shock. The second step is to confirm the adequacy of source control and dosing of antimicrobial(s). Only then, the empirical antimicrobial treatment can be either maintained, escalated, de-escalated or stopped. In patients with negative cultures, another cause of organ dysfunction should be considered. If the mechanism is non-infectious, the antimicrobial treatment can be stopped; if another source of infection is found, new samples are required, and antimicrobials should be adapted to the new clinical picture.@story_separate@In conclusion, ADE has become more clearly defined and understood, but until now, a demonstrable impact on AMR is lacking. ADE should not be used as a 'carte blanche' for the unrestricted use of (very-) broad empirical antimicrobial therapy and it is important to recognize that it may have unexpected and unwanted side effects. The impact of ADE on the microbiome needs further study while one should consider that sequential exposure to two different antimicrobials may not necessarily be better than to one. We advocate against the use of ADE as a QI in the ICU. In the meantime, ADE should clearly be regarded as an important component of ASPs. When applying ADE, planned duration of therapy, as well as sample quality and relevance need to be incorporated in the decision-making process. Efforts should also be aimed at optimising empirical therapy, which may reduce the need for ADE later on; this is where rapid diagnostic techniques may have an important role.","Antimicrobial de-escalation (ADE) is defined as the discontinuation of one or more components of combination empirical therapy, and/or the change from a broad-spectrum to a narrower spectrum antimicrobial. It is most commonly recommended in the intensive care unit (ICU) patient who is treated with broad-spectrum antibiotics as a strategy to reduce antimicrobial pressure of empirical broad-spectrum therapy and prevent antimicrobial resistance, yet this has not been convincingly demonstrated in a clinical setting. Even if it appears beneficial, ADE may have some unwanted side effects: it has been associated with prolongation of antimicrobial therapy and could inappropriately be used as a justification for unrestricted broadness of empirical therapy. Also, exposing a patient to multiple, sequential antimicrobials could have unwanted effects on the microbiome. For these reasons, ADE has important shortcomings to be promoted as a quality indicator for appropriate antimicrobial use in the ICU. Despite this, ADE clearly has a role in the management of infections in the ICU. The most appropriate use of ADE is in patients with microbiologically confirmed infections requiring longer antimicrobial therapy. ADE should be used as an integral part of an ICU antimicrobial stewardship approach in which it is guided by optimal specimen quality and relevance. Rapid diagnostics may further assist in avoiding unnecessary initiation of broad-spectrum therapy, which in turn will decrease the need for subsequent ADE."
"The recently emerged Severe Acute Respiratory Syndrome-Coronavirus-2 (SARS-CoV-2) has shown several neuroinvasive properties that have allowed the hypothesis of different pathogenic mechanisms related to both acute and chronic neurological sequelae of the ""COronaVIrus Disease-19"" (COVID- 19) . However, the neuropathological correlates respectively [18] . Accordingly, it was suggested that the persistence of CoV infection might pathogenically contribute to the onset and course of these disorders. For instance, it is known that some infective agents may trigger MS, viruses being the most likely involved in some genetically predisposed patients [20] . Conversely, among CSF from 37 subjects with optic neuritis, only four patients and one control were positive for CoV 229E and none for OC43, thus not providing evidence for an etiological role of human CoVs [19] . Figure 1 summarizes the main pathophysiological mechanisms proposed in patients with COVID-19 and acute and long-term CNS involvement. and related acute and long-term neurological manifestations. Angiotensin-converting enzyme-2 (ACE2) and iron chelated with any porphyrin, irrespective of the valence state of the iron atom (HEME) dysregulation, as well as direct action on vascular endothelium, immune response, and inflammation (with the subsequent release of inflammatory mediators), and the ""Trojan horse hypothesis"" (i.e., leukocytes-carriers of infection through the blood-brain barrier) have been proposed as the main mechanisms responsible for brain lesions. The inhibitory pathways are shown in red, while the activatory pathways are indicated in green, along with morphological changes responsible for cardiovascular and brain diseases, especially in comorbid patients. Legend: ACE2: angiotensin converting enzyme-2; AT1-R: angiotensin II receptor type; BBB: blood-brain barrier; CVs: cardiovascular manifestations; HEME: iron chelated with any porphyrin, irrespective of the valence state of the iron atom; ICAM-1: intercellular adhesion molecule 1; MCP-1: monocyte chemoattractant protein-1; MMP: matrix metalloproteinase; ROS: reactive oxygen species; VCAM-1: vascular cell adhesion molecule 1.@story_separate@Although neurological complications may arise from a direct effect of SARS-CoV-2, they usually reflect a systemic response to the infection, with severe cases of COVID-19 more often producing CNS complications with respect to mild forms (45.5% vs. 30%) [21] . Moreover, most of these patients are in the older age group, and exhibit comorbidities, especially hypertension, and the neurological involvement can occur independently of the respiratory manifestations [22, 23] . Globally, asthenia, myalgia, headache, anosmia, and ageusia are the most common symptoms, followed by encephalopathy, stroke, and seizures [9] . In extreme cases, some patients may exhibit encephalitis, flaccid paraparesis, and coma [22] . Of note, telemedicine is no longer a futuristic concept, being the new normal for an increasing number of medical and surgical specialists during the pandemic [24] . A recent meta-analysis concluded that among 70 patients with COVID-19 and neurological manifestations (mean age 61.9 ± 17.7 years, 60.6% male), 39 (53.4%) had a stroke, 18 (24.7%) a Guillain-Barré syndrome or its variants, 11 (15.1%) encephalopathy, meningitis, encephalitis, or myelitis, and 5 (6.8%) seizures. Neurological disorders presented after 8.1 ± 6.8 days from the onset of infection. The average rate of mortality was 17.8% (mortality rate of stroke 25.6%), whereas chemosensory dysfunction occurred in 59.9% and 57.5% of patients with anosmia and ageusia, respectively [25] . More recently, a systematic search on 19 studies and >12,000 adult patients with laboratory-confirmed COVID-19 [26] found that headache was reported in 7.5% of patients, dizziness in 6.1%, hypo-anosmia and gustatory dysfunction in 46.8% and 52.3%, respectively, whereas symptoms of muscular injury ranged between 15% and 30%. Three studies reported a radiologically-confirmed acute cerebrovascular disease in 2% of patients [26] . Older subjects or those with pre-existent cognitive impairment, several comorbidities, past medical conditions, poor pre-morbid functional independence, malnutrition, concomitant infections, or multiple vascular risk factors are also at higher risk for altered consciousness and encephalopathy when affected by COVID-19 [27] [28] [29] . Endocrine or metabolic disorders, such as hypo-hypercalcemia, hypo-hypernatremia, hypo-hyperglycemia, liver and/or kidney dysfunction, and sepsis confer a further risk [30] . An acute hemorrhagic necrotizing encephalopathy (AHNE) was described in a COVID-19 patient [31] . Brain magnetic resonance imaging revealed multifocal and symmetric contrast-enhanced hemorrhagic lesions in the thalamus, mesial temporal region, and insula, bilaterally [31] . A similar case occurred in a previously healthy 44-year-old woman [32] . A posterior reversible encephalopathy-like syndrome, with reversible cortical blindness [33] and mild encephalopathy, was also described [34] Older individuals with COVID-19 are at higher risk for neurovascular events [35] . In a retrospective cohort of 221 subjects [36] , eleven (5%) underwent ischemic stroke, one (0.5%) cerebral hemorrhage, and one (0.5%) cerebral venous thrombosis. Other authors reported five patients with stroke (of ischemic origin in four of them) in the context of severe COVID-19, thrombocytopenia, increased D-dimer, and multiple organ failure [29] . Of note, a previous investigation in the USA observed that young persons (<50 years) more likely underwent large-vessel strokes during the course of COVID-19, thus showing that all age groups can be affected [37] . Increased markers of inflammation and a state of hypercoagulability seem to be a feature of severe forms [29] . Indeed, systemic inflammation associated with the infection, vasculitis, and thrombosis is known to enhance the risk of stroke [38] . Finally, both systemic vasculitis and CNS vasculitis were autoptically documented in SARS-CoV patients [39] . Lastly, smell and taste disorders are often complained about by COVID-19 patients and can manifest suddenly [40] . In Italy, 19.4% of subjects had some forms of anosmia and ageusia [41] , and in a registered study on 12 European centers, including 417 patients with mild-to-moderate COVID-19, chemosensory dysfunction had a prevalence of 85.6% (anosmia) and 88% (ageusia) [42] . Notably, olfactory dysfunction was mentioned as the onset symptom in 12% of them, without any runny nose or nasal obstruction in 18% [42] . Unlike other organs and tissues, which can be easily sampled by biopsy, a comprehensive histopathological assessment of the nervous system requires a detailed examination of both the CNS and peripheral nervous system (PNS). At the onset of the pandemic, several countries, including Italy, did not allow autopsies in the attempt to contain the risk of infection and its spread. Later, although many local governments and scientific communities appreciated the missed opportunity of collecting histopathological data, the number of autopsies has only increased slowly [43] , probably due to the limited pathologists trained in the brain and the few centers able to guarantee the safety of the mortuary staff. According to the international guidelines [44] [45] [46] , indeed, autopsies must be performed within a certified biosafety level 3 Autopsy Room, and mortuary staff must be equipped with personal protection equipment (disposable headgear, double pair of disposable gloves, cut-resistant protective gloves, respiratory filter FFP3 protection, Tyvek coverall suits). During brain and spinal cord removal, particular care is also needed to avoid bone aerosolization. Another relevant concern regards the sampling. It is often unclear if it has been sufficient and if standard protocols have been adopted; therefore, the microscopic results obtained may not be comparable [47] . Furthermore, data appear to be heterogeneous since only a few papers have identified the virus with the reverse transcription-quantitative polymerase chain reaction (RT-qPCR). Even when the assay was executed, it is also uncertain whether the genome could be exclusively identified with that of SARS-CoV-2 [47] . To date, the management of neurological symptoms and therapy of COVID-19 patients seem to be more challenging than pulmonary and cardiovascular manifestations, those being more systematically investigated [48] [49] [50] [51] [52] [53] . Furthermore, in the pandemic scenario, some symptoms, and signs of CNS involvement, e.g., those underlying meningitis or encephalitis, were not always correctly diagnosed and promptly managed. Therefore, the questions we want to draw attention to are the following: How many cases were actually attributed to a direct invasion of the CNS by SARS-CoV-2? How many cases have been not diagnosed or misdiagnosed due to a failure to identify the virus in the CSF or at the post-mortem examination? Can we make a reliable differential diagnosis between direct viral damage and superposition damage by other causes or etiological agents? To properly answer these questions, it should be preliminarily stated that, although autopsies are still scarce compared to the entity of the pandemic, the most common pathological lesions in COVID-19 appear to be similar to those seen in the context of other viral and systemic diseases associated with cardiovascular involvement. Based on our autopsy experience [54] [55] [56] [57] [58] and current evidence from the literature, it seems that morphological data are still limited to a few cases of meningitis or necrotizing encephalopathy [3, 30, 58] . More recently, it has also been hypothesized that neuropathological changes might be the consequence of direct or immune-mediated damage of the virus, although a secondary lesion from the interaction through the ACE2 receptor or interference with the HEME formation cannot be excluded (Figure 1 ), as well as an indirect injury from a systemic dysfunction [50, [59] [60] [61] [62] [63] . Unbalanced homeostasis, immune-mediated action, and damage from cytokines release and inflammatory mediators (i.e., the ""cytokine storm"") seem to contribute to neurological deficits [50, [59] [60] [61] [62] [63] . It is also likely that hypoxemia plays a role in many patients with encephalopathy, it being secondary to metabolic derangements due to multiple organ failure or medication effects. Nevertheless, it should be taken into account that peripheral biomarkers showed non-specific signs of neuronal damage and reactive gliosis in moderate-to-severe patients with COVID-19. This does not seem to suggest a distinctive neuropathogenesis of SARS-CoV-2 [64] . Similarly, weak evidence supports direct viral damage, which presupposes its capacity for overcoming the BBB. Moreover, SARS-CoV-2 can infect different cell types, and data on the involvement of endothelial cells alone are enough to explain most of the pulmonary, cardiovascular, and other organ damage, including the CNS. Additional data come from the few pathological findings described in subjects with SARS-CoV and Middle East Respiratory Syndrome (MERS)-CoV: histologically, edema and non-specific neuronal degeneration were mostly observed at patients' autopsies [11, 39, [65] [66] [67] . A PubMed (MEDLINE) review was carried out to identify all studies dealing with autopsies performed in COVID-19 patients. The following search terms were used, from database inception to December 2020: COVID-19, SARS-CoV-2, autopsy, neuropathology, histology, morphology, encephalopathy, ischemic stroke, intracerebral hemorrhage, gliosis, necrotizing encephalopathy, encephalitis, myelitis, and inflammation. A total of more than 11,000 articles, including those listed in the references of the retrieved studies, were found originally. We then excluded the following items: all publications not dealing with COVID-19 autopsy or neuropathology; all studies different from original articles (e.g., reviews, case report/case series, letters, commentaries, etc.); all preclinical studies or research performed on animals or cell cultures; non-English written papers; any other publication that did not comply with the goal of the present review. After this process, 46 studies performed autopsy: 39 showing pulmonary, cardiovascular, or other organ involvement (Appendix A), and 7 showing brain pathology (Table 1) . Among them, only 3 autopsy studies demonstrated a RT-qPCR-positive viral infection [68] [69] [70] . Table 1 and Appendix A report the main autopsy studies during the COVID-19 pandemic. As expected, pulmonary and cardiovascular lesions were those investigated the most and basically appeared not to be different from those previously described in SARS-CoV and MERS-CoV [51] [52] [53] . Diffuse alveolar damage, myocarditis, myocardial infarction, thromboembolism, disseminated intravascular coagulation, among others, have been reported. Of note, thromboembolism and ischemic stroke have been explained in the context of acute events affecting the cardiovascular system. Large vessel occlusion by hypercoagulability and cerebral vasculitis were also common [74] . Taken together, these findings are in line with the authors' experience ( Figure 2 ). It clearly emerges from these studies that the CNS represents a relevant target in the disease course of some patients [47] , whose brain might be affected by both vascular-related changes and/or infection-related changes. Indeed, the consensus seems to converge on a little or no direct COVID-19 brain damage (i.e., ""pure"" encephalitis), a relevant contribution of thrombo-embolic vascular changes, a still debatable consequence of the ""cytokine storm"" in very ill elderly persons, and a few but severe encephalopathic/encephalitis syndromes of unknown causation, although rarely found to be common or representative. Nevertheless, only few of the radiologically and histopathologically identified lesions can be clearly attributed to a vascular injury, usually in the context of vascular comorbidities or risk factors predisposing to ischemic or hemorrhagic conditions. Therefore, performing a brain autopsy in COVID-19 is relevant for the differential diagnosis between a secondary vascular injury related to SARS-CoV-2 infection and primary direct virus damage [47] . Up to now, the few autopsy studies focused on the neurological involvement of COVID-19 may be ascribed to the relatively few deaths from neurological complications compared to those from pulmonary or cardiovascular disease, as well as to purely technicalprocedural reasons or safety-related issues to biopsy or extensive organ examination. Therefore, these considerations should be taken into account when considering the global epidemiology and the estimation of neurological complications causing death in COVID-19 and the effort to make a clear distinction between direct damage to the CNS from a secondary cerebral lesion due to associated comorbidities and systemic viral dissemination. Moreover, recent clinical reviews demonstrated that SARS-CoV-2 affects not only the CNS but also the PNS and muscles [66, [75] [76] [77] [78] . Based on the clinical studies and the limited neuropathological data, CNS histopathology encompasses hypoxic/ischemic encephalopathy, activated microglial cells with variable infiltration of perivascular T lymphocyte, cerebrovascular diseases, AHNE, encephalitis/meningitis, acute myelitis, demyelinating disorders, and PNS involvement [21, 68, 71, 73, , with histologically documented lesions in some studies [6, 11, 68, 69, [71] [72] [73] . Hypoxic/ischemic encephalopathy causes several changes to the brain secondary to a combination of neuroinflammatory response, decreased cerebral oxygen supply due to pneumonia during the acute phase of infection, and systemic metabolic disorders. Accordingly, CoV infections, including that by SARS-CoV-2, have been diffusely described in association with a ""cytokine storm syndrome"". Clinically, symptoms are dysphoria, headache, delirium, confusion, mental disorder, loss of consciousness, and coma in severe cases. This histopathological pattern seems to be the most common correlate in COVID-19 patients with clinical CNS involvement [116] . Regarding acute neurovascular accidents and their sequelae, SARS-CoV-2 is able to induce a systemic inflammatory response and a hypercoagulable state, as indexed by an increase in D-dimer, a prolongation of the prothrombin time, and disseminated intravascular coagulation, which confer a greater risk of ischemic and/or hemorrhagic complications [117] . Severe COVID-19 cases, therefore, are at high risk of thrombosis secondary to an indirect hypercoagulable state and a direct vascular endothelial damage [116] . In AHNE, coagulative neuronal necrosis, reactive gliosis, and perivascular lymphocytic infiltration with cell debris or inclusions were the main histological aspects [68] . Regarding meningitis and encephalitis, albeit viral encephalitis was expected to be one of the most common complications, the current evidence indicates that it is quite rare in COVID-19. In the encephalomyelitis/acute disseminated perivascular encephalomyelitis, processes of acute inflammation involving the CNS and causing demyelination were described in association with neuronal damage and some vascular lesions, such as multifocal hemorrhage and/or micro-infarcts and perivascular inflammation with histiocytes and astrocytosis infiltration. Up to now, however, clinically diagnosed cases are very scarce, although those reported were not confirmed at the histopathological exam. In general, it is also difficult to diagnose viral CNS diseases, mainly because of symptom variability, a paucity of specific markers, and challenges in distinguishing it from other viral or even non-viral encephalitis, as well as from a non-specific encephalopathy related to a systemic viral infection [116] . Since demyelination is usually caused by the autoimmune reaction triggered by an excessive and widespread inflammation [72] , there are also findings of demyelinating lesions induced by SARS-CoV and associated with MS earlier in the pandemic. However, one case only histologically documented neuronal damage with perivascular lymphocytes and microglial reaction, associated with the CD68+ histiocytic population more represented than lymphocytes [71] . Moreover, pathological correlates are not strong, thus requiring more and systematic investigations in this ""cutting-edge"" topic. Finally, reduced T lymphocytes and lymphopenia in COVID-19 have been described, a finding which may put vulnerable individuals at risk for opportunistic infections, especially if this occurs in combination with steroid treatment. However, additional evidence is needed to support the vulnerability of subjects with COVID-19 to opportunistic infections [116] . Overall, neuropathological changes in patients with COVID-19 appear to be of a mild entity, neuroinflammatory signs being the most common finding, along with reactive gliosis, astrocytosis, and microglia activation [70] . While COVID-19 patients continue to be reported, the neuropathological pattern of ischemic/hypoxic injury, cerebral hemorrhage, and mild or moderate non-specific in-flammation are unlikely to vary [118] significantly. The identification of CoVs particles by electron microscopy is also difficult because of the similar appearance of normal cell structures, an aspect which has generated several controversies [118] . The inherent bias of autoptic exams for severe, fatal cases and institutional restrictions for morphological CNS assessment have also determined that some studies might have overestimated both the extent and frequency of neuropathological findings, whereas others might have underestimated them [119] . Finally, autopsies in children, which include the multisystem inflammatory syndrome, remain extremely rare. Although they are responsible for <2% of all COVID-19 [120] , neuropathological data from pediatric patients can significantly aid in disentangling the peculiar pathophysiological basis underlying the pediatric disease, including the typical immune-response, the degree of ischemic/hypoxic damage, and the hypercoagulability state of this age-group. Further ""hot topics"" are the characterization of the effects of antiviral therapies (such as remdesivir), steroids (including dexamethasone), immunomodulatory medications (e.g., anti-interleukin-6), monoclonal antibodies, and anticoagulant agents on the brain [119] . Since therapeutic responses to COVID-19 vastly differ among centers, the understanding of how the different treatment options during hospitalization and after discharge might be responsible for the variability in both acute and long-term neurological manifestations and their histopathological correlates [121] remains. Regarding long-term sequelae, neuropathological findings in COVID-19 survivors are totally unstudied. Some CNS symptoms, such as asthenia, headache, myalgia, anosmia, and dysgeusia, may last for weeks or even months in a non-negligible portion of patients [122, 123] , and studies determining causes and mechanisms underlying long-term manifestations are required. The persistence of symptoms or development of new symptoms related to SARS-CoV-2 infection late in the course of COVID-19 is an increasingly recognized problem facing the infected population and health systems. ""Long-COVID"" or ""COVID longhaulers"" describes those persons who experience symptoms for more than 28 days after the diagnosis of SARS-CoV-2 infection [124] . Symptoms are as markedly heterogeneous as seen in acute disease and may be constant, fluctuating, or replaced by other symptoms of varying frequency and severity. Such multisystem involvement requires a holistic approach and, although many patients can be managed in primary care, others need rehabilitative care [124] . In particular, a significant proportion of patients report persistent and debilitating symptoms centered not only around fatigue but also including ""brain fog"", pain, breathlessness, and dysrhythmias, that extend several months in the postinfection period [125] . These symptoms are a characteristic of a well-documented and largely unexplained post-viral illness, i.e., the myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS). The ""long haulers"" with ME/CFS, who do not make a straight-forward recovery in the post-viral period, probably reflect damage by the host response to the initial infection [126] . A severe host response (e.g., the ""cytokine storm""), indeed, can give rise to oxidative and inflammatory damage and generalized oxidative stress. Pathophysiological changes in SARS-CoV-2 that enhance the production of reactive oxygen species might be ameliorated by free radical scavengers, also confirming that oxidative stress and SARS-CoV-2 pathogenesis may be closely linked [127] and that some antioxidant therapies might be of beneficial effect [128] . The few studies available, which also differ in terms of methodology, is the main limitation of this review. Thus, the generalization of the findings reviewed here requires further validation given the low number of cases, the limited clinical data, and the lack of sex-and age-matched controls. To date, therefore, the etiological link between SARS-CoV-2 infection and autoptic brain correlates remains inconclusive. In particular, the possibility of primary viral-related damage on neurons and glia needs to be validated by systematic identifications of the viral genome within the brain or at the CSF. Surely, the triggering of an abnormal immune response, together with the release of neurochemical factors and inflammatory mediators, significantly contribute to BBB disruption, changes in brain homeostasis, and clinical manifestations. Other critical aspects regard the sensitivity and specificity of the diagnostic techniques used, the unavoidable effect of unspecific post-mortem phenomena on brain tissue, and the challenges in studying a large number of people or particular age groups (such as children and young adults). Further, larger and independent works on autoptic exams, integrated with clinical, laboratory, CSF, imaging, and general autoptic data, are needed to elucidate the impact of SARS-CoV-2 on CNS and its role in the process, progression, and mortality of COVID-19. The authors declare no conflict of interest. Table A1 . Autopsy studies in COVID-19 individuals without brain involvement. @story_separate@Notwithstanding the limited evidence available and the heterogeneity of the studies, in this review, we provided a preliminary description of the complex relationship between SARS-CoV-2 and CNS sequelae. Despite the exponentially increasing number of publications on COVID-19, the lack of well-defined pathophysiology and systematic neuropathology of long-term neurological consequences of SARS-CoV-2 still stands. Given the peculiar features of the CNS and the challenges in collecting samples, autopsy remains the gold standard exam for studying any primary CNS disorder and its involvement in the course of systemic diseases, such as COVID-19. More autopsies would mean further knowledge, accurate diagnosis, new therapeutic approaches, and improvement in the survivors' quality of life. In this tragic pandemic, lessons from dead people can rescue lives.","Background: Neuroinvasive properties of SARS-CoV-2 have allowed the hypothesis of several pathogenic mechanisms related to acute and chronic neurological sequelae. However, neuropathological correlates have been poorly systematically investigated, being retrieved from reports of single case or limited case series still. Methods: A PubMed search was carried out to review all publications on autopsy in subjects with “COronaVIrus Disease-19” (COVID-19). Among them, we focused on histological findings of the brain, which were compared with those from the authors’ autoptic studies performed in some COVID-19 patients. Results: Only seven studies reported histological evidence of brain pathology in patients deceased for COVID-19, including three with reverse transcription–quantitative polymerase chain reaction evidence of viral infection. All these studies, in line with our experience, showed vascular-related and infection-related secondary inflammatory tissue damage due to an abnormal immune response. It is still unclear, however, whether these findings are the effect of a direct viral pathology or rather reflect a non-specific consequence of cardiovascular and pulmonary disease on the brain. Conclusions: Notwithstanding the limited evidence available and the heterogeneity of the studies, we provide a preliminary description of the relationship between SARS-CoV-2 and brain sequelae. Systematic autoptic investigations are needed for accurate detection and adequate management of these patients."
"The novel coronavirus was formally declared a global health pandemic by the World Health Organization (WHO) on 11 March 2020 (WHO 2020). Coronavirus has had a significant impact on the well-being of our communities and law enforcement agencies (LEA), who face unique challenges created by this global public health crisis (Frenkel et al. 2020; Stogner et al. 2020) . These unprecedented health challenges (Laufs and Waseem 2020) will likely have a more significant impact on mental health than physical health (Kotera 2020) , increasing the risk of anxiety, depression, and post-traumatic stress disorders (PTSD) (Greenberg 2020) , which could result in a global mental health crisis post-pandemic (Kola 2020; Shuja et al. 2020) . Survey data obtained from UK police, fire, and ambulance services reported an increase in mental health problems across the country's emergency services, with two in five police officers reporting poor mental health since the start of the pandemic (Mind 2021) . The rise in psychological illness could pose critical challenges for communities and law enforcement, with considerable and long-lasting consequences.@story_separate@Policing is widely recognized as a high-risk occupation, and compared to the general population, there is a greater risk of infection. The risk of infecting others has been reported as one of the primary causes of stress among officers during the pandemic (Frenkel et al. 2020) , compounding the organizational stress already associated with the occupation (Stogner et al. 2020) . Repeated exposure to these stressors is a cause for concern-challenges such as these place added pressure on officers. Moreover, policing is made even more difficult due to the lack of clear directives and inconsistent governmental measures (Frenkel et al. 2020; Sager and Mavrot 2020) . The rise in the number of infected or COVID-19-related fatalities in police officers is problematic. In many countries, police officers have been infected or died from contracting coronavirus while in the line of duty (National Law Enforcement Officers Memorial Fund 2020; Philippine National Police 2021). The role of a police officer has shifted since the start of the pandemic, from not only protecting our communities from crime but also playing a pivotal role in controlling the spread of the virus, maintaining public order, and promoting safer communities during the pandemic (Frenkel et al. 2020; Grover et al. 2020) . Moreover, they increasingly encounter many COVID-19-related deaths (Fisher et al. 2020) , repeatedly being called out to recover the bodies of the deceased (Grover et al. 2020; Laufs and Waseem 2020) . Officer stress may become exacerbated due to increased workload and long working hours (Laufs and Waseem 2020) . Police officers must be physically present within communities to carry out their role (Gordijn et al. 2017) , sometimes in situations that could cause them physical and psychological harm. For example, many countries have seen large public demonstrations against coronavirus restrictions and the refusal of protesters to wear masks and socially distance puts officers at increased risk (Laufs and Waseem 2020) . Police forces worldwide have had to adapt to new ways of working. The strain officers experience due to coronavirus is somewhat different from the usual occupational stresses of the job, primarily because of the perceived worry of virus exposure: in situations where there is the potential to become infected, and the constant fear of infecting others (Drew and Martin 2020; Grover et al. 2020 ). Not only that, but police stress and trauma have implications for others; it may harm relationships with their family and friends (Papazoglou and Tuttle 2018) . These findings illustrate multi-dimensional stress experienced by officers. The safest ways to protect each other from COVID-19 is social distancing and self-quarantining (Hansen and Lory 2020) . However, community confidence in the handling of the COVID-19 pandemic is at an all-time low. As such, the police will be the first to encounter angry exchanges from the general public, who feel aggravated about the present situation, and stressed and worried about the possibility of infection. They have to adapt to the constant shift in restrictions, economic uncertainty, and isolation (Stogner et al. 2020 ). Relationships will need rebuilding once communities start to re-engage post-COVID, and we must address the subsequent mental health crisis that will emerge (Hansen and Lory 2020), not just for our communities but also for emergency service personnel. It is evident that a variety of factors significantly contribute to the development of mental ill-health in officers. We must therefore understand how this public health crisis impacts their psychological well-being. It is clear from previous research undertaken that a hypermasculine culture exists in the police force (Kurtz 2008; Silvestri 2017 )-a culture that inhibits officers from seeking help (Edwards and Kotera 2020) . As increased workload (Drew and Martin 2020) and the stress of possible virus exposure continue to be tackled by officers, departmental leadership should work to minimize the stress placed on officers as much as possible. One possible way would be to implement support initiatives that would mentally prepare officers for public health emergencies (Laufs and Waseem 2020) . There is evidence emerging on the stress experiences of officers during COVID-19 (Frenkel et al. 2020; Grover et al. 2020; Mehdizadeh and Kamkar 2020) . A mixedmethod study by Frenkel et al. (2020) of officers from five European countries investigated police strain over three months following lockdown. Risk of infection, inadequate leadership, and communication emerged as the main stressors in this study. There were slight decreases in strain over time, with officers seemingly tolerating the pandemic. The findings obtained during Frenkel's study may have implications for understanding governmental, organizational, and individual coping strategies during pandemics. It is addressing these problems through the implementation of effective public health emergency plans that need to be put in place by police forces, also, to establish precrisis preparation guidelines to improve the skills and competencies necessary in the future for officers to reduce mental health issues when similar public health emergencies arise (Laufs and Waseem 2020). There may be several waves or periods of public health emergencies in the future, undoubtedly affecting police departments on more than one occasion (McCurry 2020). It may be appropriate to be less reactive and introduce a more proactive planning approach to emergencies (Jefferson and Heneghan 2020). Adjustments to this degree also have implications for operational planning while ensuring the well-being of the workforce. Coronavirus continues to spread rapidly (Kotera 2020) . At the time of writing, cases surpassed 174 million worldwide (WHO 2021). Following the easing of initial lockdowns, stringent measures to deal with coronavirus outbreaks were reinstated in several countries. For example, the UK government imposed national and local lockdowns for several months to restrict movement. Worldwide vaccination programmes have been implemented. However, the effectiveness of such measures varies across countries. More recently, lockdowns, travel bans, and strict social distancing measures have been implemented to curb the spread of the virus in many countries, with such controls being enforced by the police (Laufs and Waseem 2020) . The effect is the increased challenges and added stress faced by law enforcement officers (Mehdizadeh and Kamkar 2020) . LEAs were most likely unprepared for the mental health impact of coronavirus on police officers (Stogner et al. 2020) . For this reason, LEAs should identify and refer officers to mental health services to tackle the mental health burden imposed on them due to these continuing restrictions (Jetelina et al. 2020) . For effective post-pandemic policing, a strong emphasis on community policing and crisis management training (Stogner et al. 2020) to identify the actions that need to be taken in the event of a public health crisis, will be necessary. Returning to pre-COVID operational levels will take time, but it is essential to start thinking about strengthening resources, being prepared, and improving police training. As suggested among Singaporean LEAs, preparatory measures such as training for psychological flexibility and technological skills may be helpful (Yap et al. 2020) . Moreover, using the Operations Psychology Framework, clarifying the psychological and mental health issues at each dimension of operation (e.g. time, intervention, and command) may enhance their preparedness (Ang et al. 2011) . Lessons learned from COVID-19 must be regularly revisited and corresponded to by continuous actions. While research on the impact of COVID-19 remains to be developed, suggested strategies for dealing with future public health incidents are based on comparative examples and lessons learned from previous emergencies (Chua et al. 2021; Laufs and Waseem 2020; McCanlies et al. 2018 ). Resources such as the American Society for Evidence-Based Policing (ASEBP) are a forum for sharing worldwide COVID-19 response strategies and policies for LEAs (The American Society of Evidence-Based Policing 2020). In Italy, the rural Carabinieri moved to the cities to patrol shops and cafes, while in Spain, the military has been called upon to support LEAs (The Brookings Institution 2021). Some LEAs in the USA have introduced police chaplains, who work closely with employee assistance programmes, counselling, and other departmental resources to support agency personnel and their families during the pandemic (Police Executive Research Forum 2021). Others have suspended in-service training and roll call briefings to reduce the number of officers gathering indoors and to minimize the risk of infection; instead, virtual meetings were conducted wherever possible (Independent Office for Police Conduct 2020). In addition, limiting public access to police stations, face-toface interaction, and encouraging non-frontline staff to work from home have been implemented (The American Society of Evidence-Based Policing 2020). Lessons from disasters like Hurricane Katrina have implications for LEAs and provide important lessons and guidance on dealing with public health emergencies (Laufs and Waseem 2020). Preparing for temporary or permanent loss of human resources is critical to LEAs' ability to operate effectively during future public health emergencies (Sanberg and Mcfadden 2010) . Discussions need to take place, and the effect of those measures needs to be evaluated. Promoting resilience and improving mental health in police officers. Strengthen coping resources through additional training for police officers. Improve communication and stress-coping skills in officers. Social support Design social support structures for police officers during operational duties while maintaining officer well-being.@story_separate@The COVID-19 pandemic is likely to impact police departments around the world in many ways. There are several ways to approach such issues, some of which are highlighted in Table 1 (integrated from Frenkel et al. 2020; Laufs and Waseem 2020) ; Milliard and Papazoglou 2020; Stogner et al. 2020 ). First, current disaster plans should be revised initially, and in some cases, police departments will need to put in place effective emergency planning procedures. Second, the COVID-19 pandemic is likely to create unprecedented strains in resource and staff shortages, especially given how exposed police officers are and the challenges in maintaining social distancing measures in the field. Last, it is vital to provide officers with the tools and support through training to help them manage the pressures they face without risking their mental health while experiencing the stresses of the COVID-19 environment (Drew and Martin 2020) . Author Contribution AME wrote the message commentary. AME and YK revised the commentary. Both authors read and approved the final commentary. Frenkel et al. 2020; Laufs and Waseem 2020) ; Milliard and Papazoglou 2020; Stogner et al. 2020) Crisis management preparedness Put in place crisis and emergency management systems, plans, and processes at a governmental, organizational, and individual level. Implement post-pandemic crisis intervention training.","The role of police officers during the coronavirus (COVID-19) pandemic is challenging, faced with the difficult task of keeping communities safe and preventing the spread of COVID-19 while putting their physical and mental health at risk. Emerging evidence points to the stress experiences of officers during the COVID-19 pandemic. With cases now surpassing 174 million and close to four million deaths worldwide, as well as stringent lockdown measures, police officers are faced with unprecedented challenges resulting from the pandemic. This commentary suggests police departments strengthen resources by putting in place appropriate emergency planning for future public health incidents, in addition to preparing for temporary or permanent loss of human resources. It is important to implement robust training plans post-pandemic to allow officers to offer better care for communities when faced with future public health emergencies. Finally, police officers should be provided with the resources and support to cope with the stresses associated with COVID-19."
"Human papillomavirus (HPV) is a sexually transmitted infection [1] , with more than 200 genetically distinct types (i.e., strains) of HPV [2] . HPV types have been classified by the International Agency for Research on Cancer as per their carcinogenicity [3] . HPV types 16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58 , and 59 are considered carcinogenic, whereas HPV 68 is considered probably carcinogenic. Several other types are considered possibly carcinogenic, i.e., HPVs 26, 30, 34, 53, 66, 67, 69, 70, 73, 82 , 85, and 97 [3] . A few other HPV types, such as 6, 11, 42, 43 , and 44, cause benign lesions or asymptomatic infections [2] . Infection with carcinogenic HPV types can lead to cervical, anal, vaginal, vulvar, penile, and oropharynx cancers [1] . Vaccination against human papillomavirus infection could potentially eliminate the most important HPV types that cause cervical, anogenital, and oropharyngeal cancers [4] . Canada was one of the first countries to implement a publicly funded HPV vaccination program. Three vaccines to protect against carcinogenic strains of HPV have been licensed in Canada [5, 6] . First, a quadrivalent vaccine (4vHPV) targeting HPV types 6, 11, 16, and 18 was approved in 2006 for use in females 9 to 26 years of age and in 2010 for use in males 9 to 26 years of age. In 2011, the indication was further extended to include women up to 45 years of age. Second, a bivalent vaccine (2vHPV) protecting against types 16 and 18 was approved for females in 2010. Finally, a nonavalent vaccine (9vHPV) targeting carcinogenic types 16, 18, 31, 33, 45, 52 , and 58 and low-risk types 6 and 11 was approved for both females and males in 2015. Figure 1 shows the 10 provinces and 3 territories in Canada. Each province and territory is responsible for implementing its public HPV vaccination program. While each jurisdictional program has undergone a series of policy changes, program information is scattered across multiple sources and there is no national database that compiles this information. There have been several descriptions of HPV vaccination programs [5] [6] [7] [8] , but no analysis of the evolution of HPV vaccination policies and vaccination coverage in all Canadian jurisdictions. As Canada prepares to implement the prevention strategies designed to meet the World Health Organization's goal of eliminating cervical cancer [8] , it is essential that Canadian policymakers take stock of the current status of HPV vaccination. Hence, the objective of this study is to synthesize publicly available information for each jurisdiction and to describe the evolution of public HPV vaccination programs in Canada. @story_separate@This was a targeted literature review of HPV vaccination programs in Canada, focused on governmental databases and statistics, informal reports, Embase, and PubMed. The scope of the searches was based on the PICO (+) framework (population, interventions, comparisons, outcomes, and time) described in Supplementary Table S1 . Searches were conducted between 29 August 2019 and 12 September 2019. We searched the websites of Canadian provincial and territorial health public health authorities with keywords in English and French related to public HPV vaccination programs and vaccination coverage rates. Specific search terms were customized to each province/territory. Examples of search terms are ""HPV immunization AND name of province/territory"", ""HPV immunization program AND name of province/territory"", ""HPV eligibility AND name of province/territory"", ""HPV vaccine coverage AND name of province/territory"", ""immunization coverage AND name of province/territory"", ""immunization coverage report AND name of province/territory"", ""immunization report AND name of province/territory"", ""immunization coverage AND school AND name of province/territory"", and ""vaccine coverage AND name of province/territory"". In addition, we conducted searches of the peer-reviewed literature. PubMed was searched in a heuristic approach, involving chain-searching of related and citing articles. Articles reporting HPV-related studies in Canada were retrieved. Where information was not located using the published literature, local provincial health authorities were directly contacted. We retrieved information by province/territory on the publicly funded, school-based primary grade(s) covered for vaccination, temporary catch-up cohorts, if the programs covered girls and boys, year of program introduction, vaccination schedule, and vaccine coverage rate by year and sex, where published and available. Birth year of children eligible for school-based vaccination programs and other groups eligible for public HPV vaccination programs was also extracted when available. Program details were synthesized by province/territory and year, including vaccination schedule and vaccination coverage rate by sex, and are presented together over time. All available vaccine coverage rate data were extracted; however, this paper only presents vaccine coverage rates for cohorts that received the full course of HPV vaccination based on the recommended dose regimen at the time of administration. When not available, birth year of children eligible for school-based vaccination programs was calculated using the assumption that children are five years old when they start kindergarten. For example, if girls and boys are eligible for vaccination in Grade 6 for the 2017/18 school year, it was assumed that they were 11 years old when they were eligible for the first dose of HPV vaccine and it was calculated that the birth year of this cohort was 2006 (2017-11). Vaccination coverage rate (as a percentage) is expressed as the number of children vaccinated relative to the number eligible for vaccination. This was a review of publicly available aggregate data and information from published literature; therefore, ethical approval was not required. As shown in Table 1 , between school years 2007/08 and 2010/11, all Canadian jurisdictions initiated school-based HPV vaccination for girls within their publicly funded immunization programs. All jurisdictions began with a three-dose schedule (0, 2, and 6 months) of 4vHPV in Grades 6 or 7 (Grades 4-6 in Northwest Territories) except Quebec. In Quebec, the HPV program began with two doses (0 and 6 months) in Grade 4, with a third dose scheduled to be received in Grade 9, five years later. However, in 2013, the program was changed to remove the third dose before the first cohort vaccinated reached Grade 9. In consequence, girls in Quebec have always received two doses, six months apart. Similarly, in British Columbia, the initial three-dose schedule for girls in Grade 6 was changed in 2010 to a two-dose schedule in Grade 6, with a third dose scheduled in Grade 9. The British Columbia program was changed to a two-dose schedule in 2014. Therefore, only the cohort vaccinated in 2010/11 had their third dose in Grade 9; all other cohorts vaccinated after 2010 received two doses. Subsequently, all programs except Alberta transitioned to a two-dose schedule of 4vHPV and then a two-dose schedule of 9vHPV. Alberta was one of the first provinces to transition to 9vHPV vaccine in 2016 and remained at a three-dose 9vHPV scheme until 2018, when the province moved to a two-dose schedule. By the 2018/19 school year, all HPV programs for the 13 Canadian jurisdictions were using a two-dose schedule of the 9vHPV, except Quebec, which adopted in that year a mixed schedule approach with a first dose of 9vHPV followed by 2vHPV for the second dose 6 months later. As of September 2020, Quebec divulged plans to defer the second dose to 5 years later. In 2013/14, Prince Edward Island was the first province to include boys in the school-based vaccination program. Alberta followed in 2014/15 and Nova Scotia in 2015/16 [9] . All other provinces implemented gender-neutral programs in 2016/17 or 2017/18. By 2018/19, all jurisdictions had gender-neutral, school-based programs with a two-dose schedule of 9vHPV offered in Grades 4 to 7, except that Quebec had a two-dose mixed schedule. As shown in Table 2 , eight of the 13 Canadian jurisdictions instituted a temporary catch-up vaccination program for schoolgirls at either the first or second year of the inception of the program for varying lengths of time (between 1 and 5 years). In 2016, Ontario changed the target age of its vaccination program from Grade 8 to 7 and added one additional cohort so girls in Grade 8 would not be missed during the transition. Alberta, Manitoba, and Quebec were the only provinces that implemented temporary catch-up programs lasting 3 or 4 years for schoolboys at the first or at the second year post program inception. Table 3 shows the birth years of children that have been eligible for the school-based HPV vaccination programs since the introduction of the programs up to the end of the 2019/20 school year. In addition to the school-based programs, in most of the jurisdictions, individuals who missed the full course while at school remained eligible, as shown in Table 4 . Finally, most jurisdictions have public HPV vaccination programs for high-risk groups, which comprise men who have sex with men, transsexual persons, HIV-infected individuals, and other at-risk groups (Table 4 ). Northwest Territories h Grades 4-6 Table 2 . School-based temporary catch-up HPV vaccination programs. British HIV positive individuals aged 9-26 years. Males aged 9-26 years who are street involved. Males aged 9-18 in the care of the Ministry of Children and Family Development. Males of any age who are in youth custody services centres. Females and males who were eligible in Grade 6 are eligible up to the age of 26. Males aged 17-26 years. Hematopoietic Stem Cell Transplant recipients between the ages of 9 years up to the end of Grade 12. Solid Organ Transplant candidates and recipients between the ages of 9-26 years.  Time trends in HPV vaccination coverage rates (full course) in schoolgirls from 2007/08 to 2018/19 are shown in Figure 2 . Rates in girls ranged from 48% in Ontario in the first year of the program to 94% in Newfoundland/Labrador in 2012/13. Only Prince Edward Island had rates consistently over 80%: in girls in years 2011/12 to 2018/19 and in boys in 2014/15 to 2018/19. Rates were over 80% in Nova Scotia in boys in 2015/16 and 2016/17. For some provinces (Alberta, Ontario), there is evidence that rates gradually increased over the first several years of the program, while in Newfoundland/Labrador, rates improved consistently over the entire time period. In Saskatchewan, rates in girls declined gradually over the time period, from 75% in 2008/09 to 69% in 2016/17. There is no obvious time trend in the remaining provinces.  Since their introduction in 2007, Canadian publicly funded HPV vaccination programs have evolved generally from a three-dose to a two-dose schedule, from a quadrivalent to a nonavalent vaccine, and from a girls-only to a gender-neutral program. HPV programs have been shown to be effective, resulting in decline in the incidence of pre-cancerous cervical lesions and genital warts in Canada [40] . In addition to the school-based programs, in most jurisdictions, individuals who missed the full course while at school remained eligible for a certain period of time to receive publicly funded HPV vaccine. Research has shown that providing opportunities to receive missed doses in schools through catch-up programs is important in optimizing coverage [6] . Most jurisdictions have enhanced their public HPV vaccination programs to include high-risk groups, such as men who have sex with men, transsexual persons, HIV-infected individuals, and other at-risk groups. However, there is no standardization and programs vary across provinces and territories. The transition to a mixed HPV vaccine dose schedule in Quebec in the fall of 2018 has been widely debated, stemming from the limited clinical efficacy data of this schedule before adoption [41] [42] [43] [44] . To date, no other jurisdiction in the world has adopted a mixed dose schedule [45] . Following COVID-19 disruptions to school-based vaccine delivery in September 2020, the schedule was further modified to include a deferral of the second dose to 5 years after the first dose. In the absence of long-term durability and/or effectiveness data, the real-world impact of this schedule on HPV-related outcomes in these cohorts may not be fully understood for at least a decade [41] . The current COVID-19 pandemic has disrupted many immunization activities across Canada. School closings, originated from the need to control viral transmission, and disruptions from competing priorities in the capacity of the public health system have led to a decrease in coverage in the school-based programs. Provincial surveillance mechanisms are currently assessing the extent of the vaccination delays and losses in coverage [45] . Despite the challenges imposed by its large land area (the second largest country in the world), sparsely populated rural areas, and lack of a central coordinating authority for overseeing immunization delivery, Canadian HPV vaccination programs have largely succeeded, relative to other high-income countries. However, current vaccine coverage rates fall short of the Canadian Partnership Against Cancer (CPAC)'s target to eliminate cervical cancer by 2040. CPAC's recommendations include attaining: 90% of 17-year-olds to be fully vaccinated by 2025; 90% of eligible women to have been screened with an HPV test by 2030; and 90% of all women with an abnormal screening result to receive within 3 months appropriate diagnostic and treatment by 2030 [46] . Vaccination coverage rates have varied markedly by jurisdiction, and there have been no consistent time trends. Irrespective of jurisdiction, coverage rates in schoolboys have been similar to those in schoolgirls. Since the 2012/13 school year, only Nova Scotia, Prince Edward Island, and Newfoundland/Labrador have had vaccination coverage rates exceeding 80% in any school year. The recommendations of the Canadian Immunization Committee, that 80% of eligible schoolgirls receive the required doses of HPV vaccine within 2 years and 90% within 5 years of program introduction, have largely not been met [47] . More recently, the public health national immunization goal is to achieve by 2025 90% vaccination coverage (two or more doses of HPV vaccine) by 17 years of age in all adolescents [48] . Provinces with the same vaccination program evolution-e.g., Saskatchewan, Ontario, New Brunswick, and Prince Edward Island-have widely different coverage rates. There is no clear evidence that a change in the number of doses was associated with changes in vaccination coverage. There are eight jurisdictions that switched from a three-dose to a two-dose strategy and in only two of them was there a more than five percent increase in vaccination coverage: Saskatchewan and Nova Scotia, both in 2014/2015 to 2016/2017. Similarly, switching the vaccine (but maintaining the same number of doses) had no apparent effect on vaccination coverage. There were six instances of switching the vaccine while maintaining a two-dose schedule and in no instance was there a change in vaccination coverage of ≥ 5%. Gilbert et al. (2016) explored determinants of HPV non-vaccination and vaccine refusal in girls 12-14 years old, based on data from the Childhood National Immunization Coverage Survey of 2013 [49] . Among sociodemographic variables, a younger age (12 years) at vaccination and a European country of birth of the responding parent were associated with an increased risk of not being vaccinated, while parental knowledge, attitudes, and beliefs were associated with parental refusal of HPV vaccination [49] . A 2014 study of a nationally representative sample of Canadian parents of boys aged 9-16 examined psychosocial factors in parents' HPV vaccine decision-making for their son [50, 51] . Grounded in the Precaution Adoption Process Model, results from the online survey illustrated that discussion with a healthcare provider about the HPV vaccine, higher HPV knowledge, increased perception of risks in the absence of HPV vaccination or that others endorse HPV vaccination, and positive attitudes related to vaccines in general were associated with increased odds of being in the more advanced stages of the decision to vaccinate their son. Believing that HPV vaccination is harmful increased the odds of deciding not to vaccinate while perceiving the benefits of HPV vaccination decreased the odds [50] . More recently, Tatar et al. (2019) examined parental HPV vaccine hesitancy from a nationally representative sample of Canadian parents of 9-16 years old boys and girls in 2016 and 2017 [52] . HPV-related attitudes, behaviors, knowledge, and intentions to vaccinate changed over time in parents categorized as ""flexible"" hesitant (unengaged/undecided) compared to ""rigid"" hesitant (decided not). Higher social influence, HPV knowledge, and family income, white race, and lower perception of harms (vaccine safety) were associated with higher HPV vaccine acceptability in ""flexible"" hesitant parents. Given that healthcare in Canada is managed by the individual provinces and territories, there is substantial variation in the delivery and administration of the HPV vaccine across the 13 different programs [6] . Vaccine coverage rates and type of service delivery model may also be impacted by the diverse geography and contextual means of delivery across the Canadian landscape. In Calgary, Alberta, during the period 2008-11, the HPV vaccine was offered free of charge to all girls in Grades 5 and 9, with two different service delivery models, depending upon the acceptance of the program by the local school board-immunization against HPV was not permitted in-school in the Calgary Catholic School District and at a small number of private schools, and in these cases, vaccination was provided in the community at Public Health Clinics [53] . HPV vaccination completion rates were much higher for girls with an in-school model than for girls with a community service model (75% versus 36%) [53, 54] . Bird et al. (2017) reported a meta-analysis of vaccine uptake rates in six of the Canadian provinces based on 12 studies published in the period 2010-2016 [55] . Rates of vaccine uptake varied by age, sex, service delivery model, and funding source. Rates in those ≤18 versus >18 years old were 67% versus 14%, in female versus male (57% versus 47%), in school-based versus community-based programs (69% versus 19%), and with public funding versus out-of-pocket payment (67% versus 14%). Many countries in Europe and the Americas included HPV vaccination of adolescents in their national immunization programs, beginning in 2007 [56] . These programs differ in service delivery method (school-or clinic-based), public funding, inclusion of catch-up vaccination, etc. [56] . However, programs in some countries are similar to those in Canada. For instance, Australia implemented publicly funded, school-based HPV vaccination in 2007, targeting females 12-13 years of age, with catch-up vaccination, and extending the program in 2013 to include males [57] . As of 2015, estimated coverage with three doses was 77% for females and 66% for males [57] . Scotland initiated a similar program, but without including males, and achieved a three-dose coverage rate in the target cohort (12-13-year-old females) of 90% [58] . HPV vaccination programs in most countries, however, are clinic-based [56] . Notably, the United States initiated clinic-based HPV vaccination in 2006, targeting females 11-12 years, and adding males 11-12 years in 2011 [57] . As of 2015, estimated coverage with at least one dose was only 42% among females and 10% among males 19-26 years of age [58] . The present study is limited in that there are gaps in the available information. Not all HPV vaccine coverage rates have been reported for all years, age, and sex by each jurisdiction in Canada. Coverage in other eligible groups outside school-based is rarely available. Canada does not have a national vaccination surveillance program; therefore, there is a lack of standardized methodologies for reporting vaccination coverage rates across jurisdictions, making it difficult to compare vaccination rates [6, 59] . We reported the vaccine coverage rates as provided by provincial health authorities and did not investigate the methodology employed by each individual health unit.@story_separate@In conclusion, since their implementation in schoolgirls in 2007, provincial and territorial programs have undergone multiple policy changes, gradually increasing population protection via inclusion of boys and high-risk groups. Over time, policy changes have reflected the evolving science of HPV vaccination and funding support. While there are methodological variations across jurisdictions in how data are collected and reported, HPV vaccination rate targets clearly remain to be met in the school-based public programs in Canada. The data collated in this report may assist Canadian health authorities in addressing the shortfall in HPV vaccination coverage, with the eventual goal of eliminating HPV-related cervical and other cancers. Supplementary Materials: The following are available online at http://www.mdpi.com/1718-7729/28/1/97/s1, Table S1 : PICO(+) framework. Author Contributions: A.G., G.P.Y., V.R., P.B., and S.K. contributed to the conception, design, acquisition, analysis, or interpretation of the data and drafting or revising the manuscript critically for important intellectual content. E.L.F. contributed to the conception, design, and interpretation of the data and revised the manuscript critically for important intellectual content. All authors have read and agreed to the published version of the manuscript.","Background: Since 2007, all Canadian provinces and territories have had a publicly funded program for vaccination against human papillomavirus (HPV) infection. The objective of this study was to describe the evolution of these vaccination programs. Methods: This was a targeted literature review of public HPV vaccination programs and vaccination coverage rates, based on information provided by jurisdictional public health authorities. Results: HPV vaccination of schoolgirls began in school years 2007/08 to 2010/11 with three doses of the quadrivalent HPV vaccine in all provinces except Quebec, which started with two doses. By 2018/19, all jurisdictions were vaccinating with two doses of the nonavalent vaccine in both girls and boys, except Quebec, which used a mixed vaccination schedule with one dose of the nonavalent and one dose of the bivalent vaccines. Public HPV vaccination programs in most provinces include after-school catch-up vaccination. Immunocompromised or other high-risk individuals are eligible for the HPV public vaccination program in most provinces, but policies vary by jurisdiction. In 2017/18, vaccination coverage rates in provincial HPV school-based programs varied from 62% in Ontario to 86% in Prince Edward Island in girls and from 58% in Ontario to 86% in Prince Edward Island in boys. Conclusions: Since their introduction, Canadian school-based HPV public vaccination programs have evolved from a three-dose to a two-dose schedule, from a quadrivalent to a nonavalent vaccine, and from a girls-only to a gender-neutral policy. Vaccination coverage rates have varied markedly and only Prince Edward Island and Newfoundland/Labrador have maintained rates exceeding 80%."
"Coronavirus disease (COVID-19) is a viral disease caused by SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2), a new strain of coronavirus first identified in humans in January 2020. The WHO declared the coronavirus outbreak a pandemic on 11 March 2020 [1] . Italy was one of the first Western countries to be severely affected by the coronavirus pandemic: at first, the pandemic mainly affected the northern and central parts of the country and then spread nationwide. As of today, more than 1 million people in Italy have been infected with SARS-CoV-2 and at least 117,000 people have died [2] , but the spread of COVID-19 could be larger than officially reported [3] . Vaccination against SARS-CoV-2 represents an effective and safe tool to protect the population against the disease: as of 21 February 2021, three COVID-19 vaccines with greater than 60% efficacy to reduce symptomatic infection risk have been approved in the EU [4] [5] [6] and many other candidate vaccines are in phase 3 trials [7] . However, COVID-19 vaccine hesitancy represents a major barrier to achieving herd immunity; despite the current pandemic, the population's intention to get vaccinated against COVID-19 is still not clear. Vaccine hesitancy can be defined as a ""delay in acceptance or refusal of vaccination despite availability of vaccination services"", with several factors affecting it, according to the Strategic Advisory Group of Experts on Immunization (SAGE) [8, 9] . A systematic review of surveys of the general population revealed that the willingness to receive a pandemic vaccine ranged from 8% to 67% [10] . In Italy, vaccine hesitancy is often related to spreading fake news on the disease and related vaccinations [11] . During the first SARS-CoV-2 pandemic, fake news had a significant impact on the Italian population, and they still represent one of the most contributing factors to the skepticism on vaccinations [12] . This is a particular concern in Italy, which has been shown to be a country with a significant presence of vaccine hesitancy [13] . A mass vaccination campaign is currently the only way to boost immunity in order to protect the population from SARS-CoV-2 infection, and high rates of acceptance and coverage are needed. The aim of this pilot study was to evaluate the intention to get vaccinated against COVID-19 among a convenience sample of the general population resident in Italy and the factors associated with hesitancy and acceptance of the vaccine in the context of the current pandemic.@story_separate@An anonymous online survey was conducted from 11 December to 15 December 2020 among an adult general population. The questionnaire was shared through social platforms (Facebook, Instagram, WhatsApp, Telegram) and using a regional website that aims at enhancing the knowledge and the awareness about vaccination in the general population (https://www.vaccinarsintoscana.org accessed on 18 April 2021). Participants aged 18 or older and living in Italy were considered eligible. Incomplete questionnaires were excluded. Participation in the study was voluntary, and data were collected anonymously. All participants provided online informed consent to be included in the study. In a first section, along with socio-demographic variables, the questionnaire addressed aspects such as trust in institutions, vaccine hesitancy and adherence to the 2020-2021 influenza campaign. As stated by the WHO SAGE, vaccine hesitancy refers to a complex phenomenon defined as delay in acceptance or refusal of vaccine despite the availability of a vaccination service. According to the WHO's definition and other studies published on hesitancy, in this study, we evaluated participants' self-reported vaccine hesitancy using two adapted questions: we asked whether the participants have ever refused or postponed a vaccination recommended by a physician for themselves or a child because they considered it useless or dangerous and whether they would advise others against a vaccination, even if recommended [14] [15] [16] . Subjects answering ""yes"" to at least one of this two questions were considered hesitant. Trust in institutions was assessed with a Likert scale by asking participants how much they trust the institutions (possible answers ranged from ""1-I have no trust at all in institutions"" to ""4-I have full trust in institutions""). In a second section, we used Likert scales to first assess the intent to be vaccinated against the novel coronavirus (possible answers ranged from ""1-surely not"" to ""5surely""); then, we evaluated possible determinants of a vaccine hesitancy or acceptance such as the perceived relevance of the vaccination in fighting against the virus and the perceived safety of the vaccination against COVID-19. We also assessed the possibility that the respondent or a family member or acquaintance had contracted the disease or had been hospitalized because of it. Descriptive statistics were conducted to generate summary tables for study variables. The item related to the willingness to be vaccinated against COVID-19, previously expressed with five answers, was transformed into a binary variable, grouping answers ""1-surely not"", ""2-probably not"", and ""3-I am not sure"" in ""no/not sure"", and ""4probably"" and ""5-surely"" in ""yes/probably"". In order to assess the predictors of this new, dichotomous variable, we performed both single and multiple analyses. In particular, 12 single logistic regression analyses were performed, one for each item of the questionnaire. Variables that were found to be significantly associated with the dependent variable were included in a backward stepwise procedure to perform a multiple logistic regression model: results are presented as an odds ratio (OR) with 95% confidence interval (95% CI). For all the analyses, a p-value < 0.05 was considered significant. Statistical analyses were conducted using RStudio 1.  Of 7656 collected questionnaires, 0.25% (n = 19) had to be excluded as participants denied consent to the treatment of personal data, while 0.42% (n = 32) were excluded because respondents stated that they do not live in Italy. A total of 7605 questionnaires were considered available for data collection and analysis. Table 1 shows the summary statistics of the sociodemographic profile of the study participants. Of the total respondents, 64.9% (n = 4939) were females, while median age was 47.0 years (IQR: 34.0-58.0 years); 35.9% (n = 2729) were healthcare professionals (HCP), and three out of four (n = 5706) were employed when completing the questionnaire. With regard to the level of education, most respondents (39.4%, n = 3000) had a high-school qualification, followed by those with a master's degree (35.5%, n = 2686), a bachelor's degree (14.1%, n = 1070), those with a secondary school qualification (7.6%, n = 578), with a PhD (3.1%, n = 239), and finally those with a primary school qualification (0.4%, n = 32). When asked about their trust in institutions, the most selected answer was ""3-I have enough trust in institutions"" (65.6%, n = 4633), while the mean and the median of the item were 2.8 and 3, respectively. Table 2 shows the personal beliefs about vaccinations and the willingness to get vaccinated against COVID-19. Most of the participants declared they had never ""refused or postponed a vaccination recommended by a physician for themselves or a child because they considered it as useless or dangerous"" (88.0%, n = 6695), while 91.0% (n = 6952) stated that they would not advise others against a vaccination, even if recommended. Combining these results, as specified in the methods section, we identified 1204 hesitant individuals (15.8% of the sample). Moreover, less than a half (43.5%, n = 3309) of participants stated that they had been vaccinated against influenza in the 2020-2021 season. When asked if they will vaccinate against COVID-19, if recommended for them, 6231 (81.9%) subjects answered that they would get vaccinated (4908, 64%, answered ""I will surely get vaccinated"", while 1323, 17.4%, answered ""I will probably get vaccinated""); 1374 (18.1%) were hesitant about COVID-19 vaccination or even against it. A slightly higher percentage of subjects willing to be vaccinated against COVID-19 was found among HCPs (82.7% of the HCPs, n = 2256; Table 3 ). Table 3 . Variables associated with the willingness to be vaccinated against COVID-19. From the left: the first column shows the variables included in the analyses. The second column shows the ""willingness to be vaccinated"" among the categories reported in each item. The third and the fourth columns show the results of the single logistic regression analyses and the results of the multiple logistic regression analysis. The exploration of the personal beliefs and personal experiences about COVID-19 showed that 3417 subjects (44.9%) thought that COVID-19 vaccines would be safe (n = 3580, 47.1% declared they ""don't know"" and n = 608, 8.0% answered ""no""); most of the participants (n = 5818, 76.5%) were sure that the vaccination would be ""absolutely relevant"" in fighting the COVID-19 pandemic. Moreover, it emerged that 6546 (86.1%) participants knew someone that had been infected by SARS-CoV-2 (including themselves); almost a half of our sample (n = 3353, 44.1%) also knew someone that had been hospitalized for the same reason (including themselves). According to multiple logistic regression (Table 3) , factors independently found associated with higher COVID-19 vaccine acceptance were of male sex (OR 1.38, 95% CI 1.12-1.71) and had a higher level of trust in institutions (OR 3.93, 95% CI 2.04-7.83); moreover, subjects that got vaccinated against influenza had less chances to be against COVID-19 vaccination (OR 2.09, 95% CI 1.68-2.58). Employed individuals and hesitant subjects had less chance to be willing to get vaccinated against COVID-19 (OR 0.77, 95% CI 0.61-0.95% and OR 0.13, 95% CI 0.11-0.17). Finally, those who thought that this vaccination would be safe and/or relevant to fight the COVID-19 pandemic were more likely to be willing to be vaccinated. Knowing someone that had been infected by COVID-19 or hospitalized due to COVID-19 was not significantly associated with increased vaccine acceptance, as well as being HCP, since COVID-19 vaccine acceptance in non-HCPs was almost as high as in HCPs. Vaccination is surely one of the most relevant public health interventions, but its acceptance varies with space, time, social class, ethnicity, and contextual human behavior [17] . In the current context, COVID-19 vaccine hesitancy may be the most important limiting factor in efforts to control the pandemic and its effects on health [18, 19] . Our pilot study used a web-based questionnaire to evaluate the factors influencing SARS-CoV-2 vaccine acceptance and hesitancy in a population-based sample in Italy: there are still few studies exploring the intention to receive the COVID-19 vaccines before their availability and our results are consistent with those of other studies conducted in Italy and similar countries, although a large variability in COVID-19 vaccine acceptance rates have been reported worldwide, varying from 40% up to >90% [20] [21] [22] . In particular, 64.5% of our sample declared that they would certainly be vaccinated when possible, while 17.4% answered ""probably"", outlining a large percentage of the population (81.9%) willing to be vaccinated, a result in line with another study recently published in Italy [23] and with current data regarding those categories that were first involved in the vaccination campaign, such as the elderly category; more than 80% completed the vaccination cycle and more than 90% received at least 1 dose [24] . Identifying factors that promote acceptance can help policymakers boost COVID-19 vaccination, and the study results reported here provide further evidence of the major role that some determinants of health and personal beliefs can play in vaccine acceptance and hesitancy. Among the most described positive predictors for accepting the vaccination against SARS-CoV-2 there is the self-perception of a high risk of severe COVID-19: HCP, especially those who work with COVID-19 patients, probably perceiving a greater risk of getting infected, are generally more likely to get vaccinated [25, 26] . A total of 82.7% of the HCPs that participated in this survey were determined to get vaccinated, and this figure is consistent with current Italian data (to date, 79.4% of HCPs are vaccinated with a full two-dose immunization course, and the percentage is higher if we consider those that only received the first dose) [24] . Even if previous studies have found conflicting results concerning COVID-19 vaccine hesitancy among HCP [27] , they were slightly more inclined than non-HCP to get vaccinated in our sample, even if this result was not statistically significant (OR 1.08, 95% CI 0.96-1.22). Moreover, as reported by other studies, acceptance was higher in males than females (OR 1.38, 95% CI 1.12-1.71). Explanations may vary, including a higher risk perception in males, due to the higher risk of severe COVID-19 described for men [28, 29] . Both previous vaccination against influenza and high level of trust in government/ institutions have been studied as factors associated with COVID-19 vaccine acceptance, with results that are consistent with our findings [14, 30] . In fact, as well as previous influenza vaccination (OR 2.09, 95% CI 1.58-2.68), a high level of trust in institutions resulted in being a strong predictor of vaccine acceptance in our sample (OR 3.93, 95% CI 2.04-7.83). Similarly, other studies showed how those who did not intend to get vaccinated against COVID-19 had lower level of trust in institutions [31] . For this reason, and when considering the lack of trust in government that has been found by many studies as predictor of a higher level of COVID-19 vaccine hesitancy, it is necessary to pay attention to the fact that part of our sample (almost the 30%) claimed to have no (or low) trust in institutions: higher trust in institutions and public health authorities may enable people to accept the short-term individual costs associated with vaccination to address the solution of common problems in the community [32] . Factors independently found associated with vaccine hesitancy against COVID-19 vaccine in our sample included being hesitant about other vaccinations and being employed; this is not uncommon, and it could be possible that those who are not working consider the vaccine as a factor that could facilitate their return to work [33] . Individuals that took part in recent studies were generally concerned about vaccines' safety, usually because their fast development. The most frequently remarked safety observations included quality control and potential side effects [22, 34] . It is therefore not surprising that, in our sample, the fact of considering the vaccine as safe was a strong predictor of vaccine acceptance (OR 56.33, 95% CI 31.73-105.87); however, it is important to underline how only a small number of the participants considered it unsafe (8.0%) and that most of the participants thought that it would be a key factor in fighting against the disease (76.5%). This is quite surprising, considering that the survey was conducted in a period in which televisions, newspapers, and media were reporting conflicting information regarding the efficacy, safety, and availability of COVID-19 vaccines. Age, study level, and knowing people infected or hospitalized due to COVID-19 were not predictors of higher or lower vaccine acceptance in our sample. Our pilot study has several limitations; first, the study involved a convenience sample of Italian adults, and it was distributed online, thus being accessible to web-users only. Therefore, the findings may not be representative of the whole population. Second, it is a cross-sectional study and depicts a picture of the community response at the point of the survey; moreover, almost 1 participant out of 10 (9%) reported being ""not sure"" about the intention to uptake the COVID-19 vaccination. The real intention could be different when the vaccine is available or when information about specific COVID-19 vaccines change over time. Finally, the survey took place in December 2020, right before any COVID-19 vaccine was available. Interestingly, the survey was realized before the beginning of the Italian anti-COVID-19 vaccination campaign: on one side it represents the beliefs of our sample before the vaccination started, on 27 December, along with the polarization of the public debate about it. On the other hand, it could potentially not be representative of the current situation after months of debates about COVID-19 vaccine efficacy and safety. As a matter of fact, it only represents the general beliefs about COVID-19 vaccinations right before the beginning of the vaccination campaign, since no specific questions were asked to assess if participants were concerned about a certain type of vaccine or if the opinions towards COVID-19 vaccines have changed during the period of vaccine development. Despite these limitations, our findings are novel and relevant, and our study is among the first reporting COVID-19 vaccine willingness among a large sample of adults living in Italy and in analyzing the factors that influence COVID-19 vaccine acceptance and hesitancy before the vaccine availability and the dissemination of more in-depth efficacy and safety data.@story_separate@The results of this study show high rates of vaccine acceptance among adults living in Italy. In particular, factors such as trust in institutions, previous influenza vaccination, and the beliefs about vaccination safety and efficacy in fighting against the pandemic resulted in being strong predictors of vaccine acceptance. Large studies, representative of the Italian population, are needed to draw conclusions: if these results will be confirmed and aiming to even increase the number of individuals willing to be vaccinated against COVID-19, it will be important to reinforce some communication characteristics of the current vaccination campaign. It should not only be evidence-based, but it should also clearly and effectively explain to the population the importance of vaccination against COVID-19. The data outlined in this manuscript can help to design larger studies to better understand the factors that influence COVID-19 vaccine acceptance and to address the problem of COVID-19 vaccine hesitancy in the current pandemic.","Vaccination against SARS-CoV-2 represents an effective and safe tool to protect the population against the disease; however, COVID-19 vaccine hesitancy could be a major barrier to achieving herd immunity. Despite the severity of the current pandemic, the population’s intention to get vaccinated against COVID-19 is still not clear. The aim of this study was to evaluate the intention to get vaccinated against COVID-19 among a convenience sample of the general population resident in Italy and the factors associated with hesitancy and acceptance of the vaccine in the context of the current pandemic before the rolling out of COVID-19 vaccines. An anonymous online survey was diffused among a general adult population living in Italy. Participants aged 18 or older and living in Italy were considered eligible. Incomplete questionnaires were excluded. Overall, 7605 valid questionnaires were collected. Most of the participants (81.9%) were inclined to get vaccinated; male sex (OR 1.38, 95% CI 1.12–1.71), a high level of trust in institutions (OR 3.93, 95% CI 2.04–7.83), and personal beliefs about high safety of COVID-19 vaccines (OR 56.33, 95% CI 31.57–105.87) were found to be among the significant predictors of COVID-19 acceptance. These data could help design larger studies to address the problem of COVID-19 vaccine hesitancy in the current pandemic."
"Coronavirus disease 2019 , is a respiratory disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is currently considered a global health emergency. Since first being identified in Wuhan, China in December 2019, SARS-CoV-2 has spread widely and infected millions of people globally. On March 11, 2020, the World Health Organization (WHO) declared this outbreak to be a global pandemic. To date, there have been around 1.24 million deaths reported according to WHO as of November 7 th , 2020 (World Health Organization, 2020). SARS-CoV-2 is a single-stranded ribonucleic acid (RNA) enveloped virus from the genus Betacoronavirus, subfamily Orthocoronavirinae, and family Coronaviridae (Zheng, 2020) . Two other viruses belong to this genus, namely Middle East respiratory syndrome virus and severe acute respiratory syndrome virus (SARS-CoV). SARS-CoV-2 was identified to have 82% RNA identity with SARS-CoV . Both SARS-CoV and SARS-CoV-2 recognize the same receptor in the human body, namely angiotensin-converting enzyme 2. The SARS-CoV-2 genome encodes a trimeric structural spike protein, a homodimeric cysteine proteinase, an RNA polymerase, and several nonstructural proteins (Calligari et al., 2020) . The viral pathogenesis causes several symptoms, such as sore throat, running nose, cough, fever, and eventually respiratory failure. Currently, there is no specific therapy with curative efficacy against the disease (Centers for Disease Control and Prevention, 2020) . Several drugs, such as hydroxychloroquine and chloroquine, have been suggested as treatments. However, no studies were adequately powered to prove their efficacy (Pastick et al., 2020) . In addition, several reports of serious arrhythmia have been described in patients with COVID-19 who received these compounds (United States Food and Drug Administration, 2020 ). In addition, efforts are underway to develop vaccines to control the outbreak. However, given the complexity of the virus and the rapid mutation of its single stranded RNA, it may take years to generate effective vaccines (Huanget al., 2020) . Thereby, other compounds should be examined to identify an effective treatment with few or no adverse effects. According to Marcucci, propolis, a resinous bee product, exhibits antiviral activity based on the presence of flavonoids, caffeic acid, and esters of aromatic acids (Marcucci, 1995) . The antiviral activities of these compounds occur through the inhibition of viral transmission to other cells, inhibition of viral propagation, and destruction of the outer envelope of the virus (Marcucci, 1995) . Various experiments have been conducted to analyze the antiviral activities of the compounds. Research by Yildirim et al. revealed that propolis significantly decreased the number of copies of herpes simplex virus 2 after 48 hours of incubation (Yildirim et al., 2016) . In addition, Gekker et al. demonstrated that propolis has a destructive effect on the outer envelope of human immunodeficiency virus (Gekker et al., 2005) . In terms of antiviral activity in the respiratory tract, another study reported that some propolis compounds have lower IC 50 values against human rhinovirus than ribavirin (Kwon et al., 2019) . The potency of propolis as COVID-19 drug has also been observed through some research. Vardhan and Sahoo (2020) have shown that three propolis components, namely limonin, quercetin and kaempferol have inhibitory potential by binding to viral RNA-dependent RNA polymerase (RdRp) with binding energy −9 to −7.1 kcal/mol through molecular docking study (Vardhan & Sahoo, 2020) . Molecular docking enables the virtual screening of millions of compounds in a time-and cost-efficient manner (Pinzi & Rastelli, 2019) . Hence, this method is widely used as a preliminary study in drug discovery (Meng et al., 2011) . Another molecular docking study by Güler et al. (2020) concluded that identified compounds from alcoholic extract of propolis are able to bind with angiotensin-converting enzyme (ACE)-2. Thus, the compounds may be potential to prevent SARS-CoV-2 to bind with ACE-2 (Güler et al., 2020) . Based on these findings, propolis compounds may have potency for treating COVID-19. Further research is needed to evaluate the ability of propolis compounds to inhibit SARS-CoV-2 replication. Other protein in SARS-CoV-2 that is also considered a potential therapeutic target is main protease (Anand et al., 2003) . The enzyme cleaves polyproteins translated from viral RNA into 12 smaller proteins that participate in viral replication (Chen et al., 2020) . Therefore, viral replication can be blocked by inhibiting this enzyme . Previously, Zhang et al. developed peptidomimetic α-ketoamides as potential broad-spectrum inhibitors of main protease in Betacoronaviruses and Alphacoronaviruses . One such compound, tert-butyl(1-((S)-1-(((S)-4-(benzylamino)-3,4-dioxo-1-((S)-2-oxopyrrolidin-3yl)butan-2-yl)-amino)-3-cyclopropyl-1-oxopropan-2-yl)-2-oxo-1,2-dihydro-pyridin-3-yl) carbamate, also known as 13b, inhibited SARS-CoV-2 replication in human Calu3 lung cells. The crystal structure of SARS-CoV-2 main protease in complex with inhibitor 13b is available. Propolis compounds are diverse according to the region (Alday et al., 2016). Beforehand, Sulawesi propolis compounds from North Luwu have been identified by several research (Mahadewi et al., 2018; Miyata et al., , 2020a Miyata et al., , 2020b Sahlan et al., 2019) . Sulawesi propolis previously was known to be produced by Tetragonula aff. biroi. However, recent study rectified that it is actually produced by Tetragonula sapiens (Sayusti et al., 2020) . Some research has also proved the health benefits of Sulawesi propolis. Sulawesi propolis exhibits antifungal activity to Candida albicans, C. tropicalis, C. krusei, C. parapsilosis, C. glabrata and Cryptococcus neoformans . Other study showed that Sulawesi propolis is potential to be developed as a non-steroid antiinflammatory drug and antioxidant agent Sahlan et al., 2019) . These facts encourage us to do more investigations regarding the health benefits of Sulawesi propolis. The components of antioxidant and flavonoid in Sulawesi propolis may perform antiviral activity. Up to date, there has been no research that aims to evaluate the potency of Sulawesi propolis compounds to treat COVID-19. In this research, molecular docking was performed to analyze the molecular interaction between SARS-CoV-2 main protease (PDB ID: 6Y2F) and Sulawesi propolis compounds. Molecular docking aims to predict the conformation of the ligand within the receptor and assess the binding affinity (Guedes et al., 2014) , which is represented by the docking score (kcal/mol). The docking score and binding characteristics of inhibitor 13b were used to judge whether the propolis compounds are potential inhibitors of SARS-CoV-2 main protease. Furthermore, this research also performed molecular docking between SARS-CoV-2 main protease and 14b, a modified version of inhibitor 13b. 14b does not feature a Boc group, which predicted to protect the compound as it crosses the cellular membrane . Although 14b was mostly inactive in human Calu3 lung cells, this compound may have lower affinity for plasma proteins than inhibitor 13b .@story_separate@Molecular docking was performed using an Asus laptop with an Intel ® Core™ i7-8550U @1.80 GHz processor, 8 GB of RAM, the Windows 10 Home Single Language 64-bit operating system, and an Intel ® UHD Graphics 620 graphics processing unit. The software used in the study included  First, 20 identified Sulawesi propolis compounds from North Luwu, South Sulawesi, Indonesia were selected based on Lipinski's RO5. The rules aim to assess the solubility and permeability of drug candidates. The RO5 stated that poor absorption or permeability are more likely to occur when a compound has molecular weight greater than 500 g/mol, Log P greater than 5, H-bond donors more than 5 and H-bond acceptors more than 10 (Lipinski et al., 1997) . Sulawesi propolis compounds that violates more than 2 rules were thrown away. Lipinski's RO5 was assessed by using MarvinSketch (ChemAxon). The SARS-CoV-2 main protease in complex with the α-ketoamide 13b (PDB ID: 6Y2F) was obtained from the Protein Data Bank (http://www.rcsb.org) in the.pdb format . The Protein Data Bank is an open access digital data resource that provides 3D structure data for large biological molecules (Berman et al., 2013) . The protein and inhibitor 13b file was then separated into two different.pdb files using Visual Molecular Dynamics (Humphrey et al., 1996) . Then, the separated protein files were loaded into Autodock Tools 1.5.6 for further preparation consisting of the addition of polar hydrogens, addition of kollman charges and conversion of the file to the.pdbqt format (Forli et al., 2016) for compatibility for docking using Autodock Vina. Meanwhile, the test ligands were selected Sulawesi propolis compounds from North Luwu produced by Tetragonula sapiens and 14b using inhibitor 13b as control. The structure of inhibitor 13b and 14b are shown in Figure 1 . Excluding inhibitor 13b, the 2D structures of the ligands were constructed and converted into 3D structures in the.pdb format using MarvinSketch. Then, the ligands were loaded to Autodock Tools 1.5.6 to add polar hydrogens as well as gasteiger charges, and convert the file to the.pdbqt format. Before performing molecular docking between propolis compounds and main protease, the optimal specific search space was determined. The search space is the area within the protein in which the docking simulation will be performed. It may represent the locations of the binding sites. The optimal search space can be predicted via docking between the native ligand of the protein structure, in this case 13b, using the protein that was separated in the protein preparation step. This method is also known as ""redocking."" Redocking and the subsequent docking simulation were performed using Autodock Vina. In terms of accuracy, Autodock Vina offers more accurate binding mode predictions than Autodock Tools (Vieira & Sousa, 2019) . Adjustment was performed for three parameters to obtain the optimal specific search space, namely the center of the grid box, number of points in the x, y, and z dimensions, and grid spacing. Based on the redocking simulation, it was found that the binding site had a higher probability to be located at the following coordinates: x = 11.476, y = −1.396, and z = 20.745. The number of points in the x, y, and z dimensions were all set to 25 Å, and the grid spacing was adjusted to 1.0 Å. After the docking score was obtained, the root mean square deviation (RMSD) of the first rank ligand pose was evaluated. An RMSD of less than 2.0 Å indicates that the redocking procedure is 6 accurate (Ramírez & Caballero, 2018) . Thereby, the docking coordinates could be used to dock the propolis compounds onto the protein. To identify which propolis compounds are potential SARS-CoV-2 inhibitors, the compounds were docked into the protein individually using the coordinates and grid parameters obtained from the redocking simulation. The simulations were performed using Autodock Vina with exhaustiveness value set to 64. After the docking score for each compound was obtained, the interactions between the protein and the ligands with the lowest docking scores were analyzed using Ligplot+, which provides 2D visualization of molecular interactions. This research analyzed the molecular interactions between selected Sulawesi propolis compounds  First of all, 19 Sulawesi propolis compounds from North Luwu were selected based on Lipinski's RO5 in Table 2 . According to the result, (-)-papuanic acid, isopapuanic acid, isocalopolyanic acid and curcumene have Log P of greater than 5.0. In the meantime, a compound, namely α-tocopherol succinate, violates two out of four rules, which are molecular weight and log P parameter. A molecular weight greater than 500 g/mol will lead to poor permeability when a drug molecule penetrates biological membrane through passive diffusion process (Qiu et al., 2016) . Meanwhile, Log P represents octanolwater partition coefficient. A drug molecule with high Log P value tends to be more nonpolar and have poorer aqueous permeability (Templeton et al., 2015) . Although there are several violations, the number of rules being obeyed is sufficient to indicate that the compounds have good permeability. In addition, there are some tolerable Lipinski's RO5 parameter value for natural compounds. Selected Sulawesi propolis compounds, inhibitor 13b and 14b then were docked to main protease by using Autodock Vina. Docking simulation by using Autodock Vina produces two results, which are the most stable ligand poses and docking score. The docking score is generated from an empirical calculation that considers the number of hydrophobic interactions and hydrogen bonding (Trott & Olson, 2010) . The greater the number of interactions, the greater the tendency for the docking score to be negative. The docking score may become a representative of binding affinity, and it is inversely proportional to the binding stability (Flamandita et al., 2020) . The negative docking score indicates that the compound, which in this case acts as a ligand, has the ability to interact with the protein. Meanwhile, the most stable ligand pose will further be used to find out which amino acid residues in the protein interact with the ligand. Thereafter this is referred as interaction profile. By comparing interaction profile between SARS-CoV-2 main protease and propolis compounds with SARS-CoV-2 main protease and inhibitor 13b, the potency of propolis compounds to have the ability to inhibit the protein with the same pathway as inhibitor 13b may be evaluated. The docking score between main protease and test compounds are available in Table 3 . Based on the docking simulation, 13b and 14b bound to main protease with a binding affinity of −8.2 and −7.2 kcal/mol. Meanwhile, among the propolis compounds, the three lowest docking scores were obtained for broussoflavonol F, glyasperin A, and sulabiroins A, with values of −7.8, −7.8, and −7.6 kcal/mol, respectively. Broussoflavonol F and glyasperin A are identified as flavonoids, while sulabiroins A is a derivative of podophyllotoxin compounds (Miyata et al., , 2020a (Miyata et al., , 2020b . The interaction profiles of these three compounds, inhibitor 13b and 14b with main protease severally then were analyzed. The interaction profile analysis was focused on the existence of interaction with His 41 and Cys 145 . These amino acids have an important role as main protease catalytic sites. Thus, propolis compounds must interact with these sites in order to inhibit the activity of the enzyme. The two-dimensional (2D) visualization of the interaction of main protease with 13b, broussoflavonol F, glyasperin A, and sulabiroins A were presented in Figure 2 . The interaction profile between main protease and inhibitor 13b and 14b were firstly analyzed. According to the visualization, the docking simulation successfully captured the interaction of inhibitor 13b with both His 41 and Cys 145 , in line with the results reported by Zhang et al. (2020) as the crystallographer . The shortest interatomic distance between main protease with His 41 and Cys 145 are 3.03 and 3.27 Å respectively. In the meantime, 14b only interacts hydrophobically with His 41 . Based on this finding, although 14b may have a better in vivo potency due to lower affinity for plasma protein binding, it does not have favorable interaction towards the main protease. The interaction profile between main protease and each propolis compounds were then analyzed. For further consideration, binding similarity between interaction profile of main protease with the three propolis compounds and 14b compared to interaction profile of main protease and inhibitor 13b were calculated. Based on the calculation, 14b, broussoflavonol F, glyasperin A and sulabiroins A bound to main protease with the similarity of 71%, 75%, 63% and 44% respectively. The summary of interaction profile between main protease and each test compounds are available in Table 4 , while Table 5 summarizes the hydrogen bonds present between main protease and each test compounds. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. The research was financially supported by DRPM Universitas Indonesia through Grant Publikasi  Alday, E., Navarro-Navarro, M., Garibay-Escobar, A., Robles-Zepeda, R., Hernandez, J. Meanwhile, the arcs with spokes radiating toward the ligand atoms represent hydrophobic interactions. The atoms involve in hydrophobic interactions are indicated by the presence of spokes radiating back (Wallace et al., 1995) . @story_separate@Sulawesi propolis compounds from North Luwu that are produced by Tetragonula sapiens exhibit potential to inhibit SARS-CoV-2 main protease activity. Three compounds, namely broussoflavonol F, glyasperin A, and sulabiroins A, displayed the greatest docking score, with values of −7.8, −7.8, and −7.6 kcal/mol, respectively. The study results suggest that molecular dynamic simulation should be conducted for broussoflavonol F and glyasperin A given that both compounds interact with main protease catalytic sites and have the ability to bind with main protease with binding similarity of 75% and 63% respectively compared to potent inhibitor. Further research should be conducted to verify the potency and safety of broussoflavonol F and glyasperin A to the treatment of COVID-19.","Coronavirus disease 2019 (COVID-19), a respiratory disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is a global health concern, as the World Health Organization declared this outbreak to be a global pandemic in March 2020. The need for an effective treatment is urgent because the development of an effective vaccine may take years given the complexity of the virus and its rapid mutation. One promising treatment target for COVID-19 is SARS-CoV-2 main protease. Thus, this study was aimed to examine whether Sulawesi propolis compounds produced by Tetragonula sapiens inhibit the enzymatic activity of SARS-CoV-2 main protease. In this study, molecular docking was performed to analyze the interaction profiles of propolis compounds with SARS-CoV-2 main protease. The results illustrated that two compounds, namely glyasperin A and broussoflavonol F, are potential drug candidates for COVID-19 based on their binding affinity of −7.8 kcal/mol and their ability to interact with His(41) and Cys(145) as catalytic sites. Both compounds also displayed favorable interaction profiles with SARS-CoV-2 main protease with binding similarities compared to inhibitor 13b as positive control 63% and 75% respectively."
"As reports showed, a pandemic of unknown pneumonia started in Wuhan, China, in December 2019, but there are reports that it began by the end of November 2019 [1] [2] . A new variant of the corona virus COVID-19 similar to SARS-CoV, for which health experts have approved no valid medicine or vaccine. This virus is transmitted by the respiratory tract or by contact with infected particles or pathogenic organisms over an incubation period of 2 to 14 days [3] [4] . Until then, the world could be protected by various preventive measures; COVID-19 pandemic has taken over the entire globe. Over an early duration of 4 months, many as five million cases are publicly revealed, with much more than 300,000 causalities in over 150 countries [5] [6] . COVID-19 is not explicitly treated, and the mortality rate varies across different countries and ranges from 2 to 15 percent [7] [8] . The epidemiological data verify that the spread rate is very high, and the WHO confirmed that SARS-CoV is about 10 times fatal [9] . @story_separate@It is essential to determine the needed potential facilities, which may be necessary for terms of hospitals, beds, ventilators, medicines, etc. In regards to health personnel including doctors, nurses, and support personnel, a variety of researchers, including physicists, biologists work for thorough evaluation and further consequences of their developed model in the study of infected data, recovered data, and death rates. Infection, recovery, and mortality rates depend not only on the form of Covid-19 but also on the country's reaction. In this scenario, the researcher would like to examine data on infected, recovered, and dead people in India directly compared with other countries and the worldwide mean. The specific modeling was designed and evaluated by a few researchers, and their findings were presented [12] [13] [14] [15] . Some of the models are discussed below, along with their brief concept and shortcomings. Julia R Gog (2020) explained the classic epidemic susceptible-infectious-recovered (SIR) model, a very simple model proposed in the 1920s. It is called the SIR model, and S is the number of people who are susceptible to disease but have not been infected so far, I form the number of people who are ill, and R are the people who are either dead or healed from infection [16] [17] . SanglierContrearas (2020) suggested the prediction of COVID-19's epidemiological progress through modeling, analysis, and interpretation of data, and recommending various quadratic equations and appears to fit for critical data evaluation. The multiple factors studied and evaluated are the number of people infected, the death rate, the number of people recovered, the duration of treatment, etc. [18] . Rashad Eletrey (2020) investigates the impact of evolutionary adaptations, referring COVID-19 on the transmission of processes in complex networks [19] . If we carefully analyse and compare the figure 5 with figure 3 , we will see that on Day 130, (Aug13, 2020), the line showing number of cumulative active infected people is changing slope in having a small peak, thereafter the number will keep of reducing which indicates that the recovering people are more than the freshly infected people. It has been found that the projected and the actual data are in the good agreement exhibiting the similar pattern. Figure 5 can be employed to make estimates/projections for Covid-19 infections on days in advance.@story_separate@The following conclusions have been drawn from the investigation presented herewith:","The new outbreak of the corona virus (Covid-19) is expanding rapidly worldwide, disrupting millions and prompting authorities to take swift measures to avoid the disease. National lockdown imposed by the Indian government since 25 March 2020, the early lockdown action shows as compared to many other Countries/states can benefit from limiting the final size of the epidemic. A report on the issue of spreading the Covid-19 modeling in India is under review. This study analyzes Covid-19 infections by 20Dec 2021 and presents a mathematical approach for forecasting new cases or cumulative cases in practical situations. This forecast is much needed to schedule/continue medical set-ups for possible action to tackle the Covid-19 outbreak. It is important to mention here that the number of authors has proposed different models for predicting the expansion of Covid-19 to India and other countries; almost no model has yet to be demonstrated viable. With this mathematical model, it is simple to forecast the transfer of Covid-19. It is clear from the data that lockdown has played a significant role in controlling the transmission of the disease. A close match between the predicted empirical results and the available results proves the derived model similarity."
"Mathematical modeling approach is a strong key tool in order to handle different infectious diseases [1] [2] [3] [4] [5] [6] [7] [8] [9] . Many real-world problems in science and engineering can be modeled by stochastic differential equations [10] [11] [12] . Fractional differential equations have been widely applied in many fields such as physics, chemical, fluid dynamic and epidemic models [13, 14] . The main challenge according studying infectious diseases is the way to predict the disease behavior and how many people will be infected in the future. Different researchers from different interdisciplinary such as applied mathematics and data science have been working on studying these types of predictions so it becomes a great area of research. Such studies are now focusing on describing COVID-19 dynamics as it became an epidemic disease. Dynamic models are used to model viral waves and time of actions to calculate the total number of infections and deaths. In all studies, COVID-19 transmission rates are varied from one country to another and similarly the virus dynamics as it depends on the behaviors of individuals, temperature, relative air humidity and wind speed. Based on these predictions and studies, governments can take more suitable actions to mitigate human and economic losses. On the other hand, the classic susceptible exposed infectious recovered model (SEIR) is adopted model for characterizing the epidemic of COVID-19 outbreak in different countries. The extension of classical SEIR model with delays is another routine to simulate the incubation period and the period before recovery [15] . In [16] , authors used numerical approaches and logistic modelling technique to make complete analysis for COVID-19. In [17] , authors made a fractional order modified SEIR model of pandemic diseases and apply it on COVID-19 to predict virus spreading behavior in Pakistan and Malaysia. In [18] , authors made predictions about Coronavirus transmission dynamics in the African countries. The model parameters (protection rate, infection rate, average incubation time, average quarantined time, cure rate, and mortality rate) are selected using Metropolis-Hasting (MH) parameter optimization method [19] . Further study for modelling COVID-19 daily confirmed cases in Egypt and Iraq by using Gaussian fitting model and logistic model is reached [20] [21] [22] . The virus dynamic behavior is modelled with a new SEIR model. One of the important quantities should be calculated during modelling virus dynamics is the basic reproduction number. This value helps to eliminate the diseases and expect number of secondary infections produced by an infected individual when all individuals are susceptible to infections. In [23] , authors made a multi-strain modified SEIR epidemic model for COVID-19 and wrote a complete analysis on how to control the value of the reproduction number for next upcoming virus peaks. Based on the above-mentioned works, by using Euler method and Euler-Murayama method, we study the predictions of COVID-19 second wave in Egypt through developing different dynamic mathematical models. In particular, the fractional order dynamic model, stochastic dynamic model and the fractional order stochastic dynamic model. Finally, comparisons between actual and predicted daily infections are presented. Secondary data for Egypt first viral wave are carefully collected from references [24, 25] . The second virus wave of Egypt is assumed started on 15 November 2020. This paper is prepared as follows. In Section 2, we review some essential facts from fractional calculus and stochastic analysis. In Section 3, we describe COVID-19 dynamical models. In Section 4, we apply dynamical models on case study of Egypt and compare between actual and predicted daily infections. In the end, conclusions and future work are written.@story_separate@Definition 2.1. Riemann-Liouville fractional integral operator of order for a function can be defined as [26] : Definition 2.2. Caputo derivative of order with the lower limit zero for a function can be written as [27] : , where . A standard one-dimensional Wiener process is a stochastic process indexed by { ( ), > 0 } nonnegative real numbers t with the following properties [28] [29] [30] : (1) with probability 1. ( 0 ) = 0 (2) The function is continuous on t. → ( ) (3) If then and are independent. (4) For Ɐ , all increments , are normally distributed with mean 0 and variance i.e., represents the stochastic Wiener process . One of the simplest numerical approximations for the SDE is the Euler-Maruyama method. If we truncate Ito's formula of the stochastic Taylor series after the first order terms, we obtain the Euler-Maruyama method as follows: with the initial value . ( +1) = ( ) + ( ( )) ∆ + ( ( )) ∆ , i = 0, 1, 2, . . . , N -1 ( 0 ) = 0 Based on the epidemiological feature of COVID-19 and the several strategies imposed by the government, with different degrees, to fight against this pandemic, we extend the classical SEIR model [23] to describe the transmission of COVID-19. In particular, we partitioned the population into five classes, denoted by S, E, I, R and F, where S represents the daily Susceptible individuals; E is the daily Exposed individuals; I the daily Infected individuals, which have not yet been treated. Finally, R and I are the daily Recovered and daily Deaths. All proposed model satisfy the following assumptions: (1) All involved transmission rates, for such models, are positive and varying with time. (2) Natural birth and death rate are not considered. (3) Symptomatic patients are only who transmit the virus. (4) Susceptible individuals can move into infected class without passing by exposed class.  Caputo fractional derivative [31] of order , , is applied on our SEIRF modelling Equations (1-5) . Through varying the value of , different viral wave peaks can be reached to describe virus dynamics with more than one scenario. Taking equals 1 results in generating first order modified SEIRF so it is a special case of the fractional order modified SEIRF model. Through modelling Equations (1-5), Daily Susceptible (S) enter the Exposed class (E) due to contact between exposed individuals and infected individuals. Susceptibles move to Exposed class with daily transmission rate and into infected class with daily transmission rate . Exposed (E) enter the Infected class 1 ( ) 2 ( ) (I) with daily transmission rate . A part of infected individuals moves to Recovered class (R) with daily cure rate ( ) and the remaining part moves to Deaths class (F) with daily death rate . All rates are assumed dynamic rates ɤ( ) ( ) and varying with time. where is the average number of closed contacts between susceptible individuals per infected individuals per day and 1 is the average number of closed contacts between susceptible individuals per exposed individuals per day. The 2 assumed model dynamic rates are defined as: Where is the incubation period and is a controlling parameter decreasing from country to another according to 0 how precautionary measures are followed. and are the initial values of dynamic system rates. 10 , 20 , 0 ,ɤ 0 In this model, COVID-19 dynamic behavior is modelled using equations of the first order modified SERIF model, equals 1, after adding white noise terms satisfying Wiener process properties to all modelling equations. These additions can make modelling equations more realistic in describing COVID-19 viral waves. The new added diffusion terms are as indicated in Equations (11) (12) (13) (14) (15) . All virus dynamic rates are taking the same form as in the first order modified SEIRF model. where are respectively the diffusion coefficients for Susceptible, Exposed, Infected, Recovered and , , , , Deaths individuals. This dynamic modelling approach authorize an additional novelty and dimension of the article. Modelling equations are firstly constructed using the fractional order modified SERIF model of order . After that new white noise terms satisfying Wiener process properties are added. The new added diffusion terms are different in shape than used in the first order stochastic modified SEIRF model to reach more probabilistic scenarios covering wide range of probable viral wave dynamics. Dynamical system equations after adding the new stochastic terms are as indicated in Equations (16) (17) (18) (19) (20) (21) (22) . During creating the white noise terms, balance in equations is assumed which means that if all system equations are mathematically added then both the drift and diffusion terms will vanish as in Equation (21). = ɤ( ) ( ) = ( ) ( ) 1 + 2 = 1 (22) where is the exposed-infected diffusion coefficient and is the infected-recovered diffusion coefficient. is 1 the fractional portion of infected-recovered diffusion stochastic term affecting on recovered individuals and similarly is the remaining fractional portion of infected-recovered diffusion stochastic term affecting on deaths individuals. 2 Fractional order SEIRF models can be solved numerically using many techniques and the used technique here is Euler's Method [16] . In case of stochastic SEIRF models, Euler-Murayama method is used to solve system numerically [28, 30] . After that, All dynamic models can be applied on the Egyptian case study to predict number of Exposed, Infected, Recovered and Deaths. To estimate dynamic models parameters, first wave transmission rates are used for trial and error estimations based on data in references [24, 25, 32] . In all dynamic models, the second virus wave is assumed that is started in Egypt on 15 November 2020 (day 0). All dynamic models are applied on the case study of Egypt to reach different and more realistic scenarios for In case of expected Susceptible class, Figure 1 shows this class variation with days through using different dynamic models. All SEIRF models, except the first order stochastic model, give very near curve shapes. The daily Sucepitable individuals are decreasing with time till forecasting day no. 80 and then they settles with time. The first order stochastic model gives a swinging curve shape of Susceptible class with time. Figure 2 describes daily expected Exposed individuals under using all SEIRF dynamic models. In case of fractional order dynamic models, when the fractional order increases from 0.96 to 1 the curve peak decreases. Similarly the case in the fractional stochastic dynamic model. Stochastic models not only give different curve peaks but also different time of actions. In case of the stochastic Predicted scenarios using SEIRF dynamic models are compared with their actual values from [32] . The comparison made for both actual daily infected individuals and daily deaths with their predicted ones. Comparisons are set every 7 days starting from prediction day no. 7 till prediction day no. 56. In comparisons, it is assumed that virus incubation period is 14 days, so comparisons are made each half incubation period. Prediction day no. 46 is also considered in comparisons as it is the actual second viral wave peak [32] . From indicated results in Tables (3, 4  Othman A.M. Omar: Methodology of dynamic modelling, data Collection, analysis, software, text writing. Reda A. Elbarkouky: Conceptualization, analysis, supervision. Hamdy M. Ahmed: Data collection, conceptualization, figures preparation, supervision. All authors: Reviewing manuscript. Authors declare that they have no conflict of interests.@story_separate@In this paper, we proposed novel general modified dynamic models to evaluate COVID-19 pandemic. The fractional derivative was used to improve the mathematical modelling of COVID-19 under formulating fractional dynamic models to suggest some possible realistic scenarios. These scenarios will be helpful for public health officials to eradicate this contagious disease. Through generated dynamic models, daily Susceptible (S), Exposed (E), Infected Adding white noise to these models results in a novel fractional order stochastic dynamic model. After that, all proposed models are applied on the case study of Egypt to predict different scenarios of the second virus wave started on 15 November 2020. Numerical solution of the proposed models is valid through using Euler's method and Euler-Maruyama method then simulations are made using MATLAB software. From results, both the stochastic modified SEIRF model and the fractional order stochastic modified SEIRF model make daily curves more realistic through adding diffusion terms with different predicted peaks and time of actions. Finally, it has been found out that the reached models are very useful for the Egyptian government to control the COVID-19 outbreak for the upcoming months. Our future work will be focused on fractional stochastic differential equation SEIRF epidemic model with vaccination.","In this paper, COVID-19 dynamics are modelled with three mathematical dynamic models, fractional order modified SEIRF model, stochastic modified SEIRF model, and fractional stochastic modified SEIRF model, to characterize and predict virus behavior. By using Euler method and Euler-Murayama method, the numerical solutions for the considered models are obtained. The considered models are applied to the case study of Egypt to forecast COVID-19 behavior for the second virus wave which is assumed to be started on 15 November 2020. Finally, comparisons between actual and predicted daily infections are presented."
"The quality improvements are significant. However, several questions arising from previous results still need to be answered. Among them are: 1. What is the theoretic reason for such a significant quality improvements? 2. What are the impacts of data aggregation methods on the results? and 3. What are the impacts of probability distributions on the entropy of an interval-valued dataset? In this paper, we investigate these questions from the perspective of information theory [9] . To be able to calculate and compare entropies of interval-valued datasets, it is necessary to establish the concepts and algorithms on probability and entropy for interval-valued datasets. In our work [18] , also published in this volume, we lay down both theoretic and algorithmic foundations for the investigation reported in this work. In which, point-valued statistic, probabilistic, and entropy measures for interval-valued datasets are established in details with practical algorithms. Interested readers should refer that article for a solid theoretical foundation. In the rest of this paper, we briefly review related previous work, such as the stock market annual variability forecasting, the dataset, and information entropy in Sect. 2. We try to answer the question why interval-valued data leading better quality forecasts through comparing information entropy of intervalvalued samples against point-valued ones in Sect. 3 . We calculate and compare the impacts of two aggregation methods (min-max and confidence intervals) associated together with commonly used probability distributions (uniform, normal, and beta) in Sect. 4 . We summarize the main results and possible future work in Sect. 5.@story_separate@We first briefly review the dataset and the stock market annual variability forecasts; and then introduce related concepts and algorithms of calculating entropies of a point-valued dataset and of an interval-valued dataset. The S & P 500 index is broadly used as a indicator for the overall stock market. The main challenge in studying the stock market is its volatility and uncertainty. Modeling the relationship between the stock market and relevant macroeconomic variables, Chen, Roll, and Ross [4] established a broadly accepted model in economics to forecast the overall level of the stock market. According to their model, the changes in the overall stock market value (SP t ) are linearly determined by the following five macroeconomic factors: IP t : the growth rate variations of adjusted Industrial Production Index, DI t : changes in expected inflation, UI t : and changes in unexpected inflation, DF t : default risk premiums, and T M t : unexpected changes in interest rates. This relationship can be expressed as: By using historic data, one may estimate the coefficients of (1) to forecast changes of the overall stock market. The original dataset used in [14] and [17] consists of monthly data from January 1930 to December 2004 in 75 years for the six variables. Here are few sample lines of the data: To make an annual stock market forecast, a commonly used approach is to make a point-valued annual sample first, such as the end of each year, i.e., December data, or annual minimum for predicting the min, or annual maximum for estimating the max. Applying OLS to estimate the coefficients in (1), people are able to make a point-valued prediction. By adding and subtracting a factor (usually denoted as Z) of the standard deviation to the point-valued prediction, one form a confidence interval as an annual variability forecast. However, such confidence interval forecasting methods have never been widely used in the literature because of the poor forecasting quality [2] and [7] in forecasting the stock market. Normally, the forecasting intervals are so narrow that there is only a 50% chance, or even less, that a future point lies inside the interval [5] and [6] . In other cases, the forecasting intervals can be so wide that the forecasts are meaningless. This poor forecasting quality is deeply rooted in the methodology of point-based confidence interval forecasting. Instead of commonly used point-valued approach, an interval-valued method has been proposed and applied for the annual stock market variability forecasts [14] . In which, the annual minimum and maximum form an interval-valued (minmax) sample of the year. By applying an interval least-squares algorithm [13] with the interval-valued sample, significant quality improvements of predictions are obtained. Figure 1 illustrates the interval-valued annual forecasts comparing against the actual variations of S & P 500 from 1940-2004. In which, a ten-year sliding window was used to make an out of sample forecast. Further studies on forecasting the stock market [10] and [11] , variability of mortgage rates [12] , crude oi price prediction [29] , and others, have consistently reported that the quality of variability forecasts with interval-valued samples and interval least-squares are significantly better than that of with point-valued samples and OLS. As the main objective of this work, we want to investigate the major reason for such significant quality improvements through computing and comparing the entropies of point-and interval-valued samples. Our investigations are carried out through calculating and comparing information entropy, i.e., the average rate at which information produced by a stochastic source of data [28] . Shannon defines the entropy for a discrete dataset X = {x 1 , x 2 , . . . , x n } in his seminal paper ""A mathematical theory of communication"" [26] as: where p(x i ) is the probability of event x i . In information theory, Shannon's entropy has been referred as information entropy, and it has been used as a measure of information in data. Viewing the stock market as a stochastic source of data, we try to measure and compare the amount of information contained in datasets. For a point-valued dataset X, we may estimate its entropy practically with the algorithm below:  Applying available software tools, one can easily implement the steps in Algorithm 1 above. For example, calling the histogram method in Python numpy module returns the counts and bins in a histogram of a dataset. The rests are straightforward to implement. However, it is not that straightforward to calculate information entropy of an interval-valued dataset. By the term interval, we mean a connected subset of R. An interval-valued dataset is a collection of intervals. Using a boldfaced lowercase letter to denote an interval, and a boldfaced uppercase letter to specify an interval-valued dataset, we have X = (x 1 , x 2 , . . . , x n ) as an interval-valued dataset consisting of n intervals x 1 , x 2 , . . . x n . Applying (2) to calculate the entropy of X demands a probability distribution of X. Our paper [18] provides the theoretic and algorithmic foundations needed for calculating a point-valued probability of an interval-valued dataset. For readers' convenience, here are two related definitions and a theorem from that paper: is called a probability density function, pdf of an interval-valued dataset X if and only if f (x) satisfies all of the conditions: Using pdf i to denote the probability density function for x i ∈ X, we have the theorem below to obtain a pdf for X practically. is a pdf of X. With (4), we define the entropy for an interval-valued dataset X as Definition 2. Let P be an interval partition of the real axis and pdf (x) be the probability density function of P. Then, the probability of an interval x (j) ∈ P is p j = x (j) pdf (t)dt, and the entropy of P is Example 1. Find a pdf and entropy for the interval-valued sample dataset X 0 = { [1, 5] , [1.5, 3.5] , [2, 3] , [2.5, 7] , [4, 6] }. For simplicity, we assume a uniform distribution for each x i ∈ X 0 , i.e., The pdf of the example in (6) is a stair function. This is because of the uniform distribution assumption on each x i ∈ X 0 . The five intervals in X 0 form a partition of R in eleven intervals including (−∞, 1) and (7, ∞). Using (5), we have the entropy of the interval-valued sample dataset entropy(X 0 ) = 2.019 Example 1 illustrates the availability of a point-valued pdf for an intervalvalued dataset. For more theoretic and algorithmic details, please refer [18] . We are ready now to investigate the question: why does the interval-valued approach significantly improve the quality of variability forecasts? Previous results have evidenced that the interval-valued approach can significantly improve the quality of forecasts in different areas (such as the stock market annual variability, the variability of the mortgage rate [12] , and the variability of crude oil price [15] ). However, using the same economical model and the same original dataset but point-valued samples, the quality of forecasts are much worse. To investigate the possible cause, we should examine the entropies of interval-valued and point-valued input datasets evidently. Applying Algorithm 1 on point-valued annual samples of the six-dimensional financial dataset, we calculate their attribute-wise entropy. The four point-valued annual samples are December only, annual minimum, annual maximum, and annual midpoint 1 . With the Algorithm 3 in [18] , we calculate the attribute-wise entropy of the annual min-max interval-valued sample. Table 1 summarizes the results. In which, the first row lists each of the six attributes in the dataset. The second to the last rows provide values of attribute-wise entropy of five different samples: December only, Annual minimum, Annual maximum, Annual midpoint, and Annual min-max interval, respectively. Figure 2 provides a visualized comparison of these entropy. From which, we can observe the followings: The attribute-wise information entropies vary along with different samples. However, the attribute-wise entropies of the interval-valued sample are clearly much higher than that of any point-valued ones. Comparatively, the entropies of point-valued samples do not differ significantly. This indicates that the amount of information in these point-valued samples measured with entropies are somewhat similar. But, they are significantly less than that of the interval-valued ones. The greater the entropy is, the more information may possibly be extracted from. This is why the interval-valued forecasts can produce significantly better forecasts in [10, 11, 14] , and others. In both theory and practice, meaningless noises and irregularities may increase the entropy of a dataset too. However, it is not the case here in this study. The interval rolling least-squares algorithm [16] has successfully extracted the additional information and made significant quality improvements. The advantages of using interval-valued samples instead of point-valued ones have also been observed in predicting variations of the mortgage rate [12] , the crude oil price [15] , and others. The interval-valued samples indeed contain more meaningful information. Therefore, in making variability forecasts like the stock market, it is preferable of using interval-valued samples rather than point-valued ones. Here is an additional note. The attribute-wise entropies of the annual minmax interval-valued sample in Table 1 and the sum of entropies of the pointvalued annual minimum and maximum are similar. If one uses the point-valued annual minimum and annual maximum separately, can he obtain quality forecasts similar to that of using the min-max interval-valued sample? Unfortunately, an empirical study show that is not the case. In [11] , a comparison of the following two approaches is reported. One of the two is of applying the point-valued annual minimum and maximum samples to predict annual lower and upper bounds of the market with the OLS separately. Then, confidence intervals are constructed as annual variability forecasts. The other applies the ILS with the min-max interval-valued sample. The quality of forecasts produced in the later approach is still much better than that of the former approach. In [10] , using the sample of annual midpoints is studied for the same reason of performance comparison. The ILS with interval-valued annual sample still significantly outperform the point-valued approach in terms of higher average accuracy ratio, lower mean error, and a higher stability in terms of less standard deviation. This suggests that, to extract information from an interval-valued sample, one should use the ILS instead of OLS. Yes, an interval-valued sample may contain more information than a point-valued sample does. But, there are various strategies, such as in [1, 8] and others, to aggregate data other than the min-max method. What are the impacts of different aggregation strategies on the entropy of resulting interval-valued dataset? Furthermore, in calculating the entropy of an interval-valued dataset, Eq. (4) requires the pdf i for each x i ∈ X. What are the impacts of these pdf i s on calculating the entropy of X? We now investigate these two questions computationally again. In studying probability distribution of interval-valued annual stock market forecasts, point-valued data are aggregated with confidence intervals instead of annual min-max intervals [17] . In which, the points within a year are first fit with a normal distribution attribute-wise. Then, confidence intervals are formed at a selected level of probabilistic confidence with an intention of filtering out possible outliers. With different levels of confidence (by adjusting the Z-values), the interval-valued samples vary. So do the variability forecasts. However, we have observed that the variations are not very significant at all when Z is between 1.25 to 2, see [17] . Specifically, the average accuracy ratio associated with the Zvalues are: 61.75% with Z = 1.25, 64.23% with Z = 1.50, 64.55% with Z = 1.75, and 62.94% with Z = 2.00. These accuracy ratios are very similar to 64.19% reported in [14] with the min-max aggregation. In calculating the attribute-wise entropy of the annual min-max intervalvalued sample with Algorithm 3 in [18] earlier, we have assumed a uniform distribution for each interval. In addition to uniform distribution, we consider both normal and beta distributions in this work because of their popularity in applications. In this study, we computationally investigate the impacts of a combination of an aggregation strategy associated with a probabilistic distribution on the entropy of resulting interval-valued data. We report our numerical results on each of the following four combinations: (a) Min-max interval with uniform distribution; (b) Fitting data with a normal distribution then forming confidence interval with Z = 1.5, using normal distribution in entropy calculation; (c) Fitting data with a normal distribution then forming confidence interval with Z = 1.5, then assuming uniform distribution on each interval in entropy calculation; and (d) Min-max interval fitting with a beta distribution. Table 2 lists attribute-wise entropies for each of the four cases above. Figure 3 provides a visual comparison. Python modules numpy and scipy are used as the main software tools in carrying out the computational results. We now analyze each of the outputs from (a)-(d) in Fig. 3 .  The line (a) is exactly the same as the min-max interval line in Fig. 2 . This is because of that we have already assumed uniform distribution in calculating the attribute-wise entropy for each of the min-max intervals. The line (b) indicates that the entropies of the interval-valued sample formed with the method (b) are much less than that of the interval-valued one, i.e., the line (a). This is not by an accident. Equation (4) uses the arithmetic average of i pdf i as the pdf of an interval-valued dataset X. As we know, the sum of normal random variables follows a normal distribution. Therefore, the resulting interval-valued dataset obtained with (b) follows a normal distribution, which is determined only by its mean and standard deviation with much less irregularity. Therefore, the calculated entropy is much less than that of (a). However, one should not abolish confidence interval aggregation at all. The only thing causing the relatively less entropy is the entropy calculation, in which, we assumed normal distribution for each pdf i . This is further explained on the line (c) below. The line (c) shows the results obtained with the same confidence intervals in (b) but then assuming a uniform distribution for each interval in calculating the entropy. The Corollary 2 in [18] makes this practically doable. Notice that the lines (c) and (a) are fairly close to each other comparing against (b) and (d). This means that using a confidence interval to aggregate points can still be a valid practical approach. Computational results in [17] repeated below further verify the claim as an evidence. By adjusting the Z-values of normal distribution, several interval-valued annual samples are formed at different levels of probabilistic confidence. Using them, that work reports some changes in overall quality of the stock market annual forecasts. The average accuracy ratio associated with the Z-values are: 61.75% with Z = 1.25, 64.23% with Z = 1.50, 64.55% with Z = 1.75, and 62.94% with Z = 2.00. They are very close to 64.19% reported in [14] with the min-max intervals. The relatively overall closeness of line (c) and (a) can be an explanation for the similarity of the average accuracy ratios. The closeness of (a) and (c) also implies that adjusting the Z-value in data aggregation may slightly improve the quality of forecasts but not significantly. Lastly, the ILS algorithm [15] does not depend on any specific probability distribution but the calculation of entropy does. Therefore, in calculating entropy of samples formed with confidence intervals, assuming a uniform distribution can be a good choice like in the reported case study of stock market forecasting. Unless, each attribute follows a normal distribution indeed. The line (d) is much lower than the rests. However, we ignore it because of the reasons explained below. In our implementation, we call the beta.fit in scipy.stats module to estimate the parameters of a beta distribution, which fits the data best. During run time, we have encountered multiple run-time warnings although our implementation returns the reported attribute-wise entropy. After checking our code carefully without finding any bugs, we examine the latest available official documentation of scipy updated on December 19, 2019. Regarding beta fit, it states ""The returned answer is not guaranteed to be the globally optimal MLE (Maximum Likelihood Estimate), it may only be locally optimal, or the optimization may fail altogether"" [27] . We do not have any other explanations for the numerical results. Due to the run-time warnings and current software documentation, we accept that the specific computational results on (d) are not reliable as a fact.@story_separate@Applying interval-valued data rather than point-valued ones, researchers have made very significant quality improvements of variability forecasts. This work strongly suggests that the significant quality improvements in previous studies very much likely come from the interval-valued inputs. Figure 2 clearly shows that the attribute-wise entropies of an interval-valued sample are much higher than that of those point-valued samples. The more information contained in the input data, the higher quality outputs could be expected. Furthermore, the interval least-squares algorithm [15] can be applied to successfully extract information from an interval-valued sample rather than using the traditional ordinary least-squares approaches as reported in [11] and others. Computational results also conclude that both min-max and confidence intervals can be effectively used to aggregate point-valued data into intervals. Both of them may lead to similarly well quality variability forecasts with the evidence on the stock market reported in [3] and [17] . This is because of that they may result in interval-valued samples with similar entropies as illustrated in Fig. 3 lines (a) and (c). While the interval least-squares algorithm itself does not demand probability distribution information at all, calculating the entropy of an interval-valued dataset does. The lines (b) and (c) in Fig. 3 suggest that a uniform probability distribution on each interval can be a good choice in calculating the entropy of an interval-valued dataset. In summary, this work provides information theoretic evidences, in addition to empirical results published previously, on the followings: -Using interval-valued samples together with ILS is preferable than using point-valued ones with OLS in variability forecasts like predicting annual variability of the stock market and others. -Applying min-max interval and/or confidence interval (at an appropriate level of confidence) to aggregate points into intervals may result in interval-valued samples containing similar amount of information. -When estimating the entropy of an interval-valued dataset with (5), it can be a good choice of assuming a uniform distribution on each interval. Unless, it follows a normal distribution indeed. The future work may consist of both sides of application and theory. With the information theoretic evidence, we have validated previously published results with interval-valued data and ILS. Therefore, applying interval methods in variability forecasts with uncertainty has a high priority. On the theoretic side, we should indicate that attribute-wise entropy is not exactly the same as the entropy of a multidimensional dataset. Investigating attribute-wise entropy in this study is not only because of its simplicity, but also because [18] only provides pointvalued probability and entropy for single dimensional interval-valued datasets. Therefore, establishing point-valued probability and entropy for a multidimensional interval-valued dataset is among future works too.","Using interval-valued data and computing, researchers have reported significant quality improvements of the stock market annual variability forecasts recently. Through studying the entropy of interval-valued datasets, this work provides both information theoretic and empirical evidences on that the significant quality improvements are very likely come from interval-valued datasets. Therefore, using interval-valued samples rather than point-valued ones is preferable in making variability forecasts. This study also computationally investigates the impacts of data aggregation methods and probability distributions on the entropy of interval-valued datasets. Computational results suggest that both min-max and confidence intervals can work well in aggregating point-valued data into intervals. However, assuming uniform probability distribution should be a good practical choice in calculating the entropy of an interval-valued dataset in some applications at least."
"COVID-19 (Corona Virus Disease 2019) is a highly infectious disease with a long incubation period which was caused by Sars-Cov-2 (Severe Acute Respiratory Syndrome Coronavirus 2) [1] . The number of COVID-19 patients increased dramatically due to hundreds of millions of people traveling during the Spring Festival period. The severity of COVID-19 had been underestimated until the National Health Commission classified it as a B type infectious disease officially and took actions to fight against this disease on 20 January, 2020. Ever since then, epidemic prevention was comprehensively upgraded and marked the real beginning of universal concern, indicating widespread impacts. The uncertainty and low predictability of COVID-19 not only threaten people's physical health, but also affect people's mental health, especially in terms of emotions and cognition, as many theories indicate. According to Behavioral Immune System (BIS) theory [2] , people are likely to develop negative emotions (e.g., aversion, anxiety, etc.) [3, 4] and negative cognitive assessment [5, 6] for self-protection. Faced with potential disease threat, people tend to develop avoidant behaviors (e.g., avoid contact with people who have pneumonia-like symptoms) [7] and obey social norms strictly (e.g., conformity) [8] . According to stress theory [9] and perceived risk theory [10] , public health emergencies trigger more negative emotions and affect cognitive assessment as well. These negative emotions keep people away from potential pathogens when it refers to the disease. However, long-term negative emotions may reduce the immune function of people and destroy the balance of their normal physiological mechanisms [11] . Meanwhile, individuals may overreact to any disease in case of less appropriate guidance from authorities, which may result in excessively avoidant behaviors and blind conformity [8] . Therefore, it is essential to understand the potential psychological changes caused by COVID-19 in a timely manner. Since psychological changes caused by public health emergencies can be reflected directly in emotions and cognition [3] [4] [5] [6] , we can monitor psychological changes in time through emotional (e.g., negative emotions and positive emotions) and cognitive indicators (e.g., social risk judgment and life satisfaction). The emotions and cognition are usually measured by retrospective questionnaires, such as Oxford Happiness Inventory (OHI) [12] , Symptom Checklist 90 (SCL-90) [13] , Satisfaction with Life Scale (SWLS) [14] , and Likert Type Attitude Scale [15, 16] . However, at the time of the COVID-19 outbreak in China, it was very difficult to conduct a traditional paper survey in the affected areas; online surveys rely on the cooperation of participants, and it is difficult to meet the requirements in time, and even brings extra burdens for participants. Since we did not know the time of COVID-19 declaration, it was impossible to measure people's emotions and cognition by a traditional survey in advance. There may be a certain deviation when requiring people to recall their mental state a week or more ago. Weibo data is emerging as a key online medium and data source for researchers to understand this social problem in a non-invasive way. Sina Weibo is a leading Chinese Online Social Networks (OSN) with more than 462 million active daily users in 2019. These users use Weibo functions (e.g., reply, @function) to interact with each other, forming rich user behavior data. The aim of this study is to explore the impacts of public health emergency COVID-19 on people's mental health, to assist policy makers to develop actionable policies, and help clinical practitioners (e.g., social workers, psychiatrists, and psychologists) provide services to affected populations in time.@story_separate@The samples in this study were from the original Weibo data pool [17] . The data pool contained more than 1.16 million active Weibo users. Weibo is a popular platform to share and discuss individual information and life activities, as well as celebrity news in China [18] . The retrieved data included (1) user's profile information, (2) network behaviors, and (3) Weibo messages. Privacy was strictly protected during the procedure, referring to the ethical principles [19] . We have obtained the Ethical Committee's approval and the ethic code is H15009. The following inclusion criteria were employed to select active Weibo users from the data pool. First, they had published at least 50 original Weibo posts around a month in total from 31 December, 2019 to 26 January, 2020. Second, their authentication type is non-institutional (e.g., individual user, etc.). Third, their regional authentication is in China, not ""overseas"" or ""other"". We acquired 17,865 active Weibo users finally, then fetched all their original posts published during 13 January, 2020 to 26 January, 2020 into the two-week period for the analysis. In this study, we used Online Ecological Recognition (OER) [20] , which referred to the automatic recognition of psychological profile (e.g., anxiety, well-being, etc.) by using predictive models [17, 20, 21] based on ecological behavioral data from Weibo. We employed Text Mind system developed by the Computational Cyber Psychology Laboratory at the Institute of Psychology, Chinese Academy of Sciences to extract content features [22] , including Chinese word segmentation tool [17] , and psychoanalytic dictionary [23] . We used the Chinese word segmentation tool to divide users' original microblog content into words/phrases with linguistic annotations, such as verbs, nouns, adverbials, and objects [24] , and then extracted psychologically meaningful categories through the simplified Chinese LIWC (Language Inquiry and Word Count) dictionary [23] . These lexical features were data sources for word frequency analysis. After feature extraction, we used the psychological prediction model [25] obtained from the preliminary training to predict the psychological profile of these active Weibo users. These predictive models are tools developed for online psychology research based on big data and deep learning technologies, including emotional indicators (anxiety, depression, indignation, and Oxford happiness), cognitive indicators (social risk judgment and life satisfaction), and so on. Figure 1 portrays the procedure from feature extraction to psychological indicator prediction. All the prediction models have reached a moderate correlation with questionnaire scores. The feasibility of predictive models has been repeatedly demonstrated [26] [27] [28] . Int. J. Environ. Res. Public Health 2020, 17, x 3 of 9 In this study, we used Online Ecological Recognition (OER) [20] , which referred to the automatic recognition of psychological profile (e.g., anxiety, well-being, etc.) by using predictive models [17, 20, 21] based on ecological behavioral data from Weibo. We employed Text Mind system developed by the Computational Cyber Psychology Laboratory at the Institute of Psychology, Chinese Academy of Sciences to extract content features [22] , including Chinese word segmentation tool [17] , and psychoanalytic dictionary [23] . We used the Chinese word segmentation tool to divide users' original microblog content into words/phrases with linguistic annotations, such as verbs, nouns, adverbials, and objects [24] , and then extracted psychologically meaningful categories through the simplified Chinese LIWC (Language Inquiry and Word Count) dictionary [23] . These lexical features were data sources for word frequency analysis. After feature extraction, we used the psychological prediction model [25] obtained from the preliminary training to predict the psychological profile of these active Weibo users. These predictive models are tools developed for online psychology research based on big data and deep learning technologies, including emotional indicators (anxiety, depression, indignation, and Oxford happiness), cognitive indicators (social risk judgment and life satisfaction), and so on. Figure 1 portrays the procedure from feature extraction to psychological indicator prediction. All the prediction models have reached a moderate correlation with questionnaire scores. The feasibility of predictive models has been repeatedly demonstrated [26] [27] [28] . We calculated word frequency, scores of negative emotional indicators (i.e., anxiety, depression, and indignation), positive emotional indicators (i.e., Oxford happiness), and cognitive indicators (i.e., social risk and life satisfaction) of the collected messages. We then compared the differences of psychological characteristics before and after the declaration of outbreak of COVID-19 on 20 January, 2020 through the paired sample t-test by using SPSS (Statistical Product and Service Solutions) 22, which is published by IBM (International Business Machines Corporation), New York, USA. We calculated word frequency, scores of negative emotional indicators (i.e., anxiety, depression, and indignation), positive emotional indicators (i.e., Oxford happiness), and cognitive indicators (i.e., social risk and life satisfaction) of the collected messages. We then compared the differences of psychological characteristics before and after the declaration of outbreak of COVID-19 on 20 January, 2020 through the paired sample t-test by using SPSS (Statistical Product and Service Solutions) 22, which is published by IBM (International Business Machines Corporation), New York, USA. Among 17,865 active Weibo users, 25.23% were males and 77.95% were from Eastern China, which is considered the richest region in China. Ages of users who registered their birth date in their profile (n = 4156, 23.26%) ranged from 8 to 56 years with the median age of 33 years. The demographic profile is depicted in Table 1 .  In this study, we compare the LIWC categories between the week before (T-before) and after (T-after) 20 January, shown in Table 2 . It contains two types of LIWC categories: words of emotions and words of concerns. Words of emotions include positive emotion (e.g., faith, contentment, and blessing), negative emotion (e.g., worry, suspicion, and jealousy), anxiety (e.g., upset, nervous, and crazy), and anger (e.g., complaint). Words of concerns include health (e.g., insomnia, doctor, and exercise), leisure (e.g., cooking, chatting, and movies), family (e.g., family and house), friend (e.g., companion and guest), money (e.g., bills, cash, and borrowing), death (e.g., burial, killing, and funeral), and religion (e.g., church, mosque, and temple), which can reflect what people are paying attention to.  Results indicate significant differences of emotional indicators between T-before (13-19 January, 2020) and T-after (20-26 January, 2020), as shown in Table 3 T-before represents the predicted emotional indicators during 13-19 January, 2020; T-after represents the predicted emotional indicators during 20-26 January, 2020; M = mean; SD = standard deviation; df = degrees of freedom. ** p < 0.01, *** p < 0.001. We found significant differences in cognitive indicators between T-before (13-19 January, 2020) and T-after (20-26 January, 2020), as shown in Table 4 . After 20 January, cognitive indicators of psychological traits increased in social risk judgement (t (17,747) = 3.120, p < 0.01), but decreased in life satisfaction (t (17,747) = 5.500, p < 0.001).  Since the National Health Commission identified COVID-19 as a B type infectious disease officially, COVID-19 influenced the psychological states of people across China. This study collected active Weibo users' data, and conducted sentiment analysis during 13-26 January, 2020. We used OER to acquire the psychological states, and found that Weibo users' psychological conditions significantly changed under the outbreak of COVID-19. The findings showed that people's concerns by linguistic expression increased after January 20. We observe an increase in health and family, while a decrease in leisure and friend. Uncertainty of the upcoming situation causes cognitive dissonance and insecurity; this produces a feeling of mental discomfort, leading to Weibo's activity oriented toward dissonance reduction and keeping security on health and family relationship [29] . According to the theory of BIS, people behave in a more reticent and conservative way when they feel threatened by disease [30] . Therefore, staying at home with family and reducing recreational activities seems to be a safer way to prevent illness. It also indicated that people begin to care more about their health and were more likely to seek social support from their families rather than getting together with friends, which suggested that people' interests and attention were influenced by the restricted travel policy and self-isolation regulations from the health authorities and central government. Affected by COVID-19, messages related to death and religion became salient after 20 January. Reports showed severity and potential mortality of COVID-19. Research confirmed that people tended to respond to emergencies such as stress or death in the way of religion, which can comfort tense moods and bring more positive emotions [31] . That is why people prayed for the county through religion or other beliefs, leading to the phrase that appeared most frequently on the Internet at that time: God bless China. People showed more negative emotions (anxiety, depression, and indignation) and less positive emotions (Oxford happiness) after the declaration of COVID-19, which was supported by the theory of BIS, i.e., people did generate more negative emotions for self-protection [3, 4] . These results are consistent to previous studies as well, which found that public health emergencies (e.g., SARS) triggered a series of stress emotional response containing a higher level of anxiety and other negative emotions [32, 33] . Meanwhile, the confirmation that COVID-19 could be passed from person to person on 20 January, which was inconsistent with previous reports, lead to quite a number of people being unsatisfied with misinformation published from provincial governments (e.g., Hubei) and ineffective regulatory actions, causing an increase in indignation. However, it's worth noting that the word frequency of positive emotions increased after 20 January, which seemed to be inconsistent with the theory of BIS. In fact, positive emotion includes words such as faith and blessing, which are more inclined to reflect group cohesiveness rather than pure personal emotions (e.g., happiness). Researchers found that group threats (e.g., natural disasters and epidemic diseases) made groups a community of interests, resulting in more beneficial behaviors and social solidarity, which indicated higher group cohesiveness [34] . For example, lots of provinces (e.g., Sichuan Province, Shandong Province, etc.) formed medical teams to help the Hubei province, which was the worst affected area. Many people donated money and supplies to Hubei Red Cross to support the control of COVID-19. Furthermore, social risk judgement was higher and life satisfaction was lower after the declaration of COVID-19. It is consistent with the theory of BIS, which found that when social uncertainty increased, such as unknown etiology and ambiguous route of transmission, people developed the negative cognitive assessment (e.g., higher sensitivity of risk judgment or risk perception) so that they could discover potential infection sources in time and avoid infection [2, 35] . Not only that, people's fear of potential risk and lack of controllability caused by COVID-19 brought about higher risk judgement as perceived risk theory claimed [10] . Moreover, some preventive policies and regulations in terms of travel restriction and self-isolation made the quality of life worse, reflecting in lower life satisfaction. The following briefly foregrounds some of the study's implications for policy makers and clinical practitioners (e.g., social workers, psychiatrists, and psychologists) plan and fight against COVID-19. For policy makers: (1) develop a consistent policy and procedure for reporting the latest confirmed cases, recent death toll, and other data about the epidemic situation. For example, the surge of cases on February 12th did not mean that the situation has been out of control, but because of the new diagnostic criteria introduced. It is important to let people understand the data properly to reduce excessive stress responses (e.g., anxiety, depression, etc.) brought on by inappropriate perception. (2) Expand public awareness of continuous progress in decision-making measures. Since indignation may come mainly from mistakes and deficiencies in preventing and controlling the epidemic, it can effectively decrease indignation if public awareness and involvement are provided. (3) Ensure the supply of medical treatment service. It is critical to set up medical service to treat the disease, and let people know how to access it conveniently. People can get help in time if they are infected. It can improve people's sense of control over risks, thereby avoiding excessive social risk perception. (4) Provide more in-door entertainment services to address good quality of life. People may be more willing to cooperate when their living and entertainment requirements are met, such as online shopping, entertainments, etc. For clinical practitioners: (1) adjust consultant configuration rationally and cooperate with each other. Psychological consultants should grasp the epidemic information correctly and conduct science popularization during counseling. Social workers can help solve practical problems in life. These actions can improve the sense of stability and relieve anxiety and depression. (2) Deliver necessary psychosocial therapy in various ways. Considering the particularity of self-isolation, relevant hotline counseling and online consulting should be applied in practice. Several other points should be considered when generalizing this study's findings. First, as Weibo users are mainly young people, the results may be biased to some extent. In addition, the current analysis is based on a weekly basis, with a relatively large granularity, which has certain influences on reflecting the changing trend of social mentality in a timely manner. In further studies, we will try to expand the range of sex and age and predict psychological traits in a finer granularity. Previous studies indicated that people tended to exaggerate attitudes and prejudices, especially when they felt more vulnerable to disease transmission [36] . It inspires us to try to build a prediction model which can predict people's attitudes and beliefs against the virus through online Weibo data for further understanding of psychological impacts of public health emergencies. The authors declare no conflicts of interest.@story_separate@In this study, we compared the difference before and after 20 January on both linguistic categories and psychological profile. We found an increase in negative emotions (anxiety, depression, and indignation) and sensitivity to social risks, as well as a decrease in positive emotions (Oxford happiness) and life satisfaction after declaration of COVID-19 in China. What's more, people show more concern for health and family, and less concern for leisure and friends. Using social media data may provide timely understanding of the impact of public health emergencies on the public's mental health during the epidemic period. Author Contributions: S.L., N.Z., and T.Z. conceived and planned this article. S.L. and Y.W. carried out the search and revision of the literature. T.Z. collected and provided the data. S.L. and Y.W. analyzed the data. S.L. drafted the study. J.X., N.Z., and T.Z. reviewed and edited the writing. All authors (S.L., Y.W., J.X., N.Z., and T.Z.) revised the article critically for important intellectual content. All authors (S.L., Y.W., J.X., N.Z., and T.Z.) commented on and approved the final manuscript and are accountable for all aspects of the work. All authors have read and agreed to the published version of the manuscript. Funding: This research was funded by National Natural Science Foundation of China, grant number 31700984.","COVID-19 (Corona Virus Disease 2019) has significantly resulted in a large number of psychological consequences. The aim of this study is to explore the impacts of COVID-19 on people’s mental health, to assist policy makers to develop actionable policies, and help clinical practitioners (e.g., social workers, psychiatrists, and psychologists) provide timely services to affected populations. We sample and analyze the Weibo posts from 17,865 active Weibo users using the approach of Online Ecological Recognition (OER) based on several machine-learning predictive models. We calculated word frequency, scores of emotional indicators (e.g., anxiety, depression, indignation, and Oxford happiness) and cognitive indicators (e.g., social risk judgment and life satisfaction) from the collected data. The sentiment analysis and the paired sample t-test were performed to examine the differences in the same group before and after the declaration of COVID-19 on 20 January, 2020. The results showed that negative emotions (e.g., anxiety, depression and indignation) and sensitivity to social risks increased, while the scores of positive emotions (e.g., Oxford happiness) and life satisfaction decreased. People were concerned more about their health and family, while less about leisure and friends. The results contribute to the knowledge gaps of short-term individual changes in psychological conditions after the outbreak. It may provide references for policy makers to plan and fight against COVID-19 effectively by improving stability of popular feelings and urgently prepare clinical practitioners to deliver corresponding therapy foundations for the risk groups and affected people."
"A major challenge in materials science research today is that the artifactual embodiment is primarily textual, even if it is in digital form. Researchers analyze materials through experiments and record their findings in textual documents such as academic literature and patents. The most common way to extract knowledge from these artifacts is to read all the relevant documents, and manually extract knowledge. However, reading is time-consuming, and it is generally unfeasible to read and mentally synthesize all the relevant knowledge [26, 28] . Hence, effectively extracting knowledge and data becomes a problem. One way to address this challenge is through knowledge extraction using domain-specific ontologies [18] . Unfortunately, materials science work in this area is currently hindered by limited access to and use of relevant ontologies. This situation underscores the need to improve the state of ontology access and use for materials science research, which is the key goal of the work presented here. This paper introduces Helping Interdisciplinary Vocabulary Engineering for Materials Science (HIVE-4-MAT), an automatic linked data ontology application. The contextual background covers materials science, shared ontology infrastructures, and knowledge extraction applications. HIVE-4-MAT's basic features are reviewed, followed by a brief discussion and conclusion identifying next steps.@story_separate@Materials science is an interdisciplinary field that draws upon chemistry, physics, engineering and interconnected disciplines. The broad aim is to advance the application of materials for scientific and technical endeavors. Accordingly, materials science researchers seek to discover new materials or alter existing ones; with the overall aim of offering more robust, less costly, and/or less environmentally harmful materials. Materials science researchers primarily target solid matter, which retains its shape and character compared to liquid or gas. There are four key classes of solid materials: metals, polymers, ceramics, and composites. Researchers essentially process (mix, melt, etc.) elements in a controlled way, and measure performance by examining a set of properties. Table 1 provides two high-level examples of materials classes, types, processes, and properties. The terms in Table 1 have multiple levels (sub-types or classes) and variants. For example, there is stainless steel and surgical steel. Moreover, the universe of properties, which is large, extends even further when considering nano and kinetic materials. This table illustrates the language, hence the ontological underpinnings, of materials science, which is invaluable for knowledge extraction. Unfortunately, the availability of computationally ready ontologies applicable to materials science is severely limited, particularly compared to biomedicine and biology. Ontologies have provided a philosophical foundation and motivation for scientific inquiry since ancient times [15] . Today, computationally ready ontologies conforming to linked data standards [9] offer a new potential for data driven discovery [14] . Here, the biomedical and biology communities have taken the lead in developing a shared infrastructure, through developments such as the National Center for Biological Ontologies (NCBO) Bioportal [4, 29] and the OBO foundry [6, 25] . Another effort is the FAIRsharing portal [1, 23] , providing access to a myraid of standards, databases, and other resources [31] . Shared ontology infrastructures help standardize language and support data interoperability across communities. Additionally, the ontological resources can aid knowledge extraction and discovery. Among one of the best known applications in this area is Aronson's [8] MetaMap, introduced in 2001. This application extracts key information from textual documents, and maps the indexing to the metathesaurus ontology. The MetaMap application is widely-used for extraction of biomedical information. The HIVE application [16] , developed by the Metadata Research Center, Drexel University, also supports knowledge extraction in a same way, although results are limited by the depth of the ontologies applied. For example, biomedicine ontologies, which often have a rich and deep network of terms, will produce better results compared to more simplistic ontologies targeting materials science [32, 33] . Overall, existing ontology infrastructure and knowledge extraction approaches are applicable to materials science. In fact, biology and biomedical ontologies are useful for materials science research, and researchers have been inspired by these developments to develop materials science ontologies [7, 11, 17] . Related are nascent efforts developing shared metadata and ontology infrastructures for materials science. Examples include the NIST Materials Registry [5] and the Industrial Ontology Foundry [2]. These developments and the potential to leverage ontologies for materials science knowledge extraction motivate our work to advance HIVE-4-MAT. They have also had a direct impact on exploring the use of NER to assist in the development richer ontologies for materials science [33] . The expanse and depth of materials science ontologies is drastically limited, pointing to a need for richer ontologies; however, ontology development via manual processes is a costly undertaking. One way to address this challenge is to through relation extraction and using computational approaches to develop ontologies. To this end, named entity recognition (NER) can serve as an invaluable first step, as explained here. The goal of Named Entity Recognition (NER) is to recognize key information that are related to predefined semantic types from input textual documents [20] . As an important component of information extraction (IE), it is widely applied in tasks such as information retrieval, text summarization, question answering and knowledge extraction. The semantic types can vary depending on specific task types. For example, when extracting general information, the predefined semantic types can be location, person, or organization. NER approaches have been also proven effective to biomedical information extraction; an example from SemEval2013 task 9 [24] about NER for drug-drug interaction is shown in Fig. 1 below. As shown in the Fig. 1 , the NER pharmaceutical model receives the textual input (e.g. sentences), and returns whether there are important information entities that belong to any predefined labels, such as brand name and drug name. A similar undertaking has been pursued by Weston et al. [28] , with their NER model designed for inorganic materials information extraction. Their model includes seven entity labels and testing has resulted in an overall f1-score of 0.87 [28] . This work has inspired the HIVE team to use NER, as a step toward relation extraction, and the development of richer ontologies for materials science. Goals of this paper are to: 1. Introduce HIVE 2. Demonstrate HIVE's three key features Vocabulary browsing, term search and selection, and knowledge extraction/indexing 3. Provide an example of our NER work, as a foundation for relation extraction. Hive is a linked data automatic metadata generator tool developed initially as a demonstration for the Dryad repository [16, 30] , and incorporated into the DataNet Federation Consortium's iRODS system [12] . Ontologies encoded in the Simple Knowledge Organization System (SKOS) format are shared through a HIVE-server. Currently, HIVE 2.0 uses Rapid Automatic Keyword Extraction (RAKE), an unsupervised algorithm that processes and parses text into a set of candidate keywords based on co-occurrence [22] . Once the list of candidate keywords is selected from the SKOS encoded ontologies, the HIVE system matches candidate keywords to terms in the selected ontologies. Figure 2 provides an overview of the HIVE model. HIVE-4-MAT builds on the HIVE foundation, and available ontologies have been selected for either broad or targeted applicability to materials science. The prototype includes the following ten ontologies: 1)Bio-Assay Ontology (BioAssay), 2) Chemical Information Ontology (CHEMINF), 3) Chemical Process Ontology (prochemical), (4) Library of Congress Subject Headings (LCSH), 5) Metals Ontology, 6) National Cancer Institute Thesaurus (NCIT), 7) Physico-Chemical Institute and Properties (FIX), 8) Physico-chemical process (REX), 9) Smart Appliances REFerence Ontology (SAREF), and 10) US Geological Survey (USGS). Currently, HIVE-4-MAT has three main features: • Vocabulary browsing (Fig. 3 and Fig. 4) • Term search and selection (Fig. 5) • Knowledge Extraction/Indexing (Fig. 6) The vocabulary browsing feature allows a user to view and explore the ontologies registered in HIVE-4-MAT. Figure 3 presents the full list of currently available ontologies, and Fig. 4 provides an example navigating through the hierarchy of the Metals ontology. The left-hand column (Fig. 4) displays the hierarchical levels of this ontology; the definition, and the right-hand side displays the alternative name, broader concepts and narrow concepts.  The term search and selection feature in Fig. 5 allows a user to select a set of ontologies and enter a search term. In this example, eight of the 10 ontologies are selected, and the term thermoelectric is entered as a search concept. Thermoelectrics is an area of research that focuses on materials conductivity of temperature (heat or cooling) for energy production. In this example, the term was only found in the LCSH, which is a general domain ontology. The lower-half of Fig. 5 shows the term relationships. There are other tabs accessible to see the JSON-LD, SKOS-RDF/XML and other encoding. This feature also allows a user to select an encoded term for a structure database system, such as a catalog, or for inclusion in a knowledge graph. Figure 6 illustrates the Knowledge Extraction/Indexing Feature. To reiterate, reading research literature is time-consuming. Moreover, it is impossible for a researcher to fully examine and synthesize all of the knowledge from existing work. HIVE-4-MAT's indexing functionality allows a researcher or a digital content curator to upload a batch of textual resources, or simply input a uniform resource locator (URL) for a web resource, and automatically index the textual content using the selected ontologies. Figure 6 provides an example using the . The visualization of the HIVE-4-MAT's results helps a user to gain an understanding of the knowledge contained within the resource, and they can further navigate the hypertext to confirm the meaning of a term within the larger ontological structure.  Inspired by the work of Weston et al. [28] , the HIVE team is also exploring the performance and applications of NER as part of knowledge extraction in materials science. Research in this area may also serve to enhance HIVE. Weston et al. [28] focus on inorganic materials, and appear to be one of the only advanced initiative's in this area. Our current effort focuses on building a test dataset for organic materials discovery, with the larger aim of expanding research across materials science. To build our corpus, we used Scopus API [27] to collect a sample of abstracts from a set of journals published by Elsevier that cover organic materials. The research team has identified and defined a set of seven key entities to assist with the next step of training our model. These entities have the following semantic labels: (1) Molecules/fragments, (2) Polymers/organic materials, (3) Descriptors, (4) Property, (5) Application, (6) Reaction and (7) Characterization method. Members of our larger research team are actively annotating the abstracts using these semantic labels as shown in Fig. 7 . The development a test dataset is an important research step, and will help our team move forward testing our NER model and advancing knowledge extraction options for materials science in our future work.  The demonstration of HIVE and reporting of initial work with NER is motivated by the significant challenge materials science researchers face gleaning knowledge from textual artifacts. Although this challenge pervades all areas of scientific research, disciplines such as biology, biomedicine, astronomy, and other earth sciences have a much longer history of open data and ontology development, which drives knowledge discovery. Materials science has been slow to embrace these developments, most likely due to the disciplines connection with competitive industries. Regardless of the reasons impacting timing, there is clearly increased interest and acceptance of a more open ethos across materials science, as demonstrated by initiatives outlined by Himanen et al. in 2019 [19] . Two key examples include NOMADCoE [13] and the Materials Data Facility [10] , which are inspired by the FAIR principles [13, 31] . These developments provide access to structured data, although, still the majority of materials science knowledge remains hidden in textually dense artifacts. More importantly, these efforts recognize the value of access to robust and disciplinary relevant ontologies. HIVE-4-MAT complements these developments and enables materials science researchers not only to gather, register, and browse ontologies; but, also the ability to automatically apply both general and targeted ontologies for knowledge extraction. Finally, the HIVE-4-MAT output provides researchers with a structured display of knowledge that was previously hidden within unstructured text.@story_separate@This paper introduced the HIVE-4-MAT application, demonstrated HIVE's three key features, and reported on innovative work underway exploring NER. The progress has been encouraging, and plans are underway to further assess the strengths and limitation of existing ontologies for materials science. Research here will help our team target areas where richer ontological structures are needed. Another goal is to test additional algorithms with the HIVE-4-MAT application, as reported by White, et al. [30] . Finally, as the team moves forward, it is critical to recognize that ontologies, alone, are not sufficient for extracting knowledge, and it is important to consider other approaches for knowledge extraction, such as Named Entity Recognition (NER) and Relation Extraction (RE) can complement and enrich current apporaches. As reported above, the HIVE team is also pursuing research in this area as reported by Zhao [33] , which we plan to integrate with the overall HIVE-4-MAT.","This paper introduces Helping Interdisciplinary Vocabulary Engineering for Materials Science (HIVE-4-MAT), an automatic linked data ontology application. The paper provides contextual background for materials science, shared ontology infrastructures, and knowledge extraction applications. HIVE-4-MAT’s three key features are reviewed: 1) Vocabulary browsing, 2) Term search and selection, and 3) Knowledge Extraction/Indexing, as well as the basics of named entity recognition (NER). The discussion elaborates on the importance of ontology infrastructures and steps taken to enhance knowledge extraction. The conclusion highlights next steps surveying the ontology landscape, including NER work as a step toward relation extraction (RE), and support for better ontologies."
"The pandemic of the novel coronavirus, the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection (also known as coronavirus disease 2019, COVID-19) has infected more than 59 million individuals worldwide and caused more than 1.4 million deaths. Although the main manifestations of COVID-19 are related to respiratory symptoms, the compromise of multiple organs has been described in the literature, including the digestive system [1] . There are several gastrointestinal manifestations, including anorexia, diarrhea, nausea/vomiting, and abdominal pain. Those symptoms have a pooled prevalence of 17.6% and are frequently observed in hospitalized patients [1, 2] . The liver is also one of the most common organs affected in COVID-19. In fact, between 2%-11% of affected patients have liver comorbidities and 16%-53% of cases reported abnormal liver tests [3, 4] . The most frequent abnormalities are mildly elevated alanine aminotransferase (ALT) and aspartate aminotransferase (AST) and are more common in hospitalized patients [4, 5] . Elevation of alkaline phosphatase and gammaglutamyl transferase (GGT) has also been reported [6] . Those alterations have been related to expression of angiotensin-converting enzyme 2 (ACE-2), the putative receptor of SARS-CoV-2, in endothelial cells of the liver and the biliary epithelium [7] . In addition, since the frequency of liver dysfunction increases as COVID-19 is more severe, liver damage might be directly caused by the infection of liver. Abnormal liver tests can also be partially explained by drug-induced liver injury (DILI), cytokine storm, and/or pneumonia-associated hypoxia [4] . Pathological studies in patients with SARS-CoV-2 infection have confirmed the presence of the virus in liver tissue [8] . This finding was also described in SARS and MERS infection [3] . Although histopathological information is scarce, previous reports from SARS and MERS showed steatosis, mild portal tract and lobular lymphocytic inflammation, as well as mild cellular hydropic degeneration in hepatic parenchyma [7, [9] [10] [11] . Regarding SARS-CoV-2 infection, initial reports of autopsies performed in COVID-19 patients have described steatosis, mild lobular and portal activity, lymphocytic endotheliitis, and necrosis [12] [13] [14] . However, data about the main histopathological findings in COVID-19 is still scarce. Thus, aiming to provide a comprehensive synthesis of the pathological findings in liver injury due to SARS-CoV-2 reported so far, we performed a systematic review and meta-analysis of each histopathological finding in liver samples from autopsies and biopsies of COVID-19 patients.@story_separate@This systematic review with meta-analysis was registered on PROSPERO (ID: CRD42020192813) and followed a prespecified analysis plan. This study is reported in accordance with the Preferred Reporting Items for a Review and Meta-analysis (PRISMA) guidelines [15] . Eligible trials had to include patients diagnosed with COVID-19, regardless of age and gender. The diagnosis of COVID-19 had to be based on a compatible clinical history and molecular evidence with a quantitative real-time polymerase chain reaction (qRT-PCR) for SARS-CoV-2. We included liver histopathological reports from deceased COVID-19 patients who subsequently were studied with autopsy or liver biopsies performed in alive COVID-19 patients. We planned to include all the studies that report liver histopathological data, regardless of the design (case-reports, case-series, descriptive cases, cross-sectional studies, cohort studies, and randomized controlled trials). We excluded studies performed in vitro, animal models, or lacking evidence of SARS-CoV-2 infection from this systematic review. We did not include manuscripts performed before December 1, 2019. We performed an electronic search from December 1, 2019, to June 3, 2020, in MEDLINE (via PubMed) and Embase databases. We used keywords and free-text words related to SARS-CoV-2 infection, autopsies, and liver biopsies. We reported the search strategy used in PubMed and Embase databases in the appendix. We hand searched (up to June 3, 2020) preprint servers (bioRxiv, medRxiv, and SSRN) and coronavirus resource centers of The Lancet, JAMA, and New England Journal of Medicine. We did not limit our search by language. Two investigators (Díaz LA and Idalsoaga F) independently screened the titles and abstracts to ascertain whether each study met the eligibility criteria. The full texts of the identified eligible articles were then evaluated to determine whether they should be included in the analysis. Disagreements between the two reviewers were resolved by consensus. In case of persistent disagreement, arbitration by a third reviewer (Arab JP) settled the discrepancy. Two authors (Díaz LA and Cannistra M) independently extracted data from included studies using forms specially designed for this purpose. The following data were extracted describing the study, participants, source of sample (liver biopsy or autopsy), and the main liver histopathological findings. Discrepancies were resolved by a third reviewer (Arab JP). Two investigators (Díaz LA and Idalsoaga F) independently assessed the risk of bias of each included trial with the Appraisal tool for Cross-Sectional Studies (AXIS) checklist for cross-sectional studies, the Institute of Health Economics (IHE) checklist tool for Case Series, the Newcastle-Ottawa Scale (NOS) for case-control studies and cohort studies, and the Cochrane Risk of Bias Tool for randomized trials and quasi-experimental studies. The outcomes were defined by consensus with an expert pathologist (Graham R). The main outcomes were the frequency of vascular thrombosis (presence or absence), venous outflow obstruction (presence or absence), frequency of portal and lobular Inflammation and severity (mild, moderate, or severe), Kupffer cell hyperplasia (presence or absence), steatosis (presence and fat percentage), fibrosis (based on METAVIR score, scores range from F0 to F4, with higher numbers reflecting greater fibrosis), and ductopenia (presence or absence). Secondary outcomes were the frequency phlebosclerosis of the portal vein, herniated portal vein, congestion of hepatic sinuses, and necrosis. Data were synthesized per each histopathological finding of SARS-CoV-2 infection. We estimated the prevalence of event rates in the form of a proportion (with a confidence interval of 95%). Proportions were pooled using random-effects models. Only those studies with a sample size of at least ten patients were included in the meta-analysis. We used Q statistic and I 2 to quantify heterogeneity. We planned a subgroup analysis according to the liver sample source (biopsy in alive patients or autopsy), ethnicity, and gender. We planned sensitivity analysis excluding the high risk of bias studies. A small study effect was evaluated with a funnel plot. As such, subgroup and sensitivity analyses should be considered exploratory. All statistical analyses were performed using MedCalc Statistical Software version 19.3.1 (MedCalc Software Ltd, Ostend, Belgium; https://www.medcalc.org; 2020). The quality of evidence for the outcomes was graded with the GRADE framework. The funding source only provided support for the financing of paid manuscripts during the review process. The researchers did not receive payment or other incentives. We identified 3060 records and 905 were duplicates. After a screening process against title and abstract, we obtained 58 full-text articles that were assessed for eligibility. We finally selected 18 studies for our systematic review from 7 countries (Austria, Belgium, China, Germany, Italy, Switzerland, and the United States). The selection process is described in Figure 1 . All studies were observational in design; no randomized trials were identified. Among the 18 studies, 4 were case reports and 14 case series. The study with the largest sample size included a total of 48 cases [16] . Fifteen studies reported the proportion of each finding and the other three studies presented the pooled data without the prevalence of each finding. All the studies included autopsy data (i.e., no liver biopsies from living patients were identified). The risk of bias assessment was considered high. The pooled studies included a total of 167 patients. All the patients were over 15 years old and 67.2% were male. The grade of evidence was considered very low since all the information emerged from case reports and case series. For the meta-analysis of each finding, we included only five studies with at least ten patients. There was a maximum of 116 cases included in each meta-analysis. We did not perform subgroup analyses due to the low number of studies included. Table 1 [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] summarizes the main baseline characteristics of each study. The main findings described in the selected studies were hepatic steatosis, congestion of hepatic sinuses, vascular thrombosis, fibrosis, Kupffer cell proliferation or hyperplasia, portal inflammation, lobular inflammation, venous outflow obstruction, phlebosclerosis of the portal vein, herniated portal vein, periportal abnormal vessels, hemophagocytosis, and necrosis. [20] April 2020 Not reported Hypertension (9/11), diabetes mellitus (5/11), coronary artery disease (2/11), previous malignant disease (2/11), COPD (2/11), cerebrovascular disease (4/11), and dementia (4/11) Martines et al [22] May 2020 United States [25] May 2020 Germany Autopsy 10 79 (64-90) 7 Males and 3 females Not reported Fatty liver disease (1/10), hypertension (7/10), atrial fibrillation (4/10), chronic kidney failure (3/10), COPD (2/10), heart failure (2/10), obesity (2/10), diabetes (1/10), among others We did not find ductopenia in the selected studies. Figure  A total of 78 from 139 (65.1%) patients demonstrated hepatic steatosis. Lipid droplet size was described in only 7 cases (9%): 4 macrovesicular, 1 microvesicular, and 2 mixed (macrovesicular and microvesicular). The studies of Lax et al [27] and Prilutskiy et al [28] also described the presence of hepatic steatosis without the proportion of this finding. When meta-analyzed, data showed a pooled prevalence of hepatic steatosis of 55.1% [5 studies, 116 patients; 95% confidence interval (CI): 46.2-63.8], without significant heterogeneity among the studies (P = 0.411; I 2 = 0%) ( Figure 3A ). A total of 28 from 163 cases (17.2%) reported congestion of hepatic sinuses. Additionally, the study of Prilutskiy et al [28] described the presence of mild centrilobular congestion without the frequency of this finding. In the meta-analysis, the pooled prevalence of congestion of hepatic sinuses was 34.7% (5 studies, 79 patients; 95%CI: 7.9-68.4) ( Figure 3B ). The heterogeneity was significant among studies (P < 0.001; I 2 = 90.2%). Regarding necrosis, 14 of 91 patients (15.4%) presented this finding. Necrosis was also described without a proportion in the study of Yao et al [30] . We could not perform a meta-analysis of this finding due to the low number of patients and studies that adequately described the presence or absence of necrosis. A total of 63 from 139 (45.3%) cases presented vascular thrombosis. The type of vessel thrombosed was not specified in the studies. When meta-analyzed, data showed a pooled prevalence of vascular thrombosis of 29.4% (5 studies, 116 patients; 95%CI: 0.4-87.2). The heterogeneity was significant among studies (P < 0.001; I 2 = 97.7%) ( Figure 3C ). Other vascular alterations were identified in the studies. The presence of venous Steatosis was the most frequent finding (55.1%). Studies also reported congestion of hepatic sinuses (34.6%), vascular thrombosis (29.4%), fibrosis (20.5%), Kupffer cell hyperplasia (13.5%), portal inflammation (13.2%), and lobular inflammation (11.6%), Other findings observed were venous outflow obstruction, phlebosclerosis of the portal vein, herniated portal vein, periportal abnormal vessels, hemophagocytosis, and necrosis. outflow obstruction was described exclusively by Bryce et al [29] , with a prevalence of 36.4% (8 of 22 cases), and 5 of these cases (62.5%) were acute.  A total of 51 from 139 (36.7%) patients had fibrosis. The grade of fibrosis was only described in 7 cases: 1 case was graded as F3 (METAVIR scale) and 6 cases were graded as F4. The study by Lax et al [27] and Prilutskiy et al [28] also described the presence of fibrosis without the proportion of this finding. In the meta-analysis, the pooled prevalence of fibrosis was 20.5% (5 studies, 116 patients; 95%CI: 0.6-57.9), with December significant heterogeneity among the studies (P < 0.001; I 2 = 94.5%) ( Figure 3D ). Only 12 of 115 cases (10.4%) reported Kupffer cell proliferation or hyperplasia. The studies by Sonzogni et al [16] and Prilutskiy et al [28] also described the presence of this phenomenon, but without a prevalence. Additionally, the study of Bryce et al [30] described the presence of hemophagocytosis. In the meta-analysis, the pooled prevalence of Kupffer cell hyperplasia was 13.5% (5 studies, 79 patients; 95%CI: 0.6-54.3%). The heterogeneity was significant among studies (P < 0.001; I 2 = 94.3%) ( Figure 4A ). December  Histologic evaluation in a total of 41 out of 139 (29.5%) patients showed portal inflammation. The grade of inflammation was reported in 37 of 41 cases (90.2%); all were graded as mild, and the cells were lymphocytes and plasma cells. When metaanalyzed, data showed a pooled prevalence of portal inflammation of 13.2% (5 studies, 116 patients; 95%CI: 0.1-48.8), with significant heterogeneity noted among studies (P < 0.001; I 2 = 94.7%) ( Figure 4B ). In the case of lobular inflammation, a total of 31 from 139 (22.3%) showed this finding. The grade of inflammation was reported in 28 of 31 cases (90.3%): 26 cases were mild (92.9%) and 2 were moderate (7.1%). The predominant cells were neutrophils and lymphocytes. In a meta-analysis, the pooled prevalence of lobular inflammation was 11.6% (5 studies, 116 patients; 95%CI: 0.3-35.7), with significant heterogeneity noted among studies (P < 0.001; I 2 = 89.9%) ( Figure 4C ). As previously reported for patients with SARS and MERS, the liver is frequently affected during a SARS-Cov-2-related disease (COVID-19) [3, 5] . In this systematic review, we identified 18 studies from 7 countries (case reports and case series) that December 28, 2020 Volume 26 Issue 48 include data from autopsies of deceased patients with COVID-19. We identified multiple histopathological findings, including hepatic steatosis (55.1%), venous outflow obstruction (36.4%), congestion of hepatic sinuses (34.7%), vascular thrombosis (29.4%), fibrosis (20.5%), necrosis (15.4%), Kupffer cell hyperplasia (13.5%), portal inflammation (13.2%), and lobular inflammation (11.6%). Several of the hepatic histopathological findings observed in this study have also been described in SARS and MERS patients. It may be related to the ongoing systemic inflammatory process and sepsis, affecting the liver rather than a direct manifestation of SARS-CoV-2 [7, [9] [10] [11] . There is still debate about whether SARS-CoV-2 directly causes liver injury or if the observed abnormalities in liver chemistries seen in COVID 19 are an indirect consequence of the disease reflecting severity, inflammation, and potentially confounding muscle injury that occurs in 19% of patients [31] . A recent study by Bloom et al [32] showed that AST-dominant aminotransferase elevation is common in COVID-19, is not associated with muscle damage markers and correlates with disease severity, probably reflecting true hepatic injury. In a series including 148 patients from a single-center in China, 55 (37.2%) had abnormal liver chemistries on admission. Patients with elevated admission liver chemistries were also more likely to have a high-grade fever and higher C-reactive protein [33] . In two large Chinese cohorts, 6.2% and 11.6% of patients with COVID-19 developed liver chemistries over 3 times the upper limit of normal, suggesting that a minority of patients experience significant biochemistry elevations [34, 35] . The most frequent histopathological finding was steatosis. This prevalence is higher than the general population [36] . This can be partially explained by the baseline characteristics of the population. Admitted COVID-19 patients suffer from more severe disease and more frequently exhibit chronic diseases (i.e., diabetes mellitus, age, hypertension, obesity, and cardiovascular disease), which are also associated with hepatic steatosis [37] . However, existing data suggest that SARS-COV-2 may affect lipid metabolism [38] . In general, all viruses alter lipid synthesis and signaling in host cells as they highjack and utilize the cellular machinery to produce lipids for their envelope. Also, it has been shown that COVID-19 patients have elevated serum levels of fatty acids and infection with other SARS viruses determine long-lasting alterations in lipid metabolism [39] . It has been shown that metabolic-dysfunction associated fatty liver (MAFLD) [40] condition is independently associated with a higher risk of severe COVID-19 (odds ratio 2.67) [41] . Also, advanced MAFLD (i.e., fibrotic disease) is associated with a more severe disease irrespective of metabolic comorbidities. Since the liver hosts a significant mass of innate immune cells, hepatic release of proinflammatory cytokines may contribute to COVID-19 severity [42] . Also, some authors suggest NAFLD progression could be accelerated or exacerbated by COVID-19 [42] . We observed an increased frequency of hepatic vascular alterations in patients with COVID-19. This may be due to endothelial dysfunction (endotheliitis), a pro-coagulant state, and direct vascular injury of the disease [13, 43, 44] . Congestion and necrosis are features of venous outflow obstruction and may also be explained by circulatory dysfunction, heart failure, and ischemia, which may complicate multiorgan failure. Liver fibrosis was also frequently found in the published series of COVID-19 patients. Whether this is related to sustained liver injury or by prior history of chronic liver disease is unclear, though the latter is favored given the acute nature of COVID-19 disease. Of note, patients with cirrhosis have been shown to have a higher risk of mortality due to COVID-19 (relative risk of 4.6) [45] . It is important to notice that no specific histologic indicator of direct infection (i.e., viral cytopathic effect) in the liver tissue. Notably, there was a report from a pediatric living donor liver allograft recipient who, on a post-operative day 4, developed respiratory distress, fever, and an approximately 5-fold elevation of liver enzymes. The patient and donor were positive for SARS-CoV-2. Liver biopsy showed moderate acute hepatitis with prominent clusters of apoptotic hepatocytes, associated cellular debris, and lobular lymphohistiocytic inflammation. Typical portal features of mild to moderate acute cellular rejection were also noted [46] . This case report raises consideration for possible direct liver injury. The possible mechanisms by which SARS-CoV-2 exerts its pathogenetic role in the liver have been speculated in the literature. There is a known tropism of SARS-CoV-2 for ACE-2 receptors, and they are abundantly expressed in cholangiocytes. Nevertheless, a cholestatic pattern of liver injury is not the most common finding on presentation, as one might expect [47] . A second proposed mechanism is the cytokine storm, which leads to a surge in inflammatory cytokines affecting the liver [48] . SARS-CoV-2 induced acute respiratory distress syndrome and systemic inflammatory response syndrome lead to hypoxemia and shock, which can cause ischemiareperfusion injury [49] . It has been reported that SARS-CoV-2 can infect the endothelial cells directly and result in widespread endotheliitis [13] . Furthermore, administration of multiple drugs attempting to treat the disease have the potential to produce DILI, exacerbating the liver involvement of the disease [50] . The main limitation of our study is the inclusion of autopsies only, resulting in bias towards the most severe disease, with great influence by pre-existing co-morbid conditions. In our literature search, there were no reported liver biopsies from nonsevere COVID-19 patients. A correlation between clinical condition, biochemistry, liver imaging, and autopsies would be desirable to explore. Another limitation of this systematic review is the high heterogeneity in the published articles. Finally, since this is a systematic review from autopsy data, relevant clinical information to interpret the causes of steatosis (such as alcohol consumption or body mass index) was not available. The liver is frequently involved during severe acute respiratory syndrome coronavirus 2 infection and coronavirus disease 2019 (COVID-19). However, there is no consensus about the main histopathological findings in COVID-19. Identifying the main histopathological findings could help understand the mechanism of liver injury frequently observed in COVID-19 patients. The characterization of the liver histopathological findings will impact the interpretation of liver chemistries and liver biopsies in COVID-19 patients. We conducted a systematic review and meta-analysis, including liver biopsies and autopsies of COVID-19 patients. Proportions were estimated using random-effects models. We included 18 studies. The major histological findings are hepatic steatosis (55.1%), congestion of hepatic sinuses (34.7%), vascular thrombosis (29.4%), fibrosis (20.5%), Kupffer cell hyperplasia (13.5%), portal inflammation (13.2%), and lobular inflammation (11.6%). Other abnormalities can be identified, such as venous outflow obstruction, phlebosclerosis of the portal vein, herniated portal vein, periportal abnormal vessels, hemophagocytosis, and necrosis. The multiple liver histopathological findings observed in COVID-19 demonstrate the susceptibility to liver injury in risk populations, the inflammatory response, and thrombosis associated with this infection.@story_separate@In summary, in this systematic review and meta-analysis of autopsies from patients with COVID-19, we found a high prevalence of hepatic steatosis and the presence of vascular thrombosis as major histological liver features. Further studies are needed to establish the mechanism and implications of these findings. Steatosis, vascular thrombosis, fibrosis, and inflammatory abnormalities are the most frequent liver histopathological findings in COVID-19 patients.","BACKGROUND: Coronavirus disease 2019 (COVID-19) disease can frequently affect the liver. Data on hepatic histopathological findings in COVID-19 is scarce. AIM: To characterize hepatic pathological findings in patients with COVID-19. METHODS: We conducted a systematic review with meta-analysis registered on PROSPERO (CRD42020192813), following PRISMA guidelines. Eligible trials were those including patients of any age and COVID-19 diagnosis based on a molecular test. Histopathological reports from deceased COVID-19 patients undergoing autopsy or liver biopsy were reviewed. Articles including less than ten patients were excluded. Proportions were pooled using random-effects models. Q statistic and I(2) were used to assess heterogeneity and levels of evidence, respectively. RESULTS: We identified 18 studies from 7 countries; all were case reports and case series from autopsies. All the patients were over 15 years old, and 67.2% were male. We performed a meta-analysis of 5 studies, including 116 patients. Pooled prevalence estimates of liver histopathological findings were hepatic steatosis 55.1% [95% confidence interval (CI): 46.2-63.8], congestion of hepatic sinuses 34.7% (95%CI: 7.9-68.4), vascular thrombosis 29.4% (95%CI: 0.4-87.2), fibrosis 20.5% (95%CI: 0.6-57.9), Kupffer cell hyperplasia 13.5% (95%CI: 0.6-54.3), portal inflammation 13.2% (95%CI: 0.1-48.8), and lobular inflammation 11.6% (95%CI: 0.3-35.7). We also identified the presence of venous outflow obstruction, phlebosclerosis of the portal vein, herniated portal vein, periportal abnormal vessels, hemophagocytosis, and necrosis. CONCLUSION: We found a high prevalence of hepatic steatosis and vascular thrombosis as major histological liver features. Other frequent findings included portal and lobular inflammation and Kupffer cell hyperplasia or proliferation. Further studies are needed to establish the mechanisms and implications of these findings."
"The global spread of Coronavirus disease of 2019 abruptly impacted the pediatric rheumatology community, prompting physicians to rapidly assess the impact of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection in children with pediatric rheumatic disease (PRD) and the implications of immunosuppressive treatment on their risk for severe disease. Numerous questions arose surrounding preventive measures, risk reduction, ongoing immunosuppressive management, and methods to minimize disruption to clinical care. Simultaneously, pediatric rheumatologists quickly found themselves amidst the discovery of an unanticipated inflammatory syndrome related to COVID-19, now termed multisystem inflammatory syndrome in children (MIS-C). In a multidisciplinary collaborative effort with pediatric physicians including infectious disease and cardiology, pediatric rheumatologists have assisted in providing guidance surrounding the diagnostic evaluation and medical management of MIS-C. In this review, we will discuss the impact and clinical management of children with PRD during the COVID-19 pandemic, review the literature related to the clinical presentation and management of MIS-C, and briefly discuss unique clinical sequelae of COVID-19 that may prompt evaluation by a pediatric rheumatologist. impact of SARS-CoV-2 infection in patients with rheumatic disease and patients on immunosuppressive therapy. Reports in the literature of patients with adult-onset rheumatic disease have demonstrated that advancing age and underlying comorbidities remain a primary factor in the risk for severe complications from COVID-19, similar to the general population [1 && ,2 && ]. Few studies have also described disease-specific factors including underlying rheumatic disease, disease activity, the presence of lung involvement and certain immunomodulatory medications (including corticosteroids, rituximab and conventional disease modifying antirheumatic drugs (DMARDs)) that may additionally predict worse outcomes [2 && ,3,4 & ]. However, to date, there is little evidence to suggest a higher risk for severe COVID-19 in children with PRD [5-9] or children receiving immunomodulatory therapies commonly used for PRD [10] [11] [12] [13] [14] [15] . In May 2020, the American College of Rheumatology (ACR) developed the ACR COVID-19 Clinical Guidance for Pediatric Rheumatology Task Force, charged to provide clinical guidance to rheumatology providers who treat children with PRD in the context of the COVID-19 pandemic [16 & ]. Recognizing that children with PRD do not appear to be at significantly increased risk of severe COVID-19 and acknowledging the need to take into account individual patient characteristics and prevalence of SARS-CoV-2 transmission in the community, these general recommendations were aimed at assuring optimal control of underlying PRD during the era of the COVID-19 pandemic. Guidance was provided to discourage physicians from modifying or delaying immunomodulatory therapy in the absence of SARS-CoV-2 exposure or infection. Similarly, in the presence of close/household exposure or asymptomatic COVID-19, recommendations were to continue medical therapy needed to control underlying PRD, with special consideration to reduce corticosteroid burden to the lowest effective dose possible to control underlying disease. Concerns related to the use of rituximab and cyclophosphamide raised by the Global Rheumatology Alliance in adults with rheumatic disease [1 && ] were acknowledged; however, given the overall reduced risk of severe COVID-19 in the pediatric population and the fact that these medications are typically reserved for severe life and/ or organ threatening disease in children, the task force agreed that the benefits of continuing therapy likely outweigh the risk in most cases [16 & ]. In contrast, in the presence of symptomatic COVID-19, the task force agreed to conform to conventional practices related to concurrent infections and recommended holding all DMARDs for the duration of symptoms and up to 7-14 days after resolution of fever and respiratory symptoms. Special considerations were made for patients with PRD on interleukin (IL)-1 inhibitors, as these patients may be particularly sensitive to medication disruptions and with the knowledge that selective IL-1 inhibitors have been safely used in other infections [17] . Another consideration that the pediatric rheumatologist faced in the era of the COVID-19 pandemic is the responsibility to assure adequate and timely access to clinical care, particularly during times of increased community transmission of SARS-CoV-2. In evaluating clinical practice and patient perspectives during the COVID-19 pandemic, both patients and families acknowledged that apprehension about in-person clinical assessments and safe access to the hospital system [18 & ] may have resulted in delays to care and exacerbation of underlying illness [19] . As a result, the rapid expansion of telemedicine during this time has been instrumental in improving access to care for children with PRD [19] [20] [21] [22] , with the development of comprehensive telemedicine assessments, including standardizing the musculoskeletal physical exam using the video version of paediatric Gait Arms Legs and Spine (pGALS), V-pGALS [23 & ]. Despite the numerous benefits of telemedicine, several limitations should also be acknowledged, specifically related to the quality and comprehensiveness of care, psychosocial evaluation, and the availability of access to technology to maintain health equity [21, 24] . In addition to concerns related to medical management of children with PRD, the COVID-19 pandemic has also raised awareness of the impact emotional distress, school closures, and limited socialization on the overall well being of children with chronic illness. Children and adolescents with PRD have a relatively high prevalence of anxiety and depression at baseline compared to the general pediatric population [25, 26] . Furthermore, there is evidence that patients within the@story_separate@Children with PRD do not appear to be at significantly increased risk of severe COVID-19; thus, treatment goals should be targeted to assure optimal control of underlying PRD during the COVID-19 pandemic. The clinical management of MIS-C requires multidisciplinary collaboration with an appreciation of distinct clinical phenotypes to assure accurate diagnosis and immunomodulatory management. Post-COVID sequelae may manifest with inflammatory manifestations, mimicking pediatric rheumatologic disease, prompting assessment and recognition by pediatric rheumatologists. Black and Latinx populations may be disproportionately impacted by the pandemic from both a medical and psychosocial standpoint [18 & , [27] [28] [29] [30] . Pediatric rheumatology providers must be mindful of the burden of the COVID-19 pandemic on both children with PRD and their caregivers, recognizing the impact of psychosocial distress on physical disease, and assist with referrals to mental health services. Similarly, with regards to in-person schooling, the ACR COVID-19 Clinical Guidance for Pediatric Rheumatology Task Force recognized the generally low rates of transmission in primary and secondary schools [31] and emphasized the benefits of attending in-person school, once taking into account individual patient characteristics and comorbidities. Finally, to date, there has been no evidence to suggest children with PRD are at higher risk of adverse reactions from the COVID-19 vaccine and thus the task force recommended that children, adolescents, and young adults with PRD should receive the vaccine in accordance with Centers for Disease Control and local recommendations. Since first described in Europe in April 2020, MIS-C (also known as pediatric multisystem inflammatory syndrome (PMIS)), has been increasingly recognized throughout the world. The presentation of MIS-C is temporally linked to COVID-19 exposure, with peaks of disease typically following surges of COVID-19 cases by approximately 3-6 weeks, and patients demonstrating evidence of prior SARS-CoV-2 infection with positive IgG serologies. Although MIS-C remains an overall rare condition, clinical presentations with cardiogenic shock and multiorgan dysfunction have created an impetus for rapid multidisciplinary collaboration and strategies to guide clinical management. Given the presentation of MIS-C as a systemic inflammatory condition with clinical symptomatology that often overlaps with Kawasaki disease, pediatric rheumatologists have been involved from the onset in attempts to understand the underlying pathophysiology, and have assisted in developing guidance for diagnostic evaluation, clinical monitoring and management with immunomodulatory therapy [32 && ,33]. In the year since the initial description of MIS-C, knowledge regarding the presentation, management, and pathophysiology has rapidly expanded; however, numerous uncertainties remain. The majority of children presenting with MIS-C are previously healthy, with reports of prior asymptomatic or minimally symptomatic COVID-19; thus, underlying predisposing factors for MIS-C remain unknown. There is a range of clinical severity, yet a majority of patients require care in pediatric intensive care units (ICUs) during their hospitalization. Furthermore, while MIS-C was initially described as 'Kawasaki-like', numerous reports have studied these overlapping phenotypes and have since described differences in both clinical and immunochemical presentations between MIS-C and Nearly, all patients presented with fever (99%) and most commonly involved organ systems included gastrointestinal (86%), cardiovascular (79%), and respiratory (50%) (Fig. 1) . In this systematic review, 23% of patients fulfilled criteria for complete Kawasaki disease, whereas 24.1% resembled incomplete Kawasaki disease. Laboratory evidence of systemic inflammation was evident in the majority of MIS-C cases, with elevated acute phase reactants including C-reactive protein, IL-6, and ferritin. Patients also had significantly elevated markers of coagulation (D-dimer, fibrinogen) and myocardial injury (troponin, brain natriuretic peptide). Many investigators have compared children with MIS-C to historical Kawasaki disease patients. MIS-C patients tend to have a broader age range, with a median age range higher than that of classic Kawasaki disease (median age: 2-2.7 years) [34 . MIS-C patients are less likely to have coronary artery abnormalities and more likely to have ventricular dysfunction [35 & ]. Evidence of systemic inflammation also appears to be substantially higher in MIS-C compared to historical Kawasaki disease cohorts. Studies have additionally compared clinical presentation, laboratory markers and outcomes in children and adolescents with MIS-C and acute COVID-19 [37, 38] . Feldstein et al. compared cases of MIS-C in a USA cohort with severe acute COVID-19 and found that patients with MIS-C were more likely to be 6-12 years old, non-Hispanic/ Black and more likely to be previously healthy [38] . Clinical symptoms that distinguished MIS-C patients from acute COVID-19 included mucocutaneous symptoms and severe cardiovascular involvement without respiratory involvement. Patients with MIS-C were more likely to have higher neutrophil-to-lymphocyte ratios, higher CRP levels, and lower platelet counts [38] . A summary of clinical and laboratory features of MIS-C, Kawasaki disease, and acute COVID-19 is presented in Fig. 1 . To date, immunomodulatory management of MIS-C has focused on its resemblance with Kawasaki disease and theoretical concerns about cardiac sequelae and potential coronary artery aneurysms, similar to those described with untreated KD. While data are limited to retrospective reports, case series, and anecdotal evidence, most reports describe widespread use of intravenous immunoglobulin (IVIG) and corticosteroids in 50-90% of patients for the treatment of MIS-C, with variability noted in treatment strategies and dosing among different centers and studies [33,34 && ,35 & ] One retrospective cohort study from France [39] compared use of IVIG alone versus IVIG plus methylprednisolone as initial therapy for MIS-C in propensity score-matched cohorts and found that combined therapy was associated with improved outcomes, including lower rate of treatment failure, lower use of second-line treatment, less need for hemodynamic support, less evidence of acute left ventricular dysfunction, and shorter ICU stay. IL-1 inhibition with anakinra has also been described in refractory disease in up to 8 ]. Although a majority of patients require ICU support (73%), almost all patients have excellent outcomes, with very few deaths reported and minimal longterm sequelae [40] . The ACR published clinical guidance for treatment of MIS-C and hyperinflammation in COVID-19 in June 2020 with revisions published in November 2020 [32 && ]. This document includes a diagnostic pathway for MIS-C and a discussion of features distinguishing MIS-C from KD. Clinical guidance is provided for laboratory evaluation and cardiac monitoring, stratified by clinical presentation. Guidance for immunomodulatory therapy recommends stepwise approach with initiation of IVIG for all children hospitalized with MIS-C and/or fulfil Kawasaki disease criteria with careful consideration of cardiac function and fluid status to prevent fluid overload. Corticosteroids are recommended as firstline therapy for shock or organ threatening disease, or as a second line therapy for refractory disease Impact of COVID-19 on pediatric rheumatology Wahezi et al. (Fig. 2) . Other treatment options for refractory disease include high dose intravenous corticosteroids and IL-1 inhibition [32 && ]. In addition to acute COVID-19 infection and postinfectious inflammatory conditions, post-COVID sequelae have also been described and are also being addressed by pediatric rheumatologists worldwide. Regardless of symptoms at diagnosis or severity of acute infection, several patients suffer from longterm effects of COVID-19. Rheumatologists are being called upon to assess a wide array of symptoms commonly seen as presentations of systemic autoimmune disease and to distinguish them from lingering effects of COVID-19. Symptoms vary among studies with fatigue, muscle weakness, sleep difficulties, and anxiety or depression commonly reported as long-term effects [41] . In a meta-analysis of studies world-wide, fatigue, headache, attention disorder, hair-loss, and dyspnea were the most common symptoms [42] . Although myalgia was the most common musculoskeletal symptom reported during acute COVID infection, many musculoskeletal findings have been reported as post-COVID Moderate-to-high consensus was reached by the Task Force in the development of this treatment algorithm for MIS-C associated with severe acute respiratory syndrome coronavirus 2. 1 Intravenous immunoglobuin (MG) dosing is 2 gm/kg based on ideal body weight. Cardiac function and fluid status should be assessed before MG is given. In some patients with cardiac dysfunction, MG may be given in divided doses (1 gm/kg da1y over 2 days). 2 Methylpremisolone or another steroid at equivalent dosing may be used. 3 Refractorry disease is defined as persistent fevers and/or ongoing and significant end-organ involvement. 4 Low-to-moderate-dose glucocorticoids (methylprednisolone 1-2 mg/kg/day) may be considered for first-line therapy in some MIS-C patients with concerning features (ill appearance, highly elevated B-type natriuretic peptide levels, unexplained tachycardia) who have not yet developed shock or organthreatening disease. 5 If the patient was given Iow-to-moderate-dose glucocorticoids as first-line therapy, methylprednisolone IV dosing should be 10-30 mg/kg/day for intensification treatment. Reproduced with permission from Henderson et al. [32 && ]. complications including myositis, neuropathy, arthropathy, and soft tissue abnormalities [43] . In children, data are scarce regarding long-term consequences of COVID-19 with few reports of fatigue, dyspnea, and heart palpitations or chest pain. Other symptoms reported include decreased concentration, prolonged fevers, and headaches [44] . Additional clinical features seen in both acute infection with COVID-19 and post-COVID sequelae are dermatologic manifestations that may mimic common rheumatologic conditions. The most common dermatologic findings in acute COVID-19 infection are maculopapular rash, urticaria, chilblains vesicular lesions, livedo reticularis, and petechiae. Interestingly, an increased incidence of chilblains has been reported during the pandemic and found to be more common in younger age patients [45] . Chilblains seem to appear after the active phase of disease, most commonly described in patients who were asymptomatic carriers of COVID -19 and found to have antibodies only after chilblains was reported. This entity has been termed 'COVID toes' and has been described in both adults and children (Fig. 3) . Most pediatric cases present with no evidence of acute COVID-19 infection, and often with negative SARS-CoV-2 PCR testing and negative antibodies [46] . Histopathologic studies of these lesions have shown variable degrees of lymphocytic vasculitis with evidence of endothelial damage [47 & ]. Coronavirus particles have also been described within the endothelium [47 & ]. Prognosis of children with chilblains is favorable, with spontaneous regression within 2-8 weeks as the most common outcome. In severe cases, a trial of topical steroids and/or oral antihistamines may be considered [46, 48] .@story_separate@Despite the numerous challenges to the medical community, the COVID-19 era has been defined by a rapid expansion in scientific knowledge and a time of extraordinary local and worldwide collaboration, both within the pediatric rheumatology community, as well as across multiple disciplines. Through collective efforts, we have learned that children with PRD, including those on immunomodulatory therapies, are not at increased risk for severe COVID-19; thus, the overall goals in the management of patients with PRD include continued therapy to assure prompt control of active disease, relief of symptoms, and prevention of long-term sequelae. Pediatric rheumatology providers must be mindful of challenges that have been exacerbated during the pandemic including healthcare inequities, impaired access to clinical care and psychosocial distress. Finally, pediatric rheumatology providers have been called upon to assist in the management of post-COVID sequelae, including MIS-C, relying on expertise in multidisciplinary collaboration, knowledge of immune responses, and the use of immunomodulatory therapies.","The purpose of this review is to discuss the clinical management of children with pediatric rheumatic disease (PRD) during the Coronavirus disease of 2019 (COVID-19) pandemic, as well as the unique role of the pediatric rheumatologist during a time of emerging post-COVID inflammatory sequelae including, multisystem inflammatory syndrome in children (MIS-C). RECENT FINDINGS: To date, there has been little evidence to suggest that children with PRD, including those on immunomodulatory therapies, are at increased risk for severe COVID-19. Clinical guidance statements have been created to support clinical providers in providing care to children with PRD during the COVID-19 pandemic. Pediatric rheumatologists have also been called upon to assist in the identification and management of post-COVID sequelae, including the rapidly emerging inflammatory illness, MIS-C. SUMMARY: The COVID-19 era has been defined by a rapid expansion in scientific knowledge and a time of extraordinary local and worldwide collaboration, both within the pediatric rheumatology community, as well as across multiple disciplines. Through collective efforts, we have learned that children with PRD, including those on immunomodulatory therapies, are not at increased risk for severe COVID-19. Pediatric rheumatologists have also worked alongside other disciplines to develop guidance for the management of MIS-C, with the majority of patients experiencing excellent clinical outcomes."
"The outbreak of COVID 19 caused by severe acute respiratory syndrome-related coronavirus (SARS-CoV-2) has thrown a pandemic threat to the humanity of the world [1]. Symptoms like cold, flu and in major cases lung failure or brain failure are shown by the infected patients [2] . This virus has a huge transmission rate, and without developing a suitable therapeutic option, the human lives can't come back in their previous rhythm [3] . Coronaviruses (CoVs) belong to the family of Coronaviride with spike glycoprotein on their outer surface, which is similar to severe acute respiratory syndrome (SARS) and middle east respiratory syndrome (MERS) [4] . SARS-CoV-2 is a large enveloped positive sense RNA virus containing structural and non-structural proteins (nsps), including several accessory proteins [5] . 82% genomic sequence identity of SARS-CoV-2 with SARS-CoV helps us to gather knowledge about the pathogenesis of SARS-CoV-2 [6] . SARS-CoV and SARS-CoV-2, S protein mediated host cell invasion occurred through binding angiotensin converting enzyme-2 (ACE-2), a receptor protein [6, 7] . ACE-2 is located at the surface membrane of the host cell. The infection process initiates with the interaction between viral S protein and ACE-2 on the surface of the host cell [8] . According to the analysis of Cryo-EM structure, the binding affinity of S protein (SARS-CoV-2) with ACE-2 is approximately 10-20 times greater than the SARS-CoV S protein [9, 10] . So higher contagiousness and transmissibility are reflected for SARS-CoV-2 with respect to SARS-CoV [11, 12] . Various attempts have been made to inhibit different proteins and enzymes that are involved in replication process of SARS-CoV-2 viz. hydroxychloroquine inhibits Mpro [13] , remdesivir inhibits RdRp [14] , Sofosbuvir, Ribavirin inhibit RdRp [15] , extract from Azadiractha Indica inhibits PL-pro [16] . Furthermore, to discover therapeutic agents for effective blocking of ACE-2 protein, Chloroquine and hydroxychloroquine are already reported [17, 18] . Systematic checking of drug-drug target interaction (DTI) is a standard method of drug repurposing. Various scoring functions (e.g. docking scoring function) are applied for drug repurpose [17] . In this study, we have selected 24 anti-bacterial and anti-viral drugs for virtual screening against ACE2 proteins of human body. Molecular docking study has been done with ACE2 receptor against these drugs. Molecular dynamics simulation was also performed to check the J o u r n a l P r e -p r o o f stability of ACE2 with that drugs by different plots like RMSD, RMSF, SASA radius of gyration analysis. The outbreak of COVID 19 caused by severe acute respiratory syndrome-related coronavirus (SARS-CoV-2) has thrown a pandemic threat to the humanity of the world [1] . Symptoms like cold, flu and in major cases lung failure or brain failure are shown by the infected patients [2] . This virus has a huge transmission rate, and without developing a suitable therapeutic option, the human lives can't come back in their previous rhythm [3] . Coronaviruses (CoVs) belong to the family of Coronaviride with spike glycoprotein on their outer surface, which is similar to severe acute respiratory syndrome (SARS) and middle east respiratory syndrome (MERS) [4] . SARS-CoV-2 is a large enveloped positive sense RNA virus containing structural and non-structural proteins (nsps), including several accessory proteins [5] . 82% genomic sequence identity of SARS-CoV-2 with SARS-CoV helps us to gather knowledge about the pathogenesis of SARS-CoV-2 [6] . SARS-CoV and SARS-CoV-2, S protein mediated host cell invasion occurred through binding angiotensin converting enzyme-2 (ACE-2), a receptor protein [6, 7] . ACE-2 is located at the surface membrane of the host cell. The infection process initiates with the interaction between viral S protein and ACE-2 on the surface of the host cell [8] . According to the analysis of Cryo-EM structure, the binding affinity of S protein (SARS-CoV-2) with ACE-2 is approximately 10-20 times greater than the SARS-CoV S protein [9, 10] . So higher contagiousness and transmissibility are reflected for SARS-CoV-2 with respect to SARS-CoV [11, 12] . Various attempts have been made to inhibit different proteins and enzymes that are involved in replication process of SARS-CoV-2 viz. hydroxychloroquine inhibits Mpro [13] , remdesivir inhibits RdRp [14] , Sofosbuvir, Ribavirin inhibit RdRp [15] , extract from Azadiractha Indica inhibits PL-pro [16] . Furthermore, to discover therapeutic agents for effective blocking of ACE-2 protein, Chloroquine and hydroxychloroquine are already reported [17, 18, 51] . Systematic checking of drug-drug target interaction (DTI) is a standard method of drug repurposing. Various scoring functions (e.g. docking scoring function) are applied for drug repurpose [17] . In this study, we have selected 24 anti-bacterial and anti-viral drugs for virtual screening against ACE2 proteins of human body. Molecular docking study has been done with ACE2 receptor against these drugs. Molecular dynamics simulation was also performed to check the stability of ACE2 with that drugs by different plots like RMSD, RMSF, SASA radius of gyration analysis.@story_separate@The crystal structure of SARS-CoV-2 spike binding site angiotensin converting enzyme-2 (ACE-2) (PDB ID:6M0J) receptor was obtained from protein data bank (http://www.rcsb.org). The structure was then cleaned using Autodock tools by removing heteroatoms and by adding necessary hydrogen atoms. The structures of the 24 drug molecules were obtained from PubChem. Using UCSF Chimera [19] the pdb files of the drugs were created for docking. Only chain-A of ACE-2 receptor was selected for docking with drugs. Autodock Vina [20] package was used for docking between the best binding sites of ACE-2 and drugs. 10ns MD-simulation was performed with the minimum energy conformer of the ACE-2 and Cefpiramide (CPM) complex using Gromacs (5.1) [20] with CHARMM36-march2019 force field [21] . The TIP3P water model [22] was used for solvation of the complex. Necessary topology and parameter files for the drug (CPM) were generated by using CGenFF server. A cubical box with a buffer dimension 10 x 10 x 10 Å 3 was created and adequate number of Na + ions were added to maintain electro neutrality. After performing energy minimization of the ACE-2-drug complex to 10 kJmol -1 nm -1 , a 100 ps NVT equilibration was then performed at 300 K followed by another equilibration NPT for 100 ps, keeping 2fs time step. Modified Berendsen thermostat was used for the NPT ensemble. Here also the time step was 2 fs. For both NVT and NPT equilibration, cut-offs for electrostatic and van der Waals interactions were kept at 1.0 nm. Long range interactions were calculated using smooth particle mesh Ewald (PME) method [23] . The equilibrated ensembles were finally subjected to MD simulation for 10 ns, with electrostatic and van der Waals cut off as before. PME method was used to calculate long range electrostatic interactions. A modified Berendsen thermostat and a Parinello-Rahman barostat were used with reference temperature and pressure at 300K and 1 bar respectively. Snapshots of the trajectory were saved every 1 ns for each case. Molecular mechanics Poison-Boltzmann surface area (MM-PBSA) method [24] , implemented on Gromacs tool (g_mmpbsa) [25] was used for the calculation of binding free energies. The binding energies were calculated by using the following formulae (1) (3) Where, G w-complex is the total free energy of the ACE2 and drug complex, G w-protein , G w-drug are the free energies of the protein and drug respectively. E MM is the average MM potential energy including bonding, non-bonding energies, G sol is the free energy of solvation including polar and non-polar energies. SASA is the solvent accessible surface area, γ is the coefficient of surface tension of solvent and b is the fitting parameter. TS is not considered by g_mmpbsa. 24 potentially active drugs were selected for virtual screening against human angiotensin converting enzyme-2 (ACE-2) receptor. According to the previous studies, all these drugs have either anti-bacterial or anti-viral activities as shown in J o u r n a l P r e -p r o o f Pharmacological analysis of these compounds showed interesting results. ADME toxicity analysis has been performed against these selected compounds. ADMET (i.e. Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiling of the compounds were performed with the help of pkCSM online server [50] . J o u r n a l P r e -p r o o f  Compounds which showed potential binding affinities with ACE2 are shown in Fig. 2 . The interaction site of the docked structure of the drugs with ACE2 is given in the figure. Furthermore, we also note that after 2 ns the RMSD fluctuation of the docked structure is relatively low with respect to the undocked one suggesting during the progress of MDsimulation, the drug moiety interacts strongly within the cavity of the ACE-2. RMSF plot as shown in the Fig. 4b reveals that the fluctuations of residues for the docked structure are quite low compared to the undocked one. Radius of gyration (Rg) indicates the compactness of a system. With increasing the value of Rg, the compactness of the system also increases. Rg for the docked and undocked structure is shown in the Fig. 5a . Rg for the docked structure is quite high as compared to the undocked one which confirms that after docking the drug (CPM) is nicely fitted within the cavity of ACE-2. We also analyzed the surface accessible surface area (SASA) plot for undocked and docked ACE-2. The binding energy of CPM against ACE2 showed a high value of -79.958 ± 18.653 kJ/mol. Table 3 all the interaction energies between ACE2 and CPM showed a high value confirming profound conformational changes of ACE2 by CPM. Conformational changes during MD-simulation is represented in Fig. 9 . These changes are captured at each nanosecond and revealed that these changes are profound. RMSD plot indicates a significant change in the structure after two ns. Hence it is clear that CPM has a considerable impact on the conformation of ACE-2. J o u r n a l P r e -p r o o f indicates that the drug has a significant impact on the receptor. Considerable stabilization and effective blocking of ACE-2 by CPM are confirmed by RMSD, RMSF analysis along with the binding energy calculation. We believe that the drug, CPM can be anticipated as an effective blocker for ACE-2 receptor and showed potential inhibitory activity against SARS-CoV-2. The authors declare no conflicting interest in the present work. Data is available upon request to the corresponding author. [1] Xu X, Chen P, Wang J, Feng J, Zhou H, Li X, et al. Evolution of the novel coronavirus from the ongoing Wuhan outbreak and modeling of its spike protein for risk of human transmission. Science China Life Sciences. 2020;63:457-60. [2] Jain V, Yuan J-M. Predictive symptoms and comorbidities for severe COVID-19 and intensive care unit admission: a systematic review and meta-analysis. International Journal of The crystal structure of SARS-CoV-2 spike binding site angiotensin converting enzyme-2 (ACE-2) (PDB ID:6M0J) receptor was obtained from protein data bank (http://www.rcsb.org). The structure was then cleaned using Autodock tools by removing heteroatoms and by adding necessary hydrogen atoms. The structures of the 24 drug molecules were obtained from PubChem. Using UCSF Chimera [19] the pdb files of the drugs were created for docking. Only chain-A of ACE-2 receptor was selected for docking with drugs. Autodock Vina [20] package was used for docking between the best binding sites of ACE-2 and drugs.  10ns MD-simulation was performed with the minimum energy conformer of the ACE-2 and Cefpiramide (CPM) complex using Gromacs (5.1) [20] with CHARMM36-march2019 force field [21] . The TIP3P water model [22] was used for solvation of the complex. Necessary topology and parameter files for the drug (CPM) were generated by using CGenFF server. A cubical box with a buffer dimension 10 × 10 × 10 Å 3 was created and adequate number of Na + ions were added to maintain electro neutrality. After performing energy minimization of the ACE-2-drug complex to 10 kJ mol − 1 nm − 1 , a 100 ps NVT equilibration was then performed at 300 K followed by another equilibration NPT for 100 ps, keeping 2fs time step. Modified Berendsen thermostat was used for the NPT ensemble. Here also the time step was 2 fs? For both NVT and NPT equilibration, cut-offs for electrostatic and van der Waals interactions were kept at 1.0 nm. Long range interactions were calculated using smooth particle mesh Ewald (PME) method [23] . The equilibrated ensembles were finally subjected to MD simulation for 10 ns, with electrostatic and van der Waals cut off as before. PME method was used to calculate long range electrostatic interactions. A modified Berendsen thermostat and a Parinello-Rahman barostat were used with reference temperature and pressure at 300 K and 1 bar respectively. Snapshots of the trajectory were saved every 1 ns for each case. Molecular mechanics Poison-Boltzmann surface area (MM-PBSA) method [24] , implemented on Gromacs tool (g_mmpbsa) [25] was used for the calculation of binding free energies. The binding energies were calculated by using the following formulae  G sol = G polar + G non-polar = G polar + (γSASA + b) Where, G w-complex is the total free energy of the ACE2 and drug complex, G wprotein , G w-drug are the free energies of the protein and drug respectively. E MM is the average MM potential energy including bonding, non-bonding energies, G sol is the free energy of solvation including polar and non-polar energies. SASA is the solvent accessible surface area, γ is the coefficient of surface tension of solvent and b is the fitting parameter. TS is not considered by g_mmpbsa.  24 potentially active drugs were selected for virtual screening against human angiotensin converting enzyme-2 (ACE-2) receptor. According to the previous studies, all these drugs have either anti-bacterial or anti-viral activities as shown in Table 1 . Among the selected drugs four drugs (Formoterol, Cefpiramide, Mitoxantrone and Tigecycline) are FDA approved. In the present study we have made a comprehensive analysis of the inhibitory activity of these drugs against ACE-2 receptor. Docking scores, summarised in Table 1 clearly indicate the binding efficiency of these drugs with ACE-2 receptor. All the 24 drugs showed binding affinities with ACE-2 receptor and 12 of them showed high binding affinities with a docking score greater than or equals to − 7.0 kcal/mol. Cefpiramide, which showed a broad spectrum antibiotic activity showed the highest docking score against the human ACE-2 receptor of − 9.1 kcal/mol. The binding affinities of the studied drugs against ACE-2 are shown in Fig. 1 . Pharmacological analysis of these compounds showed interesting results. ADME toxicity analysis has been performed against these selected compounds. ADMET (i.e. Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiling of the compounds were performed with the help of pkCSM online server [50] . All the studied compounds have skin permeability ranging from − 2.665 to − 4.3. Most of the compounds do not inhibit P-glycoprotein I and II. Blood-brain barrier (BBB) permeability values are between − 2.083 and +0.087, whereas CNS permeability values appear between − 5.4 and − 1.632. Most of the drugs do not inhibit CYP1A2, CYP2C19, CYP2C9, CYP2D6, CYP3A4 enzymes and do not interact with renal OCT2 substrate. Along with that, most of the drugs neither show AMES toxicity nor inhibit the hERGI inhibitor. The highest value of LD50 toxicity of these drugs are 3.328 mol/kg. Few of them are hepatotoxic in nature and almost all of them do not create skin sensitization. The highest value of minnow toxicity level of these drugs is 5.424. These values are tabulated in Table 2 . Compounds which showed potential binding affinities with ACE2 are shown in Fig. 2 . The interaction site of the docked structure of the drugs with ACE2 is given in the figure. Fig. 3 represents the docked structure of Cefpiramide (CPM) with ACE2. From Fig. 3 it is clear that there is strong binding interaction between the drug and ACE2 receptor due to the formation of H-bonding, electrostatic and van der Waal interactions. The nearest residues are shown in the 2D contour plot (left panel) as well as in the 3D structure is shown in right panel. The H-bonding distances (in angstrom) are given in the 2D structure and the donor and acceptor sites within the docked cavity are given in the right panel 3D structure. Analysing the ADME data and binding energies obtained from docking results we have chosen the drug Cefpiramide (CPM BE = − 9.1 kcal/mol) to study the MD-simulation against ACE-2. The RMSD plot of the docked CPM against ACE-2 is shown in Fig. 4 . We found a profound stabilization of the docked structure after 2 ns?as compared to the undocked one. Furthermore, we also note that after 2 ns the RMSD fluctuation of the docked structure is relatively low with respect to the undocked one suggesting during the progress of MD-simulation, the drug moiety interacts strongly within the cavity of the ACE-2. RMSF plot as shown in Fig. 4b reveals that the fluctuations of residues for the docked structure are quite low compared to the undocked one. Radius of gyration (Rg) indicates the compactness of a system. With increasing the value of Rg, the compactness of the system also increases. Rg for the docked and undocked structure is shown in Fig. 5a . Rg for the docked structure is quite high as compared to the undocked one which confirms that after docking the drug (CPM) is nicely fitted within the cavity of ACE-2. We also analyzed the surface accessible surface area (SASA) plot for undocked and docked ACE-2. Fig. 5b represents the SASA plot of undocked and docked ACE-2. A closer look to Fig. 5b revealed that after 4 ns the docked structure corresponds to the higher SASA value compared to the undocked one suggesting the entry of the drug stabilizes ACE-2 conformation. Fig. 6 represents the sequence analysis of undocked and docked ACE-2. From Fig. 6 it is clear that there is a substantial structural alternation on amino acids in ACE2 before and after docking. Residue numbers from 280 to 381 of ACE2 were mostly affected by the drug CPM. This result is further elevated by the contribution energy with respect to residue number, as shown in Fig. 7 . The binding energy of CPM against ACE2 showed a high value of − 79.958 ± 18.653 kJ/mol. As shown in Table 3 all the interaction energies between ACE2 and CPM showed a high value confirming profound conformational changes of ACE2 by CPM. Fig. 8 represents the binding free energy, MM energy, polar solvation and non-polar solvation energy. The binding free energy of polar and non-polar parts of the docked structure with respect to time is shown in Fig. 8 c & d respectively. During MD-simulation non-polar binding free energy (van der Waal interaction) decreased indicating much stronger binding of the drug CPM in the ACE2 cavity. A stronger binding between CPM and ACE2 is indicated by the substantial structural change in ACE2 receptor. Fig. 8 a & b represented the binding energy and MM energy of CPM against ACE2 during MD-simulation. We found the average binding energy of − 79.958 kJ/mol and the average MM energy of − 231.327 kJ/ mol. Conformational changes during MD-simulation is represented in Fig. 9 . These changes are captured at each nanosecond and revealed that these changes are profound. RMSD plot indicates a significant change in the structure after two ns. Hence it is clear that CPM has a considerable impact on the conformation of ACE-2. Data is available upon request to the corresponding author. @story_separate@In the present work, we have virtually screened 24 potentially active anti-bacterial and anti-viral drugs against SARS-CoV-2 binding receptor, ACE-2. ADMET profiling confirms that these drugs are suitable to use against COVID-19 treatment. The screening results revealed that cefpiramide (CPM) showed a decent binding affinity SARS-CoV-2 human ACE-2 receptor. CPM entry to the cavity of ACE-2 is facilitated by forming H-bonding interactions and electrostatic interactions. Furthermore, MD-simulation of CPM against ACE-2 showed a striking result by stabilizing ACE-2 conformation. The total disruption of ACE-2 sequence indicates that the drug has a significant impact on the receptor. Considerable stabilization and effective blocking of ACE-2 by CPM are confirmed by RMSD, RMSF analysis along with the binding energy calculation. We believe that the drug, CPM can be anticipated as an effective blocker for ACE-2 receptor and showed potential inhibitory activity against SARS-CoV-2.","Till date millions of people are infected by SARS-CoV-2 throughout the world, while no potential therapeutics or vaccines are available to combat this deadly virus. Blocking of human angiotensin-converting enzyme 2 (ACE-2) receptor, the binding site of SARS-CoV-2 spike protein, an effective strategy to discover a drug for COVID-19. Herein we have selected 24 anti-bacterial and anti-viral drugs and made a comprehensive analysis by screened them virtually against ACE-2 receptor to find the best blocker by molecular docking and molecular dynamics studies. Analysis of results revealed that, Cefpiramide (CPM) showed the highest binding affinity of −9.1 kcal/mol. Furthermore, MD study for 10 ns and evaluation of parameters like RMSD, RMSF, radius of gyration, solvent accessible surface area analysis confirmed that CPM effectively binds and blocks ACE-2 receptor efficiently."
"In December 2019, several cases of respiratory syndrome caused by a new coronavirus were reported. (1) Subsequently, the virus called severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and the disease associated with it, COVID-19, spread worldwide. The first case in Brazil was confirmed on February 26 th , 2020, and since then, the number of new cases and deaths from COVID-19 have been on a rise in the country. Around 20% of patients with COVID-19 require hospital admission, mostly due to respiratory distress. (2) From these, about 15% require intensive care unit (ICU) admissions to receive support for organ dysfunctions such as mechanical ventilation for respiratory failure. (2) Erica Aranha Suzumura 1 The COVID-19 pandemic has brought concerns to managers, healthcare professionals, and the general population related to the potential mechanical ventilators' shortage for severely ill patients. In Brazil, there are several initiatives aimed at producing alternative ventilators to cover this gap. To assist the teams that work in these initiatives, we provide a discussion of some basic concepts on physiology and respiratory mechanics, commonly used mechanical ventilation terms, the differences between triggering and cycling, the basic ventilation modes and other relevant aspects, such as mechanisms of ventilator-induced lung injury, respiratory drive, airway heating and humidification, crosscontamination risks, and aerosol dissemination. After the prototype development phase, preclinical benchtests and animal model trials are needed to determine the safety and performance of the ventilator, following the minimum technical requirements. Next, it is mandatory going through the regulatory procedures as required by the Brazilian Health Regulatory Agency (Agência Nacional de Vigilância Sanitária -ANVISA). The manufacturing company should be appropriately registered by ANVISA, which also must be notified about the conduction of clinical trials, following the research protocol approval by the Research Ethics Committee. The registration requisition of the ventilator with ANVISA should include a dossier containing the information described in this paper, which is not intended to cover all related matters but to provide guidance on the required procedures. Given the rapidly progressing number of patients with severe COVID-19 requiring ICU admission and the use of mechanical ventilation, in addition to patients who require this type of support for other reasons, there is a growing concern regarding possible mechanical ventilators shortage. Therefore, physicians and other healthcare professionals may face a difficult decision: who out those critically ill patients will be placed on mechanical ventilation if there is not enough for all? (3) (4) (5) National and international companies report that the number of orders for mechanical ventilators skyrocketed since pandemic was declared by the World Health Organization on March 11 th , 2020. The high demand for mechanical ventilators, associated with the high complexity in manufacturing and limited production capacity of the industry, has delayed and rendered difficult delivering the requested machines. Mechanical ventilators are medical devices that have complex technology with volumetric and pressometric control. They feature digital manometer for pressure control and real-time pressure, volume and flow curves. The available settings and functions allow operation in different ventilation modes. These devices have standard alarms (blackout, oxygen drop, low battery, inverse inspiratory/ expiratory (I:E) ratio, and disconnection from the patient) and settable alarms (maximum peak inspiratory pressure, volume, respiratory rate -RR, positive end-expiratory pressure -PEEP and apnea) that must be adjusted according to the individual needs of each patient. Some mechanical ventilators have additional functions, such as compensation for tube resistance, for air leakage and circuit compliance, or noninvasive ventilation mode. Also, overpressure valves are needed on the gas supply and ventilator systems. The development of a mechanical ventilator involves numerous steps and tests, which can require commercial manufacturers to expend up to 2 years to complete. Thousands of experts, entrepreneurs, and volunteers worldwide are working to create alternative ventilators in an attempt to prevent COVID-19 patients' deaths for ventilator shortage. In Brazil, there are several initiatives focused on rapidly producing large-scale alternative lowcost ventilators to be used in these patients. Prototypes based on automation of the artificial manual breathing unit (AMBU), on Boussignac valve, and on bellows are currently being developed, as well as reproductions of the Takaoka mini pneumatic ventilator, and attempts to develop more complex mechanical ventilators with microprocessor technology. (6) These prototypes would have a life-saving potential for patients who would have no other chance for survival if no conventional mechanical ventilator is available. However, a medical device aimed at saving lives, if not appropriately developed to provide the required functions, can cause irreversible or fatal harm.@story_separate@One of the challenges for alternative ventilator developers who have not a health science background, is the lack of expertise in physiology, respiratory mechanics, and ventilator-induced lung injury mechanisms. Biomedical engineering courses emphasize the need for acquiring information related to medical concepts, the reason why many classes are given by healthcare professionals. (7) The detailed knowledge on the available ventilation modes is the best way to get safe and effective mechanical ventilation (8) as well to develop medical devices that meet the patients' needs. If not, the medical device may become a mere potentially lethal air pumping device. In this sense, some basic concepts are presented, and it is still essential that the teams that propose to develop alternative ventilators involve professionals with respiratory pathophysiology knowledge and expertise in management of patients in mechanical ventilation. Breathing aims to provide oxygen to and remove carbon dioxide produced by the body tissues. (9) Under normal conditions, inspiration is achieved by expanding the chest, which occurs by contraction of inspiratory muscles (diaphragm and external intercostal muscles). This chest expansion decreases the pressure within the respiratory system. Air enters the lungs when the alveolar pressure is lower than the pressure at the opening of the upper airways. (9) Normal expiration is passive and requires no work. The muscles relax, the diaphragm rises, returning to the resting position, the chest volume is reduced, and the air flows out the alveoli, exiting through the upper airways. (9, 10) The gas exchange between the alveolar air and the blood present in the pulmonary capillaries surrounding the alveoli occurs through diffusion. (9, 11) The higher oxygen concentration in the alveoli causes this gas to diffuse into the pulmonary capillary, while the higher carbon dioxide concentration in capillary blood, causes carbon dioxide to diffuse into the alveoli. (9, 11) The alveolar-capillary membrane, where this exchange takes place, is extremely thin and delicate. After gas exchange, blood circulates through the body, transporting oxygen to be delivered to the cells, and capturing carbon dioxide produced by them ( Figure 1 ). (9) -Peak inspiratory flow: the maximum flow at which a set TV breath is delivered by the ventilator. The flow can be either fixed during the inspiration (e.g. a constant flow wave or ""squared"" on volume control ventilation mode) or variable (e.g. a decelerating flow wave on volume control ventilation mode, or pressure control ventilation and pressure support ventilation modes). -PEEP: a positive pressure maintained within the lungs (alveolar pressure) at the end of expiration, aimed at preventing alveolar closure at the end of expiration. -Fraction of inspired oxygen (FiO 2 ): the oxygen concentration in the inspired air. It may range from 0.21 to 1.0 (21% to 100%). -Sensitivity: reflects the patient's drive to start an inspiration. The detection may occur either by monitoring the pressure, the flow, or a neuronalrelated signal. -Inspiratory/expiratory (I:E) ratio: the ratio between the inspiratory time over the expiratory time. It may range according to the patient´s demand and mechanical ventilation objective, usually ranging between 1:1 and 1:3. -Ventilator breathing circuit: the set of tubes and connectors leaving the ventilator and taking the air to the patient, and from the patient to the expiratory valve. During positive pressure mechanical ventilation, a breath has four phases ( Figure 2 ): (14, 15) 1. Inspiratory phase: this phase begins once the flow valve is open and a given volume of air is provided, overcoming the elastic and resistive loads of the respiratory system, and the lungs are inflated. 2. Cycling: the change from the inspiratory to the expiratory phase (end of inspiration). 3. Expiratory phase: this phase begins once the expiratory valve is open, and the lungs are passively deflated. 4. Triggering: the change from the expiratory to the inspiratory phase (start of inspiration). As previously described, it is the change from the inspiratory to the expiratory phase. There are five types of cycling: (8, 12, 16) 1. Pressure cycling: the inspiration is interrupted when a given pressure threshold is reached. This is the During mechanical ventilation, the pressure gradients leading the air into the lungs are obtained through positive pressures applied to the opening of the airways, literally pushing air into the respiratory system. This mechanism is opposed to the physiological spontaneous breathing. Therefore, mechanical ventilation should be carefully adjusted to avoid harmful effects. In mechanical ventilation, there are specific concepts and terminologies. The commonly used terms are: (12, 13) -Tidal volume (TV): the volume of air delivered into the lungs with each breath by the mechanical ventilator. The volumes involved in ventilation are usually expressed in liters (L) or milliliters (mL). -Respiratory rate (RR): the number of breaths in one minute, which may be either determined by the ventilator, or by the patient, or both. -Minute ventilation (V E ): the total air volume mobilized in one minute. Therefore, V E is the product of TV times RR (V E = TV * RR). It can be calculated for either expiration or inspiration. -Peak pressure (P peak ): the highest level of pressure in the respiratory system generated by the TV during inhalation. The pressures involved in ventilation are usually expressed in centimeters of water (cmH 2 O). -Plateau pressure (P plateau ): the pressure of the pulmonary parenchyma distension. Its assessment requires a minimum 2-minutes respiratory pause (occlusion of the respiratory system at the end of the inspiration). Bird Mark 7 ventilator's mechanism of cycling. The inspiratory time and TV are variable, depending on pulmonary compliance and resistance. 2. Time cycling: the inspiration is interrupted when a predetermined time is reached. This mechanism is used on the pressure control ventilation (PCV) mode and volume control ventilation (VCV) mode with inspiratory pause. The inspiratory time is predetermined allowing no patient interaction. On PCV mode, the pressure is set, and the TV depends on pulmonary compliance and resistance. On VCV, the TV is set, and the pressure depends on pulmonary compliance and resistance. 3. Volume cycling: the inspiration is interrupted when a predetermined TV is reached. It is used on the VCV mode without inspiratory pause. The volume is ensured, regardless of pulmonary compliance and resistance, however, the pressures may be variable. 4. Flow cycling: the inspiration is interrupted when the flow decreases to predetermined levels (e.g.: 25% of the peak inspiratory flow). It is the mechanism used on the pressure support ventilation (PSV) mode. The TV is variable and depends on pulmonary compliance and resistance. 5. Neural cycling: occurs when the respiratory center in the brainstem interrupts inspiration. This mechanism is based on the use of the signal obtained from the electric activity of diaphragm (EAdi) for ventilation control, and is exclusive for the neurally adjusted ventilatory assist (NAVA) mode. The EAdi is a direct representation of the central respiratory drive and reflects the duration and intensity with which the patient wishes to ventilate. For the recording of the EAdi signal, the system uses electrodes embedded in the distal part of an esophageal catheter. During conventional mechanical ventilation, the ventilator triggering may be either programmed or by the patients' inspiratory effort. The four most common types of triggering are: (12, 15, 16) 1. Time triggering: occurs when the patient shows no inspiratory effort to trigger the ventilator, e.g., during deep sedation or neuromuscular blockade, or when there is no central nervous system drive. The ventilator triggering is based on the RR setting programmed, and the ventilation is delivered independent of the patient's spontaneous efforts. 2. Pressure triggering: when the system pressure becomes negative, indicating a patient's inspiratory effort, the flow valve is open, triggering an inspiratory phase. 3. Flow triggering: analogous to the pressure triggering, but in this case, the device can detect an inspiratory effort through a variation of flow in the system. 4. Neural triggering: occurs when the respiratory center in the brainstem triggers an inspiration. This triggering mechanism is based on EAdi signal during NAVA. As the triggering is synchronized with diaphragm excitation, it reduces the ventilator's response time, favoring the neuroventilatory coupling. Ventilation modes refers to the method of providing and controlling the breaths. Several modes are available, including synchronized intermittent mandatory ventilation (SIMV), but the basic ventilation modes in the context of alternative ventilators are: (14, 15, 17) 1. Continuous mandatory ventilation (CMV): the ventilator only provides mandatory breaths, using either volume (VCV) or pressure (PCV) control, based on the programmed RR. The ventilator settings are adjusted according to the selected mode. No sensitivity adjustments are made, because, in this mode, patient´s effort is not allowed to trigger the ventilator. Its main disadvantage is that, even though the patient exhibits respiratory effort, the ventilator does not allow triggering synchronized with the patient's effort. Patients under this condition can develop respiratory muscle fatigue and severe adverse events, such as barotrauma. In clinical practice, CMV mode is not used. 1.1. VCV: TV is fixed and set by the operator. The other adjustable parameters are PEEP, inspiratory pause, inspiratory flow or inspiratory time (which determines the flow), RR, and FiO 2 . 1.2. PCV: the inspiratory pressure or driving pressure (pressure difference above the PEEP) is fixed and set by the operator. The other adjustable parameters are PEEP, inspiratory time, rise time (determines the time at which the ventilator achieves a target airway pressure), RR and FiO 2 ( Figure 3 ). 2.1. VCV: as with CMV, the TV is set by the operator. In addition to other parameters, the sensitivity for triggering assisted breaths is adjustable ( Figure 4) . 2.2. PCV: as with CMV, the inspiratory pressure is set by the operator. In addition to other parameters, the sensitivity for triggering assisted breaths is adjustable ( Figure 5 ). 3. PSV: it is considered a spontaneous ventilation mode. Every breath is started and maintained by the patient, and the ventilator delivers support with the preset pressure (pressure difference above the PEEP) during the inspiratory phase. Cycling takes place when the flow decreases to a given percent of the peak inspiratory flow, which can be automatic (usually 25%), or in modern ventilators, it can be set (between 5% and 80%). When this flow decrease is detected, the ventilator interrupts the pressure support and allows cycling. The parameters to be set are the pressure support, rise time, PEEP, sensitivity, termination criterion (percent of peak inspiratory flow drop for cycling, when available), and FiO 2 . 2. Assist/control (A/C) ventilation: the ventilator provides mandatory and assisted breaths, using either volume (VCV) or pressure (PCV) control. If the patient shows inspiratory effort, the flow valve will be opened, allowing an assisted breath according to the ventilator´s settings. If the patient ceases inspiratory effort, then the ventilator assumes the RR as set and triggers mandatory ventilations. Some of the alternative ventilator projects do not take into account patients who show inspiratory effort (respiratory drive). Except for exceptional conditions, most of the patients in clinical practice present inspiratory effort, and commercial mechanical ventilators provide a synchronized ""reinforcement"" to spontaneous breaths. The time-to-response to inspiratory effort has a significant influence on patient/ventilator synchrony and intrinsically depends on the type of sensor employed. (18, 19) When the mechanical ventilator only provides CMV, patients with respiratory drive, either under mild or no sedation, or those in weaning from mechanical ventilation are at great risk of severe adverse events, such as ventilation-induced lung injury, barotrauma and worsing of gas exchange, making it difficult to regain autonomous breathing without devices. Therefore, alternative ventilators not allowing synchronous breathing should not be used in long term support of critically ill patients. During spontaneous breathing, inhaled air is humidified and heated during its passage through the airway, reaching the alveoli appropriately saturated with water and at body temperature (100% relative humidity and 37ºC). Patients undergoing invasive mechanical ventilation lose natural inhaled gas heating and humidification mechanisms, which increases the risk of complications such as changes in mucociliary clearance, secretions thickening, and tracheal tube clogging. These complications may lead to hypothermia, hypoxemia, atelectasis and lung inflammation. (20) (21) (22) Therefore, during invasive mechanical ventilation external humidifiers are needed to compensate for the lack of natural heating and humidification mechanisms when the upper airway is bypassed. (23, 24) Ventilators development teams should consider the types of humidification and heating devices to be used. Active heated humidifiers, placed in the inspiratory limb of the ventilator circuit, act by allowing air passage inside a heated water reservoir before traveling to the patient's airway. Condensation of water vapor inside the ventilator circuit requires frequent disconnections to remove the accumulated liquid, causing aerosols dissemination and environmental contamination. Hence, active heated humidifiers are not recommended for patients with COVID-19. The alternative ventilator should allow the connection to a heat and moisture exchanger (HME) passive humidification system to avoid such disconnections. To prevent cross-contamination, some components of alternative ventilators should be sterilizable, such as the expiratory valve, the inspiratory valve, and the flow sensor. This is fundamental for choosing components manufacture materials and their fixating methods, allowing removal for appropriate cleaning. It is also essential that the ventilators allow the connection to high-efficiency particulate arrestance (HEPA) filters at the ventilator circuit expiratory exit, to prevent aerosol dissemination and environmental contamination. Mechanical ventilation is an essential part of therapy in patients with severe respiratory failure. However, mechanical ventilation itself may cause alveolar injury or worsen an established disease. (25) The possible injury mechanisms include barotrauma, volutrauma, biotrauma, and atelectrauma. (26) Barotrauma indicates lung injury attributed to the application of high pressures, and its gross presentations (17) being pneumothorax, pneumomediastinum, subcutaneous emphysema, and gas embolism. (27) Volutrauma means lung injury induced by high tidal volumes. (28) Biotrauma occurs when excessive pressures and volumes are used, even if not enough to cause barotrauma and volutrauma, leading to proinflammatory cytokines release, leukocytes recruitment, and inflammation. (29) Prolonged and exacerbated biotrauma can lead to multiple organ dysfunction syndrome increasing the risk of death. (29) Atelectrauma is the lung injury caused by cyclic opening and closure of the alveoli. (30) A mathematical model suggests that the pressures acting at the interface between open alveoli and closed units can be as high as 140cmH 2 O, even with airway pressure of 30cmH 2 O. (31) The protective mechanical ventilation strategy is based on avoiding such phenomena. (32, 33) Barotrauma, volutrauma, and biotrauma can be avoided by ventilation with low volumes and pressures, appropriately individualized for each patient. Atelectrauma can be avoided with the use of PEEP, aiming to keep the alveoli open, also individualized to prevent untoward alveolar hyperdistention. Therefore, it is not advisable that experimental ventilators, particularly those based on AMBU, to use only standard mechanical safety valves (also known as 'pop-off valve') limiting P peak to 60cmH 2 O. Some of these valves can be accidentally locked, with no alert to the assisting team, which allows the pressure in the respiratory system to increase considerably, potentially causing severe lung injuries. On March 20 th , 2020, the Brazilian Health Regulatory Agency (Agência Nacional de Vigilância Sanitária -ANVISA) published the Resolution of the Collegiate Board (RDC) 349, defining criteria, and extraordinary and temporary procedures for dealing with approval requests of new mechanical ventilators identified as strategic, due to the international public health SARS-CoV-2 emergency. This RDC is valid for 180 days from its publication date. According to it, ventilators undergoing registry may, as an exception, present the certificates from the Medical Device Single Audit Program (MDSAP) or from the Brazilian Association of Technical Standards (Associação Brasileira de Normas Técnicas -ABNT) NBR ISO 13485:2016, (34) instead of Good Manufacturing Practices certificates, waiving the presentation of a Brazilian System of Compliance Assessment (Sistema Brasileiro de Avaliação da Conformidade -SBAC) certificate. This does not mean that ventilator manufacturers and/or developers are exempt to comply with the applicable regulations. Therefore, the development of ventilators requires specific tests to be performed according to ABNT's criteria. It is strongly recommended carefully reading of the ABNT NBR 60601-1, ABNT NBR ISO 80601-2-12 and ANVISA's Technical Orientative Note (Nota Técnica Orientativa) 001/2020 during the development of such devices (Table  1) . (35) (36) (37) Additionally, it is recommended to read the ABNT PR 1003:2020 that refer to mechanical ventilators for critical care, containing the applicable requirements and guidance related to safety and performance for the project and manufacturing purposes; its validity matches the RDC 349/2020. (38) This document describes the several types of mechanical ventilators (for critical care, transportation/emergency, sleep apnea, etc.) and makes clear their differences from the manual resuscitator (also known as AMBU) which has no controls, protective systems, or alarms. Besides, the valves system of the AMBU device was developed for short-term manual use, not appropriate for long-term therapy. It should be stressed that, although RDC 349/2020 exceptionally exempt the products of presenting an SBAC certification, this RDC does not exempt products' manufacturers, importers, and developers from the safety and efficacy requirements according to RDC 56/2001, in addition to other applicable regulations. Additional to the above described technical rules and regulatory requirements described below, in April 2020 the Brazilian Association of Intensive Medicine (Associação de Medicina Intensiva Brasileira -AMIB) published a technical medical note regarding the features of mechanical ventilators used in COVID-19 patients. (39) In this note, AMIB emphasizes that the requirements were decided for an exceptional time due to the large number of patients requiring mechanical ventilation, and should be met regardless of the type of ventilator. (39) The minimum requirements are shown in table 1. Another challenge for developers is the lack of knowledge regarding the minimum test requirements for early research and development project phases. Currently, creative procedures and agile methodologies are widespread in product development processes, aiming to speed-up the initial prototype validation. It is important to emphasize that the results from this phase tests should be part of the required regulatory documents to start the ANVISA registration procedure and is fundamental that these tests comply with the current regulations. The developing ventilator should undergo several tests designed to check the basic safety and performance requirements. In addition to operation and safety-related tests, the performance should be tested in a lung simulator, with different load adjustments, allowing the simulation of close to real-life lung features, with different capacities, resistances, and compliances. The lung testing device provided with commercial mechanical ventilators is only suitable for bedside ventilator testing, before its connection to the patient. This lung testing device is not appropriate to assess performance related to volume, flow, pressures, and inspiratory and expiratory times. This is one of the issues for the development of alternative ventilators, as the lung simulator algorithm should be adaptative, to provide correct ventilation in real-life lungs, which, in addition to their dynamically changing characteristics of compliance and resistance during the different breaths, may also show a non-linear behavior. Fundamentally, all tested variables (TV, respiratory flow, inspiratory pressure, P plateau , PEEP, inspiratory time, expiratory time) are recorded for the different evaluated scenarios. Also, the FiO 2 provided by the gas mixer should be measured. After validation of minimum requirements according to the safety and performance technical rules, the next step refers to animal model testing. Autopsies of patients VCV and/or PCV modes Driving pressure control (above PEEP) on PCV mode (5cmH2O -30cmH2O) and inspired tidal volume on VCV mode (50mL -700mL) FiO2 control (21% -100%) PEEP (0cmH2O -20cmH2O) Inspiratory time control (PCV mode) in seconds (0.3 to 2.0 seconds) and inspiratory flow (on VCV mode) up to 70 L/minute Respiratory rate control (8rpm a 40rpm) Airway pressure measurement (analogical or digital manometer)* Expired tidal volume measurement, whenever possible; Maximal airway pressure alarm*, leakage and gas source failure High-capacity HEPA filter attachment possibility (N99 or N100) in the expiratory exit If possible, have an at least 2 hours capacity battery ABNT -Associação Brasileira de Normas Técnicas; AMIB -Associação de Medicina Intensiva Brasileira; PEEP -positive end-expiratory pressure; VBS -ventilator breathing system; VCV -volume control ventilation; PCVpressure control ventilation; FiO2 -fraction of inspired oxygen; HEPA -high efficiency particulate arrestance. *The item ""Airway pressure measurement (analogical or digital manometer) allows the use of an analogical manometer, however this has an impact on the requirements for the item ""Maximal airway pressure alarm, leakage and gas source failure"" on the use of a maximal airway pressure alarm. The use of a pressure alarm would be hardly possible if an analogical manometer is adopted, as it is much more feasible by means of a digital manometer. that have died from COVID-19 evidenced diffuse alveolar injury, characterized by the alveolar formation of hyaline membrane, inflammatory cell accumulation, interstitial congestion due to edema, and fibroblast proliferation in organization phase. (40, 41) Preclinical animal tests using healthy lungs are not likely to reflect the behavior seen in patient's lungs with SARS-CoV-2 infection while testing experimental ventilators. Therefore, it is fundamental that tests are conducted in validated acute lung injury animal models. There are several animal models described in the literature. (42, 43) The majority is based on the clinical disorders associated with acute respiratory distress syndrome (ARDS) in humans. (43) Unfortunately, no animal model of ARDS can accurately replicate the complex pathophysiological changes seen in patients with such disease. (44) Models presenting good reproducibility are based in porcine and ovine (43, 45) after lipopolysaccharide (LPS) intravenous injections, (45) total surfactant lavage followed by aggressive mechanical ventilation (using high volumes and pressures), (46) intravenous oleic acid injection, (45) or intratracheal hydrochloric acid administration. (47) It is expected that alternative ventilators provide appropriate PEEP levels to recruit collapsed alveoli, adequate tidal volumes and inspiratory pressures to protect the lungs from worsening the established injuries, and to be able to maintain satisfactory gas exchange during longterm mechanical ventilation. It is of upmost importance that all preclinical animal tests should document properly the adjusted ventilatory settings, vital signs, and arterial gasometry results, to set up a dossier for ANVISA registration. Additionally, it should be stressed that every research should be conducted under the appropriate regulations involving the care of the test animals and mandatory authorization from an Animal Research Ethics Committee should be granted. A clinical trial is a systematic investigation to test a hypothesis, involving human participants. Different clinical trials methodologies can be used. Observational studies assess patients under routine medical care, while clinical trials test an intervention and evaluate the intervention's safety and/or effectiveness. Each clinical trial has a Clinical Investigation Plan (more details can be found in the session 'Clinical trial dossier submission'). Before starting the clinical trial, the research protocol should be previously approved by the Research Ethics Committee, and in the case of multicenter clinical trials, by the National Research Ethics Committee (Comissão Nacional de Ética em Pesquisa -CONEP), submitted via Plataforma Brasil (an electronic platform for Research Ethics submission in Brazil). (48) Every participant should sign an Informed Consent Form before any study-related procedure is conducted. Any clinical trial should rigorously comply with the Good Clinical Practices. (49) The health-related regulatory process for medical devices aims the application of technical and regulatory requirements for the entire product life-cycle, either preor postmarketing. Therefore, the regulation starts from checking the good manufacturing practices requirements, from its concept, project verification and validation stages, product riskmanagement, clinical validation, trackability, preventive and corrective technical assistance, technovigilance, deactivation, and disposal. All procedures cited above are organic and receive constant feedback from those accountable for the device's development, manufacturing, and availability to the enduser. Legally, the company's Legal Responsible and the Technical Responsible are accountable for compliance with technical and legal requirements. The premarketing stages chronology involving submission requirements to ANVISA are displayed in figure 6. Extraordinary and temporary administrative acts related to the COVID-19 pandemics are highlighted. Before registering any device at ANVISA, the responsible company should be regularized at the local Sanitary Surveillance (VISA), which may conduct inspections to check on technical and operational aspects and issue a business license, and at ANVISA to obtain a Company Operating Authorization (Autorização de Funcionamento de Empresa -AFE) for manufacturing healthcare products, under the RDC 16/2014. In addition, for companies manufacturing high-and maximal-risk devices (classes III and IV), RDC 15/2014 requires the obtention of a Good Manufacturing Practices Certificate (Certificação de Boas Práticas de Fabricação -CBPF) issued by ANVISA. RDC 185/2001 is related to medical device registration, including risk III class, as critical care and emergency and transport ventilators are rated. Registration of these devices should be requested to ANVISA with the documents and information required in RDC 185/2001. ANVISA's technical staff evaluates the process and decides on approval; when necessary they can request additional information and documents. For the agility of the process, it is important that the registration rules are appropriately interpreted. To render this process easier ANVISA provided a complete manual for medical device manufacturers and importers. (50) This manual has overall information on the submission process and a detailed description of the registration workflow, as briefly described below: (50) -Step 1 -manufacturer regularization at the VISA: obtaining the AFE from ANVISA, and an Operating License (OL) at the city or state VISA (also known as Operating Permit -OP). Also, the manufacturing company must comply with Good Manufacturing Practices. -Step 2 -device's sanitary identification: corresponds to its identification and categorization (according to the risk and objective). It is also checked if the device requires complementary certifications and reports, registration or notification, e.g. sanitary identification or economic information report. -Step 3 -request identification: if the device is required to be registered or to be notified to ANVISA. -Step 4 -electronic submission: the action that effectively starts the medical device registration process at ANVISA. -Step 5 -protocol submission. The manual provided by ANVISA regarding medical device registration processes (50) fails to include all the most recent RDCs related to medical devices and the SARS-CoV-2 virus pandemic. Below we present a brief description of each normative change issued since the pandemic started, in March 2020. -ANVISA RDC 346/2020: establishes criteria and extraordinary/temporary procedures for good manufacturing practices certification for registration issues and post-registration changes for pharmaceutic supplies, drug products, and healthcare products related to the international public health emergency due to the new coronavirus. -ANVISA RDC 349/2020: establishes extraordinary and temporary procedures for dealing with personal protective equipment  It is important to highlight that, in addition to the ANVISA regularization of the medical device, a clinical assessment is required, using clinical trials in cases when the used technology or device purpose is innovative. Innovative is considered, e.g. the device is adapted to add a new not previously existing function or inclusion of a new indication for a previously marketed device. In April 2020, the RDC 375/2020 was issued regarding the submission of clinical trials designed to validate class III and IV medical devices. Those class of devices were identified as health services priority related to the international public health emergency related to the SARS-CoV-2 virus. Clinical trials meeting these criteria can be submitted as a clinical trial notification. This clinical trial notification should comprise the following documents: (51) a ) Completed presentation form -available at the ANVISA website. ( trial site submitting the protocol. ANVISA should provide previous clinical trial consent, considering that it will serve for future registration of a medical device or changing the originally registered clinical indications. Therefore, ANVISA should assess the trial methodological issues, regarding the ability to support safety and efficacy evidence as well as Good Clinical Practice issues related to the clinical trial. The clinical trial submission flow should follow the sequence displayed in figure 7 . (52) It should be stressed that, regarding the COVID-19 pandemic, changes may occur in these regulatory settings, and updated information is available on the ANVISA website.  EA Suzumura wrote the original manuscript with contributions from AD Zazula, HT Moriya, CQA Fais, and AL Alvarado. All the authors provided substantial intellectual contributions to the manuscript. All the authors have revised and approved the final manuscript. A pandemia por COVID-19 tem deixado os gestores, os profissionais de saúde e a população preocupados com a potencial escassez de ventiladores pulmonares para suporte de pacientes graves. No Brasil, há diversas iniciativas com o intuito de produzir ventiladores alternativos para ajudar a suprir essa demanda. Para auxiliar as equipes que atuam nessas iniciativas, são expostos alguns conceitos básicos sobre fisiologia e mecânica respiratória, os termos comumente utilizados no contexto da ventilação mecânica, as fases do ciclo ventilatório, as diferenças entre disparo e ciclagem, os modos ventilatórios básicos e outros aspectos relevantes, como mecanismos de lesão pulmonar induzida pela ventilação mecânica, pacientes com drive respiratório, necessidade de umidificação de vias aéreas, risco de contaminação cruzada e disseminação de aerossóis. Após a fase de desenvolvimento de protótipo, são necessários testes pré-RESUMO Descritores: Ventiladores mecânicos; Respiração artificial; Respiração com pressão positiva; COVID-19; Síndrome respiratória aguda grave; Engenharia biomédica; Brasil clínicos de bancada e em modelos animais, a fim de determinar a segurança e o desempenho dos equipamentos, seguindo requisitos técnicos mínimos exigidos. Após, é imprescindível passar pelo processo regulatório exigido pela Agência Nacional de Vigilância Sanitária (ANVISA). A empresa responsável pela fabricação do equipamento deve estar regularizada junto à ANVISA, que também deve ser notificada da condução dos testes clínicos em humanos, seguindo protocolo de pesquisa aprovado pelo Comitê de Ética em Pesquisa. O registro do ventilador junto à ANVISA deve ser acompanhado de um dossiê, composto por documentos e informações detalhadas neste artigo, que não tem o propósito de esgotar o assunto, mas de nortear os procedimentos necessários.@story_separate@Mechanical ventilation is much more than just pumping air into patients' lungs. Several precise and simultaneous adjustments are required, on the airway pressures, tidal volume, the fraction of inspired oxygen, respiratory rate, I:E ratio, among others. Developing a low-cost and quick manufacturing ventilator able to meet these requirements is not an easy task. Medical device development and research require scientifically validated methods to be followed. The challenges are increased by the regulatory requirements. A genuine effort from the regulatory authorities is in place to reduce the requirements and speed up the evaluation process, however, the safety exigences can't be waived in the best interest of the population. However, managers, front line healthcare professionals and the population are confident that some (or several) of the ongoing projects will meet the required technical specifications and will, soon, contribute to avoid that more lives are lost in this pandemic.","The COVID-19 pandemic has brought concerns to managers, healthcare professionals, and the general population related to the potential mechanical ventilators’ shortage for severely ill patients. In Brazil, there are several initiatives aimed at producing alternative ventilators to cover this gap. To assist the teams that work in these initiatives, we provide a discussion of some basic concepts on physiology and respiratory mechanics, commonly used mechanical ventilation terms, the differences between triggering and cycling, the basic ventilation modes and other relevant aspects, such as mechanisms of ventilator-induced lung injury, respiratory drive, airway heating and humidification, cross-contamination risks, and aerosol dissemination. After the prototype development phase, preclinical bench-tests and animal model trials are needed to determine the safety and performance of the ventilator, following the minimum technical requirements. Next, it is mandatory going through the regulatory procedures as required by the Brazilian Health Regulatory Agency (Agência Nacional de Vigilância Sanitária - ANVISA). The manufacturing company should be appropriately registered by ANVISA, which also must be notified about the conduction of clinical trials, following the research protocol approval by the Research Ethics Committee. The registration requisition of the ventilator with ANVISA should include a dossier containing the information described in this paper, which is not intended to cover all related matters but to provide guidance on the required procedures."
"Since the beginning of the 21st century, organizations must respond flexibly, rapidly, and dynamically to the demands of ever-changing customers, market opportunities, and external threats. This context arose with the increase in technology that allowed people and organizations to seek improvements that fit their needs. In this context, traditional forms of software development can present some disadvantages, such as reduced integration, delays in deliveries, and problems in sharing information among stakeholders [25] . These problems can compromise the quality of the process as a whole. Then, how to develop software with quality and rapid deliveries in an ever-changing environment? This paper presents the NatVi Framework for Agile, Service-Oriented Architecture (SOA), and Quality Assurance (QA). It is a study for a software engineering solution to deal with the presented context, using concepts in terms of architectural design and development methods that are capable of maintaining or improving Quality Assurance (QA). The Framework was the result of background research, including our previous work, a Systematic Literature Review (SLR) [8] that identified articles dealing with SOA and Agile and the challenges inherent in this combination of solutions. The SLR identified papers that addressed solutions for problems that arise in the presented context, and this work took the shortcomings found in those solutions and incorporated the necessary QA concept. The Framework construction used the solutions found in the background research. It tried to take advantage of the strength and address the weakness and gaps of those solutions. It also sought to deal with the trade-offs among the central concepts identified in the studies, e.g., degree of formality, documentation, and planning. The NatVi Framework does not merely combine agile development and SOA. It works as a single life-cycle with the necessary phases to implement services solutions in an Agile way. The NatVi Framework addresses in its four layers and 13 phases the values and principles of Agile and SOA. Likewise, it allows people to execute a well-planned and structured process that continually seeks to QA. The people involved in the development process perform specific roles in each phase, e.g., sponsor, end-user, developer, QA manager, and tester. Each phase of the Framework presents inputs, tasks, and outputs. As a Framework, it is not exhaustive, and its concepts, inputs, tasks, and outputs can change and adapt to each environment. From the requirements' elicitation to the system monitoring, the Framework seeks to balance the principles and values of the three central concepts, Agile, SOA, and QA, putting them together in a well-constructed process. This balance sometimes seems to break some principles and values in some phases, e.g., initial phases can present more planning and documentation than in a normal Agile process, but, less documentation than an SOA process. This apparent breakdown of concepts is just enough to maintain balance, trying not to lose the essence of the concepts. The Framework appears to be suitable for environments that experience rapid changes in business needs, reflecting the requirements of software systems. In a governmental setting, the Framework can help adapt systems to changes in the law and the interest of the citizen. In a service industry scenario, it can help organizations find end-user satisfaction. Nevertheless, all scenarios in which business changes occur quickly can exploit the use of the Framework. As a theoretical work, the Framework requires validation. To that end, a case study will be carried out and presented in future work. The evolution of QA in the use of the Framework will be evaluated through the monitoring of error metrics in the source code, the involvement of the development team and customer satisfaction with the new development process. Section 2 provides an overview of Agile, SOA, and QA. Section 3 discusses some previous solutions that helped to build the NatVi Framework. Section 4 presents the NatVi Framework, and Sect. 5 presents the conclusions.@story_separate@This section presents some background about Service-Oriented Architecture (SOA), Agile Software Development, and Quality Assurance (QA). The values and principles introduced are suitable for understanding these concepts and how they are essential in the Framework, but they are not exhaustible. Building new functionalities and reusing existing systems, all integrated, can be an issue. This context worsens by adding the expectation of a large number of stakeholders with different goals that can change at any given moment. In this context, at the end of the 20th century, the concept of services appeared. A new way of thinking about systems architecture, breaking complex structures into small parts of functionality [4, 11, 24] . Service-Oriented Architecture (SOA) arose as a solution to handle this context, integrating and uniforming old and new solutions, and simplifying business processes. SOA is an architectural pattern that presents service providers and service consumers, kept together by elements of Information Technology (IT) [4, 5, 11] . It aims to enhance the efficiency, agility, and productivity of an enterprise [1] . To achieve these aims, it works in eight services principles [13] : Standardized Service Contract; Service Loose Coupling; Service Abstraction; Service Reusability; Service Autonomy; Service Statelessness; Service Discoverability; and Service Composability. SOA also can be seen as a system development process [24, 29] . However, the complexity of implementing SOA presents some challenges. Its flexibility represents a complex integration of many different resources, which can exhibit a large number of critical interdependencies across many of the resources. This complexity can also bring issues to its deployment, mainly in reliability [5, 30] . Stakeholder interests also may be an issue while they can demand conflicting requirements that have to be rightly managed [7] . Performance is also a concern. If the processes present an undetermined number of requests, SOA may generate bottlenecks in peaks of a request [27] . Service providers may present various issues in an SOA environment, e.g., reusability, interoperability, and scalability [6, 30] . In the 1990s and early 2000s, many writers were looking for a lightweight method for software development. They wrote articles where they were concerned with problems in the software development processes that were heavy and oriented to vast documentation [7] . These concerns led to Agile Software Development that came in the form of the Agile Manifesto, published in 2001 [2] . According to [36] , 'Agile methods are the solution to the problems that can be caused by traditional methodologies'. This software development method is rapid, flexible, and stakeholders' goalsdriven. The Agile manifesto [2] values are as follows: '(1) individuals and interactions over processes and tools, (2) working software over comprehensive documentation, (3) customer collaboration over contract negotiation and (4) responding to changes over following a plan'. 'That is, while there is value in the items on the right, we value the items on the left more'. This software development method is rapid, flexible, and stakeholders' goalsdriven. Agile methods value the integration and collaboration between selforganized and multifunctional teams. It also values informal communication among stakeholders. It also emphasizes tacit knowledge over explicit knowledge and promotes the teams' improvement in the software process [3] . Among the main advantages of Agile are quick responses to changes in requirements, customer satisfaction with the fast and continuous delivery of useful software [18] , lower error rates, and a shorter development cycle [36] . It has to be iterative, incremental, cooperative, and adaptable to changes in environments and requirements. To achieve the necessary flexibility, Agile is formal documentation-less and uses informal communication [1, 9, 14] . These properties of Agile also can be seen as a disadvantage of the method regarding the bureaucratic organization, critical systems, and maintainability [31] . Another disadvantage is the difficulty of assessing the effort required at the beginning of the software development life cycle [18] . Regardless of this disadvantage, Agile Software Development makes use of good programming practices that are considered one of the pillars of quality assurance [15] . Some advocate that agile development is oriented to a higher quality compared to plan-driven development [26] . High-quality delivery is expected in any software engineering process, agile or otherwise, so Quality Assurance (QA) is itself a necessity. Software quality is one critical criterion used to measure the success of a software development project [22] . QA, for software engineering, means that the requirements need to be met, and the final product needs to be appropriate for the use [32] . QA is a process to guarantee quality in a software development process, which leads to quality in the final product, and it is also a process to evaluate the development itself [16, 20, 32] . There are many descriptions in the literature of QA [26] and some other concepts that share the same description, such as Software Assurance [33] , Quality Attributes [12] , and Software Quality [23] . In a broad concept, QA represents the degree to which a product or process meets established requirements. This definition may refer to the ability to meet functional requirements, but it may also refer to non-functional requirements, i.e., performance, reliability, availability, portability, and maintainability [26] . Hence, Quality Assurance (QA) is at the same time a planned and systematic process to provide adequate confidence that software conforms to requirements [16] and a set of activities designed to evaluate the process by which the software is developed. As a planned and systematic process, it is possible to list from literature some activities that help ensure the quality: demonstration of the software, test-driven development, automated acceptance testing, daily builds with Testing, pair programming, coding standard, refactoring, peer review, defect analysis, defect reporting, unit testing, test automation, continuous integration, testing level, defect prevention and static analysis [17, 34] . For each activity, a set of features must be defined according to each environment: guidelines, benefits, processes, best practices, templates, and customization [17] . QA depends upon the degree to which those established requirements accurately represent stakeholder needs, wants, and expectations [20] . It depends on a good plan and documentation. It depends on a good process and a proper evaluation of this process. In other words, the entire process of software engineering is committed to QA [26] . While Service-Oriented Architecture (SOA) and Quality Assurance (QA) follow more traditional concepts, e.g., comprehensive documentation and up-front design, Agile Software Development strongly refutes these concepts and aims a light way of developing software, mainly valuing people, arising a trade-off. Once Agile Software Development is a trend, the papers presented in this subsection aim to find a solution to this trade-off. Our previous work, the Systematic Literature Review (SLR) in [8] , presents papers that deal with Agile and SOA, and in [15] , the authors show papers that deal with Agile and QA. In the preparation of this paper, the researchers did not find an article that deals straightforwardly with the three concepts together, Agile, SOA, and QA. [28] exhibits a Service-Oriented Framework of Interface Prototype Driven Development that deals rapidly with changes in requirements and improves the quality and efficiency of analysis and design for a data-centered application system. [35] shows the 'SOA based Model-driven Rapid Development Architecture -SMRDA'. The authors performed a combination of SOA, Model-driven Architecture (MDA), proposed by Object Management Group (OMG), and agile methods. They concluded that this combination is the ""main trend of modern software development in enterprise applications. The key of which is modeling services correctly and applying agile development techniques"". In [9] , authors present a platform called FraSCAti to be used with their proposed framework, the FASOAD, a Framework for Agile Service-Oriented Architectures Development. [10] shows the Agile and Service-Oriented Software Development Method (ASOSDeM) that aims to overcome web-based development projects' complexity by dividing it into sub-projects to allow the application of agile methods' practices. It defines how a self-organizing team should execute an SOA agile development project. It describes the concepts that may be used in an SOA project such as 'Artifacts', 'Tasks', and 'Roles'. It addresses: 'development, analysis, architecture elaboration, granularity identification, components assembling, deployment, integration tests, and business processes assembling'. From [15] , is highlighted some papers that deal with Agile and QA. [18] presents the scope of the ISO 9126 quality attributes (Correctness, Maintainability, variability, efficiency, availability, portability, testability, and reliability)through agile practices derived from XP. In [19] , the authors present Agile practices responsible for quality assurance: test-driven development, acceptance testing, code inspection, pair programming, refactoring, continuous integration, collaborative work, system metaphor, continuous feedback, and coding best practices. [21] presents 3C approach, in which the authors achieved quality assurance from an improvement of the practice of continuous integration, continuous measurement, and continuous improvement. [22] the author identifies some practices that promote quality assurance in the Scrum framework and continuous collaboration, integration testing, continuous feedback and development, knowledge sharing, retrospective, and daily meetings. The Background, Sect. 2, presents some paper for dealing with Agile, Service-Oriented Architecture (SOA), and Quality Assurance (QA). Nevertheless, none of the work presents a substantial solution for the entire Software Development Life-Cycle (SDLC) with these three concepts. Few of the papers deal directly with the principles of SOA and Agile. None of them present a complete process for QA. The NatVi Framework proposes to address these gaps not covered by the other solutions. In this way, the NatVi Framework came to seek to improve Quality Assurance (QA) while dealing with Service-Oriented Architecture (SOA) and Agile during the Software Development Life-Cycle. As a framework, it is not exhaustive and presents a flexible set of phases. In this way, people responsible for a new project need to assess the phases in front of their reality and decide whether to use it. Furthermore, they can decide to customize some phase or create a new one and aggregate it to the Framework. The NatVi Framework is composed of four layers with respective phases. The 'Stakeholder Support' is an extra layer that permeates all other layers and exists only to remember that the integration of each system stakeholders, whether clients, users and IT teams is critical to the success of the software project. The layers' position indicates they work sequentially, but the 'Iteration between layers' is indicated in the point of contact. In this way, if the development is facing problems in some layers, the work can jump virtually to any other layer to fix it (Fig. 1) . The Framework seeks to keep present the essential concepts of Agile, SOA, and QA throughout the process. From Agile, the values and principles (background Sect. 2.2). From SOA, the eight principles (background Sect. 2.1). From QA, the planning, and the process execution. Despite this, in some phases, some are highlighted, indicating that special attention is needed. Phases of the Framework. As a framework, each iteration may need adjustments depending on the environment. Therefore, the terms used here are examples. In other environments, these terms may change and may extend. At each phase, a set of roles should be present, people who should be involved with the tasks of the phase. Each environment will be its own set of roles, defined by the environment according to their needs and organizational structure. Table 1 depicts examples of leading roles for each phase. Sponsor X X Client X X X X X X End-user X X X X X Business analyst X X X X X X Software designer X X X X Database administrator X Developer X X X X X Tester X X IT operator X X X QA manager X X X X X X X X X X Table 2 depicts examples of a tool-set that may be present in each phase of the Framework. These tools can be electronic or not and include techniques that assist the execution, control, and documentation of tasks or the whole process. The following subsections describe each phase of the Framework in terms of name, intention, inputs, tasks, and outputs. The items presented here are not exhaustive. For each environment, people responsible for roles may add new items and may exclude others, adapting these concepts to their reality. Inputs are all available material that can help people develop the tasks of the phase, e.g., the results of the previous phases, documents by the organization, and even outside documents. Tasks are specific actions that the roles need to do to achieve the phase objective. Outputs are the material result of the phase, e.g., report of elicited requirements, use cases, test cases, records in a control tool, developed code committed, ready system. Project management tools X X X X X X X X X X X X Version control tool X X X X X X X X X X X X Communication tool X X X X X X X X X X X X Design tool X X X X X X Test tool X X X X X X X SOA management tools X X X SOA building tool X X X Development tool X X Continuous integration tool X X X Documenting tool X X X X X X X X X X X X Phase 1 -Initiate Project. Intention: This phase is composed of a set of tasks that allow for a general understanding of the project. At this stage, people document the origin of the demand and its main concerns. The resulting document does not need a more precise technical definition. Stakeholders and the technical team must carry out this phase in conjunction. The phase can follow a standardized protocol. Here, stakeholders can be free to expose all their thoughts and concerns without much interference from the technical team. The technical team will make technical adjustments to subsequent tasks. In this phase, the Agile values 'Individuals and interactions' and 'Customer collaboration' should receive special attention to guarantee QA since the beginning of the project. Inputs: Relation of business goals. The organization's strategic plan. Industry contracts and regulations. Deadlines and available budget. Relation of stakeholders who will participate in the project. Tasks: Formalize demand in a control system. Create initial documentation. Register the project in a specific project control tool. Grant access to project information to all stakeholders. Outputs: The formalized project. The documentation with the key characteristics recorded, e.g., people involved, schedules, budget, the main concern with business objectives, problems to be resolved, expectations about the possible new system. Possible, because the solution may be other than developing a new system. In this phase is the time to decide whether to make a whole new system, adapt the functionality of an existing system, or purchase a Commercial off-the-shelf (COTS). The formalities of this document depend on the level of formalization required by the organization. It can be a record in an information system, or it can be a record in the project management tool 'read-me' file. In this phase, people who perform roles elicit requirements and break them down into concrete business needs, meaning that they identify the problems and concerns of the clients and end-users. The main concern of people is to identify the problem domain and not the solution to those problems. They need to identify and extract information from the business needs and interpret, analyze, validate, and gather together to compose the services. In this phase, people who perform roles identify all stakeholders and technical teams. They also initiate a more precise technical definition of the requirements. All stakeholders need to understand the goals and divide them as much as possible. Each of these parts defines a service. Thus, each service represents a specific functionality that explicitly maps to a step in a business process. Inputs: The outputs generated in Phase 1 -Initiate project. Business process mapping, if any, or detailed description of the phases of how the work is done. Other documents that support business objectives. Some information from Phase 1 -Initiate project is vital for all next phases and should appear as input for all of them, e.g., schedule and budget. Tasks: Elicit requirements for services. Design the initial tests. Link requirements, services, and tests. Outputs: Relation of requirements. Record of backlogs, stories, and use cases. Draft of the architectural views of the services. Draft of the case tests. Phase 3 -High-Level Architecture. Intention: In this phase, people who perform roles elaborate on high-level architecture as SOA as an architectural pattern does not have a fixed way to be implemented. In this phase, if SOA is new, the technical team should elaborate on the whole architecture. If SOA is a well-established environment, the work here will probably be simplified, and the technical team should do only a few setups. Many of the SOA principles are planed in this phase, but two principles need more attention, 'Service Reusability' and 'Service Discoverability' (see background Sect. 2.1 about SOA). Because, if other services already achieve the goal of some new service, these other services must be discovered and reused, even with some level of adaptation. Inputs: The outputs generated in previous phases. Information from Phase 2 -High-level requirements and Service definition, e.g., drafts of the architectural views and other infrastructure information, including information about external clouds, if they exist. Tasks: Design the architecture for building the SOA. If SOA already exists, design the customization for the project. Decide between developing a new service or using or adapting an existing service. Outputs: The built architecture. The documentation registered in the project management tool. Intention: In this phase, people design the services (Consumer and Provider) and address the solutions to the SOA issues. This phase represents the beginning of the first loop, or iteration, in NatVi Framework. This iteration will repeat until the service design is considered good enough to go to the next iteration, the construction layer. Even if the process is already in the next layers, people can assess the requirements at any time, and the process can return to that layer. All the eight SOA principles must be addressed in this phase (see background Sect. 2.1), but if someone needs to prioritize one, it will be 'Standardized Service Contract', because this principle can drive the final service to high-level quality, mainly in the security aspect. Inputs: The outputs generated in previous phases, mainly the high-level requirements and the drafts of the architectural views from Phase 2 -High-level requirements and Service definition and the blueprint of the high-level architecture from Phase 3 -High-level Architecture. Tasks: Refine elicited requirements. Identify the business objectives in their smallest parts. Design the services. Find alternative treatments if a principle cannot be achieved. Outputs: All the necessary documentation to built the services, e.g., service design and refined requirements. Information about architecture customization needed. All documentation recorded in a specific tool. Phase 5 -Architecture Customization. Intention: At this phase, people who perform roles evaluate adjustments in the architecture. Because SOA is a design pattern, not a whole architecture, people may need to do some customization in architecture, e.g., a new interface or a new term in the standardized service contract. Inputs: The outputs generated in previous phases, mainly the documentation of service design from Phase 4 -Service design and the blueprint of the high-level architecture from Phase 3 -High-level Architecture. Tasks: Refine the architecture. Update the architecture documentation. Implement the architectural changes. Outputs: The built architecture and the documentation registered in the project management tool. Intention: At this phase, before the Construction phase (code), people create the tests. The technical team uses information gathered in the previous phases to create these tests. Creating the test first can help developers make the code closer to service needs, that is, close the business goals. Inputs: The outputs generated in previous phases, mainly the high-level requirements, use cases, and draft of the case tests from Phase 2 -High-level requirements and Service definition. The refined requirements and the documentation of the design services from Phase 4 -Service design. Tasks: Refine the drafts of the case tests. Elaborate the unit tests, integration tests, system tests, and acceptance tests. Select a metric to guarantee the QA. Document and register the tests in a specific tool. Outputs: Tests designed and registered in specific tools and tests documented in the project management tool. Intention: This phase is the bottom of the first loop, the end of the first iteration and a control point. After the previous phases, the technical team has more understanding of the needs of the services. The clients, end-users, and the technical team evaluate the elicited requirements and all the done work until here. If they decide to make changes, they restart the loop. Otherwise, it is time to pass to the construction layer. Tasks: Evaluate all the work done. Make decisions and document what will be done. Outputs: A document with the assessment of the work. Record of the decision made. This phase is where the developers, technical team members, make the code itself, that is, build the service. All the artifacts generated in the previous iteration 'Analysis and design' are used in this phase as input to create the service. This 'Construction' layer phase is the first one of a new loop or iteration. This iteration will repeat until the service passes acceptance tests, made by clients and users. Inputs: The outputs generated in previous phases, mainly the documentation from Phase 4 -Service design, e.g., service design, and refined requirements. The test design from Phase 6 -Test design (test first). Tasks: Build the code of the services. Build the code of the unit tests. Evaluate unit tests. Daily deploy code, i.e., commit, pushing, build, and release in a development environment. Document and register the code in a specific tool. Outputs: the source code and test code recorded in a version control tool, available to the pipeline of continuous integration tools. Phase 9 -Continuous Integration and Compose Service. Intention: As already seen, the phases 8, 9, and 10 of the construction layer work almost at the same time. In this phase, people set tools that are responsible for continuous integration. These tools receive the new code, compile it, test it, e.g., unit test and integration test, build the software, and release in a development environment. In this phase, the services are composed according to the designed architecture and the customization, if one. Composing services means putting the service in the right place into the SOA and accessible for the other services, even the external ones. Inputs: The outputs generated in previous phases, mainly source code and the test code from Phase 8 -Code and test design from Phase 6 -Test design (test first). Tasks: Set the continuous integration tools for build and test. Release the software in a testing/staging environment. Evaluate the tests. Give feedback to developers. Document the results. Outputs: The services composed and released in a testing/staging environment. Testing report result. Phase 10 -Test. Intention: This phase in the framework is more about giving special attention to the tests. As seen in the previous phases, the concern about tests is constant, before and after coding. So this test phase, in particular, has an academic meaning. In fact, in the real world, these three phases, Phase 8 -Code, Phase 9 -Continuous integration and compose service and Phase 10 -Test are implemented at the same time, in a constant iteration. At these three phases, the testers make and test the services based on the tests designed in Phase 6 -Test design (test first). The developers make the unit tests, and they execute these tests in their machine to identify the errors of the first before committing to the code. The unit tests will be executed again, in Phase 9 -Continuous integration and compose service. The testers, QA manager, and operator still make the integration and system tests in this phase. The operators' participation is fundamental because they can think in a set of services and consider the SOA. These tests will be executed only in Phase 9 -Continuous integration and compose service, where is expected a more significant number of errors than the unit tests. Phase 11 -Acceptance Test. Intention: The end of the loop or iteration of the construction layer. The services are already composed and tested by clients and end-users, according to the definitions of Phase 6 -Test design (test first). These tests can be done manually or using specific tools for this. The software released in a testing/staging environment. The acceptance tests design in Phase 6 -Test design (test first). Tasks: Evaluate the software against the acceptance tests. Give feedback to the technical team. Do formal approval or disapproval. Document the results and decisions. Outputs: Testing report result. Feedback report. Formal approval or disapproval for the services to go into production. Phase 12 -Production. Intention: In this phase, the services are finalized and approved. The built services are ready to release in a production environment, preferably in an automatic way. Inputs: The source code from Phase 8 -Code. The documentation of the architecture, from Phase 3 -High-level Architecture and Phase 5 -Architecture Customization. The formal approval from Phase 11 -Acceptance test. Tasks: Adjust the architecture. Set the production environment, Make automatic pipelines for production. Release services in the production environment. Document the activities. Outputs: The adjusted or the new SOA ready. the services composed and ready for a production environment. The documentation up to date. Phase 13 -Release. Intention: The service is released in the production environment to end-users. These two phases, 12 and 13, represented the framework's monitoring layer, where services are already in a production environment and need to be tracked to identify problems in their execution.@story_separate@This work carried out research to seek a solution to better deal with developing software with quality and rapid deliveries in a software environment that tries to follow business objectives in constant change. The solution appears in the form of a Framework that deals with Agile Software Development, Service-Oriented Architecture (SOA), and Quality Assurance (QA). The construction of the Framework made use of previous works found in background research and assessed the strengths, weaknesses, and gaps in the found solutions. The background research did not find studies that deal straight with the concepts of SOA, Agile, and QA together. However, it found studies that deal with these concepts in pairs: SOA and Agile, SOA and QA and Agile and QA. We made an effort to join those works made in pairs and fit our purpose, that is, a greater understanding of the three concepts working together. Background research also identified trade-offs that generally arise when working with these concepts, but also in pairs, mainly the principles and values of SOA and QA versus the principles and values of Agile. These trade-offs were also a concern in the Framework construction that tried to address them. The NatVi Framework presented in this paper is composed of four layers and 13 phases that combine in one life-cycle Agile, SOA, and QA. It concatenates each of its 13 phases through the outputs and inputs and defines specific roles and tasks for each phase. The fundamental of the Framework is continuously, in all phase, pay attention to values and principles of the three concepts. In conclusion, the NatVi Framework is suitable for building a well-structured process to deal with the software development scenario with quality and fast deliveries in an ever-changing environment. The process created from the Framework can act on essential factors for fast delivery with quality of the final product: sufficient initial plan; adequate software architecture; business objectives divided into small pieces and directed to functional parts of the software; iterative development capable of absorbing changes instantly; code and integration tests to find errors during development; monitoring to detect problems while running the system. However, there is a need to implement the Framework in real-world scenarios and gather empirical evidence to validate and improve on it. The authors are preparing a case study to expose the NatVi Framework to a real development environment, assessing its application, employing some measures before and after the use, and appraise the acceptance by stakeholders.","This research presents the NatVi Framework for Agile Software Development, Service-Oriented Architecture (SOA), and Quality Assurance (QA). The research took place in a context of profound and rapid changes in business environments that affect the software development environment. Our previous work did a Systematic Literature Review trying to find articles dealing with SOA and Agile and the challenges inherent in this combination of solutions. In order to build the Framework, this work took the shortcomings found in the solutions presented in the papers and further incorporated the necessary QA concept. In this context, the Framework attempts to provide an answer to how to develop software with quality and rapid deliveries in an ever-changing environment, where the traditional forms of software development may not handle it. Background research identified trade-offs among SOA, QA, and Agile, e.g., formality, documentation, and planning. The background research also identified strengths, weaknesses, and gaps in papers that addressed solutions to problems that arise in software development in the presented context. The results of the background research were assessed and exploited in Framework construction. In a single life-cycle, the NatVi Framework combines Agile, SOA, and QA and addresses the values and principles that guide them in 13 phases distributed in four layers. Each phase is carried out by people who perform specific roles, expressed in terms of inputs, tasks, and outputs. As a Framework, it is not exhaustive, and its main concepts may change and adapt to each environment. The Framework validation is underway, and a forthcoming paper will present the results."
"Continuous and long-term monitoring of the ozone concentration (О 3 ) in the near-surface atmosphere is performed in the pure (background) region of the central Russia over the past 12 years [1] . The monitoring station is arranged at the point with coordinates 56°13′ N, 51°4′ E at the Vyatskie Polyany, Kirov oblast. This place is removed from the nearest large cities for more than 100 km; there are no industrial enterprises. The station is equipped with instrumentation for automated monitoring of ozone concentration and meteoparameters. The devices are used, certificated for applications in high-accuracy monitoring and subjected to regular certification and calibration. To measure the О 3 content, a 3.02P-A chemiluminescent analyzer (OPTEK Company, Russia) is used. Analyzed atmospheric air was continuously pumped through a device measuring cell. The analyzer sensitivity was 1 μg/m 3 , the measurement periodicity was 1 min [2] . Ozone is formed in the surface atmosphere due to a cycle of photochemical reactions in the presence of precursor compounds, i.e., nitrogen oxides, carbon monoxide, and volatile hydrocarbons. The presence of ozone in the surface atmosphere indicates atmospheric air contamination by combustion products ejected by car and air transport, thermal power plants and other industrial sources, as well as by forest fires. Ozone in the surface atmosphere is an integral indicator of the ambient air quality. The tropospheric ozone formation rate increases with temperature and light and is maximum in the spring-summer period. Main features of seasonal and diurnal ozone variations caused by environmental temperature and humidity variability, solar illumination intensity and duration are studied in sufficient detail. Furthermore, the ozone content balance in the surface atmosphere is controlled by many other factors, among which are the air pollution, horizontal transfer and vertical transfer from upper layers, scavenging and dry deposition during the interaction with the ground surface, and others. The typical annual variation of the ozone concentration in the surface atmosphere is shown in Fig. 1 . Rapid and significant variations in the ozone concentration in the atmosphere, shaped as random noise in this figure, are caused by regular diurnal variations. There are easily seen a gradual increase in the ozone concentration in air in spring and high maximum levels in the summer months, reaching ~200 μg/m 3 . In autumn and in winter, when the solar illumination intensity decreases, the ozone concentration in the surface atmosphere decreases. The long-term continuous monitoring of the ozone concentration in the surface atmosphere in a relatively pure (background) region allows observation of long-term trends caused by an increase in the total atmospheric pollution and climatic changes (temperature increase, an increase in the frequency of heat waves and an increase in their amplitude).@story_separate@Our close attention was directed to an analysis of the ozone content in the surface atmosphere at the beginning of 2020, when the COVID-19 pandemic began all over the world, and the activity of road and aviation transport, as well as some industrial enterprises, was limited in many countries were published [3] [4] [5] [6] . In particular, it was fixed by satellite monitoring methods [7] . In this context, we performed a comparative analysis of the ozone concentration in the surface atmosphere in 2019 and 2020 at the Vyatskie Polyany station. Figure 2 shows the time variation of daily maximum 1-hour О 3 concentrations in January-May 2019 and 2020. We note that the observed rapid fluctuations of the data shown in Fig. 2 are caused by day to day variations of meteo parameters (temperature, illumination, humidity, and wind direction). We can see in Fig. 2 that the ozone concentrations observed in 2020 are lower than in 2019 over the entire observation period. A noticeable deviation begins as early at the end of January. In this case, the air temperature at the beginning of 2020 was on average slightly higher than in 2019, which would cause higher O 3 concentrations. In February, the disagreement of measured values gradually increases; at the beginning of March 2020, an abrupt decrease in the ozone content in the atmosphere is observed. This time point coincides with the introduction of ""high alert measures"" in Russia since March 5, 2020, associated with the COVID-19 pandemic, i.e., the first limitations for the activity of road and aviation transport. To improve the accuracy of the analysis of the observed phenomenon, we used time series processing based on the calculation of the hourly daily variations averaged over a month. In addition to a decrease in the random spread of observed data in amplitude due to averaging over a long time interval (month), this approach make it possible to retain information about diurnal dynamics and to separate contributions of Date photochemical mechanisms of ozone formation and production, prevailing in the daytime, and ozone transport and sink processes in the night. The data processing results are shown in Fig. 3 , are shown. As seen in Fig. 3 , the daily variations of 2019 obey the usual trend, i.e., a characteristic spring increase in the ozone concentration from month to month is observed. The maximum average values increase from 90 μg/m 3 in January to ~155 μg/m 3 in May. In 2020, anomalous dynamics is observed. For January 2020, the values are ~20% less than in 2019. In February, a still larger (by ~35%) difference between maxima of 2019 and 2020 is observed, which reaches ~50% in March.  G Despite the increase in the seasonal temperature, solar radiation, and daylight duration, a still larger deviation from the values of 2019 is observed. In April 2020, a maximum decrease in ozone concentrations is observed, i.e., the amplitude of the day maximum which is usually caused by air pollution by ozone precursors, becomes almost three times lower than in April 2019, and is ~60 μg/m 3 . In May 2020, the observed ozone concentrations also remain significantly (more than two times) lower than those fixed in 2019. Figure 4 compares of the diurnally acquired maxima (Fig. 4a ) and morning minima (Fig. 4b) for monthly daily variations shown in Fig. 3 . These values are denoted in the April section in Fig. 3 by arrows P and G, respectively. Furthermore, Fig. 4c shows the monthly averaged maximum temperature. The morning minimum G is defined by the regional background and depends on the total mass of ozone photochemically formed in the region on the eve, the intensity of scavenging and dry deposition of ozone and its transfer from neighboring regions. At constant deposition and transfer, the morning minimum will gradually decrease with decreasing air pollution leading to a decrease in the formed ozone mass. The amplitude of the daily acquired maximum is controlled by the local diurnal concentration of pollutants involved in the photochemical formation of ozone. As seen from the dynamics of daily variations, the locally acquired ozone rather rapidly decays at nightfall. Figure 4 shows that the dynamics of the ozone content generated in the daytime differs significantly in 2019 and 2020. In 2019, this value increases correlating with the temperature increase. In 2020, such a clear dependence is not observed. From January to April 2020, this value varies rather insignificantly. Fig. 4b , also differs significantly. In 2019, the seasonal variation of minimum ozone concentrations, typical of winter and spring, is observed: initially, this parameter increases reaching a maximum in March, then it abruptly decreases until autumn similarly to that shown in Fig. 1 for 2015 . The minimum ozone concentrations observed in spring months 2020 are two times lower than in 2019, and their monotonic decrease in time is observed. If it is considered that the ozone deposition in 2019 and 2020 differs only slightly, significantly lower morning ozone concentrations in 2020 can indicate a considerable decrease in the regional background of atmospheric air pollution in comparison with 2019. The daily smaller ozone production in the daytime leads to a gradual decrease in the background regional concentration. The most probable cause of a decrease in both background and peak local and regional ozone concentrations in ambient air in spring 2020 can be a decrease in the atmospheric pollution, caused by a decrease in the economic activity in China, Europe, and Russia in connection with the COVID-19 pandemic. The development of the anomalous dynamics of the tropospheric ozone in central Russia coincide with the introduction of road transport restrictions and a global decrease in the economic activity in the world, resulted in a considerable decrease in combustion product emissions into the atmosphere all over the world.@story_separate@In central Russia, April 2020, anomalously low ozone concentrations in the surface atmosphere were recorded. The increase in the ozone concentration, usual for spring, caused by a seasonal increase in the temperature and light was followed by a monotonic decrease. The monthly averaged maximum diurnal values observed in April 2020 were almost three times lower than those fixed at the same time in 2019. In spring of 2020, both a decrease in regional background ozone concentrations in the surface atmosphere, and a decrease in the intensity of its photochemical formation were observed. The most probable cause of the observed phenomenon is a decrease in combustion product emissions into the atmosphere in China, Europe, and Russia due to the COVID-19 pandemic.","Anomalously low ozone concentrations in the surface atmosphere in central Russia were recorded in spring of 2020. The increase in the ozone content usual for spring due to a seasonal increase in the temperature and light was followed by a monotonic decrease. The maximum daily values of surface ozone concentrations averaged over the month in April 2020 were three times less than the values recorded at the same time in 2019. Both a decrease in regional background ozone concentrations in the near-surface atmosphere and a decrease in the intensity of its photochemical formation were observed. The most probable cause of the observed phenomenon is a decrease in combustion product emissions into the atmosphere in China, Europe, and Russia due to the introduction of special regimes in connection with the COVID-19 pandemic."
"Understanding of the molecular principles driving the coronavirus disease 2019 associated with the severe acute respiratory syndrome (SARS) [1] [2] [3] [4] [5] has been at the focal point of biomedical research since the start of the pandemic a year ago. SARS-CoV-2 infection is transmitted when the viral spike (S) glycoprotein binds to the host cell receptor, leading to the entry of S protein into host cells and membrane fusion. [6] [7] [8] Structural and biochemical studies have shown that the mechanism of virus infection may involve spontaneous conformational transformations of the SARS-CoV-2 S protein between a spectrum of closed and receptoraccessible open forms, where RBD continuously switches between ""down"" and ""up"" positions where the latter can promote binding with the host receptor ACE2. [9] [10] [11] The full-length SARS-CoV-2 S protein consists of two main domains, amino (N)-terminal S1 subunit and carboxyl (C)-terminal S2 subunit. The subunit S1 includes an N-terminal domain (NTD), the receptor-binding domain (RBD), and two structurally conserved subdomains (SD1 and SD2). The S2 subunit is an evolutionary conserved modlue that contains upstream helix (UH), an N-terminal hydrophobic fusion peptide (FP), fusion peptide proximal region (FPPR), heptad repeat 1 (HR1), central helix region (CH), connector domain (CD), heptad repeat 2 (HR2), transmembrane domain (TM) and cytoplasmic tail (CT). 12 The S1 regions serve as dynamic protective shield of the fusion machine whereby upon proteolytic activation at the S1/S2 and dissociation of S1 from S2, structural rearrangements in S2 mediate the fusion of the viral and cellular membranes. [13] [14] [15] [16] [17] [18] [19] The cryo-EM structures of the SARS-CoV-2 S proteins characterized distinct conformational arrangements of the S protein trimers in the prefusion form that are manifested by a dynamic equilibrium between the closed (""RBD-down"") and the receptor-accessible open (""RBD-up"") form required for the S protein fusion to the viral membrane. [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] A population-shift between a spectrum of closed states that includes a structurally rigid closed form and more dynamic closed states can precede a transition to the fully open S conformation. [30] [31] [32] [33] [34] The cryo-EM structures and biophysical tomography characterized the structures of the SARS-CoV-2 S trimers in situ on the virion surface and confirmed a population shift between different functional states, showing that conformational transitions can proceed through an obligatory intermediate in which all three RBD domains are in the closed conformations and are oriented towards the viral particle membrane. 35, 36 Conformational events associated with ACE2 binding may include progression from a compact closed form of the SARS-CoV-2 S protein that becomes weakened after furin cleavage between the S1 and S2 domains to the partially open states and fully open and ACE2-bound form priming the protein for fusion activation. 37 Cryo-EM structures of SARS-CoV-2 S protein in the presence and absence of ACE2 receptor demonstrated that pH-dependent refolding region (residues 824-858) at the interdomain interface displayed dramatic structural rearrangements and mediated coordinated movements of the entire trimer, giving rise to a single 1 RBD-up conformation at pH 5.5 while all-down closed conformation was favorable at lower pH. 38 SARS-CoV-2 S mutants with the enhanced infectivity profile including D614G mutational variant have attracted an enormous attention in the scientific community following the evidence of the mutation enrichment via epidemiological surveillance, resulting in proliferation of experimental data and a considerable variety of the proposed mechanisms explaining functional observations. [39] [40] [41] The biochemical studies suggested a phenotypic advantage and the enhanced infectivity conferred by the D614G mutation. 42 The D614G the closed state in the S-GSAS/D614G structure. 44 These experiments suggested that the D614G mutation in the SD2 domain can induce allosteric effect leading to structural shifts between the up and down RBD conformations. 44 The electron microscopy analysis also revealed the higher 84% percentage of the 1-up RBD conformation in the S-G614 protein. 45 The retroviruses pseudotyped with S-G614 showed a markedly greater infectivity than the S-D614 protein that was correlated with a reduced S1 shedding, greater stability of the S-G614 mutant and more significant incorporation of the S protein into the pseudovirion. 46 It was also proposed that D614 S mutation may allosterically impact RBD-ACE2 binding and alter the transitions of ""up"" and ""down"" forms. 47 48 According to this study, D614G eliminates a salt bridge between D614 of one subunit and K854 of the adjacent subunit but can promote ordering of the partly disordered loop ( residues 620-640) which can strengthen the intra-and inter-domain interactions and enhance the stability of the mutated S protein. This study provided support to the reduced shedding mechanism induced by the D614G mutation that inhibits a premature dissociation of the S1 subunit which eventually leads to the increased number of functional spikes and stronger infectivity. 48 Structure-based protein design and cryo-EM structure determination established that D614G and D614N mutations can result in the increased stability due to a decrease in the premature shedding of the S1 domain. 49 Nonetheless, a consensus view on the exact mechanism underlying the functional effects and increased infectivity of S-D614G spike mutant is yet to be established. Several prevalent mechanisms are actively debated in the field offered to explain diverse experimental data, including D614G-induced modulation of cleavage efficiency of S protein; ""openness"" scenario advocating mutation-induced shift to the open states favorable for RBD-ACE2 interaction; ""density"" hypothesis suggesting a more efficient S incorporation into the virion; and ""stability"" mechanism that implicates mutation-induced enhancement in the association and stability of prefusion spike trimers as a driving force of greater infectivity. 42 Molecular dynamics (MD) simulations recently probed the effects of the D614G mutation, suggesting that mutational variant favors an open conformation in which S-G614 protein maintains the symmetry in the number of persistent contacts between the three protomers. 50, 51 The normal mode analyses combined with Markov model and computation of transition probabilities characterized the dynamics of the S protein and mutational variants, predicting the increase in the open state occupancy for the more infectious D614G mutation due to the increased flexibility of the closed state and the enhanced stability of the open spike form. 52 Computer-based mutagenesis and energy analysis of the thermodynamic stability of the S-D614 and S-G614 proteins in the closed and partially open conformations showed that local interactions near D614 position are energetically frustrated and may create an unfavorable environment that is stabilized in the S-G614 mutant through strengthening of the inter-protomer association between S1 and S2 regions. 53 Using time-independent component analysis (tICA) and protein graph connectivity network, another computational study identified the hotspot residues that may exhibit long-distance coupling with the RBD opening, showing that the D614G could exert allosteric effect on the flexibility of the RBD regions. 54 Structure-based physical model showed that the D614G mutation may induce a packing defect in S1 that promotes closer association and stronger interactions with S2 subunit, thereby supporting the reduced shedding hypothesis. 55 Computational modeling and MD simulations have been instrumental in predicting dynamics and function of SARS-CoV-2 glycoproteins. [56] [57] [58] [59] [60] [61] [62] [63] [64] The growing body of computational modeling studies investigating dynamics and molecular mechanisms of S mutational variants produced interesting but often inconsistent data that fit different mechanisms. In this study, we used examine molecular mechanisms underlying the functional effects of the D614G mutation in different states of the SARS-CoV-2 S protein. We combined MD simulations with the ensemble-based mutational scanning and energetic sensitivity cartography analysis to simulate cryo-EM structures of the SARS-CoV-2 D614 and G614 proteins. By employing mutational sensitivity mapping and network-based community analysis for characterization of protein stability and allosteric interactions, we show that the D614G mutation can improve local stability of the spike protein and promote the larger number of stable communities in the open form by also enhancing the S1-S2 inter-domain interactions. This study offers support to the reduced shedding hypothesis of the D614G mutant function and provides a novel insight into the molecular mechanisms of the D614G mutation by examining SARS-CoV-2 S protein as an allosteric regulatory machine.@story_separate@Atomistic simulations were performed for the cryo-EM structure of SARS-CoV-2 S- the cleavage site at the S1/S2 boundary (residues 677-689) and residues in the S2 subunit (residues 828-853). The missing loops in the studied cryo-EM structures of the SARS-CoV-2 S protein were reconstructed and optimized using template-based loop prediction approaches ModLoop, 69 ArchPRED server 70 and further confirmed by FALC (Fragment Assembly and Loop Closure) program. 71 The side chain rotamers were refined and optimized by SCWRL4 tool. 72 The protein structures were then optimized using atomic-level energy minimization with a composite physics and knowledge-based force fields using 3Drefine method. 73 In addition to the experimentally resolved glycan residues present in the cryo-EM structures of  The structures of the SARS-CoV-2 S trimers were simulated in a box size of 85 Å × 85 Å × 85 Å with buffering distance of 12 Å. Assuming normal charge states of ionizable groups corresponding to pH = 7, sodium (Na+) and chloride (Cl-) counter-ions were added to achieve charge neutrality and a salt concentration of 0.15 M NaCl was maintained. All Na + and Clions were placed at least 8 Å away from any protein atoms and from each other. All-atom MD simulations were performed for an N, P, T ensemble in explicit solvent using NAMD 2.13 package 74 with CHARMM36 force field. 75 Long-range non-bonded van der Waals interactions were computed using an atom-based cutoff of 12 Å with switching van der Waals potential beginning at 10 Å. Long-range electrostatic interactions were calculated using the particle mesh Ewald method 76 with a real space cut-off of 1.0 nm and a fourth order (cubic) interpolation. SHAKE method was used to constrain all bonds associated with hydrogen atoms. Simulations were run using a leap-frog integrator with a 2 fs integration time step. Energy minimization after addition of solvent and ions was carried out using the steepest descent method for 100,000 steps. All atoms of the complex were first restrained at their crystal structure positions with a force constant of 10 Kcal mol -1 Å -2 . Equilibration was done in steps by gradually increasing the system temperature in steps of 20K starting from 10K until 310 K and at each step 1ns equilibration was done keeping a restraint of 10 Kcal mol-1 Å-2 on the protein Cα atoms. After the restrains on the protein atoms were removed, the system was equilibrated for additional 10 ns. An NPT production simulation was run on the equilibrated structures for 500 ns keeping the temperature at 310 K and constant pressure (1 atm). In simulations, the Nose-Hoover thermostat 77 and isotropic Martyna-Tobias-Klein barostat 78 were used to maintain the temperature at 310 K and pressure at 1 atm respectively. To compute protein stability changes in the SARS-CoV-2 S structures, we conducted a systematic alanine scanning of protein residues in the SARS-CoV-2 trimer mutants as well as mutational sensitivity analysis at the mutational site for both SARS-CoV-2 S-D614 and SARS-CoV-2 S-G614 structures. Mutational scanning of protein residues was performed using BeAtMuSiC approach. 79  A graph-based representation of protein structures 82 is used to represent residues as network nodes and the inter-residue edges to describe residue interactions. We constructed the residue interaction networks using both dynamic correlations 83 and coevolutionary residue couplings 84 that provide complementary descriptions of the allosteric interactions and yield robust network signatures of long-range couplings and communications. The details of network construction were described in our previous studies. 85 The ensemble of shortest paths is determined from matrix of communication distances by the Floyd-Warshall algorithm. 86 Network graph calculations were performed using the python package NetworkX. 87 The Girvan-Newman algorithm 88-90 is used to identify local communities. An improvement of Girvan-Newman method was implemented where all highest betweenness edges are removed at each step of the protocol. The algorithmic details of this modified scheme were presented in our recent study. 91, 92 The network parameters were computed using the python package  The conformational dynamics profiles of the SARS-CoV-2 S-GSAS/D614 and SARS-CoV-2 S-GSAS/G614 showed a more flexible S1 subunit and very stable S2 subunit in both closed form ( Figure 2A ) and 1-up open state ( Figure 2B ). The thermal fluctuations of both S1 and S2 regions were smaller in the closed state with RMSF < 1.0 Å. In the closed state, the RBD (residues 331-528) and CTD1 (residues 528-591) corresponded to the most stable regions in the S1 subunit, while UH (residues 736-781) and CH (residues 986-1035) were the most stable regions in the S2 subunit ( Figure 2A ). Only marginally larger fluctuations were seen in the CTD2 region (residues 592-686) that connects S1 and S2 subunits. Our analysis showed that the Several previous studies suggested that the molecular basis of the increased infectivity for the D614G mutant may be explained via ""openness"" hypothesis [50] [51] [52] (Tables S1-S4) using a contacts-based Prodigy approach. 96, 97 This analysis showed that the number of interprotomer contacts is consistently greater in the closed state of the S-D614 and S-G614 proteins as compared to the partially open state. According to this assessment, D614 and Q613 residues can anchor the intra-protomer clusters with V597, S596 and T315 of the same protomer, while establishing contacts with T859, L861 and S735 residues of the adjacent protomers (Tables   S1,S2 ). It is worth noting that in agreement with several computational studies advocating for the ""openness"" mechanism 50 we also found a preferentially symmetric nature of the inter-protomer contacts in the closed state, while the number of inter-protomer contacts can be considerably  Previous mutagenesis analysis suggested that the D614G mutant may improve the stability of the S protein by strengthening the inter-protomer association between S1 and S2 regions. 53 We employed the equilibrium ensembles generated by MD simulations of the SARS-CoV-2 protein structures to perform deep mutational scanning of the spike protein residues and mutational sensitivity analysis of the D614 S trimer ( Figure 4 ) and G614 S mutant proteins ( Figure 5 ). Using energetic perturbation analysis, we test the shedding hypothesis suggesting that stabilization of the S-G614 protein may preclude a premature dissociation of the S1 subunit, leading to the increased number of functional spikes and stronger infectivity. 48 We constructed mutational sensitivity heatmaps for the major hinge centers in the S-D614 closed trimer ( Figure 4A ) and partially open form ( Figure 4B ). Mutational cartography revealed several interesting trends and highlighted the energetic effect of D614 site. First, we noticed that the energetic hotspots corresponded to hinge centers F592 and Y855 that showed larger contributions in the closed state ( Figure 4A) . Mutational sensitivity mapping of the hinge residues in the S-G614 mutant trimer revealed a redistribution of the energetic hotspots ( Figure 5A,B) . In the closed form of the G614 trimer the stability hotspots corresponded to F592, Y855 and G614 sites as mutations in these positions consistently resulted in large destabilization changes ( Figure 5A ). Of particular interest is emergence of G614 as one of the stability hotspots that together with F592 and Y855 strengthen the inter-protomer hinge cluster ( Figure 5C ). Although D614G eliminates favorable interactions with T859 and K854, the tight packing is maintained between residues of the interprotomer hinge cluster ( Figure 5C ). In the open form of the S-G614 trimer the stability hotspots These findings are consistent with the latest differential scanning fluorimetry studies showing that the D614G and D614N induced changes have a very similar effect, leading to a considerable improvement of thermal stability which may be explained by a decrease in premature shedding of S1 domain. 49 Consistent with this experimental study, we found that D614G/N mutations can moderate the repulsive charge interactions at the interface between S1 and S2 domains and allow for the tighter inter-domain packing. 49 Strikingly modifications resulted in the improved stability of the closed form ( Figure S4A ). This was particularly interesting given the overall rigidity of the locked closed S-D614 trimer, indicating that mutations in the D614 position can be accommodated even in this constrained state. In the locked closed form of S-G614 the largest destabilization changes were induced by the G614K, G614D and G614E mutations ( Figure S4B ), highligting the fact that the reverse G614D mutation could significantly destablize the S-G614 trimer. In the intermediate conformation, we observed larger destabilization changes for the moving protomer but also an appreciable loss of stability for the closed-down two protomers ( Figure S4C ). An interesting pattern of protein stability changes was seen in the 1 RBD-up open state of the S-G614 trimer ( Figure   S4D ). We also employed the FoldX approach with the all-atom representation of protein structure [99] [100] [101] [102] to evaluate residue-based folding free energy contributions in the D614 and G614 S trimers In the open form of the D614 S trimer, the stability of the RBD core and S2 regions was partially reduced while maintaining the overall stabilizing signature in these positions ( Figure   7B ). By highlighting positions of circulating mutations K417, E484, N501 and D614 we noticed that D614 corresponds to a moderately stable position while other sites are marginally destabilizing ( Figure 7A,B) . Hence, mutations in these sites can be easily tolerated without impairing protein stability of the S trimer ( Figure S1A partly reconcile the ""openness"" and ""S1-shedding"" mechanism underpinning the D614G effects. Mechanistic network-based models allow for a quantitative analysis of allosteric molecular events in which conformational landscapes of protein systems can be remodeled by various perturbations such as mutations, ligand binding, or interactions with other proteins. Using this framework, the residue interaction networks in the SARS-CoV-2 spike trimer structures were built using a graph-based representation of protein structures in which residue nodes are interconnected through both dynamic 83 and coevolutionary correlations. 84, 85 Using community decomposition, the residue interaction networks were divided into local interaction modules in which residues are densely interconnected and highly correlated, while the local communities are weakly coupled through long-range allosteric couplings. A community-based model of allosteric communications is based on the notion that groups of residues that form local interacting communities are correlated and switch their conformational states cooperatively. We explored community analysis as a network proxy for stability assessment. The number of communities for the S-D614 protein was greater in the closed form ( Figure 8A) These results are consistent with the latest experimental data that demonstrated the improved stability of the D614G mutant as compared to the S-D614 protein allowing for reduction in a premature shedding of S1 domain. 49 Table 2 . The local interacting communities of the protomer A in the structure of SARS-CoV-2 S-GSAS/D614 in the closed state (pdb id 7KDG). Local Community Interacting Residues A_1005_GLN A_759_PHE  13  HR1-HR2  A_1024_LEU A_1028_LYS A_1042_PHE A_1032_CYS  14  S2-HR1-HR2  A_1062_PHE A_1029_MET A_1033_VAL A_1053_PRO  A_877_LEU  15  HR1-HR2  A_1032_CYS A_1043_CYS A_1048_HIS A_1051_SER  A_1064_HIS  16  S2-HR1-HR2  A_819_GLU A_1055_SER A_874_THR  17  HR1-HR2  A_1075_PHE A_1096_VAL A_1110_TYR A_714_ILE  18  HR1-HR2  A_1106_GLN A_1109_PHE A_915_VAL  19  NTD  A_231_ILE A_130_VAL A_168_PHE  20  NTD  A_193_VAL A_204_TYR A_37_TYR  21  NTD  A_285_ILE A_279_TYR A_38_TYR  22  NTD-CTD1-CTD2  A_299_THR A_315_THR A_597_VAL  23  RBD  A_338_PHE A_342_PHE A_368_LEU  24  RBD  A_451_TYR A_401_VAL A_442_ASP  25  RBD  A_353_TRP A_398_ASP A_464_PHE  26  RBD-CTD1  A_543_PHE A_585_LEU A_576_VAL  27  CTD2  A_666_ILE A_650_LEU A_670_ILE A_645_THR  28  HR1  A_977_LEU A_749_CYS A_993_ILE A_997_ILE  29  S2  A_878_LEU A_806_LEU A_882_ILE  30  S2  A_906_PHE A_923_ILE A_916_LEU  31  HR1  A_1000_ARG A_977_LEU A_996_LEU  32  S2-HR1-HR2  A_741_TYR A_1004_LEU A_962_LEU A_858_LEU  33  S2-HR1-HR2  A_902_MET A_1050_MET A_898_PHE  34  RBD  A_342_PHE A_511_VAL A_374_PHE A_436_TRP  35  RBD  A_353_TRP A_400_PHE A_423_TYR A_512_VAL A_410_ILE  36  RBD  A_392_PHE A_515_PHE A_395_VAL  37 RBD-CTD2 A_314_GLN A_596_SER A_613_GLN suggesting the stronger S1-S2 interfacial interactions and tighter packing between S1 and S2 domains in the closed form of the S-D614 protein. We also detected the larger number of communities in the NTD and RBD S1 regions in the closed form, indicating that the allosteric interaction network in the S1 and S1-S2 regions is stronger in the closed form, protecting alldown closed form in the S-D614 protein ( Figure 8A) . A weaker connectivity of local communities near S1-S2 interfaces and in the NTD/RBD regions was observed in the open state of S-D614 ( Figure 8B ). These findings suggested that efficiency of allosteric communication in the residue interaction network could favor the closed-down form of the S-D614 spike protein. The analysis revealed the increased number of stable communities for the S-G614 mutant in both the closed form (Table 4 ) and partially open state (Table 5 ). Both forms of S-G614 mutant featured the appreciable number of communities in the CTD1 and CTD2 regions involved in stabilization of the S1-S2 interfaces ( Figure 8C,D) . These inter-domain local communities bridging S1 and S2 regions included T315-T299-V597, R328-S530-Q580, Q314-S596-Q613, Figure 8D , Table 5 ). This local subnetwork may be instrumental in bridging the individual domains in the S1 subunit and ensure a more efficient inter-connectivity between S2 regions, N2R and S1-RBD regions in the S-G614 mutant. -HR1-HR2  A_1062_PHE A_1029_MET A_1033_VAL A_1053_PRO  A_877_LEU  17  S2-HR1-HR2  A_905_ARG A_1050_MET A_898_PHE A_901_GLN  18  S2-HR1-HR2  A_819_GLU A_1055_SER A_874_THR  19  NTD  A_194_PHE A_238_PHE A_106_PHE A_117_LEU A_201_PHE  A_235_ILE A_86_PHE A_231_ILE A_90_VAL  20  NTD  A_193_VAL A_204_TYR A_37_TYR  21  NTD-RBD-CTD1  A_328_ARG A_533_LEU A_578_ASP  22  RBD  A_338_PHE A_342_PHE A_368_LEU  23  RBD  A_353_TRP A_398_ASP A_464_PHE  24  RBD  A_365_TYR A_387_LEU A_515_PHE  25  RBD  A_406_GLU A_403_ARG A_495_TYR  26  RBD  A_509_ARG A_401_VAL A_451_TYR A_442_ASP A_438_SER  A_507_PRO  27  CTD1  A_543_PHE A_585_LEU A_576_VAL  28  S2-HR1  A_977_LEU A_749_CYS A_993_ILE A_997_ILE  29  S2  A_878_LEU A_806_LEU A_882_ILE  30  HR1  A_973_ILE A_984_LEU A_992_GLN  31  S2-HR1  A_1000_ARG A_977_LEU A_996_LEU  32  S2-HR1  A_741_TYR A_1004_LEU A_962_LEU A_858_LEU  33  HR1-HR2  A_1065_VAL A_1052_PHE A_802_PHE A_927_PHE  34  NTD-RBD-CTD1  A_328_ARG A_543_PHE A_579_PRO  35  RBD  A_342_PHE A_511_VAL A_374_PHE A_436_TRP A_347_PHE  36  RBD  A_353_TRP A_400_PHE A_423_TYR A_512_VAL A_410_ILE  37  NTD  A_104_TRP A_194_PHE A_238_PHE A_92_PHE  38 NTD-CTD2 A_664_ILE A_312_ILE A_598_ILE A_342_PHE A_511_VAL A_374_PHE A_436_TRP A_347_PHE  A_399_SER A_509_ARG  17  RBD  A_351_TYR A_452_LEU A_454_ARG A_492_LEU  18  NTD  A_91_TYR A_35_GLY A_56_LEU  19  RBD  A_464_PHE A_429_PHE A_425_LEU A_426_PRO A_514_SER  20  RBD  A_451_TYR A_401_VAL A_497_PHE A_448_ASN  21  RBD  A_454_ARG A_457_ARG A_467_ASP  22  CTD2  A_611_LEU A_666_ILE A_650_LEU  23  S2  A_888_PHE A_789_TYR A_880_GLY  24  HR1-HR2  A_1000_ARG A_977_LEU A_996_LEU  25  HR1-HR2  A_1024_LEU A_1028_LYS A_1042_PHE A_1032_CYS  26  S2-HR1-HR2  A_1062_PHE A_1029_MET A_1033_VAL A_1053_PRO  A_877_LEU  27  HR1-HR2  A_1032_CYS A_1043_CYS A_1048_HIS A_1051_SER  A_1064_HIS  28  S2-HR1-HR2  A_819_GLU A_1055_SER A_874_THR  29  HR1-HR2  A_1115_ILE A_1104_VAL A_1119_ASN  30  S2-HR1-HR2  A_1106_GLN A_1109_PHE A_915_VAL  31  NTD  A_193_VAL A_204_TYR A_37_TYR  32  NTD  A_104_TRP A_194_PHE A_238_PHE A_92_PHE A_267_VAL  A_84_LEU  33  NTD  A_238_PHE A_106_PHE A_117_LEU A_201_PHE A_235_ILE  A_86_PHE A_90_VAL  34  RBD-CTD1  A_579_PRO A_330_PRO A_544_ASN  35  RBD  A_338_PHE A_342_PHE A_368_LEU  36  RBD  A_353_TRP A_398_ASP A_423_TYR A_464_PHE  37  RBD  A_365_TYR A_387_LEU A_515_PHE  38  CTD1  A_543_PHE A_585_LEU A_576_VAL  39  S2-HR1  A_997_ILE A_749_CYS A_993_ILE  40  A_878_LEU A_806_LEU A_882_ILE  41  S2  A_906_PHE A_902_MET A_923_ILE A_916_LEU  42  S2-HR1  A_741_TYR A_1004_LEU A_962_LEU A_858_LEU  43  S2-HR1-HR2  A_905_ARG A_1050_MET A_898_PHE A_902_MET  44  HR1-HR2  A_1065_VAL A_1052_PHE A_802_PHE A_927_PHE  45  HR1-HR2  A_1081_ILE A_1115_ILE A_1137_VAL  46  NTD  A_285_ILE A_38_TYR A_279_TYR  47 NTD The can be determined by two main factors : (a) through strengthening of the S1-S2 interactions and (b) by reducing functional movements of the NTD and RBD regions exposed to binding with the host receptor. Accordingly, the D614G mutation may exert its effect through allosteric stabilization of the S1-S2 interfaces limiting shedding of the S1 domain and by reducing flexibility and enhancing thermodynamic preferences of the open state that is central aspect of the ""openness""-based mechanistic scenario. Hence, dynamic network modeling and community analysis of the S-D614 and S-G614 proteins revealed that D614G mutation can induce a partial rearrangement of the residue interaction networks and promote the larger number of stable communities in both the closed and open forms by enhancing the S1-S2 inter-domain interactions. Furthermore, the network analysis suggested a differential stabilization of the S-G614 mutant, favoring the open form in which strengthened allosteric couplings between mutational sites, S1-S2 regions and NTD/RBD regions in S1 could contribute to a decrease in premature shedding of S1 domain. Although recent studies indicated that D614G variant did not itself drive escape from antibody binding, it was found that D614G can remarkably potentiate escape mutations at some positions in certain patients, supporting an allosteric mechanism of action triggered by this mutation on dynamics and function in remote regions exposed to interactions with antibodies. In light of these newly emerging experimental data, our results argue that the D614G mutation may exert its global impact on other sites by acting as an important mediating center governing regulation of the SARS-CoV-2 machine. This material is available free of charge via the Internet at http://pubs.acs.org. Phone: 714-516-4586; Fax: 714-532-6048; E-mail: verkhivk@chapman.edu The authors declare no competing financial interest.@story_separate@We combined several simulation-based approaches with dynamic network modeling and community analysis to quantify the effect of D614G mutation on dynamics, stability and network organization of the SARS-CoV-2 S proteins. The results of this study provide a novel insight into the molecular mechanisms underlying the effect of D614G mutation by examining SARS-CoV-2 S protein as an allosteric regulatory machine. Using mutational sensitivity analysis of the SARS-CoV-2 S-D614 and S-G614 proteins we demonstrated the improved stability of the D614G mutant as compared to the S-D614 protein, offering support to the reduced shedding mechanism underlying functional effects of the D614G circulating mutation. Figure S3 shows functional dynamics of these SARS-CoV-2 S-D614 and S-G614 trimer structures. Figure S4 highlights mutational sensitivity profiles of theses SARS-CoV-2 S-D614 and S-G614 trimer structures. Supporting information contains Tables S1-S4 that characterizes the inter-protomer contacts in the SARS-CoV-2 S-D614 and SARS-","Structural and biochemical studies SARS-CoV-2 spike mutants with the enhanced infectivity have attracted significant attention and offered several mechanisms to explain the experimental data. In this study, we used an integrative computational approach to examine molecular mechanisms underlying functional effects of the D614G mutation by exploring atomistic modeling of the SARS-CoV-2 spike proteins as allosteric regulatory machines. We combined atomistic simulations, deep mutational scanning and sensitivity mapping together with the network-based community analysis to examine structures of the native and mutant SARS-CoV-2 spike proteins in different functional states. Conformational dynamics and analysis of collective motions in the SARS-CoV-2 spike proteins demonstrated that the D614 position anchors a key regulatory cluster that dictates functional transitions between open and closed states. Using mutational scanning and sensitivity analysis of the spike residues, we identified the evolution of stability hotspots in the SARS-CoV-2 spike structures of the mutant trimers. The results offer support to the reduced shedding mechanism of as a driver of the increased infectivity triggered by the D614G mutation. By employing the landscape-based network community analysis of the SARS-CoV-2 spike proteins, our results revealed that the D614G mutation can promote the increased number of stable communities in the open form by enhancing the stability of the inter-domain interactions. This study provides atomistic view of the interactions and stability hotspots in the SARS-CoV-2 spike proteins, offering a useful insight into the molecular mechanisms of the D614G mutation that can exert its functional effects through allosterically induced changes on stability of the residue interaction networks."
"A novel coronavirus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) emerged in Wuhan, Hubei Province, China, in December 2019 causing a syndrome termed by the World Health Organization (WHO) ""COVID-19"" on February 11, 2020 [1] [2] [3] [4] and recognized as a pandemic on March 11, 2020 [5] . Coronaviruses are enveloped viruses with a positive sense single-stranded RNA genome (26-32 kb) [6] , one of the largest known genomes among the RNA viruses [7] . Within the last 2 decades, 2 coronaviruses have been introduced into the human population, the SARS-CoV and the Middle East respiratory syndrome-CoV (MERS-CoV) and, by crossing the species barrier and causing severe disease spread from person to person. SARS-CoV emerged in China in 2002 from Chinese horseshoe bats and transmitted to human by human civet intermediate reservoirs and resolved within a year, causing 8,437 infected people with 813 deaths in 27 different countries worldwide, and resulting in a 9.6% fatality rate [8] . Bats may be the primary reservoir, and camels the intermediate host for MERS-CoV, originated in the Middle East and remaining largely restricted in the Arabian peninsula, resulting in 2,229 cases 2 DOI: 10.1159/000515341 and 858 deaths with a case mortality rate of 35% [9] . Human coronaviruses (HCoVs), namely, HCoV-E299 (-CoV), HCoV-NL63 (-CoV), HCoV-OC43 (-CoV), and HCoV-HKU1 (-CoV), usually cause mild, both upper and lower respiratory tract infections in humans. HCoVs are thought to have emerged from an animal source, and then, other amplifier hosts might have played a role of becoming epidemiological animal reservoirs. Interestingly, evidence indicates 95% genetic homology of RNA sequence of HCoV-OC43 to the one of the bovine BCoV, speculating transmission from cattle, the zoonotic ancestor, to humans 100 years ago [10] . The identification of animal reservoirs plays a crucial role in effective disease control.@story_separate@Most of viral diseases are caused by zoonotic pathogens maintained by wildlife reservoir hosts [11] . Bats are a natural reservoir host of different families of viruses, many of which cause severe human diseases [12, 13] . Viruses can spill over from the host to other animals and humans, causing disease outbreaks worldwide. The bat Rhinolophus (horseshoe bat) is speculated as the source of SARS-CoV-2 [2] ; however, the possibility exists for additional interspecies transmissions, and the precise route of transmission of the virus into the human population has not been established yet. Phylogenetic analysis revealed SARS-CoV-2 as a newly emerged strain of β-coronavirus that shared 79.6 and 96.2% sequence identity with SARS-CoV-1 and bat Co-RaTG13, respectively, suggesting a zoonotic origin from the likely bat reservoir [14, 15] . RaTG13 was detected in bat Rhinolophus affinis from Yunnan Province, far away from Hubei Province. Divergence dates between SARS-CoV-2 and RaTG13 were estimated between 40 and 70 years ago in horseshoe bats. Similar to what was observed with SARS and MERS, the possible involvement of an intermediate host as a plausible conduit for transmission to humans has been considered. Specifically, Guangdong Pangolin-CoV genome is very closely related to SARS-CoV-2, sharing 92.4% sequence similarity; thus, pangolin could be responsible for the zoonotic event [16, 17] . SARS-CoV-2 could have been for decades in bats and been transmitted to other hosts such as pangolins. Unlike SARS-CoV, the spike (S) glycoprotein of SARS-CoV-2 harbors a unique furin cleavage site (PRRA) before the S1/S2 region, which increases the transmissibility of this virus that is absent in bats and pangolins [15, 18] . Although the exact origin of the PRRA motif from an animal virus has not yet been determined, this insertion is an independent natural event, probably due to a recombination, and fixed by the natural selection [18] . The currently available data do not clarify whether the origin of the virus is due to a natural selection in the wild animal reservoir before zoonotic transfer or natural selection in humans following zoonotic transfer. Naturally infections with SARS-CoV-2 have been documented in various animal species. SARS-CoV-2 infection in dogs occurred in Hong Kong after close contact with people infected with the virus developing detectable antibodies against SARS-CoV-2 [19, 20] . In addition, genetic analysis revealed that dog and human viral sequences were closely related. Other human-to-dog transmissions have been reported in the Netherlands and in New York State confirmed by analysis of neutralizing antibodies against SARS-CoV-2 [21] . Conversely, neither France nor Spain tested positive dogs living with an individual infected with SARS-CoV-2 [22, 23] . Several human-tofeline transmissions have been documented [24] [25] [26] . In January 2020, in Wuhan, antibodies against SARS-CoV-2 were detected in domestic cats using ELISA and/or neutralization assay. Between March and June 2020, in Hong Kong, New York State in the USA, Belgium, and France, asymptomatic and/or symptomatic cats were tested positive for SARS-CoV-2 by RT-qPCR. Cats may infect other cats in close contact. In April 2020, in the Bronx Zoo in New York City, 1 tiger and 3 lions were found positive after interaction with SARS-CoV-2-infected zookeeper [27] . In April 2020, minks in 2 separate farms in the Netherlands developed signs of breathing and gastrointestinal disorders, with a mortality of 1.2-2.4% for a presumed human-to-mink transmission of SARS-CoV-2, workers at the farm having been previously tested positive for coronavirus disease 2019 (COVID-19). In November 2020, the Danish National Institute of Public Health announced the alert for back spillover of SARS-CoV-2 from mink farms into the community. During the passage through minks, the virus mutated in the S protein-encoding gene. In the viral samples from minks and humans, researchers identified mutations that were then used to confirm that the people on the farm were infected by the viruses from the animals, and the mutations were suggestive of the virus adaptation to this new host [28, 29] . It is unclear how SARS-CoV-2 has been introduced in the farms. A likely scenario for the infection of minks is human-to-animal transmission, although virus could potentially be introduced by wild mustelids or other wildlife, as shown by other virus infections like influenza viruses. Minks are Mustelidae-like ferrets that have been used as animal models of COVID-19 owing to their susceptibility to the virus and transmission between ferrets. The virus's rapid spread in mink makes this animal a potential nonhuman reservoir of viral source that can easily infect humans, and surveillance at the human-animal interface is highly recommended. Natural or experimental animal host susceptibility to SARS-CoV-2 is shown in Figure 1 . Animal models are important tools both for studying the pathogenesis of infectious diseases and for the preclinical evaluation of vaccines and antivirals against virus infections [30, 31] . Several animal models have been used to study COVID-19 with varying susceptibility of the host to SARS-CoV-2 infection, suggesting a species-specific role for angiotensin-converting enzyme 2 (ACE2). The ACE2 is the host cell receptor responsible for mediating infection of SARS-CoV-2. The S glycoprotein mediates binding to ACE2, followed by proteolytic cleavage by protease serine 2 (TMPRSS2), which triggers fusion of viral and cellular membranes. In this regard, species-specific differences in the ability of ACE2 to bind the S protein for virus entry has been predicted in silico and demonstrated experimentally in vitro and correlated with susceptibility to SARS-CoV-2 infection of some species tested in experimental protocols [32] [33] [34] . In accord with this, mice are resistant to SARS-CoV-2 infection [35] . Animal models currently being used in COVID-19 research are shown in Table 1 . Mice are widely used in infectious disease research, even though often, in order to be susceptible to the infection, they need to be genetically modified. Otherwise, the virus needs to be adapted for the growth in different species. SARS-CoV-2 uses cellular surface ACE2 protein as a primary receptor for cell attachment and entry, and mouse ACE2 does not bind efficiently to the viral S protein [33, 34] . The ways to overcome this problem are to adapt either the host or the virus. Mouse-adapted SARS-CoV-2 strain with binding affinity to mouse ACE2 has been obtained after sequential passaging of virus in mouse lung tissues [36] . Infection of young adult and aged BALB/c mice with High susceptibility: felines, NHPs, hACE2 mice, pangolins, ferrets, minks, hamsters; low susceptibility: dogs and pigs (in one experimental study [68] ); no susceptibility: pigs (in one experimental study [52] ) to SARS-CoV-2 infection. SARS-CoV-2, severe acute respiratory syndrome coronavirus 2; hACE2, human ACE2; NHP, nonhuman primate. adapted SARS-CoV-2 resulted in replication in both upper and lower airways, with severe symptoms in the aged mice. The disadvantage of mouse-adapted viruses may be in the clinical features of the infection that do not recapitulate all aspects of the human disease. Another approach has been to modify the receptor-binding domain in the virus S protein to bind mouse ACE2 protein. The virus replicated in animals with a limited degree of clinical illness and mild disease symptoms, suggesting that even though ACE2 may be necessary for infection, it is not sufficient to determine the outcome of infection. Transgenic mice expressing human ACE2 (hACE2) receptor support SARS-CoV-2 infection. Different strategies have been adopted for expressing hACE2 in mice, including the use of the mouse ACE2 promoter or a heterologous gene promoter, as well as the transduction by means of adenovirus or adeno-associated virus expressing hACE2 as a transgene [31, [37] [38] [39] [40] [41] . Successfully studies showed mice that mimic human COVID-19 symptoms; thus, a variety of antivirals and vaccine candidates have been tested [42] [43] [44] [45] . Thus, mice are potentially good candidates for evaluating the efficacy of antiviral drugs. Syrian hamsters intranasally inoculated are highly permissive to SARS-CoV-2 infection, with high levels of virus replication and histopathological evidence of disease that closely mimic those displayed by human COVID-19 patients. High levels of viral RNA were evident in the nasal mucosa, lower respiratory epithelial cells, and small intestine, which could be useful for the evaluation of therapeutic agents and vaccines. Analysis of neutralizing antibodies, detected as early as 7 days after infection, revealed protection against rechallenge with SARS-CoV-2, while naive hamsters were efficiently protected by passive immunization against high dose of SARS-CoV-2. Virus transmission to naive cohoused hamsters has been successful observed [46] [47] [48] . Recently, 2 different groups, by adopting the hamster model, have been able to show that the D614G variant in S protein results in increased virus infectivity in the upper airway of the animals and in enhanced transmissibility [49, 50] . In addition, sera from variant-infected hamsters can efficiently neutralize the virus, suggesting that SARS-CoV-2 vaccines, all of which are based on the D614 variant, will protect against the  Ferrets are of special relevance to laboratory studies of respiratory viruses since their respiratory tract is anatomically comparable to the human one. Ferrets are susceptible to experimental infection by SARS-CoV-2, via direct contact and via the air, and are capable of replicating and transmitting the virus to other noninfected animals. Shedding of SARS-CoV-2 is observed in nasal and oropharyngeal swabs. Infectious virus was detected in the upper respiratory and gastrointestinal tract, and viral RNA was found in the saliva, urine, rectal swabs, and feces. All ferrets possessed serum with anti-SARS-CoV-2 antibody, and high titers of neutralizing antibodies were detected at day 21. Virus causes a milder respiratory syndrome in ferrets than in humans, with undetectable or mild clinical alterations. It should be noted the consistency of the results obtained from different studies [51] [52] [53] [54] . Ferrets are a valuable model for better understanding transmission and pathogenesis of COVID-19. The animal model of infectious diseases should reflect clinical course and pathology observed in humans to characterize viral pathogenesis and to evaluate antiviral agents and vaccines. For COVID-19, several animal species were investigated as models of human disease including the nonhuman primates (NHPs). NHPs are closely related to humans; they are invaluable models for studying emerging and re-emerging diseases. NHPs could be susceptible to SARS-CoV-2 infection, being symptoms of fever, diarrhea, and pneumonitis reported in rhesus macaques (Macaca mulatta), cynomolgus macaques (Macaca fascicularis), and African green monkeys (Chlorocebus aethiops). Replication of virus at high titers was observed in both the upper and lower respiratory tracts for 7-14 days [55, 56] . Additionally, infected rhesus macaques developed humoral and cellular immune responses as well as robust protection against 28 days postinfection rechallenge, indicating full protection from reinfection [57] . In addition, as observed in older SARS-CoV-2-infected individuals, an adverse clinical outcome is associated with old rhesus macaque infected as compared to young rhesus macaque ones [53] . The use of different challenge stocks, dosages, and routes of infection in NHP models may contribute to a significant variation in the level and duration of viral replication observed in the control groups; therefore, comparative studies will need stan-dardized protocols and challenge virus stocks. NHP models have been useful for the testing of therapeutic agents as well as of several vaccine candidates [43, 45, [58] [59] [60] [61] [62] [63] [64] [65] . One experimental infection study showed that dogs have a low susceptibility to SARS-CoV-2 [54, 67] . No clinical signs were detected. Some animals produced antibodies against the virus, and viral RNA was detected in rectal swabs while it was absent in organs of euthanized dogs. These data suggest that dogs are not promising animal models to study SARS-CoV-2. Cats are susceptible to experimental infection [66, 67] . Viral RNA was evident in the upper and low respiratory tracts and small intestines, and seroconversion was detected by using the ELISA test. Naive cats were shown to be susceptible to the infection if they were in close contact with infected cats shedding viral RNA in tissue and feces, representing a potential model for droplet transmission. Domestic swine have the potential to significantly impact public health; thus, determination of the susceptibility of pigs to SARS-CoV-2 is critical. To this end, different research groups inoculated these animals intravenously, intranasally, ocularly, or orally with SARS-CoV-2. No clinical signs and no viral RNA in swabs or tissue samples have been detected, and there was no seroconversion. Naive contact animals were also seronegative, indicating that pigs could not transmit the virus. These findings suggested that swine are not susceptible to SARS-CoV-2 infection [52] . A recent study contradicts the above reported one. Indeed, of the 16 animals experimentally inoculated, 2 of them seroconverted; virus was isolated from a pig, while another one displayed mild symptoms [67] . Viral RNA was detected in oral fluids and nasal wash from 2 animals. It should be taken into account that the considerable variation observed in this study may be due to the viral isolate, infectious dose, or breed of swine. Further investigations regarding the susceptibility of these species are needed. Experimental infection of chicken and ducks with SARS-CoV-2 has been described. Animals did not develop clinical signs, all swabs and organ samples were negative for SARS-CoV-2 RNA, and seroconversion was not observed. Cohoused naive animals were found to be seronegative, indicating no animal-to-animal transmission [68, 69] . These finding suggested that poultry are not sus-Parolin/Virtuoso/Giovanetti/Angeletti/ Ciccozzi/Borsetti Chemotherapy 6 DOI: 10.1159/000515341 ceptible to the virus, and therefore, they do not represent suitable animal models for studies of SARS-CoV-2 infection. Rousettus aegyptiacus fruit bat is genetically divergent from the predicted host reservoir Rhinolophus horseshoe bat. Experimental studies revealed that R. aegyptiacus is susceptible to SARS-CoV-2. Intranasal inoculation resulted in viral replication in the upper respiratory tract, with shedding of live virus and bat-to-bat transmission. Viral RNA was detected at a lower level in other organs, including the heart, skin, and intestine. Bats developed weak antibody responses, and clinical signs were absent [52] . The capacity of fruit bats to carry and transmit the virus makes this species a reservoir host, and therefore, it represents a useful model. Cell models of SARS-CoV-2 are essential for understanding the viral life cycle, tropism, and pathogenesis. Immortalized cell lines are widely utilized for virus isola-tion and for screening inhibitors against SARS-CoV-2. The simian kidney epithelial derived Vero E6 cell lines, expressing ACE2 and low levels of TMPRSS2, are highly susceptible and permissive to virus replication, and they are commonly used for virus isolation [70] . The human cell lines, among which renal HEK293T, intestinal Caco-2, pulmonary Calu-3, and hepatic Huh-7, produce low titers of infectious virus and have been utilized in SARS-CoV-2 infection experiments [70, 71] . Nonhuman cell lines, such as feline kidney CRFK and rhesus macaque kidney FRhK4 and LLCMK2, are adopted to SARS-CoV-2 replication studies [71] . The limitation of immortalized cell lines is that they do not accurately mimic human physiological conditions, and resulting biological observations need to be validated in primary human cells or in animal models. Primary human airway epithelial cells have been found to display cytopathic effects 96 h after SARS-CoV-2 infection [72] [73] [74] . Other primary human cells, nasal epithelial, large and lower airway epithelial, type I and type II pneumocytes (AT1 and AT2), ciliated and secretory airway epithelial (HAE), and gut enterocytes, are permissive to SARS-CoV-2 infection [75] [76] [77] . Recently, an important finding has been obtained in human lung epithelial cells and in primary hu- [49] . These data suggest that the D614G mutation gives rise to enhanced virus transmissibility. In addition, serum samples from D614 virus-infected hamsters can efficiently neutralize the G614 virus from infecting cells, indicating that SARS-CoV-2 vaccines, which are all based on the D614 variant of the S protein, will protect against G614 virus variants. Organoids are self-organized 3-dimensional assemblies of cells, generated by the primary tissue or pluripotent stem cells that exhibit physiological features of an organ. Organoids derived from human cells are particularly helpful for preclinical studies, by obviating the need to extrapolate results from one species to another. Organoids are also of interest for recapitulating the physiological effects of SARS-CoV-2, for investigating virus tropism and pathogenesis and for screening SARS-CoV-2 inhibitors. SARS-CoV-2 affects several organs causing severe damage of the lung; cardiovascular, intestinal, and neurological, and endothelial systems; kidney; and liver, showing direct effects on these tissues. Recent studies showed that human bronchial, lung, kidney, liver, intestinal, and vascular organoids are all permissive to SARS-CoV-2 and may represent viral reservoir [78] [79] [80] [81] [82] [83] [84] . Antiviral effects of COVID-19 candidate therapeutic compounds have been evaluated in organoid systems [85] [86] [87] [88] . Vascular and kidney organoids have been used to identify clinical-grade soluble ACE2 as an inhibitor of SARS-CoV-2 infection [87, 88] . Liver ductal organoids support SARS-CoV-2 replication and virus infection impaired the bile acid transporting functions of cholangiocytes, resulting in the bile acid accumulation and consequent liver damage in patients [83] . A limitation of organoid models may be the lack of relevant immune cells -e.g., macrophages, that modulate severe disease. Cell lines and organoids and currently being used in COVID-19 research are summarized in Table 2 .@story_separate@In order to answer important questions on COVID-19 including pathogenesis, transmissibility, and immune response to SARS-CoV-2, as well as comorbidities and viral coinfections, in vivo investigations adopting the best animal models are needed for validation and translation in human studies. Animal models may also serve to evaluate therapeutic countermeasures and vaccines. Current literature data indicate that although most of the animal models could mimic many features of human COVID-19, however, none of them is capable of replicating the clinical outcomes related to high mortality and morbidity of the human disease. Furthermore, it has to be considered that the innate immune response, including the defense system against viruses, diverged during evolution among the animals, which may explain the differences in the rate of infection. Additional studies will be required to refine animal models of chronic diseases to gain further insights into molecular immune pathogenesis, diagnosis and treatment of COVID-19. NHPs are closer to humans and are the most relevant animal models to test interventions before deployment to human treatment. On the other side, ferrets, mice, and hamsters developed clinical signs of the infection that are very similar to the one developed in humans. These animals might be useful in answering many questions regarding the mechanism of action of antivirals, and safety and efficacy of vaccines. Animal and in vitro/ex vivo models can be adopted and pathogenesis and interventions assessed. Finally, wild animals such as bats and/or pangolins or a yet-to-be-identified animal host could serve as reservoirs of the SARS-CoV-2 or route of transmission to humans, and surveillance should be extensively done.","Viruses arise through cross-species transmission and can cause potentially fatal diseases in humans. This is the case of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) which recently appeared in Wuhan, China, and rapidly spread worldwide, causing the outbreak of coronavirus disease 2019 (COVID-19) and posing a global health emergency. Sequence analysis and epidemiological investigations suggest that the most likely original source of SARS-CoV-2 is a spillover from an animal reservoir, probably bats, that infected humans either directly or through intermediate animal hosts. The role of animals as reservoirs and natural hosts in SARS-CoV-2 has to be explored, and animal models for COVID-19 are needed as well to be evaluated for countermeasures against SARS-CoV-2 infection. Experimental cells, tissues, and animal models that are currently being used and developed in COVID-19 research will be presented."
"Unconscious or implicit biases, which are unfair ""prejudice[s] in favor of or against one thing, person, or group compared with another"" [14] , continue to be a pressing social issue. People can hold biases against or for others based on characteristics such as the individual's age, gender, gender identity, physical abilities, religion, sexual orientation, or weight [14] . Due to their automatic and unintentional nature -and often unrecognized impact on judgment and behavior -implicit biases can lead people, even those most committed to egalitarian ideals, to be unsuspecting perpetrators of discrimination. Existing bias interventions have two primary shortcomings: (1) they have been designed as ""one-shot"", ""one size fits all"" solutions and (2) they have not successfully aided individuals in identifying bias-triggering contexts or situations as they occur in real time. These shortcomings are in large part due to the difficulty and lack of feasibility of assessing or detecting bias as it manifests itself in daily life. Our research aims to understand and detect the physiological and behavioral associates of implicit bias and, ultimately, to build technology that uses this knowledge to integrate bias mitigation strategies into personal devices, such as smartphones and wearables. In this study, we use machine learning modeling to understand the links between racial bias and involuntary physiological responses using Photoplethysmography (PPG), Electrodermal Activity (EDA), Skin Temperature (SKT), and Accelerometer (ACC) signals. To our knowledge, this is the first study to investigate the feasibility of using machine learning to detect racial bias from physiological signals collected from a wearable wristband. In the following sections, we discuss prior research on implicit bias and its physiological counterparts. We then describe our data collection study, our analysis methods, and our preliminary results.@story_separate@Background and Related Work Implicit biases are typically measured through the Implicit Association Test (IAT) [2], a test that measures how long it takes participants to categorize contrasted concepts, like Black and good, or white and kind [11] . For example, a participant is described as having an unconscious bias towards white appearing faces if they are faster at completing the task when white faces are paired with words like ""good"" and Black appearing faces are paired with words like ""bad."" Based on the differences in participants' speed in pairing faces and evaluations in Black-good/White-bad and Black-bad/White-good conditions, participants are categorized as having no bias, a slight bias, a moderate bias, or a strong bias [12] . A meta-analysis of 122 studies of the IAT found that the predictive validity of the IAT is greater for social sensitive topics, such as race, where the test has greater predictive validity for bias than self-report measures [11] . Even small differences between individuals on the IAT have been linked to differences in discrimination in hiring, in salary, in criminal proceedings, in school discipline and grading, and in health care decisions [10] . What the IAT fails to tell us, however, is whether there are also physiological responses associated with bias. Our ultimate goal is to identify the triggers and internal indicators of bias so that we can provide individuals with just-in-time information they can use to reduce the impact of biases on their judgments and actions. Prior work has begun to establish one avenue for such interventions: the linking of unconscious bias activation with distinct patterns of concomitant physiological activity, patterns that are consistent with threat responses in intergroup contexts. Researchers have found that learned threat responses to outgroup members are indicated by distinct patterns of neural activity (e.g., amygdala activation: [3, 4] ); cardiovascular reactivity [6, 13] ; and skin conductance [8] ). For example, the intensity of participants' neural activity is related to differences in assessed levels of prejudice. Participants' anterior cingulate cortex activity is greater for participants with more assessed prejudice than their less prejudiced peers when they are asked to engage in a labelling activity after being shown Black appearing faces [5] . Similarly, in another study, when participants were confronted with individuals who appeared Asian, but had Southern American accents, the participants demonstrated unusual cardiovascular responses, had poorer task performance, and manifested negative and defeat-related behaviors [6] . In studies of bias against individuals with mental illness, researchers found that the reactions and decisions of participants when interacting with individuals with mental health concerns were correlated with skin conductance measures, indicating a possible relationship between skin conductance measures and bias [8] . While these early studies on the relationship between implicit biases and physiological responses are encouraging, our research is motivated by gaps in the literature. We are not aware of any research in this domain that captures and investigates a combination of physiological responses and measures of implicit biases. Our study helps to 1) better understand and confirm that such relationships exist, 2) identify the most significant physiological responses to bias, and 3) identify the overlapping and combined physiological responses to bias. We used Empatica [1] , a wristband designed to gather high-quality physiological signals to capture physiological responses. The Empatica has four sensors: Electrodermal Activity (EDA), Photoplethysmography (PPG), Infrared Thermopile (TEMP), and Accelerometer (ACC). The PPG sensor is capable of measuring Blood Volume Pulse (BVP) from which Heart Rate (HR), Heart Rate Variability (HRV), Inter-Beat-Interval (IBI) and many other cardiovascular features can be extracted. The EDA sensor is used to measure sympathetic nervous system arousal and monitor emotional states, such as stress, excitement, and focus. The accelerometer and Infrared Thermopile are used to track body movements and to monitor changes in skin temperature. As part of a larger study on simulated practice, the data collection took place in two of eight sections of an Introduction to Teaching course at a large university in the Southeast. Of the 76 students in the two sections, 46 consented to wearing an Empatica E4 wristband while completing the study measures and had enough biometric data for analysis. All participants completed the IAT and surveys during course sections, observed by the researchers. The participants resembled their course peers in gender, interest in teaching, and languages spoken at home, but were slightly more white than the students in the course sections as a whole (61% to 51%) and from slightly wealthier families (54% to 42%). Overall, the participants were almost equally split between men and women, generally spoke only English in the home, and were more likely to have attended a primarily white high school than a mixed race school or one primarily consisting of students of color (50% primarily white, 39% mixed race, and 11% primarily students of color). IAT data processing. The initial IAT test results had eight categories. The IAT categorized participants as having either a strong preference, moderate preference, slight preference, or no preference for either white or Black appearing faces. We designed our machine learning analysis as a binary classification task to infer whether or not a person has bias towards any racial group. We, therefore, categorized all strong and moderate preferences as 'biased' and all slight or no preferences as 'unbiased'. Out of the 46 participants, 26 scored as biased and 20 scored as unbiased. Feature Extraction. We calculated a total of 91 features from SKT, HR, BVP, EDA, and ACC signals as described in Table 1 . Statistical features such as maximum, mean , and standard deviation were common across all signals, and other features such as rms and skewness were extracted from EDA and BVP signals. The temperature and blood volume pulse signals (BVP) from the Empatica were measured at a sampling frequency of 4 Hz and 64 Hz respectively. Heart rate (HR) was derived from BVP signals by an algorithm built into the Empatica E4. The 3-axis acceleration signal was measured at a sampling frequency of 32 Hz. Because we were more interested in the degree of movement than the direction of the movement, we derived the magnitude of the x, y, and z axes from the accelerometer signals using the following equation: The EDA signals were measured at a sampling frequency of 4 Hz. The cvxEDA library [9] was used to separate the phasic and tonic components of EDA signals. This allowed us to extract features from the main, phasic, and tonic parts of the EDA signal, which is helpful in analyzing stimulus responses. The tonic part of the signal is more correlated to physiological arousal and alertness, while the phasic component is more connected to attention, significance, and novelty [7] . The features of EDA and BVP signals, including rms, kurtosis, skewness, zero_cross , and pow_spec were extracted using Python libraries HeartPy [15] and pyphysio [5] . We used machine learning to understand 1) how accurately we could classify the biased group from the unbiased sample, 2) what physiological responses were most indicative of the existence of racial bias in individuals, and 3) what temporal patterns could be observed in the physiology of the sample population during the IAT that might provide useful insights into the onset of an individual's bias-reaction. We used XGBoost, a machine learning ensemble meta-algorithm that uses decision trees as base-learner algorithms, under the framework of Bootstrap Aggregation (Bagging) and Boosting mechanisms. This method can also report the ranking of each feature from its average importance and its contribution to the algorithm's decision-making process. Since the number of biased and unbiased participants was imbalanced (26 biased vs. 20 unbiased), we applied oversampling technology in the leave-one-participant-out cross validation to balance the number of participants in both groups. We divided the signal stream of each sensor into 5 second intervals and extracted features as discussed in previous sections. We used this short time window to capture fine-grained and micro features from data. This process generated a dataset of 5-second samples as rows and sensor features as columns. We then applied our machine learning algorithms on the 5-second sample dataset in a leave-one-person-out cross validation. This approach classified each 5-second sample of each participant's data into the two labels of 0 (unbiased) or 1 (biased). The process resulted in sequences of 0 and 1s for each participant (see Figure 1 ). We developed an algorithm to iteratively smooth the sequence and generate consecutive blocks of 0 and 1 labels. First, the sequence was parsed and blocks of labels with length of 1 were replaced with the neighboring majority label on both sides. For example, 1101 was smoothed to 1111. Then segments with length of 2 were checked and smoothed in the same way. The process continued until either at most three consecutive windows were created or the frequencies of the remaining segments were greater than the mean value of the original segments' frequencies. We then chose the label that had the largest consecutive block in the sequence as the final label. This strategy generated the ranking of features during the cross validation. We further analyzed the set of features that had a high importance rate in more than half of the iterations. To measure the performance of the algorithms, we calculated accuracy, F1 score, precision, and recall statistics for the biased and unbiased groups.  Our machine learning analysis focused on 1) demonstrating the feasibility of using physiological responses to measure racial bias, 2) identifying physiological features and their importance in influencing the decision of algorithms in classifying biased vs. unbiased participants, and 3) identifying patterns of significant change in participants' time series data that might relate to their racial bias. In addition to the accuracy of classifying biased vs. unbiased samples, we wanted to identify temporal patterns that might emerge in participants' physiology during the IAT. With the ratio of majority class (biased samples) as baseline (56.5%), our results indicated that XGBoost provided an overall accuracy of 19.6% above the baseline respectively (Accuracy = 76.1% and F1 =75.8%,). The algorithms could accurately label the majority of samples belonging to the biased class (Recall of 76.9%) compared to the unbiased class, which was expected given that the majority of the samples belong to the biased class. The higher impact of accurate classification of samples in the biased class to some degree compensates for more false negatives in the unbiased samples (Recall of 75%). Figure 2 shows the features that had highest ratings in the majority of the iterations during the cross-validation. We only show the features with the frequency above 23, which is half the number of participants (46). The majority of those features were from the EDA signals. To understand the temporal patterns in the physiological responses, we looked at the location of the majority window of true predicted labels in smoothed sequences belonging to participants whose sequence was accurately labeled as biased to see if any patterns emerged. For 66% of participants, the majority window was placed at the end of the sequence, indicating that physiological reactions in biased participants became stronger as the test progressed. While more analyses will be necessary to draw conclusions on this observation, this finding suggests that timing and duration of physiological responses could serve as additional indicators of bias reactions. Our machine learning analyses provide preliminary evidence of the feasibility of using physiological responses to measure and understand racial bias. The EDA measures of skin conductivity had the strongest related to bias. The EDA measures constituted the majority of highly ranked features, and the EDA measure of standard deviation differed the most between the biased and unbiased groups. Further, the increase in participants' physiological reactions over the duration of the IAT indicates that bias responses themselves might increase over prolonged interactions, which has important practical implications for designing bias interventions. Our findings on skin conductivity demonstrate the need for more research on physiological indicators and bias, so that we can learn if this finding is limited to just the race IAT, just these participants, or if the finding is broadly true for more biases, situations, and samples. While the findings of this study have broad implications for the study of implicit bias, they are only preliminary and more research is needed to confirm them. The sample for this study consists of only 46 students, and the findings might vary with a larger sample. In addition, the small sample size created a severe class imbalance and limited our ability to do multi-class classification to infer different levels of bias. While we mitigated this problem by grouping the data samples into two categories of biased and unbiased, ideally, we would only use data samples that were labeled as highly biased or not biased to have a clear distinction between the two categories, as would be possible with a larger sample. Further, the data we collected came when students were taking the IAT and not when they were in everyday situations where bias might be triggered. The Covid-19 pandemic interrupted the planned data collection and the next steps for research are to both collect data from more participants and to collect data when participants are engaged in real or simulated environments that might trigger bias to confirm the real-world applications of the work.@story_separate@The purpose of this paper was to understand the relationship between bias as measured by the IAT and involuntary physiological responses. Our preliminary findings with a sample of undergraduate students both build on and expand the literature on the relationship between bias and physiological responses and the use of the wearable devices for understanding psychological constructs. These preliminary findings on the relationship between bias as measured by the IAT and physiological indicators have profound implications for bias mitigation and measurement and warrant further investigation.","Despite the evolution of norms and regulations to mitigate the harm from biases, harmful discrimination linked to an individual's unconscious biases persists. Our goal is to better understand and detect the physiological and behavioral indicators of implicit biases. This paper investigates whether we can reliably detect racial bias from physiological responses, including heart rate, conductive skin response, skin temperature, and micro-body movements. We analyzed data from 46 subjects whose physiological data was collected with Empatica E4 wristband while taking an Implicit Association Test (IAT). Our machine learning and statistical analysis show that implicit bias can be predicted from physiological signals with 76.1% accuracy. Our results also show that the EDA signal associated with skin response has the strongest correlation with racial bias and that there are significant differences between the values of EDA features for biased and unbiased participants."
"In December 2019, about one year ago, 27 cases of pneumonia of unknown etiology were identified in Wuhan City, China [1] . Noticeably, Dr. Li Wenliang, an ophthalmologist, first recognized the symptoms of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2, now known as coronavirus disease 2019 (COVID-19)) in seven of his patients while he developed the disease himself and passed away in February 2020 [2] . The outbreak of COVID-19 was sudden and unprecedented since its spread was very quick and extensive; therefore, the World Health Organization (WHO) has declared COVID-19 as a pandemic [3] . The pandemic changed the whole world. Since social distancing is a key measure to slow the transmission of the virus, in many countries, governments decided to implement lockdowns [4] [5] . Although necessary to control the pandemic, these measures have resulted in major interruptions in the economy, social life, and healthcare provision [4] . Specifically, hospitals have altered general wards into intensive care units (ICUs), reduced outpatient clinics, canceled elective surgeries, and re-deployed healthcare providers while treatment has been restricted to urgent or emergency conditions [4] [5] . In Greece, a partial lockdown was initiated on March 16, 2020, and became a complete lockdown on March 23, 2020, lasting until May 10, 2020. During the lockdown, the national healthcare system focused on the prevention and management of COVID-19-related disease and emergency services only. In the majority of 1 2 2 2 2 1 ophthalmology clinics in Greece, regular clinic visits, elective surgeries, scheduled intravitreal injections, and non-urgent eye conditions were deferred, as it also occurred in most countries worldwide [4] [5] [6] [7] [8] . In addition, many patients have themselves postponed their visits to ophthalmology clinics in order to avoid being exposed. Diabetic retinopathy (DR) is a microvascular complication of diabetes mellitus and one of the leading causes of blindness in the working-age population, especially due to the development of diabetic macular edema (DME) or proliferative DR (PDR), both of which require prompt management and regular follow-up [9] [10] . The standard of care for DME is anti-vascular endothelial growth factor (anti-VEGF) injections, which have been shown to be safe and effective in large pivotal clinical trials [11] [12] . Accordingly, panretinal photocoagulation (PRP), anti-VEGF injections, or a combination of them are used today for the treatment of PDR [13] [14] [15] [16] before ending up in advanced-stage disease, including vitreous hemorrhage (VH) and tractional retinal detachment (TRD) [10, 17] . It is worthy to note that failure to visit clinicians and to undergo the appropriate treatment may result in worse outcomes and potential irreversible visual loss in patients with DR [18] [19] . In light of the above, the purpose of this study was to evaluate the effect of COVID-19-related lockdown in the management of patients with DR in a tertiary reference center for ""diabetic eye disease"" in Greece. We hope that this analysis will provide valuable insight into the management of patients with DR in real-life emergency settings, such as another wave of COVID-19 outbreak or other future pandemics.@story_separate@Participants in this retrospective observational study were patients who attended the ""diabetic eye clinic"" or the emergency ophthalmology department with a diagnosis of DR, DME, or VH/TRD due to PDR during the COVID-19-related lockdown and during the same period in the previous year at a tertiary reference center in Greece (2nd Department of Ophthalmology, National and Kapodistrian University of Athens, Athens, Greece). In addition, the data of patients, who were regularly followed up and treated for DME or DR in the ""diabetic eye disease"" clinic of our department and were supposed to attend the clinic, but deferred due to the COVID-19-related lockdown and were examined after the lockdown, were collected. The study was in adherence with the Declaration of Helsinki and no approval by the Institutional Review Board of our hospital was needed since it was a retrospective study. Informed consent was obtained from participants in this study. For all participants, their medical records were reviewed and analyzed. Demographic data, medical history, dates when they attended clinics and received treatment, as well as clinical data regarding best-corrected visual acuity (BCVA; Snellen charts), dilated fundoscopy findings, and optical coherence tomography (OCT) assessments were recorded. OCT had been performed in all patients using the Heidelberg Spectralis HRA+OCT device (Heidelberg Engineering, Heidelberg, Germany). Statistical analysis was done using the Statistical Package for Social Sciences (version 24.0, IBM Corp, Armonk, NY). Comparisons between the two years were performed using the student's t-test or Mann-Whitney-Wilcoxon test, as appropriate. Accordingly, comparisons for the same patients between the periods before and after lockdown were performed using the paired sample t-test or the Wilcoxon signed-rank test. Qualitative variables were assessed using the chi-square test. The association between the change in BCVA between the last visit before lockdown and the first visit after lockdown (dependent variable) and other demographic and clinical characteristics of patients (independent variables) was evaluated using multivariate regression analysis. Statistical significance was set as a p-value of <0.05. During the complete lockdown period, we did not run the retina and the ""diabetic eye disease"" clinics, and all regularly scheduled visits were deferred. No intravitreal injections were performed in our hospital. We only accepted emergencies and performed urgent surgeries, always using suitable personal protective equipment and measurements. Specifically, six patients visited the emergency department due to PDR with only neovascularization, eight patients due to VH or TRD, and seven patients due to DME, while five patients were diagnosed with non-proliferative DR (NPDR). Regarding treatment, four patients received PRP and one patient underwent PPV for TRD. Table 1 shows the data during the same period in 2019 and 2020. As compared to the previous year, there is a significant decrease in patients diagnosed with both NPDR and PDR with only neovascularization, as well as in patients with DME, while there was no statistically significant difference in patients with VH or TRD who attended the emergency department. Accordingly, there was a statistically significant decrease in the number of intravitreal injections, PRP sessions, and PPV for the treatment of PDR complications performed in our department during the lockdown period as compared to the same period of the previous year. Non-proliferative diabetic retinopathy 183 5 Proliferative diabetic retinopathy (only neovascularization) 21 6 Vitreous hemorrhage or tractional retinal detachment 9 8 Diabetic macular edema 147 7 Intravitreal anti-VEGF injections 132 0 Intravitreal steroids injections 7 0 Panretinal photocoagulation 18 4 Pars plana vitrectomy 6 1 In this study, we also included 62 consecutive patients with previously diagnosed DME and 107 patients with DR who were supposed to be examined and treated during the lockdown since they were regularly followed up in our department, but their appointments were deferred due to the COVID-19-related lockdown. Regarding patients with previously diagnosed DME, the mean BCVA at the last visit before the lockdown was 0.42±0.14 (decimal scale) and differed significantly compared to the mean BCVA at the first visit after the lockdown (0.27±0.17, p<0.001). Of note, 11 out of 62 patients with DME (17.7%) presented a very low BCVA of ≤0.1 decimal prior to the lockdown while after the lockdown, 21 out of 62 patients (33.9%) presented BCVA ≤0.1 decimal, a difference that was statistically significant (p=0.040). Accordingly, the mean central retinal thickness (CRT) at the last visit before the lockdown was 379.3±51.9 μm and differed significantly with the CRT at the first visit after lockdown (481.4±69.2 μm, p<0.001). Since no intravitreal injections were administered for patients with DME/DR during the lockdown period, there was a delay in attending their original intravitreal injection appointment of 8.2±2.3 weeks. Figure 1 shows a case of our study sample, presenting worsening of DME in both eyes after lockdown. As far as patients with previously diagnosed DR without DME, there was no statistically significant difference in BCVA at the first visit after lockdown compared to the last visit before lockdown (0.64±0.13 vs. 0.70±0.09 decimal scale, respectively, p=0.059). Table 2 shows the progression in disease stage at the first visit after the lockdown compared to the last visit before the lockdown. Specifically, one patient progressed from mild to moderate NPDR and one from moderate to severe NPDR. However, three out of 10 patients with severe NPDR (30%) progressed to active PDR, one of whom developed vitreous hemorrhage during the lockdown period while one patient with previous quiescent PDR (8.3%) exhibited active PDR again. Additionally, four out of 107 patients (3.7%) developed DME during the lockdown period. The results of the multivariate regression analysis are shown in Table 3 . Only the time interval (in weeks) between the last visit before the lockdown and the first visit after the lockdown was found to be associated with BCVA change, with a longer interval to be associated with worse visual acuity (β coefficient=0.451, p=0.017) while there was a trend for the DR stage, which did not reach statistical significance (p=0.052  The principal message of this study was that a COVID-19-related lockdown has a negative impact on patients with DR, including DME. First of all, there was a significant decrease in the number of patients with DR who visited our clinic during the lockdown period as compared to the same period during the previous year, as was seen in previous studies as well [4, [6] [7] 20] . Moreover, patients with previously diagnosed DR and DME could not attend the clinic due to lockdown since all regular follow-up visits and intravitreal injections appointments deferred. This resulted in a significant worsening in BCVA and CRT in patients with DME, as well as in progression to active PDR in 30% of patients with severe NPDR and in 8.3% of patients with previously quiescent PDR. Our findings seem to be independent of glycemic control since diabetic patients were found to exhibit a small but significant improvement in glycemia, body weight, and total cholesterol while the other metabolic parameters remained stable [21] . Several organizations have published general guidance for ophthalmologists on managing patients during the pandemic, including the American Academy of Ophthalmology, the French Society of Ophthalmology, the German Ophthalmological Society, and the Royal College of Ophthalmologists [22] [23] [24] [25] while the Vision Academy Steering Committee has provided specific guidance for intravitreal anti-VEGF injections during the COVID-19 pandemic [26] . All these guidelines conclude that strategies for managing patients with a retinal disease during this uncertain time should focus on minimizing the risk of exposure to COVID-19 for both patients and healthcare staff, prioritizing treatment for those at greatest risk of irreversible vision loss, and simplifying anti-VEGF treatment regimens [26] [27] . Overall, there is a general consensus that protective measures should be applied in ophthalmic practice due to the increased risk for transmission of COVID-19 with the close proximity that is often required between physicians and patients. Additionally, risk assessment, postponement of non-urgent cases, and teleophthalmology virtual services have been proposed [26] . Regarding patients with DR or DME, the Royal College of Ophthalmologists guidelines recommend deferring anti-VEGF injections and reviewing them in the clinic after four months, with the exception of patients with severe NPDR and active PDR, who may require anti-VEGF agents and PRP, while a virtual review with OCT and wide-field color photography seems to be the preferred option to review these patients [25] . However, as the Vision Academy Steering Committee underlined, these guidelines are specifically relevant to the UK healthcare system and application outside the UK may be confounded by local regulations, practice capacities, and other country-specific factors [26] . Anti-VEGF therapy for patients with DME needs ongoing, perpetual treatment in most eyes. Therefore, suspending intravitreal injection appointments or examination of patients with DME may result in a significant visual loss in some patients [28] , as was observed in our study as well. The same applies to patients with severe NPDR or active PDR, who should receive their treatment at the earliest convenience, to avoid the devastating complications of PDR. Since our results showed that delays in treatment were associated with a change in BCVA, prolonged treatment postponement and indefinite deferral of the appointments without rescheduling within a reasonable time should be avoided, depending on measures in each individual country [26, 28] . Previous studies have shown similar results in patients with neovascular age-related macular degeneration during the COVID-19-related lockdown. Specifically, Borelli et al. found that the COVID-19 pandemic resulted in a significant delay in neovascular age-related macular degeneration patient care, which was associated with worse short-term outcomes in these patients, suggesting more flexible treatment regimens in patients, who need frequent treatment [6] . A potential limitation of our study pertains to its retrospective nature, as selection bias could be anticipated. Moreover, we did not include data about modifiable factors, such as hypertension, kidney disease, anemia, or glycemic status, which could affect DR. In addition, the study sample seems to be relatively small and derived from a single center. However, this is a real-life study, investigating the impact of a COVID-19related lockdown in patients with DR at a tertiary reference center for ""diabetic eye disease."" Human subjects: Consent was obtained or waived by all participants in this study. Attikon University Hospital IRB issued approval NA. No approval by the Institutional Review Board of our hospital was needed since it was a retrospective study. Informed consent was obtained from participants in this study. Animal subjects: All authors have confirmed that this study did not involve animal subjects or tissue. Conflicts of interest: In compliance with the ICMJE uniform disclosure form, all authors declare the following: Payment/services info: All authors have declared that no financial support was received from any organization for the submitted work. Financial relationships: All authors have declared that they have no financial relationships at present or within the previous three years with any organizations that might have an interest in the submitted work. Other relationships: All authors have declared that there are no other relationships or activities that could appear to have influenced the submitted work.@story_separate@In conclusion, to our knowledge, this is the first study in Greece evaluating the impact of a COVID-19related lockdown in patients with DR, showing that unintentional interruption of follow-up and treatment can result in significant deterioration in visual acuity. Of note, since visual acuity outcomes were found to be associated with the time interval between the visits before and after lockdown, reflecting the length of treatment interruption, prolonged postponement of examination or treatment should be avoided. Our duty is to balance the desire to treat our patients while protecting them from being harmed by inadvertent viral transmission. Since the COVID-19 pandemic is still in progress and other global hazards may occur with restrictions in healthcare groundwork, our study provides useful information about the management of patients with DR in real-life clinical practice. Long-term studies are needed to expand our results and suggest guidelines for individualized treatment.","Purpose To evaluate the effect of the coronavirus disease 2019 (COVID-19)-related lockdown in the management of patients with diabetic retinopathy (DR), including diabetic macular edema (DME), in a tertiary reference center in Greece. Methods In this retrospective study, we first compared the number of patients who were diagnosed with DR or DME in our clinic during the period of the lockdown and during the same period of the previous year. In addition, we included consecutive patients with DR or DME, who were followed up and treated regularly in our clinic and their appointments deferred due to lockdown, so as to compare the visual acuity, fundoscopy, and optical coherence tomography (OCT) findings prior to and post lockdown. Results During the lockdown period, there was a statistically significant decrease in patients with DR and DME as compared to the same period in the previous year. Regarding patients with previously diagnosed DME, there was a statistically significant worsening in their visual acuity and central retinal thickness after lockdown as compared to the last visit before lockdown (p<0.001 for both comparisons). Concerning patients diagnosed with DR and without DME before lockdown, 30% of patients with severe non-proliferative diabetic retinopathy (NDPR) and 8.3% of patients with quiescent proliferative DR (PDR) progressed to active PDR while four out of 107 patients (3.7%) developed DME during the lockdown. Multivariate regression analysis revealed that only the time interval between the last visit before lockdown and the first visit after the lockdown was associated with the best-corrected visual acuity (BCVA) change (p=0.017). Conclusions The COVID-19-related lockdown was related to the postponement in patient care, which resulted in significantly worse visual acuity outcomes in patients with DR."
"Two-sample tests ask, ""given samples from each, are these two populations the same?"" For instance, one might wish to know whether a treatment and control group differ. With very low-dimensional data and/or strong parametric assumptions, methods such as t-tests or Kolmogorov-Smirnov tests are widespread. Recent work in statistics and machine learning has sought tests that cover situations not well-handled by these classic methods [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] , providing tools useful in machine learning for domain adaptation, causal discovery, generative modeling, fairness, adversarial learning, and more [8, [17] [18] [19] [20] [21] . Perhaps the most powerful known widely-applicable scheme is based on a kernel method known as the Maximum Mean Discrepancy, MMD [1] -or, equivalently [22] , the energy distance [3] -when one learns an appropriate kernel for the task at hand [10, 16] . Here, one divides the observed data into ""training"" and ""testing"" splits, identifies a kernel on the training data by maximizing a power criterionĴ, then runs an MMD test on the testing data (as illustrated in Figure 1a ). This method generally works very well, when enough data is available for both training and testing. In real-world scenarios, however, two-sample testing tasks can be challenging if we do not have very many data observations. For example, in medical imaging, we might face two small datasets of lung computed tomography (CT) scans of patients with coronavirus diseases, and wish to know if these Figure 1 : Comparison among (a) traditional kernel learning, (b) meta kernel learning and (c) meta multi-kernel learning for kernel two-sample testing, wherek/k i is the learned kernel. patients are affected in different ways. If they are from different distributions, the virus causing the disease may have mutated. Here, previous tests are likely to be relatively ineffective; we cannot learn a powerful kernel to distinguish such complex distributions with only a few observations. In this paper, we address this issue by considering a problem setting where related testing tasks are available. We use those related tasks to identify a kernel selection algorithm. Specifically, instead of using a fixed algorithm A to learn a kernel (""maximizeĴ among this class of deep kernels""), we want to learn an algorithm A θ from auxiliary data ( Figure 1b) : where P, Q are distributions sampled from a meta-distribution of related tasks τ , with corresponding observed sample sets S P , S Q . In analogy with meta-learning [23] [24] [25] [26] [27] [28] , we call this learning procedure meta kernel learning (Meta-KL). We can then apply A θ to select a kernel on our actual testing task, then finally run an MMD test as before (Figure 1b ). The adaptation performed by A θ , however, might still be very difficult to achieve well with few training observations; even the best A θ found by a generic adaptation scheme might over-fit to S tr P , S tr Q . For more stable procedures and, in our experiments, more powerful final tests, we propose meta multi-kernel learning (Meta-MKL). This algorithm independently finds the most powerful kernel for each related task; at adaptation time, we select a convex combination of those kernels for testing (Figure 1c ), as in standard multiple kernel learning [29] and similarly to ensemble methods in few-shot classification [30, 31] . Because we are only learning a small number of weights rather than all of the parameters of a deep network, this adaptation can quickly find a high-quality kernel. We provide both theoretical and empirical evidence that Meta-MKL is better than generic Meta-KL, and that both outperform approaches that do not use related task data, in low-data regimes. We find that learned algorithms can output kernels with high test power using only a few samples, where ""plain"" kernel learning techniques entirely fail.@story_separate@We will now review the setting of learning a kernel for a two-sample test, following [16] . Let X ⊂ R d and P, Q be (unknown) Borel probability measures on X , with samples from these distributions. We operate in a classical hypothesis testing setup, with the null hypothesis that P = Q. Maximum mean discrepancy (MMD). The basic tool we use is a kernel-based distance metric between distributions called the MMD, defined as follows. (The energy distance [3] is a special case of the MMD for a particular choice of k [22] .) Definition 1 (MMD [1] ). Let k : X × X → R be the bounded kernel of an RKHS H k (i.e., sup x,y∈X |k(x, y)| < ∞). Letting X, X ∼ P and Y, Y ∼ Q be independent random variables, If k is characteristic, we have that MMD(P, Q; k) = 0 if and only if P = Q. We can estimate MMD using the following U -statistic estimator, which is unbiased for MMD 2 (denoted by MMD 2 u ) and has nearly minimal variance among unbiased estimators [1] : Testing. Under the null hypothesis H 0 , m MMD 2 u converges in distribution as m → ∞ to some distribution depending on P and k [1, Theorem 12] . We can thus build a test with p-value equal to the quantile of our test statistic m MMD 2 u under this distribution. Although there are several methods to estimate this null distribution, it is usually considered best [10] to use a permutation test [32, 33] : under H 0 , samples from P and Q are interchangeable, and repeatedly re-computing the statistic with samples randomly shuffled between S P and S Q estimates its null distribution. Test power. We generally want to find tests likely to reject H 0 when indeed it holds that P = Q; the probability of doing so (for a particular P, Q, k and m) is called power. For reasonably large m, [10, 16] argue that the power is an almost-monotonic function of Here, σ 2 H1 is the asymptotic variance of √ m MMD 2 u under H 1 ; it is defined in terms of an expectation of (3) with respect to the data samples S P , S Q , for i, j, distinct. The criterion (4) depends on the unknown distributions; we can estimate it from samples with the regularized estimator [16]  Kernel choice. Given two samples S P and S Q , the best kernel is (essentially) the one that maximizes J in (4). If we pick a kernel to maximize our estimateĴ using the same data that we use for testing, though, we will ""overfit,"" and reject H 0 far too often. Instead, we use data splitting [2, 5, 10] : we partition the samples into two disjoint sets, S P = S tr P ∪ S te P . We obtain k tr = A(S tr P , S tr Q ) ≈ arg max kĴλ (S tr P , S tr Q ; k), then conduct a permutation test based on MMD(S te P , S te Q ; k tr ). This process is summarized in Algorithm 2 and illustrated in Figure 1a . This procedure has been successfully used not only to, e.g., pick the best bandwidth for a simple Gaussian kernel, but even to learn all the parameters of a kernel like (8) which incorporates a deep network architecture [10, 16] . As argued by [16] , classifier two-sample tests [8, 34] (which test based on the accuracy of a classifier distinguishing P from Q) are also essentially a special case of this framework -and more-general deep kernel MMD tests tend to work better. Although presented here specifically for MMD u , an analogous procedure has been used for many other problems, including other estimates of the MMD and closely-related quantities [2, 5, 35] . When data splitting, the training split must be big enough to identify a good kernel; with too few training samples m tr ,Ĵ λ will be a poor estimator, and the kernel will overfit. The testing split, however, must also be big enough: for a given P, Q and k, it becomes much easier to be confident ; kernel architecture (8) and parameters ω0; regularization λ 1. Initialize algorithm parameters: θ := [ωstart] ← [ω0] 2. Define a parameterized learning algorithm A θ (S P , S Q ) as: ω ← ωstart; for t = 1, . . . , nsteps do ω ← ω + η∇ωĴ λ (S P , S Q ; kω); end for; return kω for T = 1, 2, . . . , Tmax do 3: Sample I as a set of indices in {1, 2, . . . , N } of size n batch for i ∈ I do 4: Split data as S P i = S tr P i ∪ S te P i and S Q i = S tr Q i ∪ S te Q i ; 5: Apply the learning algorithm: ki ← A θ (S tr P i , S tr Q i ) end for 6: Update θ ← θ + β ∇ θ i∈IĴ λ (S te P i , S te Q i ; ki); # update θ, i.e. wstart, to maximize J (A θ , τ ) end for 7: return A θ that MMD(P, Q; k) > 0 as m te grows and the variance in MMD u (P, Q; k) accordingly decreases. When the number of available samples is small, both steps suffer. This work seeks methods where, by using related testing tasks, we can identify a good kernel with an extremely small m tr ; thus we can reserve most of the available samples for testing, and overall achieve a more powerful test. Another class of techniques for kernel selection avoids the need for data splitting via selective inference [15] . At least as studied by [15] , however, it is currently available only for restricted classes of kernels and with the much-less-accurate ""streaming"" estimates of the MMD, which for fixed kernels can yield far less powerful tests than MMD u [36] . In Section 5.4, we will demonstrate that in our settings, the data-splitting approach is empirically much more powerful. To handle cases with low numbers of available data samples, we consider a problem setting where related testing tasks are available. We use those tasks in a framework inspired by meta-learning [e.g. 23], where we use those related tasks to identify a kernel selection algorithm, e.g. (1). Specifically, we define a task as a pair T = (P, Q) of distributions over X we would like to distinguish, and assume a meta-distribution τ over the space of tasks T . drawn from a task distribution τ , and observe meta- Our goal is to use these meta-samples to find a kernel learning algorithm A θ , such that for a target task (P , Q ) ∼ τ and samples S P ∼ (P ) n , S Q ∼ (Q ) n , A θ (S P , S Q ) returns a kernel which will achieve high test power on (P , Q ). We measure the performance of A θ based on the expected test power criterion for a target task: If τ were in some sense ""uniform over all conceivable tasks,"" then a no-free-lunch property would cause M2ST to be hopeless. Instead, our assumption is that tasks from τ are ""related"" enough that we can make progress at improving (7) . We will propose two approaches to finding an A θ for M2ST. Neither is specific to any particular kernel parameterization, but for the sake of concreteness, we will follow Liu et al. [16] in choosing the form where φ is a deep neural network which extracts features from the samples, and κ is a simple kernel (e.g., a Gaussian) on those features, while q is a simple characteristic kernel (e.g. Gaussian) on the input space; ∈ (0, 1] ensures that every kernel of the form k ω will be characteristic. Here, ω represents all parameters in (8): most will be in φ, but κ and q may have a few parameters (e.g. length scales), and we can also learn . Algorithm 2 Testing with a Kernel Learner 1: Input: Two samples: S P , S Q ; algorithm A θ 2: Split data as S P = S tr P ∪ S te P and S Q = S tr Meta-KL. We first propose Algorithm 1 as a standard approach to optimizing (7), à la MAML [23] : A θ takes a small, fixed number of gradient ascent steps inĴ λ for the parameters of k ω , starting from a learned initialization point ω start ∈ θ (lines 1-2). We differentiate through A θ , and perform stochastic gradient ascent to find a good value of θ based on the meta-training sets (lines 3-6, also illustrated in Figure 2 ). Once we have learned a kernel selection procedure, we can again apply it to a testing task with Algorithm 2. As we will see in the experiments, this approach does indeed use the meta-tasks to improve performance on target tasks. Differently from usual meta-learning settings, as in e.g. classification [23] , however, here it is conceivable that there is a single good kernel that works for all tasks from τ ; improving on this single baseline kernel, rather than simply overfitting to the very few target points, may be quite difficult. Thus, in practice, the amount of adaptation that A θ actually performs in its gradient ascent can be somewhat limited. As an alternative approach, we also consider a different strategy for A θ which may be able to adapt with many fewer data samples, albeit in a possibly weaker class of possible kernels. Here, to select an A θ , we simply find the best kernel independently for each of the meta-training tasks. Then A θ chooses the best convex combination of these kernels, as in classical multiple kernel learning [29] and similarly to ensemble methods in few-shot classification [31] . At adaptation time, we only attempt to learn N weights, rather than adapting all of the parameters of a deep network; but, if the meta-training tasks contained some similar tasks to the target task, then we should be able to find a powerful test. This procedure is detailed in Algorithm 3 and illustrated in Figure 1c . We now analyze and compare the theoretical performance of direct optimizing the regularized test power from small sample size with our proposed meta-training procedures. To study our learning objective of approximate test power, we first state the following relevant technical assumptions, made by Liu et al. [16] ; they discuss these assumptions in detail, particularly Assumption (C). (A) The kernels k ω are uniformly bounded as follows. For the kernels we use in practice, ν = 1. (B) The possible kernel parameters ω lie in a Banach space of dimension D. Furthermore, the set of possible kernel parameters Ω is bounded by The kernel parameterization is Lipschitz: for all x, y ∈ X and ω, ω ∈ Ω, Proposition 3 (Direct training with approximate test power, Theorem 6 of [16] ). Under Assumptions (A) to (C), letΩ s ⊆ Ω be the set of kernel parameters for which σ 2 ω ≥ s 2 , and assume ν ≥ 1 is constant. Take the regularized estimateσ 2 ω,λ =σ 2 Since m is small in our settings, and s may also be small for deep kernel classes as noted by Liu et al. [16] , this bound may not give satisfying results. The key mechanism that drives meta-testing to work, intuitively, is training kernels on related tasks. How do we quantify the relatedness between different two-sample problems? Definition 4 (γ-relatedness). Let (P, Q) and (P , Q ) be the underlying distributions for two different two-sample testing tasks. We say the two tasks are γ-related w. wherek = arg max k J(P, Q; k),k = arg max k J(P , Q ; k). The relatedness measure explicitly quantifies how good the ""best"" kernel learned from one two-sample testing problem applying to another two-sample testing problem, through the approximate test power objective. It also implies the two problems are of similar difficulty, since for small γ, the ability of our MMD test statistics to distinguish the distributions (with optimal kernels) are similar. Definition 5 (Adaptation by choosing one best kernel). With the set of base kernels {k 1 ω , . . . , k N ω }, k ω = arg max iĴλ ne (S tr P , S tr Q ; k (i) ) is said to be the best kernel adaptation. Definition 6 (Adaptation with Meta-MKL). Given a set of kernels {k 1 ω , . . . , k N ω }, the Meta-MKL adaptation is the kernelk = i∈[N ]ŵ i k i ω whereŵ = arg max wĴλ ne (S tr P , S tr Q ; i w i k i ω ). These adaptation steps use the same learning objective,Ĵ λ , as directly training a deep kernel in Proposition 3. Choosing one kernel from well-trained base kernels is expected to be easier than training the deep kernel parameters, based on the following theorem (proved in Appendix A). Theorem 7 (Adaptation by choosing one best base kernel). Suppose the kernel parameterization k ω satisfies Assumptions (A) to (C), and letΩ s ⊆ Ω be the set of kernel parameters for which σ 2 ω ≥ s 2 . Assume ν ≥ 1 is constant. Denote the relatedness between meta-task (P i , Q i ) and target task (P, Q) as γ i , and set γ = min{γ 1 , . . . , γ N }. For meta-training with sample size of order n, take λ = n −1/3 ; for adaptation phase, take λ ne = m −1/3 . Denote the chosen kernel ask ω . Let . Then, we have s > s, and with probability at least 1 − δ, From the theorem, we can see that the β n term depends on the meta-training sample size n m. With enough (relevant) meta-training tasks, γ goes to 0 quickly with the number of meta-training tasks N . So, the overall uniform convergence bound is likely to be dominated by the termβ. Notice that bothβ and the β of Proposition 3 have the same m −1/3 rate. This is roughly to be expected; similar optimization objectives are applied for both learning and adaptation, which are limited by sample size m. However, each of the other terms inβ is much smaller than the corresponding term in β n : we expect that the variance term s s, that log N is far smaller than D log(R ω n) since D will be roughly the number of parameters in a neural network, and also that L k in β n will be quite large: the bound of Proposition 23 of [16] is exponential in the depth of the network. Combining all of these factors, we expect adaptation by picking the one best related kernel to be far more efficient than direct training. In the appendix, we show a similar result for adaptation with Meta-MKL. Following Liu et al. [16] , we compare the following baseline tests with our methods on synthetic and real-world datasets: 1) 1) MMD-D: MMD with a deep kernel whose parameters are optimized; 2) MMD-O: MMD with a Gaussian kernel whose lengthscale is optimized; 3) Mean embedding (ME) test [5, 37] ; 4) Smooth characteristic functions (SCF) test [5, 37] , and 5) Classifier two-sample tests, including C2ST-S [8] and C2ST-L as described in Liu et al. [16] . None of these methods use related tasks at all, so we additionally consider an aggregated kernel learning (AGT-KL) method, which optimizes a deep kernel of the form (8) by maximizing the value ofĴ λ averaged over all the related tasks in the meta-training set S. Again following the the setup of Liu et al. [16] , we evaluate tests as follows. For synthetic datasets, we take a single sample set for S tr P and S tr Q , and learn a kernel/test locations/etc once for each method on that training set. We then evaluate its rejection rate (power or Type-I error, depending on if P = Q) using 100 new sample sets S te P , S te Q from the same distribution. For real datasets, we select a subset of the available data for S tr P and S tr Q and train on that; we then evaluate on 100 random subsets, disjoint from the training set, of the remaining data. We repeat this full process 10 times, and report the mean rejection rate of each test and the standard error of the mean rejection rate. Implementation details are in Appendix B; the code is available at github.com/fengliu90/MetaTesting. We use a bimodal Gaussian mixtures dataset proposed by [16] , known as high-dimensional Gaussian mixtures (HDGM): P and Q differ in the covariance of a single dimension pair but are otherwise the same. Specifically, where µ h 1 = 0 d , µ h 2 = 0.5 × 1 d . In this paper, our target task is T = (P, Q(0.7)) and meta-samples are drawn from T = {T i := (P, Q(0.3 + 0.1 × i/n))} N i=1 , where N = 100; note that the target task is well outside the scope of training tasks. In experiments, we set d to 2; given the small number of data samples, this is already a difficulty task for existing kernel two-sample tests. To evaluate all tests given limited data, we set the number of training samples (S tr P , S tr Q ) to 50, 100, 150 per mode, and the number of testing samples (S te P , S te Q ) from 50 to 250. Figure 3 illustrates test powers of all tests. Meta-MKL and Meta-KL are the clear winners, with both tests much better when m te is over 100 per mode. It is clear that previous kernel-learning based tests perform poorly due to limited training samples. Comparing Meta-MKL with Meta-KL, apparently, we can obtain much higher power when we consider using multiple trained kernels. Although AGT-KL performs better than baselines, it cannot adapt to the target task very well: it only cares about ""in-task"" samples, rather than learning to adapt to new distributions. In this subsection, we distinguish the standard datasets of small images CIFAR-10 and CIFAR-100 [38] from the attempted replication CIFAR-10.1 [39] , similar to Liu et al. [16] . Because only a relatively small number of CIFAR-10.1 samples are available, it is of interest to see whether by meta-training only on CIFAR-10's training set (as described in Appendix B), we can find a good test to distinguish CIFAR-10.1, with m tr ∈ {100, 200}. Testing samples (i.e., S te P and S te Q ) are from test sets of each dataset. We report test powers of all tests with 200, 500, 900 testing samples in Table 1 (CIFAR10 compared to CIFAR10.1) and Table 2 (CIFAR100 compared to CIFAR10.1). Since Liu et al. [16] have shown that CIFAR10 and CIFAR10.1 come from different distributions, higher test power is better in both tables. The results demonstrate that our methods have much higher test power than baselines, which is strong evidence that leveraging samples from related tasks can boost test power significantly. Interestingly, C2ST tests almost entirely fail in this setting; it is hard to learn useful information with only a few data points. In Appendix B, we also report results when meta-samples are generated by the training set of CIFAR100 dataset. This subsection studies how closeness between related tasks and the target task affects test powers of our tests. Given the target task T in synthetic datasets, we define tasks T with closeness C as It is clear that T (0) will contain our target task T (i.e., the closeness is zero). In Figure 4 , we illustrate the test power of our tests when setting closeness C to 0.1, 0.2 and 0.3, respectively. It can be seen that Meta-MKL and Meta-KL outperforms AGT-KL all the figures, meaning that Meta-MKL and Meta-KL actually learn algorithms that can quickly adapt to new tasks. Another phenomenon is that the gap between test powers of meta based KL and AGT-KL will get smaller if the closeness is smaller, which is expected since AGT-KL has seen closer related tasks.  In previous sections, we mainly compare with previous kernel-learning tests and have shown that the test power can be improved significantly by our proposed tests. We now show that each component in our tests is effective to improve the test power, and (as previously mentioned) data splitting in Meta-MKL is much better than using the recent test of Kübler et al. [15] which avoids data splitting. The comparison has been made in synthetic datasets studied in Section 5.1, and results are shown in Table 3 . Meta-MKL-A is a test that takes all w i = 0 in Algorithm 3, so that kernels are weighted equally. AGT-MKL uses multiple kernels in AGT-KL (learning weights like Meta-MKL), and AGT-MKL-A does not learn the weights but assign ""average"" weights 1/K directly to all base-kernels. Meta-MKL SI is a kernel two-sample test using the selective inference technique of Kübler et al. [15] rather than data splitting in its A θ . From Table 3 , we can see that introducing multiple kernel learning (MKL) scheme extensively improves test power, as it combines useful features learned from base kernels covering different aspects of the problems. Moreover, learning with approximate test power with data-splitting in the meta-setting also outperforms the non-splitting testing procedure MetaMKL SI , since MetaMKL SI requires a poor linear estimator of MMD. The result also indicates that leveraging related tasks is also important to improve the test power even though we need a small set of training samples (e.g., 10). Proposition 3 shows uniform convergence ofĴ λ for direct adaptation of a kernel class, whether a deep kernel or multiple kernel learning. For our analysis of choosing the best single kernel, however, we only need uniform convergence over a finite set, where we can obtain a slightly better rate. Lemma 8 (Generalization gap for choosing one best kernel adaptation). Denote the set of objectives for base kernel learning from meta-training procedure as ω ) and the objectives used to determine the one best kernel adaptation, from m samples, in Definition 5 aŝ J i =Ĵ λ ne (S P , S Q ; k (i) ω ). Then, with probability at least 1 − δ, Proof. To bound max i∈[N ] |Ĵ i − J i |, we consider high-probability bounds for concentration ofη ω and σ 2 ω with McDiarmid's inequality and a union bound, as developed within the proofs of Propositions 15 and 16 of Liu et al. [16] . With probability at least 1 − δ, we have Then, taking σ 2 i,λ = σ 2 i + λ, we can decompose the worst-case generalization error as Taking the upper bound on the kernel to be constant, in our case ν = 1, the above equation reads Taking the regularizer λ = 1 m 1/3 to achieve the best overall rate, We note that since the adaptation step is based on m samples from the actual testing task, our generalization result is derived based on the sample size m. As explained in the main text, even though the sample size is still small, the adaptation result benefit from a much better trained base kernel set, giving rise to large s compared to s from directly training from the deep kernel parameters with m samples. With all the building blocks, we proceed to prove our main result, Theorem 7. Proof of Theorem 7. Since the meta-training follows the same optimization objectiveĴ, the generalization result in Proposition 3 applies. With much larger sample size (of order n), the generalization error for each meta-training task is of order n −1/3 . The relatedness assumption ensures the closest problem is γ away; Lemma 8 shows the generalization error for adaptation. We then show the result via the decomposition where (a) ≤β = O  We implement all methods with Pytorch 1.1 (Python 3.8) using an NIVIDIA Quadro RTX 8000 GPU, and set up our experiments according to the protocol proposed by Liu et al. [16] . In the following, we demonstrate our configurations in detail. We run ME and SCF using the official code [5] , and use Liu et al.'s implementations of most other tests. We use permutation test to compute p-values Table 3 . We set α = 0.05 for all experiments. We use a deep neural network g • φ as the classifier in C2ST-S and C2ST-L, where g is a two-layer fully-connected binary classifier, and φ is the feature extraction architecture also used in the deep kernels in MMD-D, AGT-KL, Meta-KL, Meta-MKL, and methods in Table 3 . For HDGM, φ is a five-layer fully-connected neural network. The number of neurons in hidden and output layers of φ are set to 3 × d, where d is the dimension of samples. These neurons use softplus activations, log(1 + exp(x)). For CIFAR, φ is a convolutional neural network (CNN) with four convolutional layers and one fully-connected layer. The structure of the CNN follows the structure of the feature extractor in the discriminator of DCGAN [40] (see Figures 6 and 7 for the structure of φ in our tests, MMD-D, C2ST-S and C2ST-L). We randomly select data from two different classes to form the two samples (n i is 100) as meta-samples in CIFAR-10/CIFAR-100. Thus, there are C 2 10 and C 2 100 tasks when running Algorithm 1 on training sets of CIFAR-10 and CIFAR-100. For each task, we have 200 instances. DCGAN code is from https://github.com/eriklindernoren/ PyTorch-GAN/blob/master/implementations/dcgan/dcgan.py. We use the Adam optimizer [41] to optimize network and/or kernel parameters. Hyperparameter selection for ME, SCF, C2ST-S, C2ST-L, MMD-O and MMD-D follows Liu et al. [16] . In Algorithm 1, λ is set to 10 −8 , and the update learning rate η (line 2) is set to 0.8, and the meta-update learning rate is set to 0.01. Batch size is set to 10, and the maximum number of epoch is set to 1, 000. In line 6 in Algorithm 1, we use Adam optimizer with default hyperparameters. In line 1 in Algorithm 3, we adopt Adam optimizer with default hyperparameters and set learning rate to 0.01. Besides, we use the algorithm from Algorithm 1 to initialize parameters in the optimization algorithm. Note that we do not use dropout.  CIFAR10.1 is available from https://github.com/modestyachts/CIFAR-10.1/tree/ master/datasets (we use cifar10.1_v4_data.npy). This new test set contains 2, 031 images from TinyImages [43] . In this section, we report results when meta-samples are generated by the training set of CIFAR100 dataset, which are shown in Tables 4 and 5 . It can be seen that our methods still have high test powers compared to previous methods. Besides, we can get higher test power on the task CIFAR100 vs CIFAR10.1 compared to results in Table 2 , since meta-samples used here are closer to the target task. This phenomenon also appears in Section 5.3.@story_separate@This paper proposes kernel-based non-parametric testing procedures to tackle practical two-sample problems where the sample size is small. By meta-training on related tasks, our work opens a new paradigm of applying learning-to-learn schemes for testing problems, and the potential of very accurate tests in some small-data regimes using our proposed algorithms. It is worth noting, however, that statistical tests are perhaps particularly ripe for mis-application, e.g. by over-interpreting small marginal differences between sample populations of people to claim ""inherent"" differences between large groups. Future work focusing on reliable notions of interpretability in these types of tests is critical. Meta-testing procedures, although they yield much better tests in our domains, may also introduce issues of their own: any rejection of the null hypothesis will be statistically valid, but they favor identifying differences similar to those seen before, and so may worsen gaps in performance between ""well-represented"" differences and rarer ones.","Modern kernel-based two-sample tests have shown great success in distinguishing complex, high-dimensional distributions with appropriate learned kernels. Previous work has demonstrated that this kernel learning procedure succeeds, assuming a considerable number of observed samples from each distribution. In realistic scenarios with very limited numbers of data samples, however, it can be challenging to identify a kernel powerful enough to distinguish complex distributions. We address this issue by introducing the problem of meta two-sample testing (M2ST), which aims to exploit (abundant) auxiliary data on related tasks to find an algorithm that can quickly identify a powerful test on new target tasks. We propose two specific algorithms for this task: a generic scheme which improves over baselines and amore tailored approach which performs even better. We provide both theoretical justification and empirical evidence that our proposed meta-testing schemes out-perform learning kernel-based tests directly from scarce observations, and identify when such schemes will be successful."
"Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the infective agent behind the current coronavirus disease pandemic (Zhou et al., 2020; Zhu et al., 2020) , is an enveloped ssRNA virus with a corona-shaped surface layer of spikes that are thought to play an important role in the infection mechanism (Hoffmann et al., 2020; Shang et al., 2020; Walls et al., 2020; Wang et al., 2020; Watanabe et al., 2020) . Structural information about the spike protein has been acquired either on crystals of purified protein (Henderson et al., 2020; McCallum et al., 2020; Walls et al., 2020; Wrapp et al., 2020) or on fixed and frozen virus particles (Ke et al., 2020; Turonova et al., 2020; Yao et al., 2020) . It has been suggested that the spike hinges provide structural flexibility (Ke et al., 2020; Turonova et al., 2020) . Highresolution cryoelectron tomography observations indicate that the ribonucleoprotein (RNP) of SARS-CoV-2 is partitioned into spherical, basketlike structures (Yao et al., 2020) . However, the surface dynamics and mechanical properties of native virions remain to be understood. Here we employed atomic force microscopy (AFM) and molecular force spectroscopy (de Pablo and Schaap, 2019; Kellermayer et al., 2018; Kiss et al., 2020) to investigate the topographical and nanomechanical properties of native SARS-CoV-2 virions immobilized on an anti-spikeprotein functionalized substrate surface. The unique single-particle approach revealed that the surface layer of spikes on SARS-CoV-2 is highly dynamic, the virion is unusually compliant and resilient, and it displays an unexpected global thermal stability.@story_separate@SARS-CoV-2, the virus responsible for the current COVID-19 pandemic, displays a coronashaped layer of spikes which play fundamental role in the infection process. Recent structural data suggest that the spikes possess orientational freedom and the ribonucleoproteins segregate into basketlike structures. How these structural features regulate the dynamic and mechanical behavior of the native virion, however, remain unknown. By imaging and mechanically manipulating individual, native SARS-CoV-2 virions with atomic force microscopy, here we show that their surface displays a dynamic brush owing to the flexibility and rapid motion of the spikes. The virions are highly compliant and able to recover from drastic mechanical perturbations. Their global structure is remarkably temperature resistant, but the virion surface becomes progressively denuded of spikes upon thermal exposure. Thus, both the infectivity and thermal sensitivity of SARS-CoV-2 rely on the dynamics and the mechanics of the virus. The native coronavirus 2 displays a dynamic surface layer of spikes, a large mechanical compliance and unique self-healing capacity. The topographical structure of individual SARS-CoV-2 virus particles bound to substrate surface was imaged by using AFM (Fig.1.a) . To increase the efficiency and specificity of binding we employed a monoclonal anti-spike-protein antibody, which resulted in a nearly hundred-fold enhancement in the density of substrate-bound virions (Fig. S1) . AFM images of glutaraldehyde-fixed SARS-CoV-2 revealed spherical virions ( Fig.1.b) with somewhat variable dimensions (Table S1 ) and a rugged surface (Fig.1.c) . The mean central height of the virions (Fig. S2) , the structural parameter least sensitive to AFM tip convolution, was 62 nm (± 8 nm, S.D.) The height was smaller than the virion diameter measured in cryo-electron microscopic images (Ke et al., 2020; Turonova et al., 2020; Yao et al., 2020) , suggesting that the virus particles were partially flattened on the substrate. The 3D-rendered AFM image ( Fig.1 .d) supported this interpretation and revealed that the rugged surface is due to the presence of protrusions which we identify as the spikes (S protein trimers). In high-resolution (pixel size 5 Å) AFM images ( Fig.1 .e) individual S trimers were resolved. Visual inspection of the S trimers pointed at their positional, rotational and flexural disorder in the viral envelope. The mean nearest-neighbor distance between the S trimers, and their topographical height were 21 nm (± 6 nm, S.D.) and 13 nm (± 5 nm, S.D.), respectively (Fig. S3, Table S2 ). From the mean nearest-neighbor distance and the virion dimensions we calculated that an average 61 spikes cover the SARS-CoV-2 virus particle surface. This number exceeds those reported recently (24 (Ke et al., 2020) , 26 (Yao et al., 2020) and 40 (Turonova et al., 2020) ), suggesting that the spike number is highly variable and may be regulated during virus assembly and maturation. The flexural disorder observed here supports the interpretation of cryo-electron microscopic data (Ke et al., 2020; Turonova et al., 2020; Yao et al., 2020) , revealing a high degree of spike flexibility. We propose that the positional and rotational disorder of S trimers are due to their mobility in the virus envelope. To circumvent the effects caused by chemical fixation and uncover the spike dynamics in situ, we investigated the topography of unfixed, native SARS-CoV-2 virions (Fig.2) . Unexpectedly, we were unable to resolve S trimers on the virion surface at any of the investigated scanning strengths (Fig. S4) ; rather, the virus particles displayed a blurred, smooth surface ( Fig.2.a) . The mean central height of the native virions was 83 nm (± 7 nm, S.D.), which is significantly greater than that observed for the fixed (Fig.2.b) . We interpret the blurring of virion topography as the result of aperture error caused by time averaging of spike movement within the sampling region of each image pixel, hence the increase in virion height is caused by the AFM tip scanning an apparent dynamic surface (Fig.2.c) . Considering that the typical pixel sampling frequency (fs) in these images was 308 Hz, the frequency of spike dynamics exceeds the Nyquist frequency of fs/2 (154 Hz). Because the pixel size (d) is 2 nm, the spikes dynamically fluctuate, within a space limited by their flexible neck, with a speed (v=dfs/2) exceeding ~0.3 nm/ms. Most plausibly, spike motion is dictated by the Brownian dynamics of the receptor-bindingdomain (RBD) trimer which may then be thought of as a tethered particle. Spike mobility in the virus envelope may contribute further to the observed dynamics. An alternative explanation for the observed blurred virion surface is that the spikes evade the moving AFM cantilever tip which then scans the envelope surface. We exclude this possibility, however, because, while it relies on a similarly dynamic spike behavior, a reduced virion height should have been observed. We speculate that the rapid spike motion revealed by these experiments contributes to an efficient dynamic search by the virion on the surface of the targeted host cell, which explains why SARS-CoV-2 is at least as infective as the influenza virus (Petersen et al., 2020) in spite of its fewer spikes (up to ~60 in SARS-CoV-2 versus up to ~350 in influenza A (Harris et al., 2006) ). We investigated the mechanical properties of SARS-CoV-2 by lowering the cantilever tip on the vertex of individual virions selected on the AFM image ( Fig.3 .a). The virion was indented by pressing the tip downwards ( Fig.3 .b) with constant velocity (typically 0.5 µm/s) until a preset maximum force, measured by the cantilever deflection, was reached (typically 2-3 nN). Such a nanomechanical manipulation did not result in permanent topographical changes ( Fig.3 .c) in spite of completely compressing the virion so that the tip reached all the way to the substrate resulting in a wall-to-wall deformation ( Fig.3 .d). In the initial stage of indentation, immediately following the landing of the tip on the virion, we observed a linear force response that allowed us to measure virion stiffness (Table S3) . Mean stiffness was 13 pN/nm (± 5 pN/nm, S.D.), which makes SARS-CoV-2 the most compliant virus investigated so far (Cieplak and Robbins, 2013; Mateu, 2012) . Virion stiffness is somewhat lower than that measured for the influenza virus lipid envelope (Li et al., 2011) , suggesting that the elasticity of SARS-CoV-2 is dominated by its envelope, and the RNP contributes little to the overall viral mechanics. The elastic regime was followed by a yield point marking the deviation from the linear force response and the onset of force-induced structural transitions which continued to take place until total compression. Unlike in other viruses (de Pablo, 2018; de Pablo and Schaap, 2019), force did not drop to near zero values following mechanical yield, indicating that virion collapse or breakage was not evoked in spite of the drastic mechanical perturbation. Conceivably, the mechanical yield is made possible by the force-induced rearrangement of the basketlike structures of the RNPs (Yao et al., 2020) . Subsequent to indentation we retracted the cantilever. Remarkably, the virion generated forces up to several hundred pN during retraction, suggesting that structural recovery was taking place, possibly driven by the structural restitution of the initial RNP arrangement. The process continued until the initial virion height was reestablished. The mean force-spectroscopical height was 94 nm (± 10 nm, S.D.), which is comparable to that obtained from topographical data. The differences between the indentation and retraction force traces revealed a force hysteresis indicating that part of the mechanical energy invested in distorting the virion was dissipated as heat. We were able to continue the indentation-retraction cycles up to 100 times, but the virions never broke or collapsed (Figs.3.ef). Rather, both the indentation and retraction force traces relaxed after about a dozen mechanical cycles, resulting in a minimized force hysteresis. The force response may potentially be explained by two other mechanisms alternative to virion compression. The first one is the force-induced virion rolling on the substrate. We exclude this possibility, because, due to the presence of anti-spike-protein antibodies on the surrounding surface, the process is expected to be completely irreversible. The second is the sideways slippage of the cantilever tip, off of the virion surface. We exclude this possibility based on a calibration of the cantilever's lateral torsion (Fig. S5) and because this process is expected to be completely reversible. We can only speculate about the mechanisms behind the persistent structural self-healing of SARS-CoV-2. Conceivably, the process involves the dynamic interaction between the RNA, protein and lipid components. Notably, in some (~1 %) of the retraction traces we observed sawtoothshaped force responses, the peak forces of which fall between ~210-330 pN (Fig.3.f) . The most plausible explanation is that the force peaks correspond to the mechanically-driven unfolding of S protein domains (Moreira et al., 2020) . Altogether, the SARS-CoV-2 virion is a mechanically stable, remarkably compliant and surprisingly resilient nanoparticle. To assess the thermal stability of SARS-CoV-2, we explored the topographical changes of virions exposed to high-temperature treatment (Fig.4) . The sample was exposed to temperatures of 60 ( Fig.4.a) , 80 ( Fig.4 .b) and 90 ˚C (Fig.4 .c) for ten minutes then cooled back to 20 ˚C for AFM imaging. Remarkably, virions remained on the substrate surface, and their global appearance was only slightly altered. Virion topography became somewhat faceted, but the particles retained their blurred, smooth surface. To test whether spikes were still present following thermal exposure, we fixed the sample with 5% glutaraldehyde (Fig.4.d) . Although the rugged topography, seen in chemically fixed SARS-CoV-2 virions (Fig.1.c) , was partially restored, the spikes were much fewer, less distinct, and their trigonal shape (Fig.1.e) could not be resolved (Fig.4.e) , suggesting they became thermally denatured. Furthermore, the smooth areas interspersed between rugged regions indicate that thermal treatment resulted in a progressive dissociation of the S trimers from the virion surface. Thus, the SARS-CoV-2 virion displays an unexpected global thermal stability, which is likely related to their aerosol and surface stabilities (van Doremalen et al., 2020) . However, the conformational response of the spike proteins observed here eventually leads to the heat-induced inactivation of SARS-CoV-2. B.K., Z.K., B.P. and M.S.Z.K. conceived experiments; Z.K. and B.P. purified the SARS-CoV-2 samples; B.K. and M.S.Z.K. performed AFM imaging and force spectroscopy measurements and analyzed data; B.K. and M.S.Z.K. wrote paper; M.S.Z.K. secured financial support for the project. All authors critically read and revised the manuscript. The authors declare no conflict of interest. SARS-CoV-2 was isolated from the oropharyngeal swab of a laboratory-confirmed COVID-19 patient in Hungary and passed two times in VeroE6 cell line (European Collection of Authenticated Cell Culture, Salisbury, U.K.) in Dulbecco′s Modified Eagle′s Medium (Lonza, Basel, Switzerland) supplemented with 5% fetal bovine serum (EuroClone, Pero, Italy) and Cell Culture Guard (PanReac AppliChem, Darmstadt, Germany). To remove the disturbing effects of fetal bovine serum albumin, an additional passage was carried out in VP-SFM serumfree, ultra-low protein medium (Gibco, ThermoFisher Scientific) supplemented with Lglutamine (Sigma-Aldrich, Merck, Darmstadt, Germany). Four days after inoculation, when full cytopathic effects were observed, the virus-containing medium was collected and centrifuged (3000 x g) to remove debris. To concentrate the virus, the supernatant was ultracentrifuged (70,000 x g, 1.5 hours, 4°C) in 13.5-ml lockable plastic tubes using a Sorvall MTX-150 ultracentrifuge. The supernatant was removed and the pellet was resuspended in 100 µl VP-SFM. All sample preparation steps were performed in biosafety level-3 (BSL-3) conditions. 100 µl of 0.1% w/v poly-L-lysine (PLL) (Merck, Darmstadt, Germany) solution was pipetted onto a freshly cleaved mica surface (Ted Pella, Redding, CA) and incubated for 20 minutes, following which the surface was rinsed with distilled water and dried in N2 stream. 100 µl of 25% w/v grade I glutaraldehyde (GA) (Merck, Darmstadt, Germany) was then pipetted on the surface and incubated for 30 min, followed by further rinsing with distilled water and drying in N2 stream. Subsequently, 100 µl of 10 µg/ml recombinant protein G (Merck, Darmstadt, Germany) was added and incubated for 30 minutes, then the surface was washed five times each with 100 µl phosphate-buffered saline (PBS). 100 µl of 10 µg/ml SARS-CoV-2 Spike Glycoprotein Antibody (#abx376478, Abbexa Ltd, Cambridge, UK) was then added, and the surface was incubated for 1 hour. All surface functionalization steps were performed at room temperature. Unbound antibodies were removed by repeated rinsing with PBS. The anti-spike glycoprotein antibody-coated surfaces were stored under PBS for up to 5 days at 4 °C. An aliquot (20 µl) of purified SARS-CoV-2 sample was pipetted onto the anti-spike antibodycoated substrate surface and incubated at 37 ˚C for 30 minutes. To increase virion surface density, another aliquot was added and incubated. The process was repeated twice. Subsequently, the surface was rinsed gently with PBS to remove unbound virions. All the sample-loading and washing steps were carried out in a laminar-flow hood in BSL-3 conditions (at the National Biosafety Laboratory, National Public Health Centre, Hungary). For AFM imaging of chemically fixed SARS-CoV-2, 100 µl 5% GA solution (in PBS) was added, and the sample was incubated for >1 hour, ensuring both fixation and virus inactivation. Then, the sample was carried to the AFM laboratory (Department of Biophysics and Radiation Biology, Semmelweis University) for loading in the environmental scanner unit of the Cypher ES AFM instrument. For imaging native virions, the fixation step was omitted and the sample was loaded directly in the Cypher ES Scanner. To ensure compliance with safety measures, a closed cantilever holder and gas-tight sample chamber was used, and the sample was loaded into the scanner in a laminar-flow hood in BSL-3 conditions. Then the scanner was carried to the AFM laboratory and inserted into the AFM instrument for imaging. To further comply with safety measures, following AFM imaging the native virus samples were discarded in either of two ways. In the first, the AFM scanner was taken to the BSL-3 laboratory for removal and chemical desctruction of the sample. In the second, the sample was heated to 90 ˚C for >10 minutes with the temperature-controller unit of the Cypher ES scanner. Then, the sample was removed for immediate chemical destruction (5% NaClO). Atomic force microscopy imaging was carried out with an Asylum Research Cypher ES instrument (Oxford Instruments, Santa Barbara, CA) Voros et al., 2017) . Resonant-mode (AC-or non-contact mode) scanning was performed under liquid in PBS with BL-AC40TS (Olympus Corporation, Japan) cantilevers. The cantilever was oscillated near its resonance frequency (tyically around 20 kHz) by using photothermal excitation at a typical free amplitude of 0.5 V. Imaging was carried out at a typical setpoint of 350 mV and with scanning speeds <1 µm/s to prevent the mechanical dislodging of virions from the substrate surface. Alternatively, we used fast force mapping (FFM, or ""jumping-mode"" AFM(Moreno-Herrero et al., 2004) ) to collect image data. In FFM the cantilever was driven sinusoidally with a typical frequency of 300 Hz and a setpoint force of 100 pN to obtain a force curve for each pixel, from which the topographical image was reconstructed. The two (AC-mode and FFM) imaging modes gave similar results. Thermal exposure of the virions was achieved by heating the sample stage to pre-set temperatures . Force spectroscopic measurements were carried out on virions selected on the AFM images. The cantilever was lowered, in contact mode, onto the vertex of the virion with 0.5 µm/s velocity until the setpoint force was reached (typically 2-3 nN). Cantilever spring constants were determined by the thermal method (Hutter and Bechhoefer, 1993) prior to imaging. Spring constants were between 90-120 pN/nm. Image postprocessing and data analysis were performed by using the AFM driving software AR16, IgorPro 6.37 (Wavemetrics, Lake Oswego, OR). Particle analysis was carried out in subsequent analytical steps. First, the particles were demarcated by masking at the full width at half maximal height. Second, the mask was eroded, then dilated to gently smooth particle edges, while also getting rid of small scanning artefacts. We ignored particles with extreme deformities, ones too close to each other or with an area smaller than 3000 nm 2 . Height was calculated at the center of the particles. Volume was calculated as the sum of the heights of individual pixels of the particle multiplied by the spatial (x, y) scaling values to correct for image pixel resolution. Particle diameter was calculated as the diameter of a circle with the same area as the particle itself. reached. c. AFM image of the same overview sample area following nanomechanical manipulation. We could not detect any topographical sign of permanent structural change. d. Example of a force versus distance curve obtained during a single indentation-retraction cycle. From the slope of the indentation curve (gray dotted line) and the distance between the landing point and substrate limit of the trace we obtained the stiffness and the forcespectroscopic height of the virions, respectively. Red and blue dotted lines indicate indentation and retraction data, respectively, obtained in the 100 th nanomechanical cycle. e. Force versus distance curves obtained during repeated indentation of a single SARS-CoV-2 virion. f. The matching force versus distance curves obtained during retraction. In some traces force sawteeth (red trace) corresponding to the unfolding of domains in a surface protein, plausibly within the S trimer. Fig. 4 . Effect of temperature change on the topographical structure of SARS-CoV-2. The sample was heated for ten minutes at 60 (a), 80 (b) and 90 ˚C (c), then cooled back to 20 ˚C prior to AFM imaging. The virions persist on the substrate surface with their global structure nearly intact, but the topography becomes progressively more rugged, pointing at the gradual disappearance of the dynamic surface smoothing hence reduction of spike dynamics. d. AFM image of an overview (1 x 1 µm) sample area following thermal treatment (90 ˚C for ten minutes) and glutaraldehyde (5%) fixation. Inset, topographical profile plot measured along the center of one of the virions (dotted line). The rugged surface topography is partially restored, but large areas on the virions are devoid of spikes. e. High-resolution AFM image of a heat-exposed (90 ˚C for ten minutes) and fixed (5% glutaraldehyde) SARS-CoV-2 virion. Shallow surface protrusions are present.@story_separate@The atomic force microscopic imaging and nanomechanical measurements revealed that the SARS-CoV-2 virion is highly dynamic, compliant and resilient, and it displays remarkable mechanical and global thermal stabilities. While the dynamics of the surface spikes may play an important role in the unusually high infectivity of the virus, its mechanical and self-healing properties may also ensure adaptation to a wide range of environmental circumstances. Considering its capability of exploring viruses under native conditions, the single-particle approaches employed here may be important in uncovering not only the mechanistic details behind viral infection but the viral response to potential therapies as well.","SARS-CoV-2, the virus responsible for the current COVID-19 pandemic, displays a corona-shaped layer of spikes which play fundamental role in the infection process. Recent structural data suggest that the spikes possess orientational freedom and the ribonucleoproteins segregate into basketlike structures. How these structural features regulate the dynamic and mechanical behavior of the native virion, however, remain unknown. By imaging and mechanically manipulating individual, native SARS-CoV-2 virions with atomic force microscopy, here we show that their surface displays a dynamic brush owing to the flexibility and rapid motion of the spikes. The virions are highly compliant and able to recover from drastic mechanical perturbations. Their global structure is remarkably temperature resistant, but the virion surface becomes progressively denuded of spikes upon thermal exposure. Thus, both the infectivity and thermal sensitivity of SARS-CoV-2 rely on the dynamics and the mechanics of the virus. One sentence summary The native coronavirus 2 displays a dynamic surface layer of spikes, a large mechanical compliance and unique self-healing capacity."
"Avulsion of permanent teeth is a common dental injury and it represents about 16% of dental injuries [1, 2] . Maxillary central incisors are the teeth most frequently prone to avulsion [1] . Contact sports are the major cause of avulsion trauma with a higher prevalence among boys than girls [3] . Dental avulsion results in: (i) complete displacement of the tooth from its alveolus; (ii) injury to the soft tissues; (iii) fracture in the supporting bone [2] . In Italy, about one third and one fourth of all pre-and schoolchildren, respectively, have experienced a trauma to the permanent dentition, with a prevalence of 20.3% [4] . It is well established that the prognosis of avulsed teeth depends on appropriate management in the first moments after the trauma, according to the International Association of Dental Traumatology (IADT) guidelines, in order to ensure a standardization in the first aid procedures and a good long-term prognosis after replantation [1] . Schoolteachers and sports trainers as well as children's parents are the first people involved in the initial decisions of treatment. Despite no agreement among lay people regarding the first place to be contacted after avulsion trauma, the majority would contact a nearby dentist rather than a general hospital directly [5] . In this scenario, the role played by a general practitioner dentist, oral and maxillofacial surgeon, pediatric dentist and orthodontist is of paramount importance in order to ensure a good prognosis and treatment outcomes of the replantation. No recent data are available on the knowledge of management of avulsion trauma among Italian dentists. The primary aim of this study was to investigate the knowledge of dental practitioners about the emergency management of avulsed teeth in Italy; the secondary aim was to promote the implementation of the guidelines' dissemination through the use of new social media.@story_separate@The target population was a group of general practitioner dentists (GPDs) in Italy. The study was approved by the Ethical Committee of Sapienza University of Rome (n.3493) and informed consent was obtained from all individuals. All the procedures were in accordance with the 1964 Helsinki Declaration and its later amendments or comparable ethical standards, for the protection of human subjects and animals in research. The survey was carried out during the COVID-19 lockdown in Italy, namely during the months of March, April and May 2020. The questionnaire was developed using Google Forms and sent anonymously to a total of 600 dentists from the mailing list of the Italian scientific society ""Accademia il Chirone"". The questionnaire consisted of two parts. Part A-demographic and professional data, including training background (questions n = 11), and Part B-management of avulsed teeth (questions n = 13). The queries related to Part 1, characteristics of the respondents (Q1: age; Q2: gender; Q3: professional experience; Q4: degree; Q5: specialty; Q6: PhD; Q7: academic affiliation; Q8: setting of work; Q9: formal training in avulsion trauma; Q10: personal experience in avulsion trauma; Q11: knowledge of the International Association of Dental Traumatology guidelines), are reported in Table 1 . The queries related to Part 2, management of avulsed teeth (Q1: How should an avulsed tooth be stored after the accident and before the reimplantation? Q2: If the avulsed tooth is dirty, how to clean it? Q3: How to treat the socket prior to replantation? Q4: What type of splint to stabilize the replanted tooth? Q5: Select splinting duration for extra-oral dry time < 60 min; Q6: Select splinting duration for extra-oral dry time > 60 min; Q7: If you want to do a root canal treatment, when should you start it? Q8: Which are the possible complications after tooth replantation? Q9: Patient arrives in your studio with an avulsed tooth. Would you replant it? Q10: In case of avulsion trauma, within how long should the patient seek specialist assistance? Q11: Would you do a root canal treatment of an avulsed and replanted tooth? Q12: Duration of follow up period; Q13: Is antibiotic therapy necessary after replantation?), are presented in Table 2 . The filled in questionnaires were coded by Google Forms, that generated an Excel database. A total of 304 dental practitioners participated to this study: 40.1% (n: 122) were female and 59.9% (n: 182) were male. The response rate was 50.6% of the contacted dental professionals answered the questionnaire. The vast majority of the respondents were graduates in dentistry (82.2%) and 19 .1% in medicine. The respondents with a medicine degree (n: 58) all presented a specialty: 57 in dentistry and related and one in pediatrics. The respondents with a dentistry degree (n: 250) had one or more specialties in 44.4% (n: 111) of the cases. Academic affiliation and PhD were declared by 23.7% (n: 71) and 10.5% (n: 32) of the respondents, respectively. In Italy in 1980, a special field of study in accordance with the European Union Directives was created. Persons with a medical and surgical degree obtained before this principle came into force were dependent on the exercise of a freelance profession and having a diploma of specialization in Dentistry. Then, in 1985, a distinction was made between the profession of a doctor-surgeon from that of a dentist, recognizing a separate university course for this profession. Table 1 shows the respondents' profile. The knowledge of management of avulsed teeth was scored according to the answers to 13 questions. Eleven questions were scored 1 (correct answer) or 0 (wrong answer), while two questions had also fractional values between 0 and 1, due to the fact that they were multiple-choice questions. Answers to the 13 questions on the management of teeth avulsion are shown in Table 2 [1]. The correct answers to each question are highlighted in italics. Mean values of question scores are shown in Table 3 (in the case of 0-1 questions, mean = share of correct answers). The first part of the questionnaire included questions (Q n 1, 2, 3) regarding the optimal storage media and the proper cleaning of the avulsed tooth and dental socket management prior to replantation. The majority of the respondents indicated milk and saline (74.4%) and the patient's mouth (52%) as the correct storage media for an avulsed tooth. Rinsing the avulsed tooth with saline was the correct answer for the question ""How to clean a dirty avulsed tooth?"", given by 78.9% of the respondents. Gentle irrigation with saline was answered by 61.2% of the participants as the right pre-treatment of the socket prior to replantation. The second part of the questionnaire included questions (Q n 4, 5, 6) regarding the proper splint to use to stabilize a replanted tooth. The incorrect response ""rigid splint"" was indicated by 66.8% of the participants, while 45.1% and 42.8% of the sample answered correctly on the splint duration for extra-oral dry time < 60 min and > 60 min, respectively. The third part of the questionnaire was about the management of the tooth after replantation with questions regarding root canal treatment (RCT) complications (Q n 7, 8, 9, 11) . The correct response ratio for questions regarding the root canal treatment (Q n 8 and 11) was low: 22.1% and 40.8%. Q7 was ""If you want to do an RCT, when should you start it"" and 63% of the participants answered that after replantation of an avulsed tooth they would monitor the tooth vitality and perform an RCT in patients who develop signs and symptoms. On the other hand, the correct ratio for possible complications was high, as 84.9% and 71.1% indicated root resorption and ankylosis, respectively. The last part of the questionnaire included questions regarding follow-up duration (Q n 12) and the prescription of antibiotics (Q n 13). The correct response ratio was high, with 72.7% and 67.8%, respectively. Linear regression models were estimated for two variants of dependent variables: average score of 13 questions and weighted average score. The initial models took into account: The best set of explanatory variables (final models) was selected on the basis of the adjusted R 2 . The total score of the dentists was calculated as a mean of 13 values and can thus take a value between 0 and 1. One outstanding value (score = 0) was excluded from consideration. A histogram of the scores is shown in Figure 1 . There are two significant characteristics-qualification in dentistry and knowledge of guidelines (Table 4 ). The best set of explanatory variables (final models) was selected on the basis of the adjusted R 2 . The total score of the dentists was calculated as a mean of 13 values and can thus take a value between 0 and 1. One outstanding value (score = 0) was excluded from consideration. A histogram of the scores is shown in Figure 1 . The model estimation result suggests that, ceteris paribus, the expected score of a dentist with a qualification in dentistry is 0.058 greater (almost one correct answer more) than in the case of a medicine qualification. There is a significant correlation between score and qualification, 0.144, p = 0.012 (Figure 2 There are two significant characteristics-qualification in dentistry and knowledge of guidelines (Table 4 ). The model estimation result suggests that, ceteris paribus, the expected score of a dentist with a qualification in dentistry is 0.058 greater (almost one correct answer more) than in the case of a medicine qualification. There is a significant correlation between score and qualification, 0.144, p = 0.012 (Figure 2 ). Similarly, knowledge of the guidelines adds 0.060 to the expected score. There is a significant correlation between the score and knowledge of IADT, 0.187, p = 0.001. Plots illustrate the score distribution in corresponding groups (Figure 3 ). Similarly, knowledge of the guidelines adds 0.060 to the expected score. There is a significant correlation between the score and knowledge of IADT, 0.187, p = 0.001. Plots illustrate the score distribution in corresponding groups (Figure 3 ). There are also two characteristics with insignificant coefficients in the final modelgender (correlation 0.092, insignificant, p = 0.111) and PhD (correlation 0.127, significant, p = 0.026). Models with some interactive variables were also considered (age or years of experience and qualification or specialization). The aim was to check if time taken to gain specialization has a significant influence on knowledge. These interactive variables did not improve the model. There are also two characteristics with insignificant coefficients in the final modelgender (correlation 0.092, insignificant, p = 0.111) and PhD (correlation 0.127, significant, p = 0.026). Models with some interactive variables were also considered (age or years of experience and qualification or specialization). The aim was to check if time taken to gain specialization has a significant influence on knowledge. These interactive variables did not improve the model. This survey, conducted during the COVID-19 lockdown in Italy, enrolled 304 respondents among dental practitioners. The response rate was 50.6% and the mean fraction of correct responses was 0.524. The sample was divided between dental professionals with medicine or dentistry degree. The aim of this survey was to assess the level of knowledge of the management of avulsed teeth among Italian dental professionals and to correlate it with years of experience and the dental curriculum. The current survey showed that the storage medium, the suitable splint and the timing of root canal treatment of avulsed and replanted teeth were the critical steps of avulsion trauma management. In fact, the majority of respondents (74.7%) chose both milk and saline as the storage medium, but 52% chose saliva, which is not recommended by the IADT guidelines, as the risk of ingestion and inhalation exists (Q1). After replantation, the tooth should be splinted. The results of this survey were not satisfactory, as 66.8% of the respondents would apply a rigid splint (Q4). The IADT guidelines suggest a flexible splint for a time of two weeks to decrease the risk of ankylosis [1] . Moreover, the IADT suggests to start the root canal treatment 7-10 days after replantation of an avulsed tooth with a closed apex and mature root, to prevent inflammatory tooth resorption and obtain a good prognosis over time. By contrast, the respondents answered that they would monitor the vitality of the replanted tooth (63%) and wait for signs and symptoms before deciding to perform an RCT (57.2%). In fact, in the present survey, only 22.1% of the enrolled respondents correctly answered the question on the timing of root canal treatment after avulsed tooth replantation (Q7). By contrast, the majority of respondents were aware of the possible complications of avulsed and replanted teeth, with 84.9% and 71.7% indicating, respectively, root resorption and tooth ankylosis. It is widely documented that the timing and nature of the endodontic treatment of a replanted tooth are crucial to avoid further tough complications [6] [7] [8] [9] [10] .  This survey, conducted during the COVID-19 lockdown in Italy, enrolled 304 respondents among dental practitioners. The response rate was 50.6% and the mean fraction of correct responses was 0.524. The sample was divided between dental professionals with medicine or dentistry degree. The aim of this survey was to assess the level of knowledge of the management of avulsed teeth among Italian dental professionals and to correlate it with years of experience and the dental curriculum. The current survey showed that the storage medium, the suitable splint and the timing of root canal treatment of avulsed and replanted teeth were the critical steps of avulsion trauma management. In fact, the majority of respondents (74.7%) chose both milk and saline as the storage medium, but 52% chose saliva, which is not recommended by the IADT guidelines, as the risk of ingestion and inhalation exists (Q1). After replantation, the tooth should be splinted. The results of this survey were not satisfactory, as 66.8% of the respondents would apply a rigid splint (Q4). The IADT guidelines suggest a flexible splint for a time of two weeks to decrease the risk of ankylosis [1] . Moreover, the IADT suggests to start the root canal treatment 7-10 days after replantation of an avulsed tooth with a closed apex and mature root, to prevent inflammatory tooth resorption and obtain a good prognosis over time. By contrast, the respondents answered that they would monitor the vitality of the replanted tooth (63%) and wait for signs and symptoms before deciding to perform an RCT (57.2%). In fact, in the present survey, only 22.1% of the enrolled respondents correctly answered the question on the timing of root canal treatment after avulsed tooth replantation (Q7). By contrast, the majority of respondents were aware of the possible complications of avulsed and replanted teeth, with 84.9% and 71.7% indicating, respectively, root resorption and tooth ankylosis. It is widely documented that the timing and nature of the endodontic treatment of a replanted tooth are crucial to avoid further tough complications [6] [7] [8] [9] [10] . The statistical analysis performed on average scores showed that: I. Professionals with dentistry degree perform better overall than those with medicine degree, as those that declare to know the IADT guidelines. II. Female dentists and those with academic affiliations had an overall slightly better knowledge. Based on the findings of this study, there is a need to improve the knowledge of the management of teeth avulsion among Italian dental professionals, in accordance with the IADT guidelines [1] . Theoretical courses, specific training and continuing education programs both in the under-and post-graduate curriculum of dental schools, together with the evaluation of the acquired knowledge, are necessary to guarantee a high standard of care [11] . Moreover, dental practitioners in Italy should utilize the Italian guidelines for the prevention and management of dental trauma in children to obtain the appropriate information on evidence-based recommendations for the optimal prevention and treatment of avulsed teeth [12, 13] . A multidisciplinary panel on the behalf of the Italian Ministry of Health and in collaboration with the WHO Collaborating Centre for Epidemiology and Community Dentistry of Milan developed this document [14] . In addition, as Barrett and Kenny underlined in their review, the prevention issue should be given greater emphasis through prophylactics and eliminating fall-prone areas, installing safety measures at homes, using protective appliances in sports, education and raising knowledge about and the availability of services to address dental trauma [15] . A comparison with data from similar surveys was carried out, and, despite differences in methodologies used to report on knowledge among dental professionals, the outcomes were interesting. Table 5 shows similar surveys conducted in different countries (USA, Brazil, Malaysia, Israel, Kuwait, UK, Germany, Belgium, Morocco, China) to assess the knowledge on dental trauma management among dental professionals. The mean value of correct answers in the avulsion trauma management was always lower than the score reported for other dental trauma (crown and root fractures). The results of the current survey are in accordance to those represented in Table 5 [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] . A recent questionnaire study by Abdullah et al. conducted in Malaysia on a sample of 182 general dental practitioners (GDPs) defined the level of knowledge as adequate with a mean correct response rate of 72.7% [16] . These results, although considered to be good, were nevertheless identified as to be improved. In fact, a score of 72.7% can be defined as an adequate level of knowledge, however, it does not ensure an adequate level of competence in the entire teeth avulsion protocol in order to achieve a good long-term prognosis. Indeed, a level of knowledge which is not complete in all aspects cannot be considered sufficient to guarantee optimal results. The reported scores by Abdullah et al. were the highest [16] compared to the scores reported by Zadik [17] and Hartmann [18] , with an average correct response rate of 61.5% and 60.2%, respectively. As reported by Krastl, the treatment of dental trauma is a rare event in private practice and it is normal that GDPs are not confident in its management. Krastl conducted a survey in Germany and the vast majority of the respondents judged the frequency of dental trauma as very rare and they were unable to assess their own level of knowledge [19] . De Franca reported the results of a survey conducted in Brazil, where the correct response rate on the emergency management of avulsed teeth ranged between 16.1% and 36.6%, and the author concluded that the great majority of dental professionals would not intervene according to the literature guidelines [20] . The study by Cohenca et al. conducted in 2006 with 167 GDPs attending continuing education courses at the School of Dentistry, University of Southern California, documented a correct response rate on the trauma management of teeth avulsion of 54.6%. The authors pointed out that, although results may be similar to previous studies, extrapolation of the outcomes to other oral health providers in the USA or in other countries should be done with extreme caution [21] . Standardization in methodology is needed and recently the updated IADT guidelines underlined the development of a core outcome set (COS) for traumatic dental injuries in children and adults [1] . Further dissemination will help clinicians and researchers to implement it. The results of the present survey are in accordance with Kostopoulou et al. who showed in a sample size of 693 respondents that those who were younger and more recently graduated performed better than the others [22] . Cohenca et al. also showed a similar pattern, pointing out that those who were more experienced and those who had attended continuing education courses within the last 3 years responded more correctly [21] . The COVID-19 pandemic has called upon us to do things differently and education and teaching are going to change, as direct contact between those who learn and those who teach is not allowed on a regular basis. The development of online courses and webinars, regularly scheduled during the dental undergraduate and postgraduate programs, and continuing education, should be promoted. In fact, Zhao et al. showed a great difference in dental postgraduate education between urban and suburban dentists: 31.3% of urban dentists presented a master's or a PhD degree compared to 6.8% of the suburban dentists. Zhao et al. in accordance with the position of this paper, underlined that no difference was present between the two categories concerning continuing education programs [23] . Webinars do not create discrimination in access to education, and continuing education is of paramount importance. Moreover, webinars are interactive, easy to use, they reach a large audience, are repeatable and are inexpensive, if compared with traditional courses [23] . The webinar, in this historical moment, seems to represent the most suitable tool for the widespread dissemination of the IADT guidelines. Therefore, according to the aims of this study and, consequently, the results of the survey on the knowledge of the emergency management of the avulsion of permanent teeth among Italian dental practitioners, a webinar on dental trauma in pediatric population was developed by the Italian Society of Odontostomatology and Maxillofacial Surgery-Società Italiana di Odontostomatologia e Chirurgia Maxillo-Facciale (SIOCMF) and the Italian Society of Pediatric Emergency Medicine-Società Italiana di Medicina Emergenza Urgenza Pediatrica (SIMEUP) and it is fully available at: https://www.siocmf.it/corsi-econgressi.html and at: https://www.simeup.it/?page_id=16531. The data presented in this study are available on request from the corresponding author. The authors declare no conflict of interest.@story_separate@Based on the findings of this survey, Italian dentists' knowledge of the management of avulsed teeth should be improved. Educational programs and campaigns must be undertaken to improve their awareness and adherence to the Italian and international guidelines. Informed Consent Statement: Informed consent was obtained from all subjects involved in the study.","(1) Background: In Italy, about one fourth of all schoolchildren experience a trauma to the permanent dentition. Management of avulsion trauma is challenging and requires adherence to clinical protocols. The aim of this study was to investigate the management knowledge of avulsed teeth among Italian dentists and to promote the guidelines’ dissemination through the use of new social media. (2) Methods: The survey was carried out during the COVID-19 lockdown in Italy (March–May 2020). The questionnaire was sent anonymously to a total of 600 dentists. The questionnaire consisted of two parts. Part A—demographic and professional data and Part B—management of traumatic avulsion. (3) Results: The response rate was 50.6% and the mean fraction of correct responses was 0.524. Issues related to the therapeutic management of avulsed teeth were shown to be not well understood by the respondents. Professionals with qualifications in dentistry and those who declared to know the guidelines responded better, while other demographic and professional factors were insignificant. (4) Conclusions: Italian dentists’ knowledge of the management of avulsion trauma should be improved. Educational programs and campaigns must be undertaken to improve their awareness and adherence to the Italian and international guidelines."
"Due to the growing number of available treatment options and the rarity and heterogeneity of soft tissue sarcomas (STS), the decision-making process is very complex. In comparison to surgery alone, the addition of adjuvant radiotherapy (RT) allows obtaining a high local control rate in patients with STS of extremities or trunk wall. Nevertheless, this combination does not improve patients' overall survival (OS) [1] . Additional treatment modalities may help to improve local and distant disease control. Perioperative anthracycline-based chemotherapy (CHT) should be considered in high-risk STS [2] . However, its efficacy is still under debate. Hyperthermia (HT) may enhance the effect of both RT and CHT, but it has not been widely adopted in clinical practice [3] [4] [5] [6] . Another novel approach includes the radiosensitization of sarcoma cells by the targeted treatment given concurrently with RT [7] . Other modalities, such as radiosensitizing nanoparticles, gave promising preliminary results, but are still under investigation in trials [8] . The introduction of neoadjuvant treatments in clinical practice was initially limited to challenging cases, such as unresectable STS, albeit currently it should be considered as an equal alternative to adjuvant therapy, bringing more advantages than risks. It is highly improbable to create a universal neoadjuvant treatment regimen for STS due to the variety of molecular subtypes and related clinical factors [9] . Thus, any clinical decision must be individualized and analyzed by STS multidisciplinary tumor board (MTB). Patients with locally advanced STS should be enrolled in clinical trials with 1 Lower Higher Risk of late toxicity 1 Higher Lower Combination with chemotherapy Possible Possible 1 In conventionally fractionated radiotherapy; abbreviations: EQD2-equivalent total dose in 2-Gy fractions; GTV-gross tumor volume. In neoadjuvant RT, it is recommended to contour gross tumor volume (GTV) using magnetic resonance imaging (MRI) T1 post-gadolinium [12] . Clinical target volume (CTV) should be created by expanding GTV with adequate margins with additional coverage of tumor-related edema in MRI T2. In deep STS of extremities, CTV constitutes GTV +1.5-2.0 cm radially and 4 cm longitudinally. In the case of superficial STS or trunk STS, there are no clear recommendations; however, it seems reasonable to add at least 4 cm in each direction along paths of least resistance stopping at anatomical barriers. Besides regular organs at risk adequate to the anatomic site, attention must be paid to healthy skin and subcutaneous tissue, as well as the second limb, large joints, and bones. Margins in STS RT seem to be extensive. However, there is no reliable evidence to support their reduction. The only phase II Radiation Therapy Oncology Group (RTOG) 0630 non-randomized clinical trial showed that image-guided RT with concomitant margin reduction results in low late toxicity while maintaining reasonable local control in comparison to Canadian trial data [13] . Nevertheless, the numerical comparison between the RTOG-0630 and the Canadian trial is not relevant due to the difference in RT techniques. In the RTOG-0630 trial, 75% of patients were irradiated with intensity-modulated RT (IMRT), whereas in the Canadian trial in all patients older RT techniques were used [11, 13] . RT techniques in STS slowly evolved from 3D-conformal RT to IMRT. IMRT allows better coverage of target volumes with the prescribed dose along with higher conformity but at the cost of an increase in volume irradiated with low doses [14] . Theoretically, irradiation of the whole extremity circumference may lead to persistent lymphedema. For this reason, many radiation oncologists have avoided using IMRT in STS. This hypothesis was not confirmed in the RTOG-0630 trial, as well as in other studies with IMRT in STS. O'Sullivan et al. conducted a phase II non-randomized prospective clinical trial with RT in STS, in which IMRT resulted in favorable functional outcomes [15] . Another analysis by Peeken et al. confirmed the good toxicity profile of tomotherapy-based IMRT in patients with STS [16] . Neoadjuvant RT in STS may be prescribed in conventional and altered fractions [17] . The conventional fractionation is 50-50.4 Gy in 25-28 fractions for 5-6 weeks; however, it is not evidence-based. Several attempts of hypofractionated RT (HFRT) were described in the literature [7, 18] . The alpha/beta ratio of STS is presumably lower than 10 Gy and closer to radioresistant tumors such as prostate cancer [19] [20] [21] . As per the generalized linear-quadratic model, a higher dose per fraction applied to tumors with a lower alpha/beta ratio should result in better tumor control [22] . HFRT has other advantages such as shorter overall treatment time, better adherence to therapy, and higher cost-effectiveness [23, 24] . However, HFRT has been validated only in phase I and phase II clinical trials in STS. Thus, it is recommended to use HFRT only in further trials or in individually selected cases upon MTB decision [7] . Unfortunately, even the best perioperative treatment cannot replace high-quality R0 surgery [25] . It has been shown that the addition of a postoperative RT boost after neoadjuvant RT and non-radical surgery does not improve local control in STS [26, 27] . Nonetheless, it is still considered optional in the National Comprehensive Cancer Network (NCCN) guidelines [28] . Novel neoadjuvant RT approaches in STS include a return to abandoned spatially fractionated RT that used sieve-like collimators to produce fluctuating dose distribution with extremely high doses in selected subvolumes of a tumor. Mohiuddin et al. used spatially fractionated RT with ifosfamide-based CHT to treat bulky STS, obtaining excellent pathological responses and acceptable toxicity [29] . Dynamic RT techniques could emulate a grid-like pattern within the tumor without the use of a designed collimator. This technique is called LATTICE RT [30] . Prospective trials with LATTICE RT in locally advanced STS are awaited. Brachytherapy is not discussed in this review, because it is given intraoperatively or as an adjuvant treatment.@story_separate@According to the NCCN guidelines, neoadjuvant RT is recommended as one of the possible options in stage II, III resectable extremity, superficial trunk, or head/neck STS with acceptable functional outcomes (category 1) [28] . In the case of resectable stage II and III STS with predicted adverse functional outcomes and unresectable STS, NCCN recommends an attempt of neoadjuvant treatment, namely RT, CHT+RT, CHT, or regional limb therapy. European Society for Medical Oncology (ESMO) guidelines recommend surgery and perioperative RT as a standard treatment in high-grade (G2-3), deep, >5 cm STS [31] . The sequence of RT and surgery should be selected based on the risk of wound complications. There is no consensus regarding other clinical situations, i.e., high-grade, deep, <5 cm STS; high-grade, superficial, >5 cm STS; low-grade, deep, >5 cm STS. RT should be discussed at MTB, and the risk of local relapse, pathology, and potential toxicity should be considered. The most significant disadvantage of neoadjuvant RT is the increased risk of wound complications. In the Canadian trial, authors randomized patients to neoadjuvant or adjuvant RT [11] . Due to the higher occurrence of wound complications in the neoadjuvant group (35% vs. 17%), the trial was prematurely stopped. However, after a longer follow-up, more patients after adjuvant RT, than patients after neoadjuvant RT, developed fibrosis-related late RT toxicity [32] . What is important, even serious wound complications are reversible, whereas fibrosis-related late toxicities are usually permanent, progressive, and may severely deteriorate the patients' quality of life [33] . Several patient-dependent factors increase the ratio of wound complications, such as STS localization (lower extremity and <3 mm from the skin), tumor size >10 cm, concomitant diseases as diabetes, obesity, as well as smoking [34] [35] [36] . Treatment-related factors that exacerbate wound complications are related to RT (as described above) and surgery (split-thickness skin graft or vascularized flap closure). The role of perioperative CHT in patients with localized STS remains controversial [37] . Neoadjuvant CHT should result in tumor burden reduction and subsequently facilitate radical surgery. In particular, CHT is expected to downstage the tumor in the case of marginally resectable STS allowing for more conservative operation [38] . Neoadjuvant CHT enables the elimination of micrometastases before massive release of cytokines induced by surgery. Finally, it should improve patient survival [39] . The neoadjuvant approach eliminates the risk of adjuvant CHT-related wound complications. Moreover, in routine clinical practice performance, the status of patients is often better before extensive surgery, enabling a more toxic but also more efficient neoadjuvant CHT regimen [40] . Better performance status indirectly allows adequate CHT dose intensity and promotes compliance. However, evidence-based confirmation of these clinical hypotheses is scarce [41] . An updated large meta-analysis of 18 randomized controlled trials of adjuvant CHT for localized resectable STS showed that anthracycline-based CHT marginally improved local recurrence, distant recurrence, overall recurrence, and OS [42] . Even less evidence justifies the use of CHT alone in a neoadjuvant setting in STS. In 2001, Eilber et al. published the results of neoadjuvant CHT of patients with locally advanced STS showing a higher percentage of pathological responses and better survival in patients receiving an ifosfamide-based regimen in comparison with patients receiving doxorubicin-based CHT (without ifosfamide) [43] . There is a lack of randomized phase III clinical trials in the field. The results of available smaller studies are contradictory [44] [45] [46] . In a randomized phase II clinical trial by the European Organisation for Research and Treatment of Cancer Soft Tissue Bone Sarcoma Group (EORTC-STBSG) and the National Cancer Institute of Canada Clinical Trials Group/Canadian Sarcoma Group on neoadjuvant CHT for high-risk STS, neoadjuvant CHT followed by surgery failed to show better survival than surgery alone [47] . Thus, expansion to phase III was abandoned. In the above-mentioned trial, 134 patients with resectable high-risk primary and recurrent STS were randomized to a surgery alone arm or to three cycles of doxorubicin+ifosfamide (AI) with a subsequent surgery arm. This regimen was reasonably tolerated and did not compromise surgery. Disappointingly, it resulted in unsatisfactory benefit in 5-year disease-free survival in comparison to surgery alone (56% vs. 52%) [35] . This trial was criticized for inclusion of patients with both primary and recurrent tumors as well as patients with low grade STS [48] . Histologically driven CHT showed good preliminary efficacy of selected drugs in chosen pathological STS subtypes. Four sarcoma groups (French, Spanish, Polish, and Italian) performed a phase III randomized clinical trial (NCT01710176) with neoadjuvant histologically driven CHT [49, 50] . Two hundred seventy-eight patients with STS were enrolled in the trial. The patients received three cycles of epirubicin+ifosfamide (EI) or histologically driven CHT: for myxoid liposarcoma-trabectedin, for leiomyosarcoma-gemcitabine+dacarbazine, for synovial sarcoma-ifosfamide, for malignant peripheral nerve sheath tumor (MPNST)-etoposide+ifosfamide, and for pleomorphic sarcoma-gemcitabine+docetaxel. Surprisingly, after the median 52 months of follow-up, there were no differences between histologically driven CHT and EI in disease-free survival (47% in the tailored arm, 55% in EI arm, hazard ratio [HR] 1.23, 95% CI 0.88-1.73). Moreover, OS was shorter in the histologically driven arm in comparison to the EI arm (66% vs. 76%, HR 1.77, 95% CI 1.10-2.83). As a result of this trial, anthracycline-based regimens remain preferred CHT in STS regardless of histological subtype. This study also confirms that in STS patients with 60-70% risk of relapse, three cycles of neoadjuvant anthracycline+ifosfamide CHT are associated with an absolute benefit of 20% for relapse-free survival and OS. The optimal number of CHT cycles was addressed in a study conducted by the Italian Sarcoma Group and the Spanish Sarcoma Group [51] [52] [53] . The researchers compared three cycles of neoadjuvant AI CHT with five cycles of the same regimen given perioperatively (three neoadjuvant cycles, surgery, two adjuvant cycles). No benefit in survival was detected between the analyzed groups. Thus, three cycles of neoadjuvant anthracycline+ifosfamide may still be considered as a preferred regimen in STS including epirubicin 120 mg/m 2 with ifosfamide 9 g/m 2 [42] . The randomized trials with neoadjuvant CHT regimens in STS are presented in Table 2 . Abbreviations: AI-doxorubicin, ifosfamide; CHT-chemotherapy; EI-epirubicin, ifosfamide; N-number of patients; NR-not reported. An advantage of neoadjuvant CHT, especially if used in combination with RT, is an improvement in the effectiveness of surgery and therefore significant improvement in patient function and postoperative quality of life. Neoadjuvant CHT may be a valuable option in patients with a low probability of OS calculated using the prognostic nomogram Sarculator [54, 55] . Moreover, neoadjuvant CHT may provide benefit in patients with high-risk or marginally resectable STS [48, 56, 57] . NCCN suggests considering CHT as an addition to RT or as standalone neoadjuvant treatment in resectable stage II and III STS with predicted adverse functional outcomes, and in primarily unresectable STS [28] . ESMO recommends at least three cycles of neoadjuvant CHT with anthracycline+ifosfamide in selected high-risk STS patients [31] . However, the sensitivity of STS subtypes should be taken into account. The anthracycline+ifosfamide combination should be considered in more chemosensitive subtypes such as synovial sarcoma, pleomorphic sarcoma, liposarcoma and leiomyosarcoma. It is important to note that older patients (≥65 years) or those with comorbidities may not be able to tolerate such intensive treatment. More intensive CHT increases the risk of serious adverse events without providing significant benefit. Given the limited role of ifosfamide in leiomyosarcoma, doxorubicin+dacarbazine may be an alternative, less toxic treatment option in this group of patients [58] . Other treatment regimens cannot be recommended outside clinical trials. It is crucial to mention marginally sensitive and chemoresistant STS subtypes such as epithelioid sarcoma, extraskeletal myxoid chondrosarcoma, clear cell sarcoma, solitary fibrous tumor, alveolar soft part sarcoma and inflammatory myofibroblastic tumor, where neoadjuvant CHT should not be given because of lack of efficacy [59] . In turn, a disadvantage could be a delay in surgical treatment in the case of resistance to CHT [60] . Then, in the case of tumor progression, neoadjuvant CHT should be stopped, and definitive local treatment should be performed. There is a theoretical risk of disease progression during CHT that will make surgery in primary resectable STS impossible. Furthermore, complications of neoadjuvant CHT can considerably delay surgery and prolong overall treatment time. Toxicities depend on the used chemotherapeutic agents. Anthracyclines bring the risk of myelosuppression and cardiotoxicity, whereas ifosfamide may cause hemorrhagic cystitis, neurotoxicity, extensive vomiting, and myelosuppression [61] [62] [63] [64] [65] . Moreover, a combination of doxorubicin with ifosfamide could increase the frequency of the above-mentioned toxicities and febrile neutropenia [66] . CHT+RT may be considered in patients with high-risk or marginally resectable STS [67] . The main aims are as follows: Improve local control; Symptom control, including pain relief; Improve local response allowing for conservative surgery and negative resection margins; Limit the metastatic spread and improve survival; Sensitize cancer cells to RT with a possibility of RT dose reduction to avoid RT-related toxicity; Obtain data regarding response to neoadjuvant treatment that may serve as a prognostic factor. Guidelines favoring neoadjuvant CHT+RT are not based on high-level evidence [28, 31, 56, 68] . The optimal sequence of treatment is unknown. CHT can be given before, after, or concomitantly with RT [55] . CHT administration before or during RT planning could serve as ""stop-gap measure"" giving more time for generation of complicated highly conformal RT plan [38] . Concomitant use of anthracyclines and RT brings the risk of extensive skin, mucosal, and cardiac toxicity. However, data from breast cancer studies do not confirm this hypothesis [69, 70] . Studies on CHT+RT in adjuvant treatment for STS do not suggest a significant increase in toxicity, even in a subgroup of elderly patients [71, 72] . In the sub-analysis of a phase III clinical trial that compared three neoadjuvant vs. three neoadjuvant and two adjuvant AI cycles, the investigator was allowed to add neoadjuvant or adjuvant conventionally fractionated RT (CFRT) to the CHT, with or without postoperative boost [53] . Neoadjuvant RT was started after the first CHT cycle and continued within the second and third CHT cycle. One hundred fifty-two patients received neoadjuvant CHT+RT. It has been shown that this combination is safe and does not have a negative impact on CHT dose intensity. The only noted significant toxicities were the slightly increased likelihood of wound complications and grade 4 thrombocytopenia. In the literature, several attempts of the integration of neoadjuvant CHT+RT were investigated [7, 18] . Various CHT regimens may be combined with both CFRT and HFRT. Such regimens are under investigation in highly locally advanced or marginally resectable STS [57] . The long-term results of novel CHT+RT regimens are presented in Table 3 .  According to NCCN guidelines, CHT+RT may be considered as a treatment option for patients with stage II, III resectable extremity, superficial trunk, or head/neck STS with acceptable functional outcomes; however, the recommendation is based upon lower-level evidence (category 2B) [28] . Furthermore, it is a treatment of choice in resectable stage II and III STS with predicted adverse functional outcomes, and primarily unresectable STS. Neoadjuvant RTH-CHTH may also provide a substantial clinical benefit allowing radical surgery in primary marginally or non-resectable STS [57] . The addition of CHT to RT may increase toxicity. Anthracycline-based regimens may exacerbate RT mucosal and skin reactions as well as cardiac toxicity when RT is applied to the thoracic area. Moreover, even in the case of sequential CHT+RT, a phenomenon called radiation recall may occur [80] . It is defined as an inflammatory reaction in previously irradiated volume after the administration of CHT agents. Severity and length of radiation recall may vary. Finally, CHT toxicity may interrupt RT administration causing gaps in the selected fractionation regimen. Additionally, RT-induced toxicity may simultaneously decrease CHT dose intensity. Targeted therapies inhibit specific molecules involved in the proliferation and growth of cancer cells, as well as intratumoral endothelial cells. Both vascular endothelial growth factor (VEGFR) and epidermal growth factor receptor (EGFR) are overrepresented in neoplastic cells playing an essential role in tumor angiogenesis, as well as the promotion and progression of oncogenesis and metastatic spread. Intratumoral endothelial cells are crucial in the biological effects of RT [81] . It was shown that inhibition of VEGFR and EGFR signaling pathways is an effective strategy in particular STS subtypes, and it also enhances the effect of RT in STS [82, 83] . Various targeted drugs are currently investigated in metastatic STS, but also in neoadjuvant treatment, mostly with neoadjuvant RT. Despite promising efficacy, available data come from early-phase clinical trials, thus must be interpreted with caution. Moreover, some studies reported worrisome toxicity of neoadjuvant targeted therapy [7] . Further studies are necessary to use this combination in everyday clinical practice. Available reports on targeted therapies with or without neoadjuvant RT in STS are presented in Table 4 .  The combination of targeted drugs with neoadjuvant RT or targeted therapy alone should be used in prospective clinical trials or individually in selected patients [7] . Potential toxicity of neoadjuvant targeted therapy with or without RT in STS is poorly investigated, thus unexpected early or late toxicities may occur. For example, the combination of CFRT with sunitinib or pazopanib leads to a high proportion of patients who experienced grade 3 or higher hepatotoxicity [86, 92] . Various radiosensitizers were tested in clinical trials in the past, mostly in head and neck cancers [93] . However, none of them is widely used in clinical practice. In recent times, in a phase II/III multicenter, international randomized clinical trial, the authors assessed the efficacy of hafnium oxide nanoparticle (NBTXR3) as a local radiosensitizer added to neoadjuvant CFRT (2 Gy to 50 Gy, 25 fractions, five weeks) [8] . The treatment was offered to patients with locally advanced resectable STS of extremities or trunk wall. In the study group patients received a single intratumoral injection of NBTXR3 before CFRT, whereas in the control group, patients received the same CFRT alone. The primary endpoint of the study was the proportion of patients with a complete pathological response. After enrollment, randomization, and exclusion of ineligible patients, 176 patients were analyzed. Among them, 87 were in the study group and 89 in the control group. Pathological complete response was observed in 14 patients in the study group and seven patients in the control group (p = 0.044). Statistically significant differences were found in the proportion of patients with R0 resection, which was more frequent in the NBTXR3 group than in the CFRT-alone group. Grade 3+ CFRT-related toxicity occurred in five patients from the study group and four patients from the control group. Grade 3+ NBTXR3-related toxicity was observed in eight patients, i.e., post-injection pain in four cases and hypotension in six cases. Serious adverse events were noted in 35 patients who received NBTXR3 and 27 patients who received CFRT alone. The most frequent serious adverse event related to RT was postoperative wound complication. To sum up, the NBTXR3 injection before neoadjuvant CFRT resulted in a higher proportion of patients with a pathologically complete response with no increase in RT-related toxicity and is a promising radioenhancer in further clinical applications. There is a lack of long-term outcomes of such treatment; thus, the real effectiveness of nanoparticles+RT in STS and the late toxicity profile are still unknown. In the case of approval, the nanoparticles could be used in most patients with localized advanced STS who are eligible for intratumoral injection. In the study mentioned above, patients with STS localized in the anterior abdominal region and those with a tumor volume over 3000 mL were excluded [8] . The authors explained that in the case of tumors >3000 mL, the injection of NBTXR3 would probably be unfeasible. The injection of nanoparticles may cause anxiety, pain, or infection within the treated site. Pain could be managed with adequate analgesia. The authors of the NBTXR3 trial reported no grade 3+ acute allergic reactions. Nevertheless, premedication with glucocorticoids should be considered before injection. There are still no data about late toxicity. The term ""hyperthermia"" describes a variety of methods of controlled heat delivery to cancer cells. Local or regional HT is used in conjunction with RT or CHT as a potent radio-or chemo-sensitizer [94, 95] . It damages cancer cells by a direct cytotoxic effect and also provides indirect additional molecular effects, such as better tissue oxygenation, induction of apoptosis, instability of the cell membrane, dysfunction of intracellular proteins, and impairment of DNA repair [96] . Preclinical studies have confirmed this effect [97] [98] [99] . Routine use is limited by the low number of phase III clinical trials and poor availability of HT equipment. In a phase III randomized multicenter clinical trial organized by the European Society for Hyperthermic Oncology and the EORTC-STBSG, the authors analyzed a group of 341 patients with localized high-risk STS [4] . The patients were randomly assigned into two arms. In the study arm, 169 patients received neoadjuvant CHT with etoposide, ifosfamide, and doxorubicin (EIA) concurrently with regional HT; in the control arm, 172 patients received EIA alone. The primary endpoint of the study was local progression-free survival. Local progression or death occurred more frequently in the control arm than in the study arm (relative hazard 0.58, 95% CI 0.41-0.83; p = 0.003). In the HT+EIA arm, the treatment response rate was significantly higher (28.8%) than in the EIA alone arm (12.7%, p = 0.002). Among patients who completed the whole treatment, OS was better in the study group (HR 0.66, 95% CI 0.45-0.98, p = 0.038). The main HT-related toxicities were pain, bolus pressure, and skin burn; the majority were mild or moderate. After a median of 11.3 years of follow-up, patients from the HT+EIA arm maintained the benefit from HT [5] . In comparison to patients who received EIA alone, patients from HT+EIA had significantly better local progression-free survival (HR 0.65; 95% CI, 0.49-0.86; p = 0.002), survival rate (HR, 0.73; 95% CI, 0.54-0.98; p = 0.04), 5-year survival (62.7% vs 51.3%) and 10-year survival (52.6% vs. 42.7%). In contrast, the effectiveness of HT+CHT in neoadjuvant treatment of locally advanced STS was proven in a phase III randomized clinical trial; evidence on HT+RT in STS treatment remains scarce. The toxicity and outcomes of neoadjuvant HT+HFRT (3.25 Gy to 32.5 Gy, ten fractions, two weeks, 4× HT) are validated in a prospective phase II clinical trial SINDIR, NCT03989596 [100] . A particularly challenging situation is reirradiation due to STS relapse or radiation-induced STS. HT might be added to enhance tumor response to RT, whose dose is limited by previous RT [101] [102] [103] . In a retrospective analysis, the authors assessed the outcomes of 16 patients with radiation-induced STS in the thoracic region treated with HT+HFRT using two moderately hypofractionated regimens (3 Gy to 36 Gy, 12 fractions, three weeks, 6× HT; or 4 Gy to 32 Gy, eight fractions, two weeks, 4× HT) [6] . In 13 patients, the treatment was applied with definitive intent; in three patients, it was used as adjuvant therapy to surgery. The complete response and partial response were observed in seven and two patients, respectively. The early toxicity of HT+HFRT was good. Late toxicity occurred in seven patients but was severe in only one case, i.e., ischemia of the arm on the treated side that required forearm amputation. Currently, there is one phase II clinical trial, NCT04398095, aimed at assessing the tolerance of HT+HFRT as definitive or neoadjuvant treatment for radiation-induced or recurrent previously irradiated STS [104] . What is important, HT should not be given with each fraction to avoid a reversible phenomenon called thermotolerance [105] . Cancer cells become resistant to heat-induced damage, probably due to the synthesis of heat-shock proteins and other molecular adaptation processes if heat is applied too frequently [96] . In turn, in a phase II clinical trial, the pathological response after HT+RT was better in patients who received HT twice a week than in those who received HT once a week [106] . ESMO guidelines state that HT could be a supplementary modality to CHT [31] . Thus, HT may be offered to patients who are candidates for neoadjuvant CHT. Neoadjuvant HT+RT could be useful in patients with locally advanced STS who are not candidates for neoadjuvant systemic treatment due to poor performance status, chemoresistant pathology, or disease progression on CHT. HT+RT may also be beneficial in a selected group of patients with previously irradiated recurrent or radiation-induced STS; however, extreme caution must be taken due to the high risk of potentially severe toxicity [107] . There are no convincing data regarding optimal RT fractionation. Concurrent HT is usually applied twice a week. Treatments should be separated by at least 48 h. The tolerance of HT is usually excellent. Side effects are mild and include pain at the target site, blisters, skin damage, erythema, bleeding, thrombosis, infection, edema, and neuropathy [99, 108] . In the case of deep regional HT, additionally, some general symptoms may occur, such as nausea, vomiting, or dyspnea. The late toxicity of HT is poorly investigated. HT should not be used for pregnant or breastfeeding women [109] . Other contraindications include the presence of metal implants or pacemakers within or in the proximity of heated tissues, unstable cardiovascular disease, epilepsy, significant neuropathy with a deteriorated sense of temperature, significant fever, or a large fluid compartment within the tumor [110] . A standardized approach of radiological response evaluation is crucial for a single patient who undergoes neoadjuvant treatment as well as for scientific purposes when a new regimen is assessed. Intratumoral environment variability of STS can lead to several pathological changes after neoadjuvant treatment, namely edema, hemorrhage, and necrosis, that might increase tumor volume and be wrongly misinterpreted as disease progression [75] . The analysis of 99 patients with STS treated with neoadjuvant or palliative RT has shown that in 58 patients, the tumor volume changed significantly with a volume increase in the majority of cases [111] . Moreover, a decrease in tumor size may be correlated with the presence of viable sarcoma cells, whereas ""pseudoprogressing"" tumors might be related to an extensive pathological response [79, 112] . Thus, conventional response evaluation criteria in solid tumors (RECIST 1.1) should not be used to assess the response to neoadjuvant treatment in STS, except for myxoid liposarcomas. EORTC-STBSG published guidelines on radiological examination and reporting after neoadjuvant RT in STS [113] . The emphasis is placed on MRI before and after RT as a suggested modality in response assessment. The article gives recommendations regarding the optimal timing and protocol of MRI. It is recommended that: Post-RT imaging should not be performed earlier than four weeks post-RT (later if possible); Images acquired in the same plane should be performed with identical planing and slice thickness to allow correlation between sequences; Except for myxoid liposarcomas, size and volume measurements should not be used to reflect histopathological response; Internal signal/density characteristics should be used in combination to assess response; Areas of new enhancement should be interpreted with caution as they can arise secondarily to vascular disruption following RT and do not necessarily reflect progression; Not all areas of diminished enhancement following RT represent necrosis and, therefore, attention to terminology is suggested. The term ""treatment effects"" may be more appropriate encompassing several processes, such as necrosis, cystic change (liquefaction), or hyalinization. Special attention was paid to functional imaging. The authors gave recommendations regarding parameters for reporting multiparametric MRI in clinical trials. Treatment-induced tissue necrosis is a predictive factor of patient survival in bone sarcomas. Several attempts failed to show this dependency in STS [114, 115] . At the same time, meta-analysis of 21 studies comprising 1663 patients has confirmed that tumor necrosis <90% following neoadjuvant therapy in STS is associated with increased recurrence risk and shorter OS [116] . For pathology reports, several pathological factors were considered in various studies until the proposal of a standardized pathological examination and reporting of STS resection material after neoadjuvant treatment was published by EORTC-STBSG [117] . The article describes the process of specimen documentation. The most important innovation is the introduction of the five-grade scale of microscopic evaluation based on the proportion of tumor area that is viable. The assessment is based on ""stainability,"" which means the visualization of nuclei by hematoxylin. The percentage of stainable cells should represent the whole specimen; thus, the representative slab should be supplemented with additional blocks in the final response score. The categories include: A: no stainable tumor cells; B: single stainable tumor cells or small clusters (overall below 1% of the whole specimen); C: >=1%-<10% stainable tumor cells; D: >=10%-<50% stainable tumor cells; E: >=50% stainable tumor cells. The authors were not able to give any recommendations regarding immunohistochemical markers that may be useful in the assessment of pathological response. There are as of yet no specific biomarkers that would predict patient survival or, even better, indicate the optimal type of therapy. The general importance of using proper markers has been pointed out by Mortaji and Lebduska [118] . Kondo and Kawai suggested investigating (for STS) numerous potentially useful molecular proteomic markers which have contributed to cancer therapy, but they did not follow up on their suggestions [119] . However, in 2019 Burns et al. stated that no proteomic markers for STS had reached the clinic [120] . Kane et al. examined specimens from 60 STS patients, but pretreatment samples were available for only 23 and matched samples pre-and post-treatment for 12 [121] . They analyzed the expression of several genes involved in cell-cycle regulation and hypoxia but did not find (possibly because of the small size of the group) any association of pretreatment expression of any of the markers with survival. Schenone and Van Tine have identified seven biomarkers potentially useful for therapies commonly used in STS treatment (except for gastrointestinal stromal tumors) [122] . These are TOP2A (topoisomerase IIA for anthracycline), RRM1 (ribonucleotide reductase M1 unit for gemcitabine and Taxotere), TLE3 (transducing-like enhancer protein 3 for taxanes), MGMT (O6-methylguanine-DNA-transferase for temozolomide and dacarbazine), TUBB3 (tubulin beta-3 chain for taxanes and vinca alkaloids), SPARC (secreted protein acidic and rich in cysteine for taxanes) and PTEN (phosphatidylinositol-3,4,5-triphosphate 3-phosphatase for mTOR inhibitors), and the authors have carefully analyzed earlier studies; however, most of them did not concern STS. More recently, Caruso and Garofalo analyzed pharmacogenomic biomarkers for STS therapies [123] . These biomarkers would be potentially useful in predicting drug responses in patients. They have analyzed the literature for germline and somatic biomarkers and suggest that next-generation sequencing technologies and larger gene panels would be useful in obtaining results that could be implemented in the clinic. In the clinical trial NCT01710176, Pasquali et al. used a 67-gene expression-based signature to stratify 87 patients into lower-risk and higher-risk groups; however, no differences were observed between them in disease-free and OS, even though this set of markers, CINSARC (complexity index in sarcomas) had been previously tested in retrospective studies [124] . Other STS biomarker clinical studies are ongoing. The only routinely used highly effective neoadjuvant subtype-targeted therapy in STS is imatinib for unresectable localized dermatofibrosarcoma protuberans and gastrointestinal stromal tumors [125, 126] . However, being a diversified and orphan group of tumors, STS present various genetic alterations that may be potential targets for novel targeted therapies. Possible genetic pathways include specific translocations (for example, anaplastic lymphoma kinase [ALK] fusion in inflammatory myofibroblastic tumors), gene amplifications (for example, the amplification of MYC in radiation-induced angiosarcomas), oncogenic mutations (for example, activating mutations in the KIT receptor), and complex genomic rearrangements (vast majority of STS) [127] . A clear example is an inhibitor of enhancer of zeste homolog 2 (EZH2) histone methyltransferase, tazemetostat, registered in the USA for metastatic or locally advanced unresectable epithelioid sarcoma [128] . A mutation in the INI1 suppressor gene in epithelioid sarcoma cells causes deregulation of EZH2 that leads to the activation of multiple oncogenic signaling pathways. Tazemetostat competitively inhibits EZH2, stopping epithelioid sarcoma growth. Another effective novel therapy, namely the inhibitor of tropomyosin receptor kinases A, B and C, targets NTRK fusions. An analysis of databases from three ongoing phase I or II clinical trials with entrectinib (ALKA-372-001, STARTRK-1, and STARTRK-2), which enrolled 54 patients with metastatic or locally advanced NTRK fusion-positive solid tumors, showed a high ratio of objective durable responses (31/54 patients, median duration 10 months) with good treatment tolerance [129] . In the study, STS were the predominant group of tumors (13 patients, 24%). The objective response was observed in almost half of them (6/13 patients, 46%). Several other subtype-targeted molecules are currently under investigation [127, 130, 131] . Promising results of immune checkpoint inhibitors in various cancers encouraged investigators to assess their potential in STS. This group of molecules targets the most important regulators of the immune system that are responsible for anticancer response. Immune checkpoint inhibitors are especially active in tumors with a high mutational burden. Approved molecules include anti-CTLA4, anti-programmed cell death 1 (PD-1), and anti-PD-1 ligand immunotherapies. Despite a strong theoretical basis and expectations, preliminary results from phase I and II clinical trials showed moderate activity of immunotherapy in STS [132] . The exception is alveolar soft part tissue sarcoma, a rare and radiochemoresistant STS subtype. In a phase II clinical trial with axitinib and pembrolizumab in patients with advanced alveolar soft part tissue sarcomas and other STS subtypes, the 6-and 12-months progression-free survivals were 47% and 28%, respectively [133] . The best overall response rate was described in eight patients and, among them, six had alveolar soft part tissue sarcoma. Another report described similar activity of immunotherapy in this rare STS subtype [134] . Nevertheless, immunotherapy has not been investigated in nonmetastatic or resectable STS. Thus, it should not be recommended as a neoadjuvant treatment. New clinical trials are required to assess the efficacy of immunotherapy in STS. Patients with high-risk STS may benefit from neoadjuvant therapy. Neoadjuvant RT should be a part of the treatment provided that a risk of wound complications is acceptable. Due to high risk of distant metastases, RT may be combined with neoadjuvant CHT. Anthracycline-ifosfamide CHT regimens are preferred regardless of STS subtype. In fragile patients, less intensive CHT regimens, such as anthracyclines alone, may provide adequate efficacy without the risk of a significant increase in CHT toxicity. Slowly growing locally advanced low-grade tumors bring a low risk of metastatic spread. Thus, administration of CHT is not recommended. This group of STS is relatively more radioresistant. Therefore, based on radiobiological models, hypofractionated RT might provide benefit in local response. Additional radiosensitizing modalities such as HT may enhance efficacy of RT. Neoadjuvant therapy is the gold standard for the treatment of marginally resectable or non-resectable STS. Both CRFT and HFRT may be considered. In bulky, symptomatic tumors, shorter RT regimens are preferred. RT might be combined with anthracycline-based CHT to increase local response and decrease the risk of distant metastases. Due to the usually large volume of those tumors and related symptoms, the application of regional HT or nanoparticles may be problematic or even unfeasible. Epithelioid sarcoma, extraskeletal myxoid chondrosarcoma, clear cell sarcoma, solitary fibrous tumor, alveolar soft part sarcoma and inflammatory myofibroblastic tumor constitute STS subtypes resistant to conventional cytotoxic chemotherapy. In those subtypes, neoadjuvant CHT is not recommended due to lack of efficacy and risk of disease progression. In the case of resectable chemoresistant STS, the preferred neoadjuvant approach is RT followed by surgery. More locally advanced and marginally resectable chemoresistant STS may benefit from additional radiosensitizing modalities, such as HT or nanoparticles. Although no targeted therapies have been investigated as a neoadjuvant treatment specifically in chemoresistant STS, tyrosine kinase inhibitors and antiangiogenic agents may be considered individually in selected cases. The only exception is the inflammatory myofibroblastic tumor presenting the ALK gene mutation, which is susceptible to ALK inhibitors [135] . The management of radiation-induced or in-field recurrent STS is challenging. The only curative modality in localized tumors is radical resection with wide negative margins. The role of secondary RT is unclear, mostly due to the concerns about possible severe side effects after re-irradiation. However, RT may be carefully considered in selected cases, especially after a long period from the first RT, and absence of significant late toxicity from previous irradiation. HT may allow RT dose reduction without a decrease in treatment efficacy. CHT and targeted therapy may be used in metastatic disease, but their role in neoadjuvant therapy is not established.@story_separate@There are multiple options for neoadjuvant treatment in STS that are focused on improving local and distant control. Any neoadjuvant approach should be considered individually at the MTB, taking into consideration tumor site, stage, pathology, comorbidities, age, resectability, institutional protocols, and availability of methods. The authors' consensus on available combinations that could be considered in various clinical situations is presented in Table 5 . The response to treatment should be assessed by standardized radiological and pathological criteria. New clinical trials with new combinations of methods in the neoadjuvant setting are encouraged.","Due to the heterogeneity of soft tissue sarcomas (STS), the choice of the proper perioperative treatment regimen is challenging. Neoadjuvant therapy has attracted increasing attention due to several advantages, particularly in patients with locally advanced disease. The number of available neoadjuvant modalities is growing continuously. We may consider radiotherapy, chemotherapy, targeted therapy, radiosensitizers, hyperthermia, and their combinations. This review discusses possible neoadjuvant treatment options in STS with an emphasis on available evidence, indications for each treatment type, and related risks. Finally, we summarize current recommendations of the STS neoadjuvant therapy response assessment."
"The coronavirus disease 2019 (COVID-19) has posed a global health emergency and affected the daily lives of everyone worldwide. On January 30, 2020, the World Health Organization (WHO) declared COVID-19 to be the sixth public health emergency of international concern in history. Severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2), a new enveloped single-stranded positive-sense RNA virus that has caused the COVID-19 pandemic, is a novel coronavirus member of the Coronaviridae family. [1] The main symptoms of COVID-19 are fever, cough, and fatigue. [2] Most patients confirmed as having COVID-19 present ocular manifestations consistent with conjunctivitis, along with the typical respiratory symptoms. Notably, accumulated evidence shows that the virus can be transmitted through the oculus, especially by contacting the eyes with contaminated hands. From an anatomical point of view, the nasolacrimal system provides an anatomical bridge between ocular and respiratory tissues, potentially facilitating the exchange of virus-containing fluid between the two sites. [3] Thus, it is critical to increase awareness that SARS-CoV-2 can potentially be transmitted through the oculus and characterize the ophthalmic symptoms associated with COVID-19 infection. This knowledge will help to prevent virus transmission in ophthalmology departments and provide insights into the recognition of ophthalmic symptoms of COVID-19 and appropriate therapeutic approaches.@story_separate@It has been substantiated that some respiratory viruses can cause ocular infections, including adenovirus, influenza virus, herpesvirus, respiratory syncytial virus (RSV), as well as coronaviruses. The adenovirus infection manifests the ocular symptoms as epidemic keratoconjunctivitis, pharyngeal conjunctival fever, and non-specific conjunctivitis. [4] The nucleic acid of the virus within conjunctival swabs from adenovirus-infected patients with ocular symptoms could be detected positive through the polymerase chain reaction (PCR) experiments, and the deep sequencing also indicated the existence of adenovirus sequence. [5] [6] [7] The ocular symptoms of influenza A virus, especially the H7 subtype, are red eyes, teary eyes, itching, eye pain, burning eyes, eye pus, and sensitivity to light. [8] H7N9 virus could be detected by reverse transcription-PCR (RT-PCR) from conjunctival samples of patients with conjunctivitis symptoms, and meanwhile, the influenza virus could also be successfully isolated based on the samples. [8, 9] Herpes virus-related infection also has the features of ocular symptoms, including conjunctivitis and keratitis. The presence of herpes virus in the eyes can be detected by immunofluorescence and multiplex PCR in tear samples or corneal samples of patients with ocular discomforts. [10, 11] RSV can also cause conjunctivitis. [12] Notably, ocular symptoms and the presence of viruses can be observed during the infection of coronaviruses. Some patients infected with the human coronavirus NL-63 (HCoV-NL63) show symptoms of conjunctivitis, and the virus shedding in the eyes can be identified by RT-PCR and/or virus isolation experiments. [13, 14] The virus RNA from tears can be detected in SARS-CoV-infected patients through RT-PCR. [15, 16] Herein, we summarize the characteristics of ocular symptoms, virus shedding during the infection with different viruses, and also the expression of corresponding viral receptors in the oculus ( Table 1) . Coronaviruses use their spike proteins to select and enter target cells. Angiotensin-converting enzyme 2 (ACE2) has been identified as a mediator between severe acute respiratory syndrome coronavirus (SARS-CoV) and host cells. [17] SARS-CoV-2 has the same cellular receptor as SARS-CoV. [18] Researchers have also demonstrated that SARS-CoV-2 also uses neuropilin-1 for cell entry. [19, 20] This is the first step in understanding the pathogenesis of the virus. Studies have reported the expressions of ACE2 and its co-factors transmembrane protease, serine 2 in different sites of the oculus, including the conjunctiva, cornea, and limbus, [21] [22] [23] [24] [25] [26] [27] which provides evidence that the eye may be an additional entry portal for SARS-CoV-2 and may have some role in viral spread ( Fig. 1 ). A report of the WHO-China Joint Mission, which summarized 55,924 cases of COVID-19 [28] and another study by Guan et al., which recruited 1099 confirmed cases [2] reported that 0.8% of patients had signs of conjunctival congestion, and it was higher in patients with more severe COVID-19. [29] A meta-analysis about the proportion of clinical ocular symptoms in COVID-19 patients reported an incidence of 11.2%. [30] The main ocular manifestations are consistent with conjunctivitis [29, 31, 32] including conjunctival hyperemia, chemosis, epiphora, increased secretions, foreign body sensation, blurred vision, dry eye, and itching. Furthermore, it has been reported that a few confirmed cases manifested keratoconjunctivitis [33, 34] and keratouveitis, [35] even retinal change. [36, 37] Therefore, there is speculation that this virus may infect the ocular surface first, spread, and then cause pneumonia through the nasolacrimal tube. With a view to the considerable proportion of SARS-CoV-2related conjunctivitis cases, appropriate medical treatment is of particular importance. Generally, although viral conjunctivitis tends to self-heal, antibiotics, steroids, and artificial tears can also be used clinically to relieve the signs and symptoms of inflammation. Most drugs currently available for the treatment of viral conjunctivitis mainly target herpes virus and adenovirus infections. [38] Although some clinicians have used eye drops/ ophthalmic ointment locally in the eyes of COVID-19 patients to relieve ocular symptoms, there is currently no specific antiviral drug for SARS-CoV-2-related conjunctivitis. [39] Studies have shown that theoretically, the topical use of 1% povidone-iodine (eye drops) in the eye potentially can have an effect on SARS-CoV-2-related conjunctivitis. [40] In addition, researchers have found ingredients with potential antiviral activity (including anticoronavirus) in various commonly used eye drops and ointments, indicating that ophthalmic preparations can be used as potential drug candidates for antiviral therapy. [39] Transmission evidence of SARS-CoV-2 via eyes and corresponding ophthalmic precautions A national expert on infectious diseases was infected with COVID-19 in Wuhan and stated that the most likely route of exposure was through his unprotected eyes. [41] Another case involved a nurse who was fully protected but occasionally worked with dislocated eye goggles that means her eyes may be at risk of exposure. [42] Researchers initiated an interesting animal study to investigate whether SARS-CoV-2 can cause systemic disease through the conjunctiva. [43] They inoculated two rhesus macaques with SARS-CoV-2 via conjunctiva and the other one inoculated via intratracheal route as a comparison and then detected virus load in the respiratory tract and systemic tissues including lungs and digestive tract of conjunctiva-inoculated rhesus macaques. This finding indicated that SARS-CoV-2 can cause systemic disease through the conjunctiva. Another study used SARS-CoV-2 to infect ex vivo cultures of human conjunctiva tissues and the viral titers of the culture supernatants continued to increase 24 to 48 hours after inoculation. At the same time, the virus-inoculated conjunctiva tissues were subjected to immunohistochemical staining with monoclonal antibodies against SARS-CoV-2 nucleoprotein, the results showed the presence of virus particles in the conjunctiva tissues. It indicates that the eyes may be the additional way for SARS-CoV-2 to infect the human body. [44] One previous study reported that hand-eye contact (such as eye rubbing) was a risk factor of epidemic keratoconjunctivitis [45] and that a total of 332 (62%) out of 534 COVID-19 patients had a history of hand-eye contact, [46] indicating that transmission by hand-eye contact should not be ignored. [47] Other researchers have speculated that ophthalmologists are susceptible to SARS-CoV-2 infection. [48, 49] The conjunctival mucosa is directly exposed to infectious droplets and fomites during close contact. [50] It is probable that ophthalmologists are extremely reliant on physical examination during patient consultation, and there is generally almost no distance between ophthalmologists and patients during these exams. [51] Thus, contagious droplets from patients probably are transmitted to the nose, mouth, and ocular surfaces of health care workers, which leads to infection. Thus, it is recommended that a full set of personal protective equipment be used by ophthalmologists, nonurgent ophthalmic operations should be delayed during the pandemic, [52] if possible, and telemedical consultation is considered as another good option. [53, 54] Strict hand hygiene, handwashing with soap and water carefully and regularly, avoiding touching the eyes, nose, and mouth, especially in vulnerable areas, is recommended. Moreover, various types of slit-lamp covers, face covers, indirect ophthalmoscope covers, and shielded slit-lamp cabinets to prevent the spread of the virus have been rapidly developed for use along with a strict and complete disinfection program. [52, 55] Additionally, a set of targeted patient management plans, including the establishment of triage stations, risk assessment of patients, and strict safety intervals in the waiting area of the clinic have been developed. [54] Detection of viral RNA in ocular surfaces of COVID-19 patients Previous studies have shown that SARS-CoV RNA can be detected in tears via RT-PCR. [15, 16] Continuous studies have reported positive detection of SARS-CoV-2 RNA in the tears and conjunctival secretions of COVID-19 patients. [29, 31] One metaanalysis reported that 16.7% (10/60 cases) of conjunctival samples were SARS-CoV-2-positive by RT-PCR. [30] In one case report, SARS-CoV-2 RNA was tested in the cornea. [35] Nonetheless, positive detection by RT-PCR does not reliably indicate the emergence of conjunctivitis [56] because some COVID-19 confirmed patients presenting with conjunctivitis have tested negatively for SARS-CoV-2 RNA in tears and conjunctival secretions while some cases without ocular symptoms detected positively. [57] Many factors account for this result, and overall, the positive rate of SARS-COV-2 in tears and conjunctival secretions from patients with COVID-19 via RT-PCR is low. Possible reasons include 1) the viral load in conjunctival secretions is too low to be detected; 2) the positive results of conjunctival swabs occur during the early onset of the infection in conjunction with delayed sample collecting; 3) irregular test procedures that lead to genetic material damage or sample contamination and result in false negatives and false positives, respectively, [58] and inappropriate collection techniques (including the sample container and the sample volume collected); and 4) antimicrobial agents of the host immune system and constant tear rinsing from the ocular to the nasal cavity through the nasolacrimal duct may facilitate the elimination of the virus from the eyes. [50, 59] It should be noted that positive detection of SARS-CoV-2 RNA in post-mortem cornea and sclera of COVID-19 patients via RT-PCR. [60] In another study of five fatal COVID-19 cases, virus RNA and intraocular vascular damage were found. [61] Overall, the evidence indicates that virus shedding can occur in the oculus of COVID-19 patients and the eye may have some role in the transmission of SARS-CoV-2. George F. Gao, William J. Liu and Xiang Tian Zhou conceived the manuscript. Hong Li Ran prepared the manuscript. All authors read, edited, and approved the manuscript. None. Editor note: William J. Liu is an Editorial Board Member of Infectious Diseases & Immunity. The article was subject to the journal's standard procedures, with peer review handled independently of this member and his research group.@story_separate@In this review, we showed that the ocular surface may be an additional entry portal and infection for SARS-CoV-2, supported by anatomical and clinical observations and evidence from animal studies. We emphasized the importance of protecting eyes during close contact with confirmed patients. In consideration of the continuing COVID-19 pandemic, a comprehensive understanding of how the virus spreads is vital for the prevention and control of viral propagation to decrease the risk to public health. In addition to respiratory droplets and close contact with infected patients or contaminated items, alternative routes of SARS-CoV-2 transmission have been a concern. Considering the ocular manifestations observed in COVID-19 patients and the SARS-CoV-2-positive RT-PCR results from conjunctiva and tear specimens as well as cornea, increased attention has been given to determine if ocular tissues are a SARS-CoV-2-transmission route. We presented evidence to support that the ocular surface is an additional entry portal for SARS-CoV-2 and possibly has a role in viral transmission. However, additional details of the pathogenic mechanism and immune reaction of ocular infection in COVID-19 remain unclear. Further clinical observations and detailed animal studies would be important to investigate the ability of SARS-CoV-2 to infect ocular tissues and transmit via eyes. Protecting our eyes, especially those of health care workers, is essential during this pandemic. The one common deficiency of these studies is that the sample sizes were small; thus, sufficient sample size and wellcharacterized studies are required to obtain more evidence. Although COVID-19 can cause ocular symptoms, solid evidence is still needed to confirm that SARS-CoV-2 is the cause of ocular manifestations in these cases. Differential diagnosis is necessary to distinguish symptoms related to the virus from symptoms related to the patients' own underlying eye disease or other systematic diseases. Given that COVID-19 can cause fatal viral pneumonia, most studies have focused on respiratory tract disease, and only a small number of researchers have investigated the role of the eye in the disease process. Studying the connection of SARS-CoV-2 with the ocular surface is important to understand the comprehensive transmission routes of this virus, not only to help control viral spread during the pandemic but also to provide better infection preventive measures.","In December 2019, a new coronavirus disease 2019 (COVID-19) emerged and rapidly spread globally, posing a worldwide health emergency. The pathogen causing this pandemic was identified as severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2). It is well known that SARS-CoV-2 transmits via respiratory droplets and close contact with infected individuals or contaminated items. In addition to these two major transmission routes, other modes of transmission have not been confirmed. Considering that some COVID-19 patients have presented with ocular discomforts and positive SARS-CoV-2 RNA in ocular surfaces, as well as the discovery of the SARS-CoV-2 receptors, angiotensin-converting enzyme 2, and transmembrane protease, serine 2, in the oculus, the ocular surface is now thought to be a possible alternative route of SARS-CoV-2 transmission and a replication site. This review summarizes the evidence connecting COVID-19 with ocular tissues, ocular symptoms during SARS-CoV-2 infection, the potential role of the conjunctiva in SARS-CoV-2 transmission, and the physiopathological mechanisms. Appropriate precautions in ophthalmology departments, including innovative complete and effective patient management plans, protective personal equipment, hand hygiene, and strict personal distance intervals, are essential to effectively minimize the spread of SARS-CoV-2 and control the pandemic."
"Many of us do our shopping for food, drink, and other essential items, in a supermarket or at a takeaway. During the COVID-19, or indeed any other, epidemic this leads to a possible risk of infection. The group of people going shopping is typically open in that it is drawn from a diverse population who will come from many different locations and who are, usually, unknown to each other. Whilst this group can be quite large, and diverse, the time spent in contact with each other in such a situation is often relatively short. The question remains as to what is the best way to organise the dynamics of the shoppers in a supermarket, or takeaway, so as to minimise the overall risk of infection. During the course of the 2020 COVID-19 pandemic various measures have been considered/implemented including directed shopping and the compulsory use of face masks. In this paper we make a partial assessment of the effectiveness of both of these measures, through the use of mathematical models. During the epidemic, a typical shopping experience comprises a wait (in a socially distanced queue) by the entrance outside the shop. This queue is then allowed into the shop, typically on a one-in oneout basis. Whilst inside the shop, shoppers are largely free to move as they wish. Finally, on exit, the shoppers form an ordered queue (or queues) to be served. In a takeaway, a similar procedure is involved, although customers typically move straight from the entrance to the serving queue. Unsafe interactions can occur at any of these points. It is natural to want to try and minimise the frequency and duration of any such interactions, whilst also maintaining the economic viability of the shop. These objectives are not necessarily compatible. This leads to the issue of determining the optimal way of organising both the 'free-form' shopping (with reasonable constraints consistent with modelling the shopping experience) and also the queue (or queues) being served. This is the 'managing the crowd' principle outlined in [1] and is affected by both the internal geometry of the shop and also by the way the crowd is directed around this geometry. For example, should the crowd move 'randomly' in a self-organised fashion, or should it in some way be 'directed' as an ordered queue throughout the shop. Similarly in the case of the checkout queue, the safety of the customers in the queue will be affected by the number, and level of protection, of the servers and of each other. These considerations must also be balanced against the risk to the servers themselves. In this paper, we address some of these issues by constructing a mathematical model of the above shopping process looking at both the movement in the shop and also in the queues. This model helps to determine the total viral dose experienced by an average shopper. It is based on certain simple assumptions of the way that the virus spreads within the shop and between people, and also of the way that the people move within the store as they make their shopping choices. We emphasise that these conclusions are obtained by the use of mathematical models based on certain assumptions and caveats which we describe alongside the models. At this stage we have not considered any actual data for the COVID-19 case (although we do draw in places on data for other diseases), and the conclusions are the results only of the simulations and mathematical arguments. However, we show through the modelling experiments, and looking at the level of uncertainty in the model predictions, that the conclusions are reasonably robust to changes in the assumptions themselves. We emphasise that these studies are in some senses preliminary, and we hope that they will lead to further, data-driven investigations. Our approach comprises an agent based (ABM) social force model of the crowd (of varying density) within the shop acting as 'typical shoppers' [11] , combined with a queuing model of the checkout itself, and models for the viral spread and the impact of PPE. Other approaches to studying the issues associated with the retail environment have also been considered by the Royal Society RAMP initiative. In particular, we compare the results of this approach with that considered in [27] which examines several possibilities for managing the crowd within a supermarket environment, including unidirectional aisles and enforced capacity in popular areas of the shop, and determines the viral dose as a function of the arrival rate of the shoppers. See also the use of Poisson process model for calculating the spread of COVID-19 in the retail environment [22] . In Section 2 of this paper we consider an agent-based model for free-flowing shoppers in the supermarket, with a probabilistic model for the way in which they move as they choose their goods. Such a model assumes that there are many shoppers who interact with each other over a short period of time, a proportion of whom may be infected. During such interactions the shoppers accumulate a likely viral dose which is then linked to their risk of infection. The analysis in Section 2 comprises the use of a social force model of the crowd making certain assumptions of their mode of shopping, combined with a model for the transport of the virus from one crowd member to another. In Section 3 we move the shoppers to the queue, or queues, at the exit of the shop (which they may move to directly if the shop is a takeaway) and use queuing theory to assess the risk of infection in this (as it turns out quite dangerous) situation. A clear problem in this phase of the shopping is that the queue may be slow moving, and this significantly increases the risk to both the shoppers and the servers. In the modelling of the queue we consider the relative risk to both shoppers and servers, and rigorously study how it changes with the dynamics of the queue, the number of queues/servers, and the level of protection given to both the servers and the shoppers. In particular, we model the wearing of masks by lowering the probability of an infection spread in an unsafe interaction with an infected person. Applying these mathematical models leads to a number of tentative conclusions which, as described above, are fairly robust to the modelling assumptions that we have made.@story_separate@A first conclusion from modelling the movement of the crowd in Section 2 is that minimising the duration of interactions can be more effective than focusing only on the distance between people. (This conclusion depends only weakly on the precise model used for the aerosol transmission.) Thus a 2m separation may be quite dangerous if the people interacting spend a long time at this distance, whereas a separation of less than 2m may be safer if the interaction is shorter. We find that bidirectional shopping leads to higher viral particles inhaled. However, this structure also makes a significant difference to the efficiency of shopping. Unidirectional shopping led to the lowest viral exposure, as long as shoppers went against the direction if they forgot an item. Lower efficiency of shopping leads to shoppers spending longer in store and hence to higher viral doses. Unidirectional shopping, with the assumption that shoppers did break the directional rules if they needed, was the most efficient and led to the lowest exposure in every situation. However, the difference gained or lost by different shopping mechanics was dwarfed by the effect of differing aisle widths. Shops with 2m aisle widths led to up to 10 times more viral exposure than the same system in a 3m or 4m wide aisle. These conclusions follow the reasonable assumptions that people act in response to social and physical forces and that the viral particles from an individual disperse with isotropic diffusion (proportional to 1 r 2 with distance r from an individual). This is not known about COVID-19 particles, however our conclusions are robust to changing the viral model to being proportional to a r + b r 2 + c r 3 for any positive constants a, b, c. This result complements previous work which has found efficiency to be the main disadvantages to organised shopping [27] , and suggests that venues which have already implemented such measures might reconsider their policy if it is leading to significantly longer shopping experiences. It would also be prudent to analyse shop layout and flow with a view to minimising the time spent shopping. Furthermore, venues which have the capacity to widen aisles should consider implementing this as of utmost importance in lowering risk. Organising the exit queue: In Section 3 we draw the following tentative conclusions for organising the exit queue(s): (i) unsafe interactions should be kept to a minimum, (ii) protective mask wearing should be maximised, and mandatory for the queue servers, (iii) mandatory extra protection should be provided for the servers, (iv) the number of servers should be maximised (under the constraint of keeping them safe from each other), (v) wherever possible, customers should be organised into separate, non-interacting queues, each with a single server, rather than a single queue serviced by a number of servers. These conclusions follow from the reasonable assumption that COVID-19 spreads primarily through unsafe interactions which occur between pairs of people, with these unsafe interactions occurring with greater frequency in situations in which, for example, customers are closer together on average. We assume that a given queuing system will have some time-invariant unsafe interaction rate for any pair of people who can come into contact in the exit queue. Note that, in practice, this rate is unlikely to be uniform between different pairs; for instance, certain customers may be more observant of social distancing rules than others. Additionally, we assume that, when an unsafe interaction occurs between an infected and an uninfected person, the virus spreads with some person-invariant probability. Again, in reality, it is likely that there exist ""super-spreaders"" of the virus, for whom the probability of transmission is greater, given an unsafe interaction. Transmission probabilities are also known to be affected by, for instance, the length of time since infection. Central to all of these results is the model of the safety effect of wearing masks. This effect is assumed to be asymmetric: in an unsafe interaction between an uninfected and infected person in which both wear masks, more of the benefit of the mask wearing is believed to come from the mask worn by the infected person, see, for instance, [25] . For this reason, without masks, servers have a potentially increased chance of becoming super-spreaders of the virus within an open group of shoppers. This principle is also seen in the novel theory developed in Section 3, in which we examine the spread of infection in two queuing systems: (i) multiple queues with a single server each (as in a supermarket); and (ii) a single queue with multiple servers (as in a coffee takeaway shop or a self-service queue in a supermarket). The conclusion that wearing masks is important is of course of no surprise, indeed mask wearing is now compulsory in UK shops. However, we hope that the reasoning behind this observation will help guide decisions on the future use of masks as the current crises starts to ease. 2 Many-to-one interactions in a crowded supermarket In this section, we consider a two-dimensional model for supermarket shopping which represents the shoppers moving in a crowd as small circles (which are in turn cross-sections of cylinders). The central assumption of this section is that there is a large number, N , of shoppers who come from an originally well mixed and open population, who are in close proximity with each other for a relatively short time period. To simulate, and then to analyse, the crowd of shoppers in a supermarket, we consider a particle model to represent each person in a two-dimensional domain representing a typical supermarket, as illustrated in Figure 2 . The shoppers will then act as agents, moving around the store according to certain rules governing the way that they are likely to shop, and will come into contact with other shoppers as they do so. This domain can be thought of as a supermarket with constraints such as walls and aisles. We set out to compare the behaviour of the crowd in different shopping structures of the store, and how these structures may influence viral exposure and shopping progress. In the model, each shopper, indexed by α, is considered to be a separate particle with a position x α , a velocity v α , and an acceleration a α and radius r α . The positions and velocities of the shoppers then evolve due to the acceleration of the public actors. Such motion is governed using Newton's second law of motion, f α = m α a α , where f α is the force on the public actor (a combination of social force, intelligent intent and geometrical constraint), m α is the mass, and a α is the acceleration of the public actor. The system of ODEs that governs the position and velocity is then For simplicity, we take m α = 1, for all values of α. We model each shopper as a cross-section of a cylinder of radius r α . For our model, we will consider the shoppers to be initially distributed randomly throughout the supermarket. The forces acting on each shopper are then dependent on their surroundings, the nearby shoppers and their intent and mode of shopping. Following [11] , we consider four forces that act on the shoppers: a strong repulsion force from the supermarket walls, f walls ; a strong repulsion force, f repel , which ensures that shoppers don't inhabit the same space; a weak repulsion (social) force representing social distancing, given by f sd ; and an attraction force f α representing a shopper's intent to buy a particular item. It is the force f α which is most dependent on the individual concerned, and the the hardest to model. The force on the shopper α, is then expressed as a sum of all the forces discussed, giving us The motion of the crowd arising from these forces then depends upon the precise description of each individual force. We now discuss these in detail. Each force is governed by a set of parameters which we then choose informed by established work in agent based models. However not every shopper is the same. To avoid homogeneity every parameter for each shopper is chosen on a normal distribution with a mean informed by established theory and a variance of one quarter of the mean. In the following descriptions we give mean values for the parameters. The wall repulsion force, f walls , is a force felt by the shopper α from the closest point of the wall x w provided that the distance to that point in the wall d w = |x w − x α |, is smaller than a chosen threshold distance. The resulting force is then given by The first term in the brackets in (1) is taken from [11] . This is a mid-range force and models the desire of people to not be too close to walls. The second term taken from [12] is the force adjacent to the wall/aisle and models a shopper's inability to move through then walls. In this expression, the constant f obstacle is the maximum mid-range force, g obstacle and σ wall control the short-range force. Following the recommendations of [17] for parameters p = 2 and f obstacle as around 2 times the maximal acceleration we define f obstacle to be 4. We ran experiments to define the values of g obstacle and σ)wall such that shoppers used the most amount of space when required but did not pass through the wall. We set g obstacle = 1000 and σ wall = 0.01m and define the threshold distance at 1m. The repelling force from one shopper to another is split into two forces. The first is a near-range, social distancing force, describing the situation when a shopper α comes within a short distance of another shopper β, so that they are fully aware of each other, but they are not in contact. We note that the strength of this force can reflects imposed social distancing rules, so that it will be stronger if a 2m separation rule is enforced rather than a 1m rule. This force is combined with a contact force, which occurs when then shoppers are touching, so that they are closer than r α + r β . [17] . The repelling force is therefore defined by In this expression, s, ρ α , ρ αβ and a are defined as The maximal social distancing force is governed by the constant f repel max , and the decay of the social distancing force is dictated by p 2 . Similarly, the contact force is governed by constants g repel max and p 3 . Again following [17] we define f repel max (resp. g repel max ) to be around two (resp. four) times maximal acceleration. We take maximal acceleration to be 2ms −2 and set the parameters to be f repel max = 4, g repel max = 8. We set r α to be .25m. Attractive forces modelling the intelligent intent of the shopper. Following [11] , each shopper has a desired velocity v desire α , which in the context of this paper will model the way in which the shopper will proceed with doing their shopping. The force compelling them to travel at this velocity is f attract where v α is the shopper's current velocity. Calculating the (changing) desired velocity of a shopper v desired α as they go about their shop, is a very subtle part of this modelling procedure, and the most open to the assumptions made on the way that people shop. A shopper entering a supermarket has as a main goal, the desire to pick up the products on their list. Usually a shopper does not do this randomly, nor do they usually do this with exact precision. As a consequence, whilst their passage around the store is not completely random, it is also usually sub-optimal. As a model for this behaviour we randomly generate a number of points in the store domain for each shopper. These points then become their shopping list of desired items. In a perfect world each shopper would sort their list entirely and therefore only have to travel around the aisles in the store in one direction. We model this scenario by initially fully sorting each shoppers list by considering the location of the items in the store along an optimal path. However in reality shoppers might forget something, or not be infallible in their organisation, or simply not know in advance where the item which they want to buy is in the store. (In the authors' experience this is the rule rather than the exception!) Hence they will often have to double back during their passage around the store. This motion can then represented by a partially sorted list. We implement this by taking the fully sorted list and applying a small number of random permutations to it to give a partially sorted list. The shopper will then move around the store going from one item in this partially sorted list to the next. Let x desired represent the desired point of a shopper α with desired speed s α we define the desired velocity: We take s α to be from a normal distribution with mean 1.4ms −1 , this follows data acquired by [15] from measurements in shopping centres. We have designed our model to implement three different supermarket structures. The first structure is a one-way system. Every shopper is told to go in one direction around the shop. However in this system shoppers do go back on themselves. The second structure is a two-way system where shoppers are allowed to travel either way around the supermarket. The final structure, a strict one-way structure. In this system if a shopper had to turn back more than 1m they don't break the rules and they go all the way around the loop again. We believe this is less realistic. Note that if shoppers' lists were fully sorted both one way systems would be identical. In Figure 1 we display the path of an individual in an empty aisle under different supermarket structures. We presume that each shopper is guided by their partially ordered list as follows. They are initially attracted to the position of the first item on the list. When they get within 1m of this item then at every time step they pick up this item with probability p. This models the empirical observation that shoppers may take some time to decide which precise item they want to buy. Once they pick up that item they are then attracted to the second item in their list and this continues until the list is complete. In our model we monitor the progress of the shoppers. This is defined to be the number of items a shopper has picked up in the time-frame during which the model is run. We next consider calculating the viral dose of customers in the supermarket as they move through the store. In our model, we will consider that one (random) individual is deemed to be infected. The infected individual is asymptomatic. This information originates from experiments done by [19] which agree with previous experiments [7, 9, 20] . The density of the viral particles. We next make the assumption that the viral density is proportional to r −2 , where r is the distance from the infected individual. This assumes that the viral particles disperse uniformly with distance, that is there is an equal number of viral particles in every one metre annulus around the infected individual. A better understanding of the mechanics of the Covid-19 particle transmission in the air will naturally lead to improvements on this model However, later in this paper we will consider other rules for the decay of the viral density with distance and will show that the conclusions from the model are fairly robust to the precise details of the particle dispersion. Our model of the density of the viral particles in the air will thus be initially: for a suitably chosen constant Λ. We will choose Λ = 10 3 so that ρ(0.1m) = 10 5 particles per cubic meter. This matches the experimental measurements of viral particles under normal breathing [19, 7, 9, 20] . Exposure of individuals to the viral particles. Let V inhaled be the proportion of the surrounding air inhaled by an individual. We model the viral dose σ α inhaled by a healthy shopper α at a distance r = |x α −x inf ected | from an infected individual located at the position x inf ected as The average lung capacity is 6 litres [5] and the average breath rate is 12 − 20min −1 [23] . If we assume that an individual breathes from a 1m 3 volume every 4 seconds (a rate of 15 per minute), then we can estimate V inhaled as 6×10 −3 × 1 4 = 1.5×10 −3 m 3 . For every individual we define V inhaled α to be a value taken from a normal distribution with mean V inhaled and variance 0.25V inhaled . This subsection describes what is only a rough estimation of the behaviour of the viral particles. The full aerodynamic motion of viral particles is hard to simulate, we are not able to include that in our simplified model. We have however run the exact same visualisations with viral density proportional to 1 r , 1 r 2 and 1 r 3 . Although the exact results were different for the differing exponents we found that the comparisons of viral exposure per item were still valid and robust under the change of exponent. One could view the viral density as a probability density of viral particles. If the particles decay slowly or currents spread them further then an exponent larger than −2 may be required, while if particles drop to the ground quickly then perhaps an exponent smaller than −2 is more appropriate. This is expanded in in Section B of the supplementary materials. Every parameter except Λ, σ wall and g obstacle max were set from recommendations of well established agent based models [11, 12, 17] . We ran experiments to set σ wall and g obstacle max . Slightly varying these values had the effect of narrowing the domain but did not change the results very much. Our aim is to make comparisons between different shopping mechanics, the value of Λ does not alter the comparisons that we make. We will not be using the actual values of viral exposure in our conclusions. The domain we use for the numerical experiments is a looped aisle which models a structured aisle based system as shown in Figure 2 . This is a simplification of a supermarket. The results stated here apply to a simple loop, further work would need to be conducted to rigorously conclude this applies to more complicated domains. We now consider combining the above models of the crowd dynamics, and of the viral exposure, to determine the viral dose of a typical shopper moving in the crowd. We compare the viral exposure encountered in the different structured shopping environments discussed in the section on supermarket structure 2.1.1.  For our numerical experiments we fix the looped shopping aisle to have an area of 200m 2 . We compare three different aisle widths: 4m,3m and 2m, three different populations numbers: 7, 15, 25 and five different shopping list structures: sorted unidirectional, sorted bidirectional, partially sorted unidirectional, partially sorted bidirectional, partially sorted strict unidirectional. We then ran the crowd simulation model for a series of 20 visualisations at each crowd population N , aisle width and supermarket structure. The simulation modelled a typical shopping experience for T = 15 minutes. The model described is implemented on MATLAB using the ODE solver ode45 [10] . We recorded the viral dose σ α for each individual α the number of items picked up items α , and the viral exposure per product σ α /items α .  Walking past an infected individual with various decay laws for the droplet density. The model described above also allows for simple calculations about the viral dose associated with different possible trajectories of both the infected and the susceptible shoppers. For example, we can ask whether it is better to walk past an infected individual, coming quite close briefly, or to remain for a longer period of time at a safer distance. This question may be relevant when deciding whether to allow shoppers to walk about freely, or rather to organise them into some kind of queue or structured flow. We now consider this, and also look at the robustness of our conclusions to changes in model concerning the rate of spread of the virus droplets. Consider a susceptible individual who walks in a straight line past an infectious person at a relative velocity v, passing them at a minimum distance δ. We now compare this situation with that in which another uninfected individual remains at a constant distance D from the infectious individual for a time T . The viral dose σ received by the moving individual is where r(t) = √ v 2 t 2 + δ 2 is the distance between the infected and susceptible individuals at time t, and ρ[r(t)] is given by equation (3). For the static individual the viral dose will be Whether it is preferable to be static or moving depends on the ratio σ m /σ s (regardless of the probability of infection given a viral dose, which we analyse below). There will thus be a critical shortest distance δ c such that it is safer to walk past than remain at a distance D from an infectious individual: Note that this expression does not depend on uncertain quantities such as Λ. In the above and in the previous subsections we have considered the density of virus in the air, ρ, to decay with the square of the distance from the viral source. This corresponds to making the assumption that the particles move primarily by isotropic diffusion in three dimensions. To test the robustness of the conclusions from this assumption we now generalise equation (3) to account for either a shorter or a longer range decay, to take the more general form: with γ a constant. If the virus is only carried in relatively large droplets, which tend to fall to the ground, then this would translate into taking a value of γ > 2. However, there is also evidence that these droplets can be transported on convection currents, such as wind, air-conditioning, coughing and sneezing. Such a transmission method would then decrease the value of γ. There are also reports of the virus being carried, in some settings, by aerosols, such as small droplets created by ventilation in hospitals, or other particles in polluted environments [14] . These would diffuse further than larger droplets, making γ closer to 2. The value of γ will also depend on conditions such as temperature and humidity. For instance, a drier environment will result in smaller droplets which can diffuse further. For values of γ ≥ 2, the viral doses for the static and moving individuals are, respectively given by the expressions and where β γ is a coefficient. The values of β γ for integer γ ∈ [2, 10] are given in Table 1 . The critical distance is therefore In the case that γ = 1, the expression in (4) diverges. We therefore consider that the total duration of the walk is T (instead of infinity). In this case we obtain which gives a critical distance Note that the only uncertain quantity appearing in these expressions for the critical distance, δ c , is the decay exponent, γ. Figure 4 shows δ c against exposure time T , for different values of the static individual's distance D and the exponent γ. The velocity is taken to be v = 1.4 m/s, which is a normal walking speed. As we can see, δ c is quite low for T on the order of a few minutes and D of metres. For example, in the γ = 2 case, it would be safer to walk past an infectious individual, passing at just δ = 10 cm, than to remain for T = 2 minutes at a distance of D = 2 m. Even if γ = 4, it is safer to walk past at 50 cm than to spend 5 minutes at 2 m. Thus far we have only considered the viral dose to which susceptible individuals are exposed, but ultimately we are interested in estimating whether they will become infected. Let us define P (σ) as the probability that an individual exposed to a dose σ will be infected. To the best of our knowledge, there are as yet no data on this probability for SARS-CoV-2, although expert opinion seems to support the notion that there will be a minimum infectious dose (MID) of at least a few hundred or thousand virions, and that P will increase with dose [8] . For other viruses P has been found to follow a sigmoidal curve [2, 24] . For instance, Memoli et al. [18] exposed healthy volunteers to different doses of influenza A/H1N1 via nasal inoculation. As we show in Section 3 of Supplementary Materials, the probability of becoming infected in these data conforms well to a sigmoid. For instance, none of Memoli's subjects went on to shed detectable virus when infected with σ < 10 8 virions; whereas nearly 80% of them did when the dose was σ 10 10 virions. Assuming that the transmission mechanism for different respiratory viruses is qualitatively similar, it seems reasonable to consider, in the absence of specific data, that P might also be sigmoidal in the case of SARS-CoV-2. And it is possible to derive some general conclusions from this assumption. Let us compare a situation in which a dose σ * is given to each of n individuals, with one in which a dose nσ * is given to a single individual. The expected number of secondary infections will be I 1 = nP (σ * ) and I 2 = P (nσ * ), respectively. Assuming that P is monotonically increasing and P (0) = 0, it follows that I 1 < I 2 if P is convex. If P is a sigmoid function, this will be the case for doses below the inflection point σ I . In the absence of hard data for the SARS-CoV-2 case, it is useful to consider the following heuristic argument. At its inflection point, P is likely to be significantly greater than zero. For instance, if P were the logistic function often used to model biological processes, P (σ I ) = 50%. Hence, if our situation of interest is such that even in the worst case scenario the probability of infection is relatively low for any given individual, we can assume we are in the convex regime of P (i.e. nσ * < σ I ). This is consistent with contact-tracing studies for SARS-CoV-2, which have found relatively low probabilities of infection (attack rates) even among close contacts. For instance, Bi et al. [4] report P = 12.8%, 3.0% and 0.4% for high, moderate and rare contact frequencies, respectively. Therefore, if we assume that P is sufficiently small that we are concerned with the convex regime, then I 1 < I 2 . In other words, in this situation it would be preferable to distribute the viral dose among many individuals rather than to give it only to one. This reasoning suggests, albeit tentatively, that in situations where the probability of infection remains relatively low for any contact, a more random movement of people characterised by many fleeting interactions may lead to fewer infections than constrained movement, such as structured queuing, in which a smaller number of individuals receive a greater viral exposure. 1 However, there may be real-world circumstances in which this assumption does not hold, and in any case there will be situations, such as when shoppers are waiting to enter the shop or to pay before leaving, when queuing becomes inevitable. Given that these appear to be the situations of highest risk of transmission, we go on to model them in more detail. In this section, we examine the spread of COVID-19 in a structured queue at the exit of the shop. We consider two different models for the queuing system, and two different models for infection within the queue, with various levels of personal protection. For the queue, we examine 1. k queues in parallel, each with a single server (as in a supermarket); and 2. one queue with k servers (as in a takeaway shop). For the possible infections in the queue, we examine (a) all-to-all interactions -any two individuals in the same queue have the potential to directly spread infection to one another; and (b) nearest neighbour only interactions -COVID-19 can only directly spread between adjacent customers in the same queue. We now briefly discuss some important modelling assumptions made regarding the shop and the transmission behaviour of COVID-19. Capacity: In setting (ii), we have a single queue with capacity C. This capacity includes anyone who is currently being served by one of the servers, so trivially we require C ≥ k. Note we assume that entry is controlled so that the queue never exceeds length C. In setting (i), we have k queues each with capacity C . For each queue, this capacity includes the customer being served. We may think about this queue capacity in the supermarket setting as a hard policy adopted by shop managers to prevent queuing into customer browsing space, so C would typically be on the order of 2 or 3. Unsafe Interactions: For this section, we examine only the spread of COVID-19 directly from person to person. We ignore infections which may arise from, for example, the sharing of contaminated surfaces. In addition to possible customer to customer interactions -which we assume occur either according to regime (a) or (b) -a server and the customer they are currently serving may also have an unsafe interaction. Finally, if we are in situation (ii) with a single queue and k servers, we assume that servers on duty at the same time can have unsafe interactions with one another. We take the rate at which an unsafe interaction occurs between any valid pair as ξ hour −1 , the arrival rate into the queue as µ(t) hour −1 at time t, and the service rate of an individual server as λ hour −1 , assuming that they are always busy. Masks and Additional Safety: Our final shop-based assumption is that it is possible to mandate that all servers wear masks, and that some form of additional shielding of servers from the shoppers (e.g. a screen at the counter) is possible. Unsafe Interactions: As discussed above, an unsafe interaction is taken to be central to the transmission of COVID-19. When an unsafe interaction occurs between an infected and an uninfected person, we say that the probability of infection spreading is p. Mask Protection: We assume that the probability of infection transmission will be reduced if one or both of the people in an unsafe interaction are wearing masks. This effect will be different depending on whether the infected or the uninfected person is wearing a mask. Formally, if the uninfected person wears a mask, and the infected person does not, the transmission probability is α 1 p; if, instead, the uninfected person does not wear a mask, and the infected person does, the transmission probability is α 2 p; finally, if both the uninfected and infected people wear masks, the transmission probability is α 1 α 2 p. We remark that the extent of the effectiveness of mask wearing in reducing the spread of COVID-19 is a subject of much ongoing research. A pessimistic viewpoint is possible in the following theory by letting α 1 = α 2 = 1, with no substantive change to the remainder of our conclusions. We do not attempt to estimate these two values in this article. Additional Safety: As for masks, we assume that the additional safety at the counter provides protection by reducing infection probability. If this safety is present, the probability of transmission becomes βp. This protection factor is the same regardless of whether the server or the shopper is infected in the interaction. Note that if both masks and extra protection are present, we assume that their effects are independent of transmission probability. For example, the probability of transmission from an infected server to an uninfected customer in which both are wearing masks and extra protection is present is taken as α 1 α 2 βp. Starting Assumptions: We take the initial starting proportion of the population infected by COVID-19 to be p 0 . We assume that nobody is immune. We now discuss the factors which we shall examine in our analysis of a shop queue system over a given period of time. These variables can be divided into two types: we focus on two variables with respect to the behaviour of the population • ξ hour −1 , the unsafe interaction rate; and • p M , the proportion of the shopping population who wear masks. Our other variables concern shop policy decisions associated with the management of the queuing system • k, the number of servers in a shift; • whether servers are mandated to wear masks or not (we label this with the indicator γ S , such that γ S = 1 if servers must wear masks, and γ S = 0 otherwise); and • whether additional protection (for example, a small screen) is placed between servers and customers. Again, we label this with an indicator, γ E . We explore, for both queuing models (i) and (ii), and both infection models (a) and (b), the theoretical relationship between these variables and the number of shoppers who become infected as a result of visiting the shopping queue over a period of time, say T hours of business. We emphasise that all results in this section are subject to the assumptions discussed. Let π i be the probability that, at equilibrium, there are exactly i people in a queue of type (ii). For k servers and a capacity of C, the values of π i are stated in Section D of the Supplementary Materials. Alternatively, see, e.g. [3] . Meanwhile, establishing the equilibrium probabilities for a queuing system of type (i) is a subject of much recent interest and progress. For example, [6] have successfully derived the steady state probabilities of the system when we allow C → ∞. To the best of our knowledge, the general steady state probabilities of this queuing system are still unknown when we insist upon an arbitrary finite capacity in each queue. If we let n 1 ,...,n k be the steady state probability that there are n i people in queue i for i ∈ {1, . . . , k}, then Lemma D.1 in Section D of the Supplementary Materials gives mechanism for deriving the steady state probabilities for small C and k. For convenience in some of the later results, let j = C n k =0 . . . C n 2 =0 j,n 2 ,...,n k be the steady state probability that the first queue has exactly j customers present. Note that by symmetry of the arrivals and services, the choice of the first queue is without loss of generality, with C j=0 j = 1. With these equilibrium probabilities for the length of the queue, we are in a position to find the number of people who become infected in each queuing system and infection model pairing. We begin with infections accrued by shoppers from other shoppers, assuming we start with a large denominator population of potential users of the shop.  Then Proof : See Section D of the Supplementary Materials. We remark that the expected number of newly infected shoppers is linear in both ξ and T . This is unsurprising, but underlines the importance of appropriate social distancing to minimise unsafe interactions. Additional precautions to reduce unsafe interactions such as minimising talking indoors could also be taken. See, for example, [26] . Additionally, given that 0 < α 1 , α 2 < 1, we have that the contribution from the terms involving p M decreases quadratically with increasing p M . For p M = 0, this contribution is trivially 1, indicating no benefit; for p M = 1, the expected number of infected shoppers is discounted by α 1 α 2 . Note that some recent efforts, such as [13] , have suggested that α 1 α 2 could be as low as 1/36 for COVID-19. This emphasises the possible value of mask wearing, especially when p 0 is non-trivial. We note the importance of reducing the weighted sum of the stationary probabilities in each system in keeping the expected number of new cases as low as possible. This sum is larger if the system is closer to full capacity most of the time. In setting (ii), for a fixed service rate µ, arrival rate λ, and queue capacity C, it is expedient to make the number of servers k as large as possible. We conclude our commentary on this result by comparing the two different queuing systems under each of the two models for infection spread. Under all-to-all interactions, we note that it is preferable, according to Lemma 3.1, in almost all circumstances, to separate customers into k different queues, each staffed by a different server. This is despite the fact that a single queue with k servers results in a faster average service time for a given customer. This heuristic can be seen by comparing the two relevant quantities from Lemma 3.1: suppose that the reverse is true, and in fact a single queue leads to fewer customer infections, then we have (Given that we are directly comparing the systems, we assume the same capacity in each case, so that C = kC in this instance.) To further the ""worst-case"" infection spread for the multiple queue system, let us suppose the system is always as busy as possible, i.e. C = 1. Therefore, as then to satisfy 13 we need that if π j > π for some π ∀j ≥ 2, then this π must satisfy π < 3(C − 1) (kC + 1)(kC − 1) . We therefore see immediately that unless k ≥ 4 then for a given capacity of C = kC it is always preferable to split the customers into separate queues which, while slower, are assumed to not to be able to spread infection between one another. Even for k ≥ 4, the conditions under which a single queue is preferred are strict, with the queue almost always empty, in contrast to the system of k queues which are almost always full. Given that arrival and service rates can vary considerably over the course of trading, under an all-to-all infection assumption, single-server queues should be preferred to a single, multiple-server queue. We now look at nearest-neighbour only infections, using the same heuristic comparison between the two choices of queue management. Suppose that k separate queues will again lead to more customers becoming infected, so that we have As per the previous analysis, using 14, taking C = 1 and π j > π ∀j ≥ 2, we attain This inequality is much less severe than for the all-to-all setting. Therefore, in the nearest-neighbour setting under certain circumstances (for instance, when a single queue leads to much shorter waiting times for the average customer), a single queue may be preferable to multiple queues, particularly if both C and C are large. Therefore, our advice from this result is that, if socially distancing customers may be difficult to constantly monitor, or if the layout of the shop precludes an assumption of only neighbours in a queue infecting one another, then multiple queues should be preferred. If, however, the queuing system is in a safer location which guarantees social distancing between customers with low risk of infections between non-neighbours in a queue (for example, a well-marked outdoor queuing area) then the analysis is less clear-cut. We now examine the number of servers who become infected within a given period of time under each queuing system and infection model. Suppose k * is the number of servers who are infected at the time t = 0 in queuing system (i). Then, the expected number of newly infected servers in time T is the largest l ∈ N such that where for b < k. Proof : See Section D of the Supplementary Materials. We now turn to queuing system (ii), and examine the number of infected servers in a single queue with k servers. Lemma 3.3. Suppose k * is the number of servers who are infected at the time t = 0 in queuing system (ii). Then, the expected number of newly infected servers in time T is the largest l ∈ N such that where 3) indicate the importance of mandating masks for servers for small α 1 and α 2 , as well as installing additional safety between the servers and the shoppers. Letting γ S = 1, we see that both P (b, k) and Q(b, k) are discounted by at least α 1 compared to when γ S = 0, while fixing γ E . Letting γ E = 1, we see that both P (b, k) and Q(b, k) are discounted by β compared to when γ E = 0, while fixing γ S . Therefore, if both masks and extra protection are mandated for servers, we can think of the exposure that they get from the shoppers (and each other) as equivalent to that received over a much shorter length of time in a no-protection environment. We conclude our analyses of these lemmas on interactions involving servers with a comparison between the two queuing systems. To examine which system leads to fewer servers becoming infected, note that if we let where Q(b, k) and P (b, k) are as defined in Lemmas 3.2 and 3.3 respectively, then queue system (i) leads to a longer time to the infection of the next server for a given b, k if Q (b, k) < P (b, k), with (ii) the safer system for servers if the reverse holds. Suppose a single queue in which servers mix leads to a longer infection time, so that Q (b, k) > P (b, k). This assumption leads to the inequality C n k =0 . . . . We make further pessimistic assumptions in favour of the single queue system, analogous to our heuristic discussion of customer infections above. Suppose π i > π > 0, ∀i ∈ {0, . . . , k − 1} and note that C n k =0 . . . If we let the maximum be attained, then note that so that we infer that multiple queues should be preferred from the perspective of protecting servers in the following situations in particular • when the number of already infected servers is suspected to be non-trivial; • when the proportion of the population currently with COVID-19 is low; • when the proportion of customers wearing masks is high; • when extra protection (e.g. a screen) is possible for servers. We conclude this section with two important additional caveats to these theoretical results. Firstly, we note the importance of assuming a large population and relatively small time T . With these assumptions, we can assume that p 0 remains fixed for arriving customers throughout. In practice, even for relatively small T , it may be the case that some shoppers return multiple times to the same shop, with it being increasingly likely that they are infected with each return visit. We assume implicitly in the above that this cannot happen; however, we allow for the possibility of returning shoppers in the experiments given in the Supplementary Materials. Secondly, we remark that, in practice, ξ will not be identical for all pairs of people in the shop. In particular, the compliance of the shoppers to guidelines may be difficult to achieve uniformly. Given the centrality of controlling ξ to keeping the infection rate as low as possible, even a small minority of shoppers who do not comply with individual shop regulations could represent a significant and unnecessary risk for everyone present in the shop, as well as future users of the shop. In our conclusion we found that bidirectional and strict unidirectional shopping mechanics lead to the worst viral exposure per item. While we ran these visualisations we also recorded the exposure to individuals if the viral density has been modelled proportional to 1 r , 1 r 2 and 1 r 3 , that is exponents of −1,−2 or −3. We found that the results differed but the conclusions made were robust to these differences. We emphasise that we are comparing different shopping mechanics and are not taking much significance from absolute values of different mechanics. Similarly we are not comparing absolute values between difference exponents.  In the main text we define P (σ) as the probability that an individual exposed to a dose σ will be infected. In the absence of data on this probability for the case of SARS-CoV-2, we analyse the results of Memoli et al. [18] , who exposed healthy volunteers to different doses of influenza A/H1N1 via nasal inoculation. The dose concentration ranged from 10 3 to 10 7 TCID 50 (where TCID 50 is the 50% tissue culture infectious dose). The authors then made measurements of the degree to which subjects had become infected, such as the percentages showing symptoms and shedding virus. We can convert the dose into a viral copy number by taking into account that Memoli et al. used a 1-ml syringe to inoculate volunteers (delivering 500 µl into each nostril), and the results of Parker et al. [21] , who report that one TCID 50 /ml of influenza A/H1N1 corresponds to 2, 381 ± 1, 048 virions. Figure 8 : The percentage of the subjects who became infected with influenza A/H1N1 upon inoculation with a given viral dose. Blue circles and green diamonds represent the percentage of subjects who went on to show symptoms or to shed virus, respectively. Lines are best-fit sigmoidal functions. Data from Memoli et al. [18] , after converting from viral concentration to copy number according to Parker et al. [21] . In Fig. 8 we see the data reported in Table 1 of Ref. [18] using Parker et al. 's conversion, along with least-squares best-fit sigmoid functions of the form where x = log 10 σ. Assuming that the percentage of infected volunteers -however we choose to measure infection -can be equated with an individual probability of becoming infected, we see that P (σ) appears to follow a sigmoid functional form in the case of influenza A/H1N1. We detail here the proofs of the theoretical results from Section 3 of the main paper. We start by stating the equilibrium probabilities for the number of people in the queue in the single queue setting. Let pi i be the probability that there are exactly i people in the queue, with capacity C and k servers. Then For the steady-state probabilities of a multiple queue system, we give the following new result. Exploiting detailed balance, we examine the starting state m 0 , . . . , m C . Note that from any state, the only possible events to cause a state transition are an arrival (if the system is not at capacity) or a departure (if the system is non-empty). The number of non-empty queues is (k − m 0 ) and so the flow out of the state due to departures is (k − m 0 )λ. Meanwhile, the flow out of the state due to an arrival is 1 m C < k µ. We can reach the state m 0 , . . . , m C from states of the form m 0 , . . . , m i−1 − 1, m i + 1, . . . , m C , ∀i ∈ 1, . . . , C via a departure, whenever this is possible (i.e. whenever m i−1 ≥ 1 and m i < k). There are, by definition, (m i + 1) possible queues from which the relevant departure may occur to result in this state transition, and the rate of movement from each of these states is λ. There are also states which reach m 0 , . . . , m C from an arrival. Note that a new customer will always arrive into a queue of the shortest length in the system. Therefore, a customer will only enter a queue currently of length j if there are no queues of length 0, . . . , j − 1 and at least one queue of length j. This arrival will also increase the number of queues of length j + 1 by 1, meaning that to have a valid arrival into a queue of length j to reach state m 0 , m 1 , . . . , m C , we must have that m 0 = m 1 = . . . = m j−1 = 0 and m j+1 > 0. The rate of movement from any arrival is µ. Combining these arguments in the detailed balance equation for the state, we obtain where we note that states of the form ζ 0,...,0,m i +1,m i+1 −1,m i+2 ...,m C will only be non-zero if C j=i m i = k, i.e. if m 0 = . . . = m i−1 = 0, and if m i+1 ≥ 1. Indeed, we note that a transition does not alter the number of queues present in the system, and nor can the number of queues in the system with exactly j customers present exceed k or be less than 0. Hence, the accompanying boundary conditions are that ζ m 0 ,...,m C = 0 unless C j=0 m j = k and m j ∈ {0, . . . , k} for all j. Dividing the above detailed balance equation through by λ gives the stated result. We may use this result to state and prove Corollaries D.1.1 and D.1.2 on the behaviour of the system when the capacity of each queue is set to 1 and 2 respectively. We start with Corollary D.1.1, on the C = 1 case. Corollary D.1.1. In a multiple queue system with k servers, let C = 1, and let n 1 ,...,n k and ζ m 0 ,m 1 be as in Lemma D.1. Then Proof of Corollary D.1.1: Let C = 1. The detailed balance equation for the system reduces to Therefore, we have We can resolve this system of equations by confirming the hypothesis (by induction or otherwise) that  from which the stated result follows. When k = 2, the governing equations derived from Lemma D.1 are µ λ ζ 2,0,0 = ζ 1,1,0 µ λ + 1 ζ 1,1,0 = 2ζ 0,2,0 + ζ 1,0,1 + µ λ ζ 2,0,0 µ λ + 2 ζ 0,2,0 = ζ 0,1,1 + µ λ ζ 1,1,0 µ λ + 1 ζ 1,0,1 = ζ 0,1,1 µ λ + 2 ζ 0,1,1 = 2ζ 0,0,2 + µ λ ζ 1,0,1 + µ λ ζ 0,2,0 2ζ 0,0,2 = µ λ ζ 0,1,1 . By first directly eliminating ζ 1,1,0 , ζ 0,0,2 and ζ 0,1,1 (or otherwise) we can then resolve the steady-state probability of each state in terms of, for instance, ζ 2,0,0 , from which the result follows. be the number of people of type E who become infected by people of type D between times a and b. Let C denote a customer who is not wearing a mask and C M denote a customer who is wearing a mask. Then, if I(T ) is the number of customers infected by other customers after time T , we have The number of newly infected customers is clearly dependent on the number of unsafe interactions between customers. Let U D→E 0,T be the number of unsafe interactions between customers of type D and customers of type E from time 0 to time T in which an infection could be passed from D to E, and let U 0,T be the total number of unsafe interactions between all customers in this time; then, if Q(t) is the length of the queue at time t, we have that Next, let p D,E be the probability that a person of type D passes the infection to a person of type E given that a person of type D and a person of type E interact unsafely; then we have therefore, we have the conditional probability which gives that By similar arguments, we find that adding these four terms together gives the stated result for the (b)(ii) setting. We now extend this to the single queue, nearest neighbour only setting -i.e. setting (a)(ii). Let J D→E a,b be the number of people of type E who become infected by people of type D between times a and b, under nearest-neighbour only interactions. Then, if C and C M are as above and J(T ) is the number of customers infected by other customers after time T , we have, as an analogue of Equation 18 J however we note that the number of unsafe interactions in the nearest-neighbour only setting, which we now label as V 0,T rather than U 0,T , we have so that then the argument proceeds in the same way as before, giving that For the final two cases -in the multiple queue setting -we assume that unsafe interactions cannot occur between two customers in different queues. Therefore, the problem reduces to considering the number of shoppers who become infected in a given queue in the system (and then multiplying this by k using the symmetry of the queues in the problem). Note that, by definition, the probability that the number of people is j in a given queue in the multiple queue system is j . Therefore, by an identical argument to the (a)(ii) setting, the number of shoppers who become infected from other shoppers in the multiple queue, all-to-all interaction setting is ξkT q C j=2 j j(j − 1). Meanwhile, by an identical argument to the (b)(ii) setting, the number of newly-infected shoppers in the multiple queue, nearest-neighbour interaction setting is ξkT q C j=2 2 j (j − 1). * Finally, we turn to the proofs of the results concerning the number of additional servers who become infected within time T , and the associated customers who may be infected by servers during this time. We begin with queuing model (ii) and Lemma 3.3. Proof of Lemma 3.3: Given that we are assuming that the queue is at equilibrium, the number of possible unsafe interactions at a given moment between a customer and a server is that is, the expected number of servers who are occupied at a given point in time. Note that the number of possible unsafe interactions between two servers is trivially k 2 . Hence, the expected number of unsafe interactions between servers is k 2 ξT , while the expected number of unsafe interactions between servers and customers is k − k−1 i=0 (k − i)π i ξT , giving a total number of unsafe interactions involving at least one server within time T of ξT k It is important to distinguish between these two types of interaction in order to compute the expected number of infected servers at the end of the time period. We can therefore think of the sequence of unsafe interactions involving at least one server as the result of repeatedly tossing a biased coin, with ""Heads"" indicating an interaction between two servers and ""Tails"" indicating an interaction between a server and a customer. From the above argument, this coin will be tossed ξT k 2 + k − k−1 i=0 (k − i)π i in total, and at each toss P(Server-Server Interaction|Interaction involving Server) = We use this recasting of the problem to infer the number of additional servers who become infected. Suppose that b ≤ k servers are infected, then after one interaction therefore after one interaction we have The number of interactions until the next server is infected is geometric with ""success"" probability P (b, k), so the expected number of interactions until the next spread of the virus to a server is 1/P (b, k). Hence, the expected number of infected servers by time T is the largest l such that as required.  and . Proof of Corollary D.1.3: We again use a similar argument to the above. Suppose p j is the probability that an additional customer becomes infected from an interaction which involves at least one server, assuming that there are currently j infected servers. Therefore if I is the event that the infection spreads to another customer; A is the event that the interaction is between a server and a customer; and B is the event that the interaction is between two servers, then and so the expected number of customers infected during the period in which exactly j servers are infected is p j /P (j, k). From this, the stated result follows. The proof of Lemma 3.2 follows an almost identical argument to that presented above for Lemma 3.3. We detail the differences below for completeness. Proof of Lemma 3.2: Again we assume that the queue is at equilibrium, the number of possible unsafe interactions at a given moment between a customer and a server is which is again the expected number of servers who are occupied at a given point in time. There are, in this setting, no possible unsafe interactions between any two servers, so the expected number of unsafe interactions between servers and customers is We again try to calculate the probability that an additional server becomes infected following an unsafe interaction, given that b ≤ k servers are already infected. We see that So again the number of interactions until the next server is infected is geometric with ""success"" probability Q(b, k), so the expected number of interactions until the next spread of the virus to a server is 1/Q(b, k). Hence the expected number of infected servers by time T is the largest l such that . . .  and . Proof of Corollary D.1.4: Suppose q j is the probability that an additional customer becomes infected from an interaction which involves a server, assuming that there are currently j infected servers. Let I be the event that the infection spreads to another customer from a server. Then and so the expected number of customers infected during the period in which exactly j servers are infected is p j /Q(j, k). From this, the stated result follows. Corollaries D.1.4 and D.1.3 again highlight the importance of mandating masks for the shoppers and underline that the benefits of protecting servers also extends to the shoppers visiting the shop. We see that setting γ E = γ S = p M = 1 decreases each p j and q j by a factor of 1/βα 1 α 2 . Additionally, the importance of keeping k * , the number of infected servers at the initial time, as low as possible, is emphasised by these results. For example, if the servers are tested regularly before being present in the workplace (ensuring that k * ≈ 0), then the number of additional servers who become infected in time T , as well as the number of shoppers infected as a result of using the shop, is lessened. We Note that the above settings of the parameters should not be seen as prescriptive. We remark, for instance, that while some authors such as [13] suggest that 1/α 1 α 2 = 36, it is also suspected that α 2 /α 1 << 1. However, the analysis in the following may still be useful in qualitatively comparing the number of newly-infected people for different values of p M . Other settings above should also be seen as representing a pessimistic estimation of reality, particularly for those parameters which may vary considerably across space. For example, the β parameter value (in shops where extra safety is in place) will depend on the nature of the provisions in place in an individual shop, while p 0 can be affected by many geographic factors such as population density and locally-targeted interventions. Tables 2, 3, 4 and 5 give a pessimistic estimate of the number of people who become infected given the above parameter choices, for various different values of ξ and p M , and for β = 1 2 . Note that in these simulations, unlike in the theory in Section 3 in the main paper, we also include a 'latency period' immediately after people become infected. That is, when a person becomes infected with COVID-19, they are unable to spread the infection others for a short period of time. Following the work of [16] , this has been taken to be 6 hours, which they suggest as a reasonable lower bound on the timing. Here, anyone who is infected with COVID-19 continues to behave as a normal shopper, and is as infectious after 7 hours as after 7 days. We do not assume that anyone is immune. Each individual simulation imitates a queuing system and infection spread model pair for a week's activity in the shop. Prior to each simulation, each member of the denominator population (not including servers) was infected uniformly at random with COVID-19 with probability p 0 . The simulation then computes the number of people who are infected at the end of the week who were not infected at the beginning, including those who are infected but not yet infectious (due to the latency effect stated above). The figures given in Tables 2-5 are then the 95% empirical quantiles of 40 replications of the simulation in each instance. Table 2 focuses on those instances in which extra safety is not available and servers are not compelled to wear masks (and do not do so). Table 3 examines the cases for which extra safety is available and in use, but servers do not need to wear masks, and uniformly opt not to do so. Table 4 , by contrast, gives the results for the setting in which extra safety is not available, but servers now must wear masks. Finally, Table 5 records the estimates for the number of additionally infected people under the simulations in which both extra protection and compulsory masks for servers are in force. The results in the tables suggest that, for the specific total capacity of the queuing system we investigated, dividing the customers into separate queues resulted in a much lower number of newly infected people. This is intuitive, as we assume that customers in different queues cannot have unsafe interactions with one another. We also note that this agrees with the heuristic calculations given in Section 3 which imply that multiple queues with single servers are generally safer -and potentially much safer -for many values in our defined parameter space. However, we stress that our multiple queue simulations have at most three customers in one queue at any one time, with only four such queues in parallel, meaning that the number of possible unsafe interactions is lower than in, for instance, a large supermarket. The tables also indicate that for higher values of p M and lower values of ξ -corresponding to a greater preponderance of mask wearing and more diligently observed social distancing respectively -the fewer people become infected. Indeed, even without servers wearing masks and no extra safety in place (Table 2) , the number of people becoming infected in the single queue system is much more in line with the negligible numbers seen for the multiple queues setting. It also appears to be the case that having extra safety at the counter and insisting that servers wear masks has some modest effect on the number of customers who become infected, although this appears to be marginal in our examples due to the low number of new cases overall. With these simulations in place for the β = 1 2 case, we now turn to the β = 1 20 setting, representing a substantially more effective additional safety provision. We repeat the simulations carried out above (omitting now the simulations which include no extra safety, to avoid repetition), retaining the same settings for all parameters except β. We also omit the multiple queue settings due to the very low infection spread shown even for β = 1 2 . Table 6 is an analogue of 1.00, 0.00 0.05, 1.00 0.00, 0.05 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 2 1.00, 0.05 1.00, 1.00 1.00, 0.00 0.00, 0.00 0.00, 0.05 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 4 2.00, 1.00 1.00, 0.05 0.10, 1.00 0.00, 0.05 0.00, 0.05 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 6 1.05, 1.00 1.00, 2.00 1.00, 0.05 0.00, 0.05 0.00, 0.05 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 12 1.00, 1.05 1.05, 1.00 1.00, 1.00 1.10, 1.00 0.10, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 20 3.05, 3.05 2.00, 2.05 1.05, 1.05 2.00, 2.00 0.05, 1.00 0.00, 1.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.05 0.00, 0.00 0.00, 0.00 30 3.05, 2.05 3.00, 2.00 2.05, 2.05 1.05, 1.00 0.05, 1.00 1.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 40 4.00, 3.05 2.05, 3.00 2.00, 4.00 1.00, 1.05 1.05, 1.00 1.00, 1.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 50 4.05, 4.05 2.00, 2.05 2.05, 2.00 2.00, 1.05 2.00, 1.00 1.00, 1.05 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 60 5.05, 5.00 3.00, 2.10 2.00, 2.05 2.00, 2.05 1.00, 1.05 1.00, 1.00 0.00, 0.00 0.05, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 3.05, 4.00 2.00, 2.10 3.00, 2.00 1.05, 2.00 1.00, 1.00 0.05, 0.05 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 60 5.20, 4.00 3.00, 2.05 2.05, 2.05 1.05, 1.10 1.10, 1.15 0.05, 1.00 0.00, 0.00 0.00, 0.05 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 Single Queue -All-to-all Infections, Single Queue -Nearest Neighbour ξ Average Number of Newly Infected 1 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 0.00, 0.00 2 0.00, 0.05 0.00, 1.00 1.00, 0.05 0.00, 0.00 0.00, 0.00 0.00, 0.00 4 0.00, 0.00 0.00, 1.00 0.00, 0.05 0.00, 0.00 1.00, 0.00 0.00, 0.00 6 1.05, 0.00 1.00, 1.00 0.00, 1.00 0.00, 0.05 0.00, 0.05 0.00, 0.00 12 1.05, 1.00 1.00, 1.00 1.00, 0.05 0.00, 0.00 0.00, 0.00 0.00, 0.00 20 1.00, 2.00 1.00, is additional safety in all four queue systems/infection model pairs. Table 6 differs from Table 3 in that the protection afforded from the additional safety now gives a discount factor of β = 1 20 on transmission probability, rather than 1 2 as for Table 3 . Meanwhile, Table 7 corresponds to Table 5 in the same way, as both tables focus on the setting in which servers are mandated to wear masks and extra safety is available. However, again, Table 7 uses β = 1 20 . We see by comparing Tables 6 and 7 directly to Tables 3 and 5 that by increasing the protection factor of the extra safety provided, the number of newly-infected individuals is lessened. This appears to hold across many values of ξ and p M . @story_separate@For larger aisle widths (W ≤ 3 Figure 3 ) the different shopping mechanics made little effect on the total viral dose σ α for a given density and geometry. However the mechanics made a substantial difference to the progress of shoppers items α . A strict unidirectional system was the least efficient for shoppers while the fully sorted unidirectional system was most efficient, although perhaps a little unrealistic. This suggests that a supermarket with larger aisle widths should choose rules and regulations that increase efficiency. For narrow aisle widths (W = 2 Figure 3 ), bidirectional shopping lead to higher viral doses and less efficient shopping, this was due to shoppers taking more time to get past each other. Strict unidirectional shopping was also poor because it forces more shoppers to pass each other. In every simulation when shoppers travelled in one direction and were happy to turn back if necessary the viral dose per item was lowest. However strict unidirectional shopping where shoppers would not turn back if they had missed an item often lead to the highest values of viral dose per item. When a shopper didn't turn back they had to do a full loop to retrieve an incorrectly ordered item, this meant they passed or even got stuck behind many shoppers. Aisle width made a huge impact on viral exposure. Shoppers in a 4m or 3m wide aisle were exposed to a factor of ten less viral particles to shoppers in a 2m aisle. For Aisle widths of 2m it is more important that a unidirectional system is implemented. In bidirectional models of narrow aisles shoppers struggled to move past each other, both increasing viral dose and severely decreasing the efficiency of the shopping experience. It would be prudent for supermarkets to evaluate their shoppers experience. If a supermarket can widen their aisles by 50 percent they could potentially decrease viral doses experience by a factor of ten. If this is not possible the driving factor of exposure in a supermarket appears to be time spent in the supermarket. Work should be done to make shopping as efficient as possible. This could be done by implementing a one way system as seen in many current supermarkets, however it should somehow be communicated or emphasised that breaking this one way system in the name of efficiency is good. Also supermarkets would benefit from arranging their shops in a way such that shoppers spend the least amount of time inside. These conclusions agree with [27] that the driving factor of viral dose was time spent in the supermarket. In this paper we have examined the spread of COVID-19 in various shopping settings. As discussed in Section 2, considering the viral dose accumulated by the shoppers in the worst-case scenario is a question of critical importance. Looking only at the viral dose of susceptible individuals when shopping (rather than queuing), we find that efficiency of shopping is the largest driver of viral exposure. This is most important for shops with narrow aisles. At higher densities of people or more compact shops, the movement of shoppers can become inhibited. Supermarkets with better flowing layouts and a higher efficiency of shopping will be safer for individuals. One place where flowing movement is not necessary possible in within a queue. However, queuing is often necessary in certain situations. For such cases we find in Section 3 that a system with a cautious denominator population and well-protected members of staff can lead to very limited spread of COVID-19, while ensuring the shop remains economically viable. This appears to hold even under extremely pessimistic assumptions on the behaviour of the virus. These results are necessarily based on incomplete information and many assumptions in the model about the physics and biology of the spread of COVID-19, and of the way that people go about their shopping. The principles which emerge should therefore be regarded as best estimates, to be updated when more data becomes available. Assessing Risk in the Retail Environment during the COVID-19 Pandemic SUPPLEMENTARY MATERIALS 25 A Results of supermarket modelling for different populations. In this section we record graphs of viral exposure per shopping item for populations N = 7, 15, 25, and aisle widths W = 2m,3m,4m.","The COVID-19 pandemic has caused unprecedented disruption, particularly in retail. Where essential demand cannot be fulfilled online, or where more stringent measures have been relaxed, customers must visit shop premises in person. This naturally gives rise to some risk of susceptible individuals (customers or staff) becoming infected. It is essential to minimise this risk as far as possible while retaining economic viability of the shop. We therefore explore and compare the spread of COVID-19 in different shopping situations involving person-to-person interactions: (i) free-flowing, unstructured shopping; (ii) structured shopping (e.g. a queue). We examine which of (i) or (ii) may be preferable for minimising the spread of COVID-19 in a given shop, subject to constraints such as the geometry of the shop; compliance of the population to local guidelines; and additional safety measures which may be available to the organisers of the shop. We derive a series of conclusions, such as unidirectional free movement being preferable to bidirectional shopping, and that the number of servers should be maximised as long as they can be well protected from infection."
"This study investigated compassion satisfaction (CS) and compassion fatigue (CF) in pre-licensure students among nursing and psychiatric nursing programs at a Canadian university. Pre-licensure students are learners enrolled in a baccalaureate education program where upon completion, are eligible for licensing and registration under a health regulatory body. For example, on completion of a nursing education program, a graduate becomes eligible for professional licensure as a Registered Nurse (RN). CS refers to the level of reward a helper gains when carrying out 'care' or 'help' to others. In contrast, CF, comprised of secondary traumatic stress (STS) and burnout (BO), entails the negative aspects of work-related activities. Development of CF is highly concerning given that it is associated with ""feelings of hopelessness and difficulties in dealing with work or in doing your job effectively"" (Stamm, 2010, p. 17) . This may impede provision of safe, competent, and ethical care that is in alignment with Code of Ethics for Registered Nurses (Canadian Nurses Association [CNA], 2017; Joinson, 1992) . Studies of nurses exposed to traumatic events revealed that a higher level of CS served as a protective factor against STS and BO (Hinderer et al., 2014) . According to Michalec et al. (2013) , limited attention has been paid to the vulnerabilities of nursing students to CF, the extent to which students may be suffering from BO, STS, and the associated mechanisms. Rudman and Gustavsson (2012) found that BO in undergraduate nursing students led to higher levels of intention-to-leave the nursing profession within one year upon workforce entry. A greater understanding is needed of the factors associated with CF in effort to prevent its formation and promote development of CS among nursing students prior to their entry into the workforce. Understanding what factors contribute to CF may assist nurse educators and researchers in formulating interventions and curricular planning strategies to support students and decrease the negative effects of caring for others.@story_separate@Provision of compassionate care is a core value of professional nursing practice as highlighted in the Code of Ethics for Registered Nurses (Canadian Nurses Association [CNA], 2017) . ""Nurses engage in compassionate care through their speech and body language and through their efforts to understand and care about others' health-care needs"" (CNA, 2017, p. 8) . In part, baccalaureate pre-licensure health curricula, serve to socialize students into their roles as care providers as students learn to foster and cultivate compassion. According to the American Association of Colleges of Nursing (AACN) and the Canadian Association of Schools of Nursing (CASN), a core essential of professional nursing education is the provision of competent, safe, ethical, and compassionate care delivery provided by the student nurse that is learned in baccalaureate education (AACN, 2008; CASN, 2015) . CS is defined as the positive aspects and pleasure a care provider gains despite any feelings of exhaustion and hardship (Stamm, 2002; Stamm, 2010) . CS results from a transactional dynamic understood as the positive effects or 'payments' one gains as a result of caregiving, despite the 'cost' of helping others (Stamm, 2002) . This is akin to Lazarus and Folkman's (1984) theory of stress, appraisal, and coping where stress results from perceived imbalances between demands of a situation and the availability of resources to cope. According to this theory, the perception of one's ability to cope has more importance than a particular stressor. The transactional nature of CS is evident in studies of nursing students who reported that CS is greater than CF (Mason & Nel, 2012; Mathias & Wentzel, 2017) . Feeling in control of a situation or stressor promotes coping and the perception that one has the resources to manage emotional distress. Stamm (2002) noted that only a fraction of individuals exposed to traumatic stressors developed symptoms associated with PTSD where gains in delivering compassionate care outweighs losses. Among nurses, there is often a sense of accomplishment in providing care to others that results in gaining rewards known as CS (Hinderer et al., 2014) . Thus, CS acts as a protective factor against CF, and specifically STS (Hegney et al., 2014; Hinderer et al., 2014) . According to Figley (1995) , CF is the emotional pain caused in some care providers when exposed to a suffering individual. In a concept analysis of CF in nurses, Coetzee and Klopper (2010) reported that CF occurs when compassionate energy is not adequately restored. Carla Joinson (1992) was one of the first nurses to discuss CF in the published literature and referred to CF as being ""emotionally devastating"" requiring awareness to recognize when it is occurring. Joinson also acknowledged that the ""outside sources that cause it are unavoidable"" and that ""'caregivers' personalities lead them towards it"" (1992, p. 116) . Joinson (1992) alluded that nurses may place high expectations upon themselves to provide care at an idealistic level and, when combined with other tasks such as paper work, care planning, delegation, and crisis management, these demands can leave the care provider depleted. For the purposes of the study, CF occurs when a care provider experiences greater STS and BO, rather than satisfaction, from care provision (Wijdenes et al., 2019) . CF reflects the negative side of caring that diminishes the ability of a care provider to help others and is comprised of STS and BO. STS results from the exposure of the care provider to the suffering of others who have or are experiencing stressful events (Boyle, 2011; Stamm, 2010) . STS manifests in the care provider as feelings of fear, sleep difficulties, intrusive images, or avoiding reminders of traumatic experiences regarding the person for whom care was provided (Stamm, 2010) . Figley (1995) acknowledged, ""There is a cost to caring. Professionals who listen to clients' stories of fear, pain, and suffering may feel similar fear, pain, and suffering because they care"" (p. 1). While posttraumatic stress disorder (PTSD) arises due to primary trauma; STS arises due to empathetic hardship (Stamm, 2010) . In consideration of pre-licensure health care students, approximately 40% of nursing students (Mathias & Wentzel, 2017) and midwifery students (Beaumont et al., 2016) are at risk of moderate levels of STS. BO first arose in the literature in 1974 in a publication by Herbert Freudenberger which popularized the term (Freudenberger, 1974) . Freudenberger described BO as a psychological, behavioural, and physical state that ranged from feelings of exhaustion and fatigue, frustration and anger, to physical manifestations (i.e., gastrointestinal illness); Freudenberger also noted that those who are committed to their work are at greatest risk of developing BO (Freudenberger, 1974) . A concept analysis of CF in nursing revealed similar findings that included decreased energy, exhaustion, loss of power, physical complaints, irritability, intent-to-quit, and provision of poor-quality care (Coetzee & Klopper, 2010; Peters, 2018) . Similarly, Maslach and colleagues defined burnout as ""a state of exhaustion in which one is cynical about the value of one's occupation and doubtful of one's ability to perform"" (Maslach et al., 1996, p. 20) . Where the Maslach Burnout Inventory (MBI) focused on exhaustion, cynicism, and professional efficacy, the Oldenburg Burnout Inventory (OLBI) focused on exhaustion and disengagement from work. Due to theoretical and psychometric concerns regarding a lack of theoretical depth, use of the MBI has reduced over time (Halbesleben & Demerouti, 2005) , giving rise to alternate tools to assess for BO such the OLBI and ProQOL scales. Within the current study, BO is defined as a component of CF where the care provider experiences decreased self-efficacy related to workload demands and increased perceived stress (Figley, 1995; Hegney et al., 2014; Rudman & Gustavsson, 2012; Stamm, 2010) . Figley (1995) noted that BO has a gradual onset, which occurs as a result of STS, coupled with emotional exhaustion. Stamm (2010) characterized BO in care providers as feelings of being overwhelmed, unhappy, disconnected, and disengaged which occurs with a gradual onset. Moreover, BO is comprised of ""exhaustion, frustration, anger and depression"" related to a lack of a supportive work environment and increased workload demands (Stamm, 2010, p. 12) . Figley (1995) attested that practitioners may endure feelings of deep sorrow and must understand their own limitations in alleviating pain suffered by clients who require help. Unfortunately, the factors that contribute to BO in nursing students are not well understood. It is imperative that research be conducted about effective ways to support the future nursing workforce. Babenko-Mould and Laschinger (2014) acknowledged that nursing student placements in the clinical practice environment are positively or negatively influenced by the well-being of the workforce, which may lead to stress and BO formation that influence students' career choice. Few studies have explored ProQOL variables within undergraduate nursing and psychiatric nursing programs. The aim of this study was to investigate the association of the ProQOL outcome variables comprised of CS, STS, and BO with intent-to-leave, measures of self-efficacy, perceived stress, and prior traumatizing events (PTEs). Exploring these phenomena may help nurse educators better understand the derivatives and associative factors of CS and CF, with the aim of supporting students while engaged in their undergraduate studies. The study used an exploratory design that employed a cross-sectional, anonymous online survey. The survey was comprised of demographic questions and four validated measures in a population of students enrolled in the Bachelor of Science in Psychiatric nursing (BScPN) and Bachelor of Nursing (BN) programs within years two, three, and four at western Canadian university. Two key research questions guided the study. They were: 1. What are the inter-relationships between CS and the CF subscales of BO and STS? 2. What predictor variables (or factors) are associated CS, BO and STS in a population of pre-licensure students? Several associations were anticipated between the predictor and outcome variables. As such, four hypotheses were generated related to the factors of interest. They were: 1. There will be a positive association between level of self-efficacy, a dimension of personality and emotional stability and CS. 2. There will be a positive association between perceived stress and STS. 3. There will be a positive association with PTEs and STS. 4. There will be a positive association between intentionto-leave and BO. This study was approved by the Brandon University Research Ethics Committee and the University of Saskatchewan Behavioural Research Ethics Board. All participant information was anonymous and voluntary. Only aggregate data is reported to protect the identity of student respondents that adheres to ethical considerations for research involving human participants. A statement regarding informed consent was included in the title screen of the online survey. Following receipt of ethical approval for the study, all full-time students enrolled in the BScPN and BN programs within years two, three, and four at a western Canadian university were invited to participate. Only full-time students were included in the study. Students on-leave from their program and those in their first general studies year were excluded from participating given that no clinical practica are embedded in the first year of each program. Sampling occurred using a convenience sample of enrolled students within the BScPN and BN programs at one university institution. At the time of survey, there were 341 students eligible to participate. In the BN program there were 46 students in year two, 53 students in year three, and 45 students in year four. In the BScPN program, there were 81 students in year two, 53 in year three, and 63 in year four. An invitation to participate in the SurveyMonkey V R online survey was issued through the University's learning management system (i.e., Moodle). G*Power (Faul et al., 2007) software was used to determine the estimated sample size for linear multiple regression with five to seven predictor variables. The estimated sample size calculation was generated with alpha level at 0.05, effect size (f 2 ) set at medium or 0.15 (J. Cohen, 1988 ) and power at 0.8 to reduce the likelihood of a type II error (Polit & Beck, 2017 ). An estimated 92 to 103 participants were needed to obtain adequate statistical power to avoid a type II error. The Professional Quality of Life Scale (ProQOL -Version 5). The ProQOL tool is comprised of 30 items that are scored on a Likert scale ranging from 1 (never) to 5 (very often), with higher scores indicating higher levels on each subscale. The ProQOL Scale is comprised of three subscales with 10 items each that pertain to CS and CF comprised of BO and STS over the past four weeks. Reliability and validity of the tool have been demonstrated wherein the Cronbach's alpha for CS was 0.88, burnout was 0.75, STS 0.81, and an overall alpha of 0.88 was obtained (Stamm, 2010) . The ProQOL Scale offers a research measure with adequate convergent and discriminant validity, as well as construct validity when assessed using bifactor modelling of the three subscales (Geoffrion et al., 2019) . In addition, the ProQOL Scale has been used in more than 200 peer reviewed papers (Stamm, 2010) involving studies of registered nurses, registered psychiatric nurses, and students of social work, midwifery, medicine, veterinary medicine, and nursing. In the current study, clarity was provided to participants that the terms 'work' and 'job' related to their role as a student in their program when providing care to patients, clients, and their families. The Core Self-Evaluations Scale (CSES). The CSES is a 12item, self-report, Likert scale tool that pertains to selfefficacy (Judge et al., 2003) . Each item is scored on a scale of 1 (strongly disagree) to 5 (strongly agree), with a total higher score being indicative of a person who is ""well adjusted, positive, self-confident, efficacious, and believes in his or her own agency"" (Judge et al., 2003, p. 304) . Coefficient alpha reliability estimates reported by the tool creators were 0.84 (Judge et al., 2003, p. 316) with test-retest reliability at 0.81 (Judge et al., 2003) . These findings indicate the tool is valid and reliable. The Perceived Stress Scale (PSS). The PSS is a 14-item, selfreport, Likert scale tool (S. Cohen et al., 1983) . Each item is scored on a scale of 0 (never) to 4 (very often) where a higher score indicates higher levels of perceived stress. The PSS provides a single summed score that assesses ""the degree to which respondents found their lives unpredictable, uncontrollable, and overloading"" within the last four weeks, influenced by the experience of ""daily hassles, major events, and changes in coping resources"" (S. Cohen et al., 1983, p. 387) . Coefficient alpha reliability revealed scores of 0.84 to 0.86 were reported indicating the instrument is also a valid and reliable tool for use in undergraduate student populations (S. Cohen et al., 1983) . The Life Events Checklist (LEC -Version 5). A modified version of the Life Events Checklist (LEC-version 5) was used to collect data regarding PTEs the participant has experienced (Weathers et al., 2018) . The LEC is comprised of a 17 item, six-point, self-report tool that serves as ""a screening measure of severity of trauma exposure"" (F. Weathers, personal communication, October 30, 2018) . The tool asks the participant to report if the stressful event indicated has: happened to the respondent, if they were exposed to the event as part of their job, if they witnessed the event occur to someone else, they learned about it happening to a family member or friend, if they are not sure it fits, or if it does not apply to them. Items that the respondent reports 'happened to them', 'witnessed it', or 'exposed to it as part of the job' were dummy-coded to receive a score of 1. All other responses received a score of 0. The tool is scored by determining the sum of individual questions (Jacobowitz et al., 2015) . The LEC has demonstrated convergent validity with ""adequate psychometric properties"" when dichotomized to assess potentially traumatic event exposure with adequate test re-test reliability (Gray et al., 2004, p. 336 ). An item was added to the above LEC tool adapted from the Life Stressor Checklist-Revised (LSC-R) developed by Wolfe et al. (1997) . The added item read as: 'Bothered, bullied, or harassed (i.e., jokes, verbal remarks) by someone in the work or school setting (i.e., another student, member of the health care team, manager, patient, or a teacher).' The original LSC-R item 24 read as ""Have you ever been bothered or harassed by sexual remarks, jokes, or demands for sexual favors by someone at work or school (for example, a coworker, a boss, a customer, another student, a teacher)?"" (Wolfe et al., 1997, p. 7) . The LSC-R has demonstrated an average kappa of 0.70 indicating adequate validity (Norris & Hamblen, 2004) . This added item allowed the participant to report if they have experienced any negative behaviours that constitute bullying and harassment psychological stressors which have been noted in the nursing literature related to students (Budden et al., 2017) . Participants were also asked to report their intentionto-leave their program of study on a Likert scale item ranging from 1 (strongly disagree) to 5 (strongly agree). The item read as: 'I think a lot about leaving the nursing/ psychiatric program.' The item was constructed based on studies published by Laschinger et al. (2012) and Rudman et al. (2014) . The item provided an estimate of undergraduate students' intentions of leaving their program. Laschinger et al. (2012) found that intentionto-leave was significantly correlated (r ¼ .49, p < .05) with BO. Psychometric analysis of a similar question in a study by Rudman et al. (2014) revealed a Cronbach's alpha of 0.75 indicating adequate validity. Rudman et al. (2014) argued that intention-to-leave may serve as a significant predictor of actually leaving the nursing profession. Demographic Data. The demographic data collected in the study yielded descriptive data about the sample important for exploring statistical associations (Polit & Beck, 2017) with the previously identified variables. Demographic variables included age, relationship status, current clinical setting, average amount of sleep per night in a week, average amount of any hours of paid employment per week. Paid employment could include work related or unrelated to degree studies. If the student was not employed, the value zero was entered. Participants also indicated if they had a previous diagnosis of PTSD, anxiety, depression (yes or no) to control for confounding variables. At the outset of the study, a data analysis plan was developed. This plan included using Statistical Package for the Social Science V R (SPSS) for analysis of the variables. Scores were entered in keeping the guidelines for each of the tools to be adopted within the study, including how to treat missing values and reverse score items. Descriptive statistics and tests for normal distribution of the sample were conducted to allow for description of the sample and analysis of homogeneity, ensuring assumptions for parametric analysis were met for statistical testing. Data were entered into SPSS version 26 and examined prior to analysis. Cases with missing data were explored and dependent variables were examined for homoscedasticity and normal distribution. Ninety-nine respondents accessed the survey, however, six were incomplete. The final sample size was adequate with 93/341 respondents resulting in a response rate of 27.27%. The average age of respondents was 24.6 years (SD ¼ 5.27) and female (n ¼ 89, 95.7%). Half of the respondents reported being single. There were 32 respondents (34.4%) from the BPN program and 61 (65.6%) from the BN program. Students in years two and three of their program were engaged in non-block clinical style with an alternating schedule of classes, labs, and clinical. In year four, all but four students were engaged in the final senior practicum structured in block clinical format with no other classes or coursework. Analysis of relationships among the dependent variables revealed that CS was inversely correlated with the CF subscales of BO (r ¼ À.475, p < .001) and STS (r ¼ À.164, p ¼ .117). The inverse relationship between CS and STS did not reach statistical significance, however, the CF subscale measures of BO and STS were significantly positively correlated (r ¼ .555, p < .001). Analysis of relationships among the independent and dependent variables revealed a significant positive correlation between CS and the CSES, a measure of selfefficacy, r (91) ¼ .370, p < .001. Secondly, there was a significant positive correlation between STS and the PSS r (91) ¼ .455, p < .001. Thirdly, a significant positive correlation occurred between STS and PTEs measured by the LEC r (91) ¼ .259, p < .05. Lastly, there was a significant positive correlation between BO and the intent-to-leave variable r (91) ¼ .522, p < 0.001. To view a complete correlation matrix of the variables, refer to Table 1 . Three categories of clinical placement sites were analysed using ANOVA testing. All in-patient units were clustered under one label named 'Inpatient' that included medical-surgical, paediatrics, acute psychiatry areas (n ¼ 62). The second cluster was labelled 'Episodic' (n ¼ 17) to reflect students in out-patient settings, and rural/emergency settings. The third cluster was comprised of students in long-term care (LTC)-palliative settings (n ¼ 10). Students who were not in clinical (n ¼ 4) were excluded from analysis. See Figure 1 for details. Levene's statistic was not significant (p > .05) indicating limited variance among the re-clustered groups, therefore homogeneity of the sample was met. Subsequently, a one-way ANOVA test with alpha set at 0.05 was completed for each of the dependent variables with Hochberg's post-hoc analysis due to the unequal group sizes (Field, 2013; Hochberg, 1988 Comparisons among the independent variables with outcome variables was conducted prior to multiple hierarchal regression analysis. Analysis provided insight into student characteristics within the sample. Students with high levels of perceived stress were younger (r ¼ À.233, p < .05), slept less (r ¼ À.440, p .001), and had greater intention-to-leave their education program (r ¼ .482, p .001). In contrast, students who reported high levels of self-efficacy had less perceived stress (r ¼ À.810, p .001), were less likely to have a prior diagnosis of depression (r ¼ À.359, p .001), were older (r ¼ .304, p .01), slept more (r ¼ .398, p .001), and had less intent-to-leave (r ¼ À.537, p .001). Data were analysed using hierarchal multiple regression analysis (blockwise entry method in SPSS) of CS and the CF subscales (comprised of BO and STS). According to Field (2013) , an effect size of 0.10 is considered small, 0.30 is medium, and 0.50 showcases a large effect. In accordance with the G*Power calculation five-to-six key variables were selected for entry into each model to limit a type II error. Variables were entered into the model informed by theory and hypothesis testing regarding the ProQOL Scale (Stamm, 2010) . In addition, variables known from prior research published in peerreview literature were entered first, with new variables entered sequentially in accordance with hierarchal multiple regression (Field, 2013) . Burnout Regression Model. Homoscedasticity assumptions of the BO raw data for hierarchal multiple regression of the dependent variables were met. Theoretical and known factors related to BO from the published literature were entered into Model 1, 2, and 3 for hierarchal regression analysis (Table 2) . Factors included age (Hegney et al., 2014) , sleep hours (Stamm, 2010) , depression (Harr et al., 2014) , intent-to-leave the education program, perceived stress (C. D. Lin & Lin, 2016; Rudman et al., 2014) , and core self-evaluation (Y. K. Lin et al., 2017) . Model 1 and 2 findings indicated that the younger the student, the higher the BO level. The same principle occurred for sleep, in that lower amounts of sleep were predictive of burnout. A student who reported being diagnosed with depression also carried predictive capacity for burnout. Model 3 allowed the researcher to test if perceived stress, core selfevaluation, and intent-to-leave made a difference in predicting BO, over-and-above the previously entered variables in Models 1 and 2. The findings revealed perceived stress and core self-evaluation were significant contributors adding explanatory power of the model. The final model predicting BO included age, sleep hours, depression, perceived stress, core self-evaluation, and intent-to-leave the education program. As predictors were entered into the three models, the R 2 Change value increased sequentially. The model explained 63.5% of the variance in scores (F 6,86 ¼ 24.911, p .001) fully powered with a very large effect size of 1.74. Within the third model, core self-evaluation (b ¼ À0.368, p .01) and perceived stress (b ¼ 0.418, p .001) were significantly predictive of burnout. Greater levels of student self-efficacy measured by the CSES, was inversely related to burnout; whereas, perceived stress was significantly, related to greater levels of burnout as part of CF. Following log 10 transformation of the STS dependent variable, the assumptions for regression analysis testing were met p ¼ .200; p ¼ .423 ). Subsequently, known factors of STS discussed in the nursing and allied health literature as well as theoretical variables of interest were entered into three models (Table 3 ). The predictors included age (Knight, 2010) , average amount of sleep per night in a week (Stamm, 2010) , PTEs measured by the Life Events Checklist (Jacobowitz et al., 2015; Stamm, 2010) , followed by perceived stress (Harr et al., 2014; Paspaliaris & Hicks, 2010) , intent-to-leave (Coetzee & Klopper, 2010; C. D. Lin & Lin, 2016; Peters, 2018; Rudman & Gustavsson, 2012) , and core self-evaluation (Y. K. Lin et al., 2017; Mason, 2018) . As values were entered into the STS model, the R 2 Change value increased from Model 1 and 2 and did not further increase from Model 2 to Model 3. This indicated that inclusion of the new predictors did not greatly contribute to explaining the overall variance. The final model explained 32% of the variance for STS (F 6,86 ¼ 6.756, p .001), powered at 99% with a moderate-to-large effect size of 0.47. The significant predictor of STS within Model 3 was PTEs (b ¼ 0.232, p < 0.05). All other variables were not unique, significant predictors of the STS subscale. CS Regression Model. Following log 10 transformation of the CS dependent variable, assumptions for normal distribution were met p ¼ .191; p ¼ .199 ). Subsequently, factors of interest informed by the published literature for CS were entered into Model 1, 2, and 3 for hierarchal multiple regression analysis (Table 4 ). Predictors included age (Hegney et al., 2014) , average amount of hours of sleep per night in one week (Stamm, 2010) , core selfevaluation (Mason, 2018) , perceived stress, and intentto-leave the education program (Coetzee & Klopper, 2010; Harr et al., 2014; Paspaliaris & Hicks, 2010; Peters, 2018; Rudman & Gustavsson, 2012; Rudman et al., 2014) . As the five predictive variables were entered into each model, the R 2 Change value increased from Model 1 and 2, and subsequently decreased in Model 3 as perceived stress and intent-to-leave were added. The final model for CS explained 27% of the variance for CS (F 5,87 ¼ 6.546, p .001), powered at 99.8% with a moderate effect size of 0.376. Age and levels of perceived stress were not significant predictors of CS. Hours of sleep were inversely associated with CS (b ¼ À0.305, p .01). Students with greater self-efficacy measured by the CSES was a significant predictor of CS (b ¼ À0.339, p <.05); in addition, those who reported less intent-to-leave was predictive of CS (b ¼ À0.261, p <.05). Inverse relationships of CS with CF measures of STS and BO were consistent with the published literature in undergraduate pre-licensure students (Flinton et al., 2018; Mason & Nel, 2012) . Although the inverse relationship of CS and STS was not statistically significant in the current study; these findings were akin to those by McArthur et al. (2017) in a sample of Australian veterinary medicine students. Furthermore, the direction of the relationships among the dependent variables is consistent with the transactional nature of stress and coping (Lazarus & Folkman, 1984 ) and Stamm's theoretical model of CS and CF (Stamm, 2010) . Cronbach's alpha scores achieved in the study were satisfactory with CS and STS achieving scores greater than 0.70. The BO alpha was 0.67, however, this is higher than the BO alpha of 0.48 achieved by Michalec et al. (2013) in their study of American nursing students. Students placed in LTC-palliative care settings had statistically significant higher levels of burnout as opposed to episodic care areas and acute in-patient units. According to Potter et al. (2010) nurses who work in home health settings and oncology clinics were at greater risk of CF. Berger et al. (2015) reported that feelings of exhaustion and hardship with subsequent CF were present in nurses involved in end-of-life situations. Moreover, long-term care settings breed the 'perfect storm' of clients who have numerous comorbidities, complex and demanding care needs, and often poor patient-to-nurse staff ratios combined with the rising tide of horizontal/lateral and vertical workplace violence (Littlejohn, 2012) . Students in this type of clinical environment must also contend with the additional challenge of meeting course and clinical objectives, which may be an additional source of strain and exhaustion giving rise to BO. Burnout. BO is one component of CF within the ProQOL Scale that was analysed using regression analysis. The finding that high levels of perceived stress and low selfefficacy were predictive of BO were consistent with those found by Mason (2018) in that CF is closely related to nursing students with pessimistic attitudes. This negative or pessimistic perspective is reflective of those who scored lower on the core self-evaluation measure. These findings support that negative affectivity or neuroticism are significant predictors of BO (Flinton et al., 2018; Skodova´et al., 2013) . Among university students with Type D Personality (prone to negative emotion or a pessimistic affect), Williams and Wingate (2012) found that social support and emotion-focused coping can decrease perceived stress. In a study of nurses and nursing students, Garrosa et al. (2008) found that significant predictors of BO included job stressors such as workload, experience with pain and death, conflict interactions, younger age, and role ambiguity. Within the current study, being married/partnered or single, having a prior diagnosis of anxiety, and work hours were not significantly associated with CS or CF. Dasan et al. (2015) found that being single was associated with BO where being married offers some protective effect. McArthur et al. (2017) found, in part, that female veterinary students who had paid employment in a veterinary clinic unrelated to a school placement had higher amounts of CS. In contrast, Harr et al. (2014) in their study of master and bachelor of social work students found that the number of hours employed and BO were statistically significant. These conflicting findings imply that further research is needed. Secondary Traumatic Stress. STS is the second component of CF measured within the ProQOL Scale. As indicated by the STS hierarchal regression model, having a higher number of PTEs was predictive of STS. This finding is further supported by Stamm's (2010) theoretical model of CS and CF that incorporated primary traumatic exposures as playing a role in development of STS. In a study of nursing students, researchers found that the more adverse childhood experiences (ACEs) female students had experienced, the higher the levels BO and depression when compared to men (McKee-Lopez et al., 2019) . These findings imply that students entering the nursing and psychiatric nursing profession with numerous PTEs may be at significant risk to STS. These students may require additional counselling supports during their education to develop positive coping strategies prior to entering the workforce as registered practitioners. Students should be encouraged to mobilize personal coping supports and access counselling services available at the university and/or through private counselling services often covered by student union dues or under other health plans. Compassion Satisfaction. Despite having less sleep, students with high levels of self-efficacy reported less intent-to-leave and higher levels of CS. Of note, the current study did not explore quality of sleep in conjunction with quantity of sleep highlighting an area of future study. Salmela-Aro et al. (2009) found that undergraduate students with high levels of optimism had high levels of work engagement with low levels of BO up to 17 years post-graduation. According to Garrosa et al. (2008) , students with a hardy personality who felt they had a greater sense of control and commitment were inversely associated with BO. These findings support that high levels of self-efficacy protect students from BO and potentiate CS formation. Of note, positive sleep hygiene practices improve academic performance and neurocognitive performance indicating that educators should discuss the importance of sleep for academic success (Abdulghani et al., 2012; Gilbert & Weaver, 2010) . More research is needed to explore the link between sleep and CS. Limitations. To date, this is the only study that has explored outcome variables of ProQOL within a sample of BScPN and BN students in Canada. Four key limitations are noted within the study that includes use of convenience sampling, limited generalizability, self-report screening tools, and timing of the survey. Participants were recruited from a convenience sample of currently enrolled full-time students at a single university. Only 27% of the eligible student population participated. Although a typical response rate of approximately 30% in education research is the norm, the non-randomness, small sample size, and homogeneity of the sample introduces sampling bias that limits generalization of findings (Polit & Beck, 2017) . Use of self-report screening tools and questionnaires introduce bias if responses are inaccurately reported, the reliability of the measures decrease and measurement error occurs (Polit & Beck, 2017) . Many of the tools, including the ProQOL Scale adopted within the study, serve as screening tools. While student nurses may not be considered professionals, it is important to note that students are socialized into the professional role of the nurse with expectations to adhere to values and competencies expected of registered, practicing nurses identified within the Code of Ethics (CNA, 2017). Similarly, ""The Life Events Checklist is only intended to be a screening measure to evaluate exposure to possible Criterion A events. . .. It is very challenging to measure total trauma load"" (F. Weathers, personal communication, October 30, 2018) . The PSS continues to offer a valid, reliable, and empirically tested avenue to assess stress within university students and working adults. The survey was administered during the months of February and March 2020 prior to the novel coronavirus pandemic with subsequent closure of university settings. This study adds to a growing call for undergraduate prelicensure programs to integrate measures that bolster students' coping and self-efficacy during times of distress. The study findings revealed that students entering the BN and BScPN programs may not be prepared to face the stressors encountered during care provision. Curricular strategies that include a four-week mindfulness course, coping and crisis peer-debriefing workshops, and incorporating emotional intelligence development throughout pre-licensure curricula are potential strategies to address high levels of BO and STS. Fostering student self-care in pre-licensure programs is an essential measure to promote CS prior to entry into the workforce. Teaching and advocating that students be engaged in positive coping and self-care practices to sustain student well-being while enrolled in nursing and psychiatric nursing programs are warranted for a long-lasting career. Development of mindfulness skills may serve to protect students against CF and promote the protective effects of CS (Clarkson et al., 2019) . A four-week course that addresses weekly learning objectives centered on mindfulness can be integrated early within pre-licensure programming. Ouliaris (2019) promoted mindfulness meditation and reflective writing as two methods that can be integrated into curricula to increase student capacity for self-awareness. Chung et al. (2018) recommended four components of a mindfulness-based curriculum that involved: (1) One weekly 60-minute classroom session every week for four weeks; (2) Prerequisite reading assignments to accompany the classroom sessions with topics such as foundational wellness and mindfulness concepts, stress, burnout, and healthy practices; (3) Individual meditation practice and journal assignments; and (4) Developing a personalized wellness plan. Rees et al. (2016) advocated that students are taught mindfulness skills to prevent burnout, especially for students with high levels of neuroticism and maladaptive coping. Coping and Crisis Peer-Debriefing Davies and Coldridge (2015) advocated that students should be trained in how to cope with traumatic situations. Coping and crisis peer-debriefing workshops can be integrated into curricula aligned with entry into clinical practice within their program. A student-drafted personal wellness plan assignment (Chung et al., 2018) embedded as part of a theory or clinical course may assist students in mobilizing their own coping supports during times of real and perceived crisis. In addition, crisis peer-debriefing education may serve as a benefit for students if a classmate is not comfortable seeking assistance from an instructor or if immediate counseling is not available. Grief training may also serve prelicensure health students when efforts to save a life are unsuccessful or when providing care to clients faced by life threatening circumstances (Sikstrom et al., 2019) . Interventions that foster emotional intelligence (Goleman & Boyatzis, 2017) , civility, and positive student coping resources prior to their entry into the workforce are warranted. Nurturing emotional intelligence in students can serve to reduce stressors, mitigate workplace bullying, unfriendliness, and hazing within nursing (Littlejohn, 2012) . Nurse educators and managers play a significant role in creating and leading environments that promote teamwork, positive working relationships, and positive working conditions (Hunsaker et al., 2015) . Positive faculty role-modeling and curricula that fosters a culture centered on civility (Clark, 2017 ) that addresses bullying and workplace violence are advisable. These efforts can serve to promote provider resiliency, emotional regulation, and encouragement that enables flourishing. Creating a positive teaching and learning environment that fosters openness, creativity, efficiency, organization, a sense of accomplishment, and overall positive psychology may play a role decreasing CF within student populations prior to their entry into the workforce. The PERMA Model (Seligman, 2011) comprised of Positive emotion, Engagement, Relationships, Meaning, and Accomplishments provides educators an avenue to incorporate aspects of positive psychology within preparational curricula (Slavin et al., 2012) . Integrating emotional intelligence development as part of an 'emotional curriculum' (van Zyl & Noonan, 2018) could serve as a proactive approach in developing student resilience, coping, self-management, social intelligence, leadership, and emotional self-awareness as learners and entry-level practitioners. The author would like to acknowledge feedback and comments received from her PhD advisory committee members. The individuals included: Noelle Rohatinsky, Gerri Lasiuk, Phil Goernert, Valerie MacDonald-Dickinson, Carol Bullin, and Don Leidl. The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article. This study was approved by the Brandon University Research Ethics Committee (ID: 22466) and the University of Saskatchewan Behavioural Research Ethics Board (ID: 1072). Note: the study is in compliance with the Canadian Tri-Council Policy Statement: Ethical Conduct for Research Involving Humans (https://ethics.gc.ca/eng/policy-politique_ tcps2-eptc2_2018.html). The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: Funding for this research was received from the Canadian Nurses Foundation TD Meloche Monnex PhD award and the Workers Compensation Board of Manitoba. Kathryn M. Chachula https://orcid.org/0000-0002-5026-953X@story_separate@The findings in this study highlight that pre-licensure nursing and psychiatric nursing students are not immune to low levels of CS and high levels of BO and STS that comprise CF within the ProQOL scale. Students in LTC-palliative care rotations reported significantly higher levels of BO in comparison to students placed on in-patient units such as medical-surgical areas and episodic care areas that included out-patient and emergency department settings. Regression analysis revealed that students with low self-efficacy and high perceived stress were predictive of BO. Students with increased exposures to PTEs were predictive of STS. Students with high levels of self-efficacy, less sleep, and commitment to their program with less intent-to-leave were predictive of having CS. Findings of the study assist educators, clinicians, and policy makers to better understand at-risk students and their associated clinical settings, as well as predictors of CS and CF in undergraduate nursing and psychiatric nursing students prior to entering the workforce as newly-licensed professionals. Curricular strategies that bolster students' resilience, coping, and self-efficacy during times of stress, distress, and feelings of exhaustion are warranted prior to entry into the workforce.","INTRODUCTION: Professional quality of life (ProQOL) that encompasses compassion satisfaction (CS) and compassion fatigue (CF) comprised of burnout (BO) and secondary traumatic stress (STS) has been raised as a world-wide issue for the nursing profession. Limited attention has been paid to the vulnerabilities of nursing students to ProQOL and the associated mechanisms. PURPOSE: Determine what factors are predictive of ProQOL in a population of undergraduate nursing and psychiatric nursing students. Methods: A cross-sectional survey was conducted comprised demographic questions and four validated measures: the Professional Quality of Life Scale (version 5), Core Self-Evaluations Scale, Perceived Stress Scale, and Life Events Checklist (version 5). RESULTS: Students in long-term care-palliative care rotations reported significantly higher levels of BO in comparison to other care areas. Regression analysis revealed students with low self-efficacy and high perceived stress were predictive of BO. Students with increased exposures to prior traumatizing life events were predictive of STS. Students with high levels of self-efficacy and less intent-to-leave were predictive of having CS. CONCLUSION: Findings assist educators, clinicians, and policy makers in understanding at-risk clinical settings and predictors of ProQOL in pre-licensure students. Curricular recommendations that include mindfulness, coping and crisis peer-debriefing, and emotional intelligence are discussed."
"Despite the widespread endorsement of a partnership approach indicating that collaboration is generally a ""good"" thing in the context of complex problems, there is a need to strengthen the evidence base relating to claims of its effectiveness [1] [2] [3] [4] . In practice, partnerships that bring different organizations and individuals together can generate a great deal of frustration, and those involved frequently struggle to achieve measurable, beneficial outcomes [5] [6] [7] [8] [9] . Estimates from formal evaluations suggest that up to 70% of alliances fail, and those that survive are frequently unable to reach their full potential [8, 10, 11] . These problems are not surprising given that partnerships are resource intensive, time consuming and require governance, procedures and processes that are very different from the ways independent organizations are run [8] . More research is therefore required to explore the factors, both positive and negative, that influence partnerships and mitigate success; otherwise, advocacy for partnerships will remain ""a rhetorical issue with no basis in reality"" [10] . This article presents findings from a mixed methods study of multi-stakeholder partnerships involving decision makers, academic representatives, clinicians, health system administrators, patient partners and representatives of health and social service organizations providing services to vulnerable populations with an interest in improving primary health care (PHC) accessibility. The study was conducted between 2016 and 2018 with the aim of understanding partnership processes and how various partnership factors contribute to partnership effectiveness within the context of addressing complex issues in PHC. We define a multi-stakeholder partnership as a complex human system based on voluntary collaborative relationships among stakeholders who agree to work together towards a common purpose, to combine competencies, resources and responsibilities and to share risks and benefits (adapted from [11] ). The focus of the study was on partnerships involving multiple stakeholders across organizations-transcending the boundaries between clinical practice, research, health system management and health care policy-collaborating towards a common goal of transforming PHC. In this study, we used a two-phase, exploratory sequential mixed methods design, embedded in a larger Canada-Australia research and evaluation project, to gain a comprehensive understanding of partnership processes, the interplay of partnership factors and the achievement of intended outcomes. This research adds depth to understanding the processes and approaches that support partnerships and assesses the relevance of the multi-stakeholder partnership approach in PHC. Equally, this research demonstrates how the application of theoretical frameworks can facilitate a deeper understanding of partnerships.@story_separate@PHC service delivery is the cornerstone of a high-performing health care system [12] [13] [14] . Robust PHC serves the following core functions: accessibility, continuity, comprehensiveness and coordination [15] . Among Organization for Economic Co-Operation and Development (OECD) countries, PHC is the first point of access to the health care system. It encompasses continuous and comprehensive care across diverse curative, preventative, educational and rehabilitation services with a person (micro), community (meso) and population (macro) orientation [13, 16, 17] . The centrality of PHC is underscored by its strategic coordination role that has a ripple effect on other parts of the health care system [18] . Advantages to having a comprehensive, responsive, high-quality PHC include improved population health outcomes, reduced inequities, improved patient and provider experience and satisfaction, lower health system costs and more robust health care systems overall [12, 13] . Many countries have seen major improvements in the health of their populations over recent decades. However, progress has not been even across various dimensions relevant to PHC. Accessibility, in particular first-contact accessibility, is an ongoing concern in PHC [18] . Important disparities in equitable access to PHC remain, and there is evidence that gaps among social groups within countries have widened [14] . Several studies [19, 20] have demonstrated that these gaps translate into unmet health care needs, delays in obtaining treatment, greater use of emergency departments and hospitalization and poorer health status for these patients. In addition, the resultant disparities present ethical challenges and place an unnecessary economic burden on publicly funded health systems. It is estimated that the direct economic burden of socioeconomic inequalities in health in Canada amounts to approximately $6.2 billion annually or over 14% of total expenditures on acute care inpatient hospitalizations, prescription medication and physician consultations [21] . In light of the realization of system deficiencies and rising health care expenditures, PHC in OECD countries has recently been the focus of a series of reform and reorganization efforts [22, 23] . Canada and Australia have health care systems that aim to provide comprehensive, universal and accessible health care based on equal access for equal need. However, a review of the literature from both countries indicates that the goal of equitable access has not been uniformly achieved, particularly when it comes to lower income members of society such as Indigenous groups and refugees [24] . Both countries have undertaken initiatives to strengthen the PHC sector following findings of commissioned inquiries that highlighted suboptimal performance on many PHC access and quality indicators [25] [26] [27] [28] . Many of these reform efforts have emphasized collaboration and partnerships among health care organizations, regional health authorities and other stakeholders as a way of achieving the goal of comprehensive PHC and delivering better care and implementation of policy [29, 30] . This interest in partnerships has been reinforced by published evidence that partnerships involving multiple stakeholders can help people and organizations generate outcomes that are greater than those that can be achieved working independently [8, 31, 32] . One of the recent international initiatives to address the challenges related to equitable access to PHC was the Innovative Models Promoting Access-to-Care Transformation (IMPACT) program [33] . It was a 5-year (2013-2018) Canadian-Australian funded research program that saw the development of local partnerships as a strategy to enhance access to comprehensive PHC for vulnerable populations. Within the context of IMPACT, vulnerable populations were defined as community members whose demographic, geographic, economic, social and/or cultural characteristics compromised their access to PHC and limited their capacity to maintain health and advocate for themselves in the context of a complex health care system [33] . Six local partnerships in three Australian states (i.e., New South Wales, Victoria and South Australia) and three Canadian provinces (i.e., Ontario, Quebec and Alberta) used a common approach to implement six different interventions designed to address a priority gap in access to appropriate PHC. The interventions ranged considerably in focus and mechanisms (see Table 1 ). The partnerships involved decision makers, researchers, health and social services providers and administrators as well as members of vulnerable communities facing complex challenges to the delivery of community-based PHC. The partnerships identified priority access needs of vulnerable communities, informed the design of interventions based on the local context, engaged in deliberative processes for collective decision-making, and evaluated both the interventions and the partnerships themselves. The work reported here was conducted within the framework of the IMPACT program. Table 1 . Overview of characteristics of interventions in six IMPACT * local partnerships (adapted from [34, 35]  Literature from diverse fields describes what constitutes effective as opposed to ineffective partnerships, and outlines strategies to enhance collaborative processes and increase partnership effectiveness [9, [36] [37] [38] [39] [40] . This literature offers a variety of definitions and conceptualizations of partnership effectiveness. The prevailing approach is to assess partnership effectiveness in terms of its ability to achieve planned outcomes [41] , also referred to as ""results"" or ""consequences"" [42] . These intended outcomes could be shortterm, such as increased knowledge of stakeholders, intermediate, such as capacity building and buy-in, and long-term such as changes in community programs, policies and practices and improvements in health indicators [42, 43] . An alternative approach to assessing partnership effectiveness is to focus on effective partnership as an end in itself, and as an intermediate outcome leading to the specific goals of the partnership. The assumption here is that partnerships that have a high level of internal functioning will be more likely to achieve their intended long-term outcomes [36] . The functioning of ongoing partnerships was the focus of this study, and we differentiated between the effectiveness of processes and outcomes. We captured information about the quality of the processes and relationships between partners and the health of the partnership on the one hand, and specific outcomes that could be attributed to the work of the partnerships. In addition, we collected information on the benefits of participating in partnerships for the stakeholders' respective organizations, in line with the literature that identifies outcomes to partners as an important third strand of effectiveness, different from the strategic goals of the partnership [44] . To assess how a partnership is performing and to identify strengths and areas for improvement, strategies for measuring these multiple partnership dimensions and evaluating progress towards intended outcomes are needed. Partnership evaluation literature offers partnership measurement tools, notably self-assessment tools, that help organizations assess how the partnership is evolving and to stay accountable to the partnership's stakeholders and funding bodies [45] [46] [47] . However, there is still ambiguity regarding the relationship among the various factors of a partnership's work and effectiveness, particularly which partnership factors are more critical to partnership effectiveness [8, 48] . Given the complexity and dynamic nature of partnerships and intractable problems they seek to address, inferring causal relationships between different partnership factors and effectiveness and identifying what percentage of the outcome is attributable to which factor is challenging [36, 37] . This challenge is common in evaluating complex human systems that entail a web of interacting feedback loops where cause and effect are not close in time and space [49, 50] . The Partnership Synergy framework aims to explain the link between partnership factors and partnership effectiveness [8] . Synergistic partnerships are high-performing partnerships that have created added value by leveraging the resources and capabilities of their partners. We define partnership synergy as the combined effect of complementary tangible and intangible partnership assets and enabling processes that gives partnerships unique advantages over the work of individual people or organizations working towards the same goals (adapted from [8] ). Partnership intangible or ""soft"" assets include human, informational and organizational resources, such as knowledge, competencies, connections, culture, data and information, whereas tangible assets include material and financial resources such as space and funding [51] . While it is widely acknowledged in the field of management that tangible assets are the critical building blocks of organizations and companies, the rapid expansion of the knowledge economy has shed more light on the importance of ""intangibles"" and the benefits that they generate [51] [52] [53] . In fact, it has recently been argued that it is the intangible assets, such as skilled employees and unique know-how, that give companies their competitive edge [52] . In the field of partnerships, the critical role of both tangible and intangible assets has been highlighted in prior research [8, 54] . It has also been suggested that intangible assets may constitute the outputs of partnership work, as health partnerships entail knowledge-generating activities and do not produce goods but rather knowledge, understanding and relationships [53] . For the purposes of this investigation, we considered intangible assets, produced by partnerships, such as learning, only insofar as they are reinvested back into the partnership and enhance partnership work and the achievement of intended outcomes. Certain partnership processes enable partnerships to leverage resources successfully and mobilize the complementary knowledge and expertise of all the partners to achieve partnership synergy [6, 50] . Examples of enabling processes include leadership, administration and decision-making processes as well as the degree to which a partnership optimizes the involvement of its partners [8, 50] . The literature identifying these key precipitators of partnership synergy is growing [6, 50, 55, 56] . While it is common to refer to these precipitators more broadly as ""partnership functioning factors"", we make a distinction between partnership assets that we view as partnership inputs and partnership enabling processes that act upon inputs to produce intended outcomes (adapted from [57] ). It has been proposed to measure partnership assets (such as the sufficiency of resources) and enabling processes, along with partnership synergy, as a predictor of partnership effectiveness [8] . Two validated scales measure partnership synergy: the Weiss et al. scale [50] and the Jones synergy scale [58] . These scales measure partnership synergy in different ways: the Weiss et al. scale, which looks at synergy as a product of a partnership that has achieved its full potential, and the Jones synergy scale that measures synergy as both a partnership process or experience and a partnership product [58] . Both synergy scales are embedded in instruments that include other measures of different partnership factors. The Weiss et al. synergy scale is embedded in the Partnership Self-Assessment Tool (PSAT) [59] , and both Weiss et al.'s and Jones' synergy scales are incorporated in the Partner Questionnaire [6] . We adopted the Partnership Synergy framework to look at the formation and processes of IMPACT program's multi-stakeholder partnerships. We selected the PSAT with some nuances from the Partner Questionnaire. The study entailed a two-phase mixed methods sequential exploratory design [60] . A qualitative longitudinal case study [61] (n = 2) was followed by a cross-sectional survey of IMPACT collaborative local partnerships (n = 5). Figure 1 illustrates the mixed methods design. The qualitative phase entailed exploring, in a smaller sample, the different manifestations of partnership synergy, the types of partnership assets and broad categories of partnership processes relevant to multi-stakeholder partnerships PHC [62] . Based on the qualitative findings, the quantitative phase included measuring, in a larger sample of partnerships, the achievement of partnership synergy and exploring associations between partnership synergy, partnership assets and partnership processes [63] . Qualitative findings and quantitative results were subsequently integrated to obtain a deeper understanding of the multi-stakeholder partnership phenomenon [60] . Preliminary data gathering for the project involved the review of minutes of meetings, protocols and reports produced within the scope of the IMPACT program and, more specifically, the two local partnerships, Primary Care Connection and Community Health Resources. The review of documents provided data on the operational elements, participants' roles and responsibilities, the objectives and intended outcomes of each initiative, and how the objectives and the involvement of different stakeholders evolved since the start of the IMPACT research program in 2013. Specific attention was devoted to contextual factors that might have had an influence on the two local partnerships. The information generated from the program's documents allowed the refinement of the study question, in consultation with a number of the program's stakeholders. Institutional ethics approval for the study was obtained from the St. Mary's Hospital Centre Research Ethics Committee (No. SMHC-13-30C) on 10 August 2016.  The longitudinal case study of the two partnerships, the Primary Care Connection Partnership and the Community Health Resources Partnership, was conducted between August 2016 and September 2018. This phase entailed non-participant observation of the partnerships' meetings and semi-structured, in-depth interviews with a sample of the partnerships' stakeholders. The longitudinal aspect allowed capturing the main developments in the life of the partnerships over time. The non-participant observation of meetings focused on the behaviours of partners and interactions among them. The first author conducted the observation of 11 Primary Care Connection Partnership and three Community Health Resources Partnership meetings. Specific attention was devoted to observing how the agreed upon processes common to all partnerships were implemented and to identifying any new emergent processes influencing the partnerships. The semi-structured interviews offered emic (i.e., through the lens of the participants) insights into partnership processes and helped to identify partnership elements that were relevant to the subsequent, quantitative phase. The first author conducted nine interviews with Primary Care Connection Partnership stakeholders and seven interviews with Community Health Resources Partnership stakeholders. Interview participants were asked to report on their experiences within their respective partnerships from the beginning of their involvement until and including the period when the interview was conducted. Interview candidates were purposefully selected [64] on the basis of their roles, and the nature and duration of their engagement with the local partnership. Representatives of each of the stakeholder group (i.e., researchers, research program coordinators, policy makers, clinicians, and organizational representatives/patient partners) were invited to participate in the interviews. Attempts were made to ensure a mix of seasoned and new participants. In addition, attempts were made to invite stakeholders demonstrating both high and low levels of engagement, based on prior observation of meetings. Candidates selected for interviews were initially invited to participate via blinded group e-mails. Follow up was undertaken in person at the end of partnership meetings, and over e-mail sent directly to the candidates. The duration of interviews was approximately one hour, and they were conducted either in person or over the telephone. The interviews were planned according to existing recommendations on conducting individual interviews in health research [64, 65] . The interviews contained predominantly open-ended questions and time was factored in for other questions which arose from the conversation. The questions asked about partnership synergy were inspired by Jones and Barry [58] and Weiss et al. [50] . All interviews were transcribed verbatim. The interview transcripts were not returned to participants for comments. A hybrid deductive-inductive approach to framework analysis was used to identify patterns within the data [66, 67] . The initial coding scheme was developed to reflect the concepts from the partnership synergy framework, including complementarity of skill, experience and work sharing. Words, sentences or paragraphs from transcripts were extracted into pre-determined codes with new codes emerging from the data. The final codes were grouped along the dimensions of partnership synergy and the five families of factors likely to foster partnership synergy: partner characteristics, relationships among partners, partnership characteristics, resources and external context [8] . The data management, coding and analysis were performed using NVivo 12 qualitative analysis software [68] . The transcripts were analyzed by the first author (EL), but the coding was verified by another qualitative researcher (CS). Emerging findings were discussed at regular team meetings. Phase I resulted in a thick description of the two partnerships as well as on the processes that were employed to enhance the partnerships. In line with the exploratory sequential design, the qualitative findings were then used to adapt the PSAT partnership self-assessment instrument by using qualitative codes and themes to select the domains and concepts for inclusion in the quantitative phase [69] (see Supplementary Materials Table S1 ). We selected PSAT scales and items that corresponded to the qualitative codes from the case study and excluded scales and items that were deemed non-relevant. We retained the following PSAT scales: Decision-Making (four items); Leadership (11 items); Administration and Management (11 items); Non-financial Resources (six items); Financial and Other Resources (three items); and Resource Utilization (three items, referred to as ""Efficiency"" in the PSAT). We also retained questions about perceived Benefits (11 items) and Drawbacks (five items) of participation to stakeholders' respective organizations and an overall assessment of the benefits compared to the drawbacks (1 item). We added elements that emerged in the qualitative data but were not part of the 2001 version of the PSAT: new proposed sub-scales for Communication (three items) and External Environment (two items). The case studies spoke to the importance of communication as an integral dimension of partnership work and the critical role of contextual influences. We also added one question to assess the extent to which the goal of developing a meaningful partnership had been achieved as a measure of partnership effectiveness. We supplemented synergy items from the PSAT with synergy items from the Partner Questionnaire [58] to capture more information on partnership processes. Partnership synergy was assessed using two sub-scales: (1) the adapted Partnership Synergy Processes sub-scale incorporating five items from the eight-item synergy scale developed by Jones and Barry [58] ; and (2) the adapted Partnership Synergy Outcomes sub-scale retaining two items from the nine-item synergy scale by Weiss et al. [50] . The items from these original scales were selected based upon the relevance to the types of partnerships highlighted throughout the qualitative inquiry, and with a view to reduce participant burden. Finally, the observation data and responses to qualitative interviews guided the choice of language that was used to elicit descriptive information about the stakeholder (type, role in the organization, length of time in the partnership and frequency of engagement). Phase II entailed a cross-sectional survey design. The criteria for partnership inclusion in this phase were: (a) the partnerships had at least five active academic and non-academic partners; (b) the partners had continually and collaboratively worked together to develop and modify strategies in order to achieve their goals; (c) the partners had begun to implement their plans. We distributed a self-administered questionnaire to a census sample of all multiple stakeholders within five of the six IMPACT local partnerships. All stakeholders in five of the six IMPACT local partnerships, who were active in the partnerships at the time of administration, were considered eligible and were invited to participate. The sixth partnership did not meet our inclusion criteria. Our final sample included 54 partnership stakeholders representing five IMPACT local partnerships (a response rate of 90%). The questionnaire captured the subjective input from stakeholders, reflecting their assessment of respective partnerships as separate entities, and not their individual experiences. Both paper-and web-based questionnaires were offered, in either English or French, with multiple contacts to maximize the response rate [70] . The electronic questionnaire was hosted on the Qualtrics [71] online survey platform. The scores for each sub-scale were derived through unweighted means and medians of all the component items; scores for a respondent were calculated only if at least 50% of the items in the sub-scale had been completed. We used Spearman correlation coefficients to estimate the correlation between items within purported sub-scales, and between subscales, specifically partnership processes and partnership synergy. We examined internal consistency of the purported sub-scales by calculating the Cronbach's Alpha. At the partnership level, the score for each sub-scale was calculated by aggregating the sub-scale scores of the component member respondents within each partnership. For partnership synergy, the two sub-scales of Partnership Synergy Processes and Partnership Synergy Outcomes were analyzed separately at the level of individual respondents. However, given that they were highly correlated with each other and demonstrated similar correlations with partnership processes, both sub-scales were combined at the level of the partnerships to provide a single score of (total) Partnership Synergy. We used the non-parametric Kruskal-Wallis test to determine if we were able to detect statistically significant differences in sub-scale scores between any of the partnerships. The ranks of the scores were also derived. SPSS 23 for Windows [72] was used for data analysis. Parallel to this study, an independent longitudinal qualitative developmental evaluation of the partnerships was undertaken to guide partnership development. The evaluation was designed and led by two of our authors (CS and VL), who subsequently ranked the five partnerships that were the focus of this study, based on the operational definitions and item content of the following sub-scales: Partnership Synergy, Communication, Decision-Making, Problem-Solving, Resource Utilization, and External Environment. We then compared the qualitative and quantitative ranks. Study participants represented a range of organizational expertise (see Table 2 ). The stakeholders within each partnership included a mix of decision makers, primary care physicians, health care managers, academic representatives (researchers and research coordinators, including partnership coordinators), members of vulnerable populations, and stakeholders from community-based organizations providing services to the vulnerable population. Among interview respondents, academic representatives and decision makers constituted the largest two groups (n = 10, 63%). Among survey respondents, academic representatives constituted the largest single group of stakeholders in each partnership. Interview participants and survey respondents were predominantly female (interviews: n = 13, 81%, survey: n = 42, 78%). Of note, 15 stakeholders participated in both the interviews and the survey.  Our qualitative investigation revealed that both partnerships had a variety of tangible and intangible assets. These assets were either acquired or generated as a result of working in partnership. The main acquired intangible assets in both partnerships were considered by participants to be the resources brought into the partnership by the multiple stakeholders who joined the partnership in order to tackle a complex, multi-faceted issue of common concern-accessibility to PHC for vulnerable populations: ""I think that it's well organized, and it includes health care utilizers as well, so that's important, decision makers, researchers and clinicians. So, I think it's, the structure is aligned well with the area of the study"" (Policy maker). These stakeholders contributed unique perspectives and skills, information and connections to a broader set of stakeholders and health systems exerting influence over the partnerships. The medical practitioners shared their experiences of dealing with vulnerable patients and identified health system opportunities and constrains to accommodate new vulnerable patients and to improve services provided to them. The academic team shared relevant research evidence, facilitated the overall work of partnerships and served as an interface with funding bodies and the larger IMPACT program. The research coordinators in particular supported the various components of the interventions, including communicating with stakeholders, organizing partnership activities, facilitating meetings, proactively conducting outreach and gathering and synthesizing information. Patient partners brought the lived experience point of view. Community organizational representatives shared insights into the challenges experienced by the target populations and available community services and helped the partnerships to align project activities with the priorities and capabilities of organizations serving the target populations. Finally, decision makers and health planners served as a bridge between researchers and policy-making, ensuring that research activities aligned with and responded to health policy priorities and capabilities and, conversely, that health authorities were appraised of research evidence relevant to the project. These unique perspectives and insights were deemed to be complementary in that they allowed for the exploration of issues of access from various angles, to obtain timely information in order to make necessary adaptations to the intervention models, and to attract additional resources. The partnership membership was not static, it evolved as the work of the partnerships progressed, reflecting natural stakeholder attrition and the need to attract additional expertise and resources. The acquired tangible assets included financial resources obtained through the IM-PACT research program's grant funding, space to meet, and information and other project support resources. The partnerships were part of the larger IMPACT research program that received funding in Canada and Australia. This funding covered the partnerships' coordinating infrastructure/support for research such as data collection, including the partnership coordinator position at each site, as well as the evaluation. Partnership stakeholders, other than research coordinators, were not remunerated for the time spent on partnership activities. However, expenses related to attending partnership face-to-face meetings, disseminating results and promoting the program were covered for most stakeholders. Partnership management activities were carried out by the research teams located at their respective research entities. Face-to-face partnership meetings were organized either at these research entities or at nearby locations, including partner universities, affiliated hospitals, and participating community organizations. Other tangible assets included information resources such as computer software. The generated intangible assets included the skills and relationships that the partnerships had invested in and the learning that transpired in the course of partnership work. The partnerships provided several stakeholders with educational and capacity-building opportunities. The coordinators in each partnership were critical resources who offered ongoing support for partnership activities. Given that working in partnership required skills that were different from those employed in the typical running of research projects, the partnerships made strategic financial investments into acquiring these new skills. Instead of outsourcing certain partnership-related tasks, the partnerships built capacity in-house through training. Both partnerships invested in training partnership coordinators in group process facilitation techniques and then provided them with opportunities to facilitate partnership meetings. Some coordinators highlighted the value of experiential learning: So, I've learned a lot from a research perspective from the research team, and from the LIP Core Team more specifically around just community development and how that works and participatory action research and making sure that everybody has a voice and who needs to be at the table, so that has been really rich, because I knew very little about that when I came on (Research coordinator). In addition, one of the two partnerships provided its patient partners with opportunities to attend relevant patient engagement training. These training and capacity-building investments were not only part of incentive management, but also benefitted the partnerships directly through enhanced skills and knowledge that strengthened the partnerships. Another important generated intangible asset was the collaborative relationships that were formed in the course of partnership work. While some stakeholders had a history of collaborative working relationships, relationships with other stakeholders had to be forged. Each partnership made intentional efforts to strengthen relationships within the respective partnership and with external stakeholders who were critical to partnership efforts. Relationships among some non-academic stakeholders developed more organically as a result of interactions during partnership meetings. These positive relationships benefitted the partnerships by stimulating more open conversations and by contributing to faster and deeper decision-making and enhanced project ownership. The generated tangible assets included the resources identified to implement and sustain the interventions developed by partnerships. In the absence of funding for intervention implementation under the IMPACT research program's grant funding, each partnership was required to mobilize adequate local resources to respond to regional access needs and to maintain interventions beyond the life of the IMPACT research funding. Both partnerships developed low-cost lay navigator approaches to addressing the access needs of the target populations. In order to sustain the interventions, both worked towards integrating the interventions into existing health system organizational structures, aligning the proposed models with priority health system initiatives. The Community Health Resources Partnership succeeded at securing additional funding for the initiative, which extended the project beyond IMPACT. The additional funding covered a randomized controlled trial to test the effectiveness of the model that the partnership had developed. Both partnerships that were part of our qualitative study [62] employed specific processes to activate the above-mentioned assets. The following main categories of processes emerged from our data: (1) resource management; (2) leadership; (3) administration and management; (4) communication; (5) decision-making; (6) adapting to the evolving context. Stakeholder engagement in the two partnerships occurred in a variety of ways. Both partnerships began with deliberative fora involving a broad range of stakeholders to learn about community needs around PHC access, the relevant community organizations and available resources to support interventions. As partnership work progressed, the Primary Care Connection Partnership involved stakeholders in multiple aspects of the research process, with some non-academic stakeholders fulfilling tasks outside the partnership's face-to-face meetings. Conversely, the Community Health Resources Partnership took a more traditional academic approach to collaboration with stakeholders-a research advisory approach, with limited contribution of non-academic stakeholders outside partnership meetings. Both partnerships used regular face-to-face meetings to openly discuss project progress, create a shared understanding of the project and engage in iterative collaborative learning: ""I feel like there's lots of opportunity to share what's on our minds, to express ourselves. I feel that it's a very open to dialogue type of meeting."" (Representative of a community-based service organization); ""I think there is a lot of discussion [ . . . ], we have the freedom to give our opinion, to discuss. I think it is very appropriate"" (Family physician). The research teams spearheading the initiative capitalized upon the various strengths and perspectives of stakeholders by providing sufficient time to discuss pressing issues, soliciting input from all stakeholders and offering stakeholders different mechanisms to contribute (e.g., large group discussions and small break-out sessions). Participants who could not attend meetings were appraised of what was discussed during meetings and provided with opportunities to contribute electronically. While interview respondents recognized the inherent power inequalities in the partnerships due to the fact that the research teams controlled grant resources and had more to lose if the partnerships did not achieve stated objectives, they acknowledged that the research teams made efforts to address barriers to equitable participation. Examples included the strategic choice of locations for partnership meetings, transportation fees coverage and approaches to elicit input from linguistic minority participants. Both partnerships experienced difficulty with the engagement of community-based stakeholders representing the target populations; however, after they became involved, one Community Health Resources Partnership patient partner stated: ""[ . . . ] it's a very nice invitation to have people who are not professionals, who are not involved in that kind of world, to be invited in and be allowed to give an opinion or thought or idea. I think it really empowers people"" (Patient partner). Both partnerships were largely driven by the research teams that initiated the partnerships. The leadership approaches in the two partnerships differed. The Primary Care Connection Partnership leveraged the power of leadership distributed among academic and non-academic stakeholders with different stakeholders taking responsibility for various components of the work of the partnership. In the Community Health Resources Partnership, the leadership was centralized within the research team. However, interview participants reported that the research team seemed genuinely interested in hearing from all stakeholders and made efforts to check in with various groups around the partnership Despite these differences, there were a number of common leadership processes. Within the research teams, both partnerships had formal (i.e., academic investigators and co-investigators) and informal (i.e., partnership coordinators) leaders knowledgeable about the context and skilled at mobilizing the various perspectives of stakeholders and forging common ground. The leaders did not possess all of the requisite partnership-related knowledge and skills at the outset but rather continuously learned from best practices in partnership development. The leaders were transparent about their own gaps in knowledge and eagerly welcomed input from different stakeholders. This openness contributed to creating an atmosphere of trust where differences of opinion could be voiced. The research teams, comprising academic principal investigators, co-investigators and research coordinators, were responsible for the overall management of the partnerships. Each team had dedicated (i.e., full-time) and part-time research staff supporting the work of the partnerships. The scope of the research teams' activities was broad. They were responsible for recruiting partnership members; managing information; coordinating com-munication among partners and with outside entities; facilitating the selection, adaptation and implementation of interventions at the local level; evaluating the effectiveness and potential for scalability of interventions. Both research teams performed extensive field work, gathering relevant data and promoting the interventions to external parties. The research teams also organized regular meetings of their respective partnerships. This included organizing meeting logistics, facilitating meetings and summarizing discussions in the form of minutes: ""They seem to just be very effective and very planned and organized. So, I'm learning from seeing how a well-organized meeting unfolds"" (Representative of a community-based service organization). The research teams in both partnerships employed adaptive management approaches to support iterative decision-making and facilitated the ability of partners to contribute meaningfully: ""We share, and then the research team stimulates the discussion between different territories, how can we help you, what can we do to succeed. [ . . . ] In addition, the research team is really glued to the team here to see if something is not working well"" (Policy maker). In addition, the Community Health Resources Partnership conducted meeting evaluations at the end of each partnership meeting and made timely adaptations to how the meetings were run and the type of information provided at subsequent meetings. Both partnerships had open and multidirectional channels of communication to communicate internally with stakeholders within the partnership: Yeah, we get updates, and people are communicating with us. We know that we're part of the team, we know that we're being informed, invited to meetings long ahead of time, so there's lots of opportunity, you know, it's not a last-minute invitation. So yeah, so those processes are reassuring to see that it's well run and things are happening in a timely manner (Policy maker). In-person communication with all stakeholders was mostly confined to regular partnership face-to-face meetings. The research teams synthesized relevant partnership-related information and brought it in condensed verbal and written formats to the attention of the partnership stakeholders. Outside of the meetings, regular contact was supported by partnership coordinators via electronic communication. These exchanges included minutes of meetings, meeting agendas and brief summaries of key issues upon which decisions would have to be made. In both cases, participants highlighted the importance of learning loops and having a variety of ways of soliciting input from partners. Learning loops involved requesting feedback from participants during meetings around issues relating to the project and being transparent about how this input was subsequently incorporated including being explicit about the reasons why suggestions could not be incorporated. Partnership external communications were aimed at increasing support for partnership interventions, recruiting medical practices that would be part of the interventions and disseminating information about partnership activities and achievements more broadly to the program's funders and the broader academic and medical community. External communication occurred during conferences in the form of conference posters, oral presentations and workshops as well as during scheduled meetings with relevant external entities. In addition, the Community Health Resources Partnership produced a periodic newsletter regarding project activities with a broader community reach. The Primary Care Connection Partnership stakeholders reported that the decisionmaking process was transparent and inclusive. Main decisions pertaining to the project were taken during face-to-face partnership meetings, by vote, on the basis of information compiled by the research team and following extensive stakeholder consultations: ""Decisions, I think the fact that you go to a vote [...] to decide how to make a change, I think that's good. They take all opinions into account before making a major decision"" (Family physician). This was particularly apparent in relation to adapting the intervention to its evolving context. Conversely, in the Community Health Resources Partnership the decision-making power was centralized within the research team, which was consistent with the advisory nature of the partnership. The activities of both partnerships were unfolding within the context of significant health care system reforms in their respective provinces. While both partnerships had to make adaptations to the interventions to respond to evolving contextual opportunities and threats, the extent of contextual impact and adaptation was far greater in the case of the Primary Care Connection Partnership: ""Of course, the thing that doesn't change is the changes in the [health] networks. But this is quite a major change."" (Policy maker). The partnership made several adaptations to the intervention, including its structure, delivery strategy and personnel resources. The situational analysis involved harnessing the knowledge of multiple stakeholders, instead of using formal environmental scanning analysis approaches. The active engagement in the partnership of policy makers and health system planners was critical in this respect, in that it contributed to an in-depth understanding of the context. Having stakeholders around the table with medium to high level of authority in their respective organizations allowed adaptations to be implemented in a timely manner. Partnership synergy manifested itself in the combination of the complementary skills and unique perspectives of the partners: ""I think it's a really good mix of people, and you can hear it in the discussion. The very different points of view, and they all complement each other very well"" (Representative of a community-based service organization). I honestly don't think that there's any other way to do it, because it's in primary care, and primary care is incredibly complex, there are so many players involved [ . . . ] incredibly complex problems and challenges, you know, particularly more so for the populations we are interested in, vulnerable populations [ . . . ] plus things are changing all the time, funding is changing and so on, so we always have to situate our project in a larger context. If we didn't have those other people at the Discussing the added value of partnership work some participants described the alignment of efforts of partnership stakeholders and the richness and integrative nature of collaborative work: ""These [partnership] tables are an example of integration. [ . . . ] We become more integrated and stronger, and there is a certain level of coherence between us. It has to be like this"" (Policy maker, emphasis by the participant). ""It is very rich. Not everyone has the same reality, and we inspire each other. In understanding the point of view of the other, we advance the discussion"" (Policy maker). [ . . . ] Magic happens when you get people who are going in the same direction. So, it's just about . . . it's analogous to integrating the health care system. If you have a group that is fragmented and they're doing their own thing separately the result obviously can't match the result that can happen when you're working together. So, it's all about integrating the effort. And so, it has to be better than just doing it on your own (Policy maker). Partnership synergy was also apparent in a variety of anticipated and actual benefits reported by stakeholders, stemming from their participation in the partnership, and in their sustained commitment to the partnerships: ""And there are different important constituents, it is more than a political representation of organizations, people who have continued to participate because they believe in it"" (Research coordinator). Participants described more benefits related to their respective organizations than personal benefits, highlighting a fit between the project's objectives and the organizational priorities of the entities they represented. The mutually beneficial nature of the partnership was reported by participants who stressed mutual and personal learning and satisfaction that stemmed from their involvement in the project: ""The researchers are learning from each other"" (Research coordinator). For me, this is completely new to me to be part of this type of project, so it's a learning experience that I'm enjoying tremendously. So, it's definitely good for my personal development to be part of this, I'm kinda honored to be part of this [ . . . ] (Representative of a community-based service organization). So, to be able to be part of the project which I think that they had a great idea, it's really smart, and I felt really glad to be part of that. You know because I feel like that's a good project, this is very helpful, this is a very, you know, significant issue for people. And to be able to be part of maybe, you know, exploring why it's a problem and offering my insights, I'm very excited to be able to do that (Patient partner). Partnership synergy was also evident in innovative ideas and new solutions to the presenting challenges: ""We are experimenting with innovative practices. Teamwork goes further. Alone we may be faster. Together we go less quickly, but we have good results, and likely sustainable results"" (Policy maker). There was evidence of partnership synergy in the ability of both partnerships to pull resources and problem-solve, to sustain interventions over time, despite contextual challenges and funding gaps: When the reform arrived, everything changed. We had to redevelop relationships, see who was going to be at the tables, did we want to keep our original territories or expand to the territories of the new [health authority]. [ . . . ] We then managed to identify a problem, we identified an intervention, which we put in place, which does not work, which we have adapted and are in the process of putting back in place (Research coordinator). Partnership synergy also manifested itself in the positive partnership atmosphere, in the feeling of unity, and the relationships that were forged in the process of working together: ""[ . . . ] It's a good trusting environment. People are happy to speak up and say what they need to say. Everybody seems to be happy to be involved"" (Policy maker). ""I usually see it as we all come together, sort of. I don't feel [ . . . ] that there's some difference between anyone, I feel like we're equally sitting at the same table, like this one single group"" (Patient partner). Other participants highlighted the ""feeling of belonging"" (Family physician) and increased project ownership: ""The commitment to the project is higher when you have built it together [ . . . ] When you have done it in collaboration, it is closer to your heart and I think that this is one of the advantages"" (Research coordinator). Table 3 displays correlations among partnership assets, the various categories of partnership processes and partnership synergy as assessed using Spearman's rank correlation coefficient. We used the following cut-offs for interpreting the scores: 0.10-weak correlation; 0.30-moderate; 0.50-strong (29) . The results indicate that partnership synergy outcomes were strongly associated with decision-making, leadership and administration and management, while partnership synergy processes were strongly associated with decision-making, leadership and non-financial resources. Total partnership synergy was strongly associated with decision-making, leadership, administration and management, non-financial resources and resource utilization. Table 3 . Spearman rank correlations among partnership synergy outcomes, partnership synergy processes, total partnership synergy and the different partnership assets and processes (n = 54). Values shaded in light grey denote weak correlations and in dark grey strong correlations (adapted from [63] We subsequently analyzed the scores for partnership synergy outcomes, partnership synergy processes and total partnership synergy in each of the five partnerships. Our oneway analysis of variance was suggestive of statistically significant differences between at least two of the medians of partnership synergy processes (p = 0.09) and (total) partnership synergy (p = 0.07). The differences in medians were statistically significant between the following partnerships: Primary Care Connection-Service Linkage, Community Outreach-Community Health Resources, Community Outreach-Diabetes Self-Management and Community Outreach-Service Linkage (in decreasing order of magnitude of difference). No difference was observed when sub-scales were analyzed separately. We then ranked the five partnerships on the basis of their aggregate scores for partnership synergy outcomes and processes and compared our results with independent, qualitative rankings of partnerships performed by content reviewers. The quantitative and qualitative rankings were completely coherent for partnership synergy outcomes, showing that partnership synergy was highest in the Community Outreach Partnership and lowest in the Service Linkage Partnership. However, there was a slight difference in the rankings of two partnerships by partnership synergy processes, namely, the Community Health Resources Partnership and the Diabetes Self-Management Partnership. Our quantitative analysis revealed that Partnership Synergy Outcomes and Partnership Synergy Processes sub-scales were strongly correlated with each other (r = 0.61), and there were few differences in how each of them correlated with the different dimensions of partnership work (see Table 3 ). We consequently chose to collapse the two synergy sub-scales together to a single Partnership Synergy scale (Partnership Synergy) for the analysis of associations at the partnership level. We also aggregated the scores at the partnership level for partnership assets and the following categories of partnership processes: non-financial resources, financial resources, resource utilization, decision-making, leadership, and administration and management (see Figure 2 ). Our newly developed sub-scales of Communication and External Environment did not perform well metrically, on assumptions of item-convergent validity, item-discriminant validity, and internal consistency. Therefore, we excluded these sub-scales from partnership-level analysis. The items related to the domains of communication, external environment, and benefits and drawbacks were analyzed independently for descriptive purposes only. We hypothesized that partnerships exhibiting higher partnership synergy would on average achieve higher scores for partnership assets and processes. On the other hand, partnerships exhibiting lower partnership synergy would on average achieve lower scores across various categories of assets and processes. Our data (Figure 2 ) indicate that stakeholders in the Community Outreach Partnership did report on average the highest scores on all categories of partnership assets and processes, with the exception of nonfinancial resources, and that the Service Linkage Partnership stakeholders reported on average the lowest or second lowest scores for all categories of assets and processes. Our within partnership correlation analyses to investigate relationships between partnership assets, processes and synergy did not yield conclusive results due to the small number of observations in each partnership. Figure 2 presents the complex picture of partnership work. All partnerships display variable scores, higher in some categories and lower in others. The figure illustrates that the composite elements constituting the performance of each partnership interact in a multitude of ways, and that there is no single path towards achieving a partnership synergy rank. We also observed that administration and management and decision-making processes appeared to be more critical to establishing the total partnership synergy rank than other processes. Irrespective of the variations in scores for different partnership assets and processes, the partnership-level scores for achieving the overall goal of developing a meaningful partnership were largely consistent with partnership-level partnership synergy scores. The majority of respondents in all partnerships reported that a meaningful partnership had been achieved leading to an uninformative median of four across all partnerships. However, as shown in Table 4 , there was considerable variation within the partnerships that was reflected in the means and standard deviations. The metric that corresponded best to both partnership synergy scores and the qualitative ranking was the percentage endorsing that the partnership goal was achieved very well or extremely well. Table 4 . Partnership-level assessment of the extent to which the goal of developing a meaningful partnership was achieved (reproduced from [63] ).  Table 5 below provides the comparison of our qualitative findings and quantitative results. There was a strong association between partnership synergy and non-financial resources (r s = 0.51). Both qualitative and quantitative findings suggest that intangible assets contributed to partnership synergy through having appropriately complementary and heterogeneous skill sets. Heterogeneity and complementarity were achieved by having a dynamic group composition that reflected the critical dimensions of the problem to be addressed and of the context that was likely to affect the work of the partnership. • Research funding facilitated partnership activities; other partners contributed in-kind resources such as time and space to meet and conduct partnership activities. Partners leveraged adequate financial resources to sustain interventions and partnership activities. • The majority of survey respondents (96%) reported the presence of financial and other capital resources across all five partnerships. There was a weak association between partnership synergy and financial and other capital resources (r s = 0.28). Qualitative and quantitative findings are partially consistent, suggesting that the importance of financial resources for partnership synergy related principally to supporting the coordinating infrastructure and a number of partnership activities (such as evaluation and outreach). Non-financial resources seemed to be more critical for partnership synergy than financial resources. Evidence of partnership synergy in the integration of non-financial and financial resources: in optimal and sustained level of commitment to the initiatives for most stakeholders, in collaborative learning relationships, in a variety of reported benefits for stakeholders' respective organizations, and in the ability of partners to leverage adequate financial resources. • Difficulty meaningfully engaging community-based stakeholders from or representing the target population, for both partnerships, and reduced engagement of some academic stakeholders in the Primary Care Connection Partnership. The majority of survey respondents reported high levels of engagement in partnership activities. The majority of survey respondents reported that their respective partnerships made very good or excellent use of their time (70%, excluding Primary Care Connection and Service Linkage) and other non-financial resources (67% excluding Primary Care Connection), with the highest rates reported in the partnership with the highest total partnership synergy score (Community Outreach). When asked how well the partners were able to include the views and priorities of the people affected by the partnership's work, the majority of survey respondents (59.2%) stated not so well or moderately well. There was a strong association between partnership synergy and resource utilization (r s = 0.50). The majority of respondents (78%) stated that the benefits of participating for the organizations that the stakeholders were representing in the partnership exceeded or greatly exceeded the drawbacks. The partnership with the highest total partnership synergy score reported, on average, the highest score for benefits, and the partnership with the lowest total partnership synergy score-the lowest. Both qualitative and quantitative results indicate that high levels of stakeholder engagement were important to achieve partnership synergy. The nature of engagement has to be aligned with the function of the partnership and the need to fulfil project objectives, with particular attention to meaningful engagement of end users and addressing disengagement. Benefits related to respective organizations seem to be more critical than personal benefits. Managing incentives, so that benefits outweigh costs, is an important consideration. There was a strong association between partnership synergy and leadership (r s = 0.74). Qualitative and quantitative results suggest that partnership synergy was facilitated by leadership capable of mobilizing the various perspectives of stakeholders. Qualitative results also highlight the contribution to partnership synergy of more distributed forms of leadership; however, the quantitative results suggest the limitations of the sub-scale in terms of assessing the extent to which leadership was distributed. • Research teams, including dedicated partnership resources, responsible for the operational running of the partnerships. • Adaptive management approaches, employed by the research teams, supported the work of the partnerships and facilitated the ability of partners to engage meaningfully, contributing to partnership synergy. • There was a strong association between partnership synergy and administration and management (r s = 0.61). Qualitative and quantitative results highlight the importance for partnership synergy of a core infrastructure and adaptive management approaches to support the work of the partnerships and facilitate the ability of partners to contribute meaningfully. • The majority of respondents reported being adequately or always informed regarding partnership's activities. Our newly developed Communication sub-scale did not perform well metrically. Qualitative findings point to the importance of timely and varied communication mechanisms in synergistic partnership learning. However, quantitative results were limited, precluding definitive conclusions regarding the contribution of communication to partnership synergy. • The Primary Care Connection Partnership harnessed the power of distributed decision-making and collaborative problem-solving, contributing to synergy. Stakeholders participated actively in the co-construction of the various aspects of the project, and some non-academic stakeholders fulfilled tasks outside the steering committee meetings. • Decision-making power was centralized within the research team in the Community Health Resources Partnership, consistent with the advisory nature of the partnership. • Partnerships with higher scores on the partnership synergy scale achieved higher scores on the decision-making sub-scale. There was a strong association between partnership synergy and decision-making (r s = 0.66). Qualitative and quantitative findings highlight the importance to partnership synergy of distributed decision-making and collaborative approaches to problem-solving. • The majority of stakeholders in two of the five partnerships (Primary Care Connection (75%) and Service Linkage (67%)) reported that their partnership had been affected a lot or a great deal by external factors, beyond the control of the partnership. The majority of Primary Care Connection respondents (75%) reported that the partnership had adapted to these external influences very well or extremely well. Our newly developed sub-scale of External Environment was not correlated with partnership synergy nor any other dimension of partnership work. Qualitative findings suggest that recognizing and dealing with changes in partnership context are important to achieve partnership synergy. Quantitative findings were limited in terms of assessing the contribution of external context to partnership synergy. This empirical exploratory study provided insights into complex and dynamic partnership processes and into how partnership assets are activated to produce partnership synergy. The Partnership Synergy framework posits that ""[ . . . ] synergy is the degree to which the partnership combines the complementary strengths, perspectives, values and resources of all partners in the search for better solutions and is generally regarded as a product of a partnership"" [6] . We have expanded this definition to incorporate the contribution of enabling (facilitating) processes in combining the tangible and intangible assets that are the inputs of the partnership. It is the combined effect of the inputs and enabling processes that confers advantages over the work of individual agents. Our first key finding of this study relates to the association between partnership synergy, tangible and intangible partnership assets and enabling collaborative processes. While enabling collaborative processes could be both operational and interpersonal in nature [5] , this study focused primarily on operational facilitators and system-level processes geared towards making ongoing adaptations to the evolving context. We assessed the role of facilitative leadership, supportive administration and management approaches, mechanisms of engaging partners and capitalizing on partners' resources, communication, and collaborative approaches to managing change. Our results (Table 3) indicate that partnership synergy was associated with partnership dimensions of leadership, administration and management, decision-making, and the ability of partnerships to optimize the involvement of its partners (referred to as ""Efficiency"" in the Partnership Synergy framework and referred to as ""Resource Utilization"" in our study). It was also associated with the sufficiency of non-financial resources. We could not perform correlation analyses to investigate relationships between partnership assets, processes and synergy at the partnership level due to the limited number of observations in each partnership. However, our comparison of values for partnership processes, the sufficiency of partnership assets and total partnership synergy by partnership ( Figure 2) indicates that among the partnership enabling processes investigated in our study administration and management and decision-making seem to be more critical to determining the achievement of partnership synergy. Our study findings also support the importance of recruiting partnership stakeholders with varied but complementary expertise and of benefits to the stakeholders' respective organizations. Our Communication and External Environment sub-scales did not perform well metrically [63] . However, the Spearman rank correlations between all three communication items and partnership synergy were moderate, supporting our qualitative conclusion that communication was an important dimension of partnership functioning. Conversely, our newly developed sub-scale of External Environment was not correlated with partnership synergy nor any other dimension of partnership functioning. The lack of correlation may be explained by the limitations of the sub-scale used. Alternatively, the lack of correlation in our study may be explained by the fact that the interface between a partnership and its context is complex and difficult to capture with standardized tools. Our qualitative investigation revealed that recognizing and dealing with changes in partnership context were important to achieve partnership synergy [62] . In addition, the synergy generated in both partnerships under the qualitative investigation facilitated adaptation to the challenging contextual circumstances and allowed the partnerships to continue. How these variables will interact under different types of contextual threats, such as the global COVID-19 pandemic, needs to be further examined. Given the importance of face-to-face interactions highlighted by our study participants, generating partnership synergy may be more challenging when moving to virtual forms of communication and interactions necessitated by the pandemic. Moreover, in our study, the nucleus of each partnership, which included the research team and a number of key non-academic stakeholders, remained consistent over time, while new members were invited to join based on the evolution of the projects and the need to attract specific expertise and additional resources at different points in time. Maintaining the consistent continuous core of members who provide continuity and keep the collaboration going may be impossible to achieve when there are sudden changes in priorities at all levels of the system and when key personnel and other resources need to be re-deployed. Changes in government priorities and policies may lead to a redefinition of partnership goals or may mean that the specific concerns driving the partnership have ceased to exist altogether [73] . However, the synergy generated by a well-functioning partnership that is already underway may help partners to quickly mobilize their strengths and reposition the partnership to tackle new priorities and vulnerabilities exposed by the pandemic. Our findings are consistent with prior research that has assessed relationships among different partnership functioning factors and partnership synergy [6, 50] . This research has analyzed the associations between partnership functioning factors, partnership synergy and the following partnership outcomes: effectiveness in the delivery of chronic illness care [56] , sustainability of innovative programs in community care [55] and perceived partnership sustainability and perceived community outcomes in sport-for-health partnerships [74] . Applying similar measures of partnership functioning and partnership synergy, studies demonstrated that partnership synergy was associated with the effectiveness of partnership leadership [6, 50, 55] , partnership efficiency [6, 50] , the effectiveness of administration and management [50] and the sufficiency of non-financial resources [50, 56] . It has also been established that partnership synergy acted as a mediator between partnership functioning and partnership outcomes (e.g., [56] ). In our study, partnership synergy emerged as an intermediate outcome of partnership functioning, before the intended outcomes related to PHC access could be assessed. Given the stage at which the partnerships were studied, and that identifying causal pathways was beyond the scope of this research, the study did not incorporate a separate effectiveness assessment of the intended outcomes. It therefore could not demonstrate definitively whether higher levels of partnership synergy precipitated better achievement of intended outcomes. However, the majority of survey respondents in all studied partnerships reported having developed a meaningful partnership that achieved both added value and benefits that largely outweighed the drawbacks of participation. In addition, a recent parallel study of IMPACT produced evidence that the intended goals of the program were largely met through partnerships [75] . The study by Spooner et al. [75] found that all IMPACT sites observed changes in patient abilities to access PHC and in provider capabilities to address the health care needs of vulnerable populations, even in interventions where there was no activity intentionally targeting provider behaviors. These findings are indicative that the partnerships were effective in reaching the intended outcomes. Our comparison of qualitative and quantitative findings revealed additional nuances in relation to the type of leadership that was employed in the partnerships that were the focus of our qualitative investigation. Prior studies have highlighted the critical importance of leadership to partnership synergy [6] . The study by Weiss et al. (2002) , using the same PSAT leadership scale, found that leadership was the dimension that was most closely associated with partnership synergy. The type of leadership that partnerships necessitate is facilitative and ""boundary-spanning""-enabling those in charge to appreciate and bridge the diverse perspectives of various stakeholders [8] . It needs to be shared formally and informally among stakeholders, even though there may be one ""lead"" organization [6] . This type of leadership, referred to with a variety of terms including ""plural"", ""collective"" or ""distributed"" leadership or simply DL, is located at the other end of the spectrum from individual leadership [76] [77] [78] . It refers to a ""collective phenomenon that is distributed or shared among different people, potentially fluid, and constructed in interaction"" [77] . The partnership that fit this definition based on our qualitative findings did not achieve the expected higher score for leadership. This observation suggests that while the leadership sub-scale used in our study tapped into its facilitative and boundary spanning nature, it might not have reflected its distributed power. Given the centrality of leadership to multistakeholder partnerships in general and partnership synergy in particular, future research should explore how the PSAT leadership scale could be amended to reflect DL. Taking into consideration the temporal and contingent nature of leadership, whereby certain situations and earlier partnership phases will require more centralized forms of leadership [76, 79] , it might not be possible to develop an all-encompassing leadership scale. This challenge may be addressed through the assessment of the various partnership components longitudinally and with the use of mixed methods, allowing the collection of different types of data. Our qualitative and quantitative results were also less consistent in relation to the dimension of financial resources. Our qualitative investigation demonstrated the critical importance of the funded coordinating infrastructure and that partnerships were capable of mobilizing adequate tangible resources to sustain service interventions. Our quantitative results revealed a high score for financial resources across all studied partnerships but a low correlation with most other dimensions of partnership functioning and Partnership Synergy Outcomes. This low correlation suggests that the Financial Resources sub-scale that we used might not have reflected the complexity in the types of tangible resources that were at play in IMPACT partnerships [63] . Alternatively, it may be explained by the fact that some multi-stakeholder health partnerships have few tangible resources. While financial and other capital resources are some of the building blocks sustaining partnership operations and partnership service interventions, the essence of partnership work lies in the production of knowledge and in relationship building. In fact, partnerships can be viewed as ""knowledge organizations"" that utilize intangible assets, such as the skills and expertise of partners, to convert information into knowledge [53] . While financial and other capital resources are important, non-financial resources seem to be more critical to creating partnership value including the ability of partnerships to make decisions, effective leadership and partnership management and administration. This finding is consistent with resource-based theory that suggests that intangible resources may be more valuable for they are more difficult to acquire and replicate [51] . Our second key finding relates to the contingent nature of partnership work and organizing to operationalize the partnership. We hypothesized that partnerships exhibiting higher partnership synergy would, on average, report higher levels of partnership functioning across various dimensions. On the other hand, partnerships exhibiting lower partnership synergy would, on average, report lower levels of partnership functioning across various dimensions. Our results indicate that the partnership with the highest total partnership synergy score achieved, on average, the highest scores for partnership functioning on most studied dimensions. Conversely, the partnership with the lowest total partnership synergy score achieved, on average, the lowest or second lowest scores for partnership functioning on all dimensions. However, the results for partnerships with middle total partnership synergy scores were more varied. The Primary Care Connection Partnership achieved a high synergy score despite a number of lower scores on some partnership functioning sub-scales. On the other hand, the Community Health Resources Partnership that achieved the second lowest total partnership synergy score performed well on some of the partnership functioning dimensions (such as resource utilization). These findings reinforce contingency theory concepts [80, 81] , in that they reflect the complexity in partnership work and suggest that there is more than one way of organizing and reaching partnership synergy. They also suggest that there might be certain partnership dimensions that are still lacking from our analysis. Taking into consideration the emphasis that the stakeholders in all five partnerships put on the quality of relationships, our assessment could have benefitted from incorporating measures of interpersonal collaborative processes and the quality of stakeholder relationships including such aspects as trust. In fact, prior research has highlighted relational factors, such as high levels of trust, to be important ingredients in a well-functioning partnership [82] [83] [84] , and trust has been identified as a critical determinant of partnership synergy [6] . A third key finding highlights the importance of assessing partnership synergy as an indicator of the health of the partnership. It might not be feasible or necessary to capture all possible dimensions of partnership functioning to be studied or assessed with standardized scales, for partnership synergy amounts to more than the sum of its parts [8, 50, 85] . Given that partnerships are inherently complex, highly contextual and constantly evolving organic entities, different factors may be more or less relevant depending on the specific circumstances and the goals of a partnership [78, 86] . At the start of the IMPACT program, the sites agreed to work with their respective local constituents in partnership. As the time progressed each partnership under our investigation evolved in different ways, addressing distinct PHC access needs, applying different ways of engaging stakeholders and developing unique ways of responding to contextual challenges. Therein lies the value of assessing partnership synergy as an important intermediate partnership outcome and a barometer for the health of the partnership. Confining partnership evaluation to assessing performance on individual partnership functioning indicators would provide important but partial information on how a partnership scored vis-à-vis set performance targets or, in comparative research, other partnerships. This partial analysis may lead us to argue that the Primary Care Connection Partnership that achieved lower partnership functioning scores on a number of dimensions did not perform well. However, the high partnership synergy score for this partnership depicts a different picture, pointing to the effectiveness of dynamics and processes and a more ""holistic"" view of the partnership. We could not assess partnership synergy longitudinally because this study commenced when the partnerships had already achieved maturity. Evaluating partnership synergy, qualitatively and quantitatively, as an evolving indicator of partnership health, at different points in the partnership's life, would allow the collection of data on other processes that might have contributed to higher (or lower) levels of partnership synergy over time. In addition, observing fluctuations in partnership synergy and partnership functioning dimensions scores may allow researchers to infer the relative importance of these dimensions to synergy. Initiating and sustaining partnerships is not easy; it is a complex, dynamic and time-consuming process that involves multiple tasks and requires certain vital components [36, 46] . This study contributes to the evolving body of knowledge on partnership synergy as a useful framework for studying collaborative ventures and identifies some of the critical requirements for synergistic partnerships. These requirements should be considered in order to determine if working in partnership should and can reasonably be pursued. Though the transferability and generalizability of this study's findings may be limited due to the small sample size, the findings are likely robust for academic partnerships with partners in the formal health care system. Despite the specificity of partnerships under this investigation, there was enough heterogeneity among them in terms of their contexts, their composition and how the partnerships evolved, suggesting relevance of these findings across various settings. It has been argued in the partnership literature that despite the diversity of partnerships, there are a number of common features influencing their effectiveness, and that the underlying principles behind creating and sustaining effective partnerships are generic [87] . The concepts used in this study in general and the PSAT in particular have demonstrated robustness in assessing the quality of partnership processes and outcomes in multi-stakeholder partnerships in several areas including public health, health promotion and chronic illness care [2, 6, 56] . The PSAT has also been identified as a valid tool for measuring group processes in interprofessional health and social service partnerships at the front-line service provider group level [2] . However, the evaluation approach and the PSAT will need to be tailored to different partnership types and local conditions. For example, community-driven partnerships with fewer resources, a decentralized partnership structure and more distributed decision-making may need to adapt the Leadership and Administration and Management sub-scales. Alternatively, partnerships may prioritize different aspects for evaluation altogether. The list of variables offered by the Partnership Synergy framework allows partnership practitioners and evaluators to select those relevant to a particular partnership. The findings of this research need to be considered in light of its limitations. First, our findings are based on a relatively small sample of partnership stakeholders who were part of a single funded program of research with a specific focus of enhancing accessibility to PHC for vulnerable populations. Caution is warranted when transferring these findings to partnerships operating in different political, resource and health care delivery environments. Second, the statistical power of our quantitative conclusions is limited due to the small sample size. Third, while we made attempts to ensure the collection of a diversity of opinions, given the voluntary nature of engagement and the timing of data collection, such that those who did not see value in the partnerships would have resigned, participants may have provided a more favourable assessment of their respective partnerships. Finally, our analysis only included a number of constructs, based on the selected theoretical frameworks and on our qualitative data. We may not have captured all of the manifestations of partnership synergy, assets and processes. The raw qualitative data are not publicly available due to privacy restrictions given the small sample and the qualitative nature of inquiry. The raw quantitative data are available at: [63] .@story_separate@This research was conducted with the aim of understanding partnership processes and how various partnership factors contribute to partnership effectiveness. The partnerships involved stakeholders from different organizations with an interest in implementing organizational solutions that enhance access to primary health care for vulnerable populations. Effectiveness was conceptualized both in terms of the achievement of intended outcomes and the effectiveness of processes (the internal health of the partnership). This research offers several original contributions to the theory and practice of multi-organizational multi-stakeholder health partnerships. First, it demonstrated the applicability of and further refined the theoretical concept of partnership synergy as an indicator of the health of a partnership and an intermediate outcome of working in partnership. Second, the research affirmed the importance of investing in the assessment of partnerships and contributed to the emerging knowledge on how to evaluate them. Third, by demonstrating an association between partnership assets, enabling processes and synergy, this research underlined the operational importance of investing in and paying attention to partnership processes as part of achieving both partnership synergy and the intended outcomes of multi-stakeholder partnerships. Supplementary Materials: The following are available online at https://www.mdpi.com/article/10 .3390/healthcare9081060/s1, Table S1 : Mapping of qualitative categories onto the PSAT scales. Informed Consent Statement: Informed consent was obtained from all participants involved in the study.","Partnerships are an important mechanism to tackle complex problems that extend beyond traditional organizational divides. Partnerships are widely endorsed, but there is a need to strengthen the evidence base relating to claims of their effectiveness. This article presents findings from a mixed methods study conducted with the aim of understanding partnership processes and how various partnership factors contribute to partnership effectiveness. The study involved five multi-stakeholder partnerships in Canada and Australia working towards improving accessibility to primary health care for vulnerable populations. Qualitative data were collected through the observation of 14 partnership meetings and individual semi-structured interviews (n = 16) and informed the adaptation of an existing Partnership Self-Assessment Tool. The instrument was administered to five partnerships (n = 54). The results highlight partnership complexity and the dynamic and contingent nature of partnership processes. Synergistic action among multiple stakeholders was achieved through enabling processes at the interpersonal, operational and system levels. Synergy was associated with partnership leadership, administration and management, decision-making, the ability of partnerships to optimize the involvement of partners and the sufficiency of non-financial resources. The Partnership Synergy framework was useful in assessing the intermediate outcomes of ongoing partnerships when it was too early to assess the achievement of long-term intended outcomes."
"RNA variations are increasingly in the focus of molecular biology, genetics and medicine. Known disease-causing RNA variants mainly affect proteins. Non-coding RNAs have gained interest and have been shown to have numerous functions. Nowadays, tens of types of RNA molecules are known. Those are functional or related to numerous cellular processes and functions ranging from catalysis to regulatory processes, from information transfer for protein synthesis to function in cellular machineries, from RNA base and sugar modification to RNA interactions, and so on. Terminology for RNA molecules, functions and processes is still confusing. There is not even consensus for the definition of RNA -apart from being chemically a polynucleotide [1] . Common language and concepts are needed for efficient communication. In addition to human information transmission, computational analyses are not possible without systematic data presentation. Variation Ontology (VariO, http://variationontology.org/) was developed for systematic description of variations and their consequences, effects and mechanisms [2] . VariO includes terms for all kinds of alterations in DNA, RNA and protein. DNA and protein descriptions and systematics have been more mature than those for RNA, largely because novel types of RNA molecules, their functions and effects are reported continuously. Here, RNA terms and the hierarchical structure in VariO are discussed. VariO annotations can be made on three molecular levels -DNA, RNA and protein -and for four types of information: variation type, structure, function, and properties. This article concentrates on RNA variations, detailed descriptions of DNA and protein variants have been published previously [3, 4] . VariO annotations are made in relation to a reference state, which can be a reference sequence, wild-type property or similar. A new VariO version includes all the additions and modifications made during the preparation of this article. By combining VariO with other systematics even more detailed and nuanced systematic descriptions can be made. When using VariO, systematic annotations, include the VariO prefix and a number followed by the term. The notation VariO:0319 means the same as the name of the term, i.e. 'RNA deletion'. When using annotations, both the number and the prefix 'VariO' must be included. Although the names of the terms can be obtained automatically, full names must be provided when the information is intended to be read by people. This article follows the VariO RNA term hierarchy on the four sublevels (variation type, function, structure and properties). The headings and subheadings are VariO terms. For clarity, VariO terms are written in quotation marks in the text. We have published guidelines for the annotation process [5] . The variation type annotations can be generated automatically with VariOtator annotation tool [6] that makes it easier to obtain systematic and coherent annotations. The other types of annotations are made manually, the interactive VariOtator tool can be used for that purpose. VariO is available in several ways in addition to the ontology website (http://variationontology.org/) including Ontology Lookup Service (https://www.ebi.ac.uk/ols/ontol ogies/vario), OBO Foundry (http://www.obofoundry.org/ ontology/vario.html), NCBO BioPortal (https://bioportal. bioontology.org/ontologies/VARIO) Ontobee (http://www. ontobee.org/ontology/VariO), FAIRsharing (https://fair sharing.org/bsg-s000776/) and others. Despite extensive databases for experimental RNA data are available, many features and properties of RNA molecules have to be addressed with computational prediction methods. Here, many of these tools are introduced in the context of the VariO annotations. The prediction method choice is an important step. Systematic benchmark performance assessments provide the most reliable information for the choice. The problem is that such studies are missing for many RNA related predictions, largely due to limited amount of known experimentally verified cases. Guidelines have been published for systematic reporting of predictor performance and implementation [7, 8] . This article introduces the hierarchy and use of RNA related terms in VariO. The description of almost all the terms is followed by cases from literature or from databases to provide examples for the use of annotations.@story_separate@Several databases distribute RNA information, examples are shown in Table 1 . Sequences in databases like GenBank and EMBL for coding sequences are for sense strand and thus identical to mRNA sequence except for containing thymine instead of uracil. Databases managed at Leiden Open Variation Database (LOVD) management system [9] are the major sources for genetic variation information. Strangely enough, RNA level descriptions are missing from the majority of LOVD databases, even when protein changes are included. IDbases for primary immunodeficiencies [10] are an exception as they include also the RNA changes. RNAcentral [13] is a portal that contains various information for non-coding RNAs originating from 28 databases. APASdb [14] is a dedicated resource for alternative polyadenylation sites. Many RNA molecules are matured by splicing, there are many databases about different aspects of splicing and splicing products. ASpedia [15] contains data for alternative splicing in human and DBASS3 and DBASS5 [16] for disease-related alternatively spliced RNAs. ExonSkipDB [17] is a registry for one special form of alternative splicing. SASD [18] was developed for proteomics studies to detect consequences of alternative splicing. In MiasDB there are data for interactions of molecules involved in alternative splicing [19] . Numerous web services are available for short and long non-coding RNAs (ncRNAs). In DASHR [20] there are genes and sequences as well as tissue and cell type information for six types of ncRNA sequences. Several databases have been developed for microRNAs (miRNAs) including miRmine for expression profiles [21] , miRTarBase [22] for target details, miRPathDB [23] for target and pathway details, HMDD [24] for disease associations and miRwayDB [25] for miRNA pathway associations in diseases. miRandb combines various data items for miRNAs [26] . piRBase [27] contains sequence and function annotations and piRDisease disease associations for piwi interacting RNAs (piRNAs) [28] . LncVar contains variations in long non-coding RNAs (lncRNAs) [29] . lncRNAs are called for competing endogeneous RNAs (ceRNAs) when they bind to miRNA and regulate their functions. In LncCeRBase [30] there are details for ceRNAs, and lncRNAs containing genomic variants that disturb ceRNA network regulation [31] . All kinds of interactions of lncRNAs are available in LIVE [32] . Variations in lncRNAs can be found from lncRNASNP [33] and diseaseassociations from LncRNADisease [34] . T-psi-C is a database for tRNA sequences and structures [35] . Information about circular RNA (circRNA) disease associations can be obtained from Circ2Traits [36] and circRNA disease [37] databases. In addition to splicing, RNAs undergo various other modifications, data for modifications, pathways, modifying enzymes and modification locations within sequences are available in MODOMICS [38] . siRNAmod [39] contains modified siRNAs. RNA editing-related resources include EDK for disease associations [40] , REDIportal for human A to I editing events [41] , and LNCediting for functional effects of lncRNA editing [42] . Data for N6-methyladenosine methyltranscriptome is available in MeT-DB [43] . Subcellular localization of RNA molecules varies, details are in RNALocate [44] . RNAs have been grouped to families based on sequence analysis in Rfam [45] . Rfam contains some secondary structure data, predicted secondary structural information can be found also from RNAStructuromeDB [46] . Experimentally determined RNA structures along with other details are in RNApdbee [47] . Nucleic Acid Database (NDB) [48] is a similar central portal as Protein Structure Database (PDB) for experimentally determined protein structures. Many RNA structures are also in PDB. RNA families and structures are classified in several levels in RNArchitecture [49] . BGSU RNA Site contains 3D structure information organized into RNA Structure Atlas, RNA 3D Motif Atlas and Representative Sets of structures. Several resources have been developed for RNA data in relation to cancers. YM500v3 [50] contains many data items for short RNAs and RNA-Seq datasets and information for their relevance to cancer. Another ncRNA resource is ncRNA-eQTL for expression profiles [51] . Lnc2Cancer [52] and CrlncRNA [53] contain experimentally supported lncRNAcancer associations. Chimeric RNA transcripts, their threedimensional contact maps and relevance for druggability can be searched from ChiTarRS [Balamura54]. The first category is 'VariO:0306 RNA variation type', which provides a brief description of the variation with commonly used terms (Fig. 1) . These terms are not intended to replace Human Genome Variation Society (HGVS) names [55] or the International System for human Cytogenetic Nomenclature (ISCN) [56] instead to complement them. Naming conventions are an example of additional systematics used to provide rich and informative annotation. VariO terms provide simple, yet comprehensive descriptions. There are two sublevels for the RNA variation type descriptions: RNA variation classification and RNA variation origin (Fig. 1 ). There are examples for almost all the terms (a representative example may be shown for one term in a case of very similar terms). In the examples, HUGO Gene Nomenclature Committee (HGNC) names [57] were used for genes. Variants are indicated by prefix r. in RNA, when gene, coding region, mitochondrial or protein variants are discussed, the prefix is g., c., m. or p, respectively. There are eight categories of RNA chain variation types, some of them with subcategories ( Figs. 1 and 2) . r.3g>u substitution in BTK [58] can be annotated as 'VariO:0317 initiation codon change', it is also 'VariO:0312 RNA substitution' of type 'VariO:0316 transversion'. This variation prevents Bruton tyrosine kinase (BTK) expression and causes X-linked agammaglobulinemia (XLA). RNA substitution is either of transversion or of 'VariO:0313 transition' type. Transitions are further categorized as 'VariO:VariO:0315 purine transition' or 'VariO:0314 pyrimidine transition'. RNA substitutions are further categorized based on the effect to coding region. 'VariO:0308 missense variation' is an RNA variation that causes amino acid substitution at protein level. Amino acid substitutions are frequently and erroneously called as missense variants [59] . The sense in the name refers to the information in mRNA triplet code. r.1559g>a missense variation due to c.1559G>A substitution causes p.R520Q substitution in BTK [60] . In 'VariO:0310 nonsense variation' the RNA [63, 64] and subsequent deletion of 21 residues from the protein leading to XLA. This deletion is of type 'VariO:0320 in-frame deletion'. mRNAs containing 'VariO:00321 out-of-frame deletion' are more common as the length of the in-frame deletion has to be divisible by three to retain the reading frame and are thus rarer. Out-of-frame deletion-containing RNAs are typically destroyed by NMD unless the variant is located towards the end of the coding region. c.1953del in BTK gene leads to p.L652* protein truncation and XLA [65] due to 'VariO:00321 out-of-frame deletion'. In-frame and out-of-frame terms are relevant only on the coding regions in mRNAs and not for variants in other types of RNAs and not for protein variants [59] . 'VariO:0311 RNA indel' contains both inserted and deleted components. An example is r.1682_1683delinsa in BTK [66] which is also 'VariO:0031 out-of-frame indel'. r.1401_1402delinsuu in BTK [67] is a 'VariO:0030 in-frame indel'. This variation causes a 'VariO:0029 sequence retaining amino acid indel' in BTK protein and leads to XLA. BTK r.1812_1813insgacagu is a 'VariO:0326 RNA insertion' introducing additional nucleotides. This XLA-causing variant [61] is of type 'VariO:0332 in-frame insertion'. The other type of coding region insertions is 'VariO:0327 out-offrame insertion'. When a sequence stretch in RNA is moved to a new location it is called for 'VariO:0241 RNA translocation'. In 'VariO:0244 RNA inversion' sequence is inverted to its original place. If the stop codon is altered to code for an amino acid 'VariO:0309 termination codon change' occurs. r.1195u>c transition in KISS1R for KISS1 receptor 1 modifies stop codon and causes normosmic congenital hypogonadotropic hypogonadism [68] . RNA variation origin has two subclasses, 'VariO:0307 RNA variation of genetic origin' and 'VariO:0333 RNA variation of non-genetic origin' (Fig. 1) . Variants of genetic origin appear at DNA level and are copied to RNA. BTK r.1559g>a missense variation leading to p.P520Q substitution [60] is of genetic origin. Several variation types are of non-genetic origin. Adenosine deaminase (ADA) gene has nine alternative transcripts which are of type 'VariO:0329 alternative RNA form', see LRG_16 in Locus Reference Genomic database [69] . r.1442g>c substitution causing p.C481S substitution in a BTK construct is a 'VariO:0247 artificial RNA variation' [70] . c.2978delG variation in NPC1 is recognized by NMD [62] making the RNA 'VariO:0335 decayed RNA'. 'VariO:0334 epigenetic RNA variation' has been included for consistency, although such cases are not known. This is not to be confused with cases where RNA molecules are involved in silencing and regulation of DNA expression. Many types of RNA molecules are heavily modified and alterations to modifications are related to many human diseases ranging from neurological diseases to diabetes, cancer and mitochondrial diseases [71] . A to I RNA editing modifies RNA sequence and is involved, e.g., in breast cancer [72] . This is a form of 'VariO:0336 modified RNA'. 'VariO:0436 mature RNA' is an RNA form that has been completely modified. mRNA molecules used for translation are examples of mature RNAs. During translation, ribosomes read mRNA molecules in triplets. When ribosome slips back one base pair or forward one base pair happens ribosomal frameshifting. As the consequence, the translated protein is different from this site onwards. In some organisms this process is intentionally used to generate more than one type of protein from a single mRNA with programmed frameshifting. It is beneficial especially for virus genomes, which must be compact to fit into the capsid. Random frameshifting leads to amphigoric protein sequence if the protein is translated. Human immunodeficiency virus 1 (HIV-1) gag-pol fusion protein [73] is due to 'VariO:0409 frameshifted RNA'. HIV-1 gag-pol is also an example of 'VariO:0408 RNA chimera'. Chimeras can emerge with different mechanisms including, e.g., read-throughs of adjacent genes, juxtaposition of transcripts encoded by genes on different chromosomes, and from noncontiguous genes within the same chromosome. Human yippee like 5 (YPEL5) and protein phosphatase 1 catalytic subunit beta (PPP1CBI) genes form recurrent and reciprocal chimeras in chronic lymphocytic leukaemia [74] . 'VariO:0396 variation affecting RNA function' has six categories. Mitochondrial m.1616A>G substitution in tRNA-Val gene and its RNA form r.1616a>g cause mitochondrial myopathy, encephalopathy, lactic acidosis and stroke-like episodes (MELAS) syndrome [75] . It is an example of 'VariO:0401 effect on amino acid transfer of RNA'. Ribozymes are RNA molecules with catalytic activity. These molecules are involved in sequence-specific intramolecular cleavage of RNA. r.a28u transversion inactivates hammerhead ribozyme [76] and has a 'VariO:0398 effect of catalytic RNA activity'. miRNAs silence genes and regulate gene expression. miR-140 regulates expression of several genes in chondrocytes. Seed region variation r.24a>g in MIR140 leads to gain of function activity and causes human skeletal dysplasia [77] due to 'VariO:0400 effect on regulatory function of RNA'. r.3g>u substitution in BTK [58] initiation codon stops the flow of genetic information via RNA to protein and has a 'VariO:0402 effect on RNA information transfer'. r.777_839del in-frame deletion in BTK causes XLA because of deletion of residues 260-280 from the protein distorts the structure of the Src homology 3 (SH3) domain [64] . This is an example of 'VariO:0397 effect on RNA splicing function' due to exon skipping. r.1559g>a missense variation causes p.R520Q substitution in BTK [60] as 'VariO:0399 effect on translation'. Variations affect several properties of RNAs and have general annotation 'VariO:0298 variation affecting RNA property'. XLA-causing r.1559g>a missense variation in BTK has 'VariO:0304 association of RNA variation to pathogenicity' [60] . It has also 'VariO:0302 conservation of variation site' property as the position is highly conserved. Nieman-Pick disease-causing c.2978delG variation in NPC1 mRNA is degraded by NMD and thus has 'VariO:0010 effect on RNA abundance' [62] . RNA catalytic activity changes are described with 'VariO:0439 effect on RNA activity' and terms at three more specific terms. Replacements at conserved adenosines 248 or 249 in the J5/15 region of RNase P, a ribozyme, have 'VariO:0441 effect on RNA affinity' [78] . Variants were introduced to Tetrahymena thermophila group I ribozyme to optimize and to have 'VariO:0442 effect on RNA specificity' [79] . These variants have also 'VariO:0440 effect on RNA reaction kinetics'. Degradation of RNA molecules is important for control of their function. Most RNA molecules are short-lived. m.1625c>t substitution in mitochondrial tRNA-Val gene MT-TV leads to very low steady-state levels (<1%) of normal mt-tRNA-Val because the variant tRNA remains deacylated and is rapidly degraded [80] . Individuals who carry the variant have profound metabolic disorder that often causes neonatal death due to 'VariO:0299 effect on RNA degradation'. Repeats of pentapeptide microsatellites in the shared exon of brain expressed associated with NEDD4 1 (BEAN1) and thymidylate kinase 2 (TK2) are responsible for spinocerebellar ataxia type 31 (SCA31) [81] . These RNAs are toxic and form aggregates called RNA foci that disrupt structure of RNA-binding proteins and have 'VariO:0300 effect of RNA folding' at RNA level. Iron-responsive elements (IREs) are RNA interaction motifs. Variations at IRE in the transcripts for ferritin light chain (FTL) gene cause hereditary hyperferritinemia -cataract syndrome (HHCS) with increased serum ferritin levels and early-onset cataracts [82] . IRE motifs in RNA interact with IRE binding proteins, which regulate the translation and stability of target transcripts in the iron metabolic pathway. r.g41c variant (Verona) in the IRE region causes 'VariO:0305 effect on RNA interaction'. CNG-triplet repeats (N indicating any nucleotide) are frequently behind neuromuscular diseases. CUG-repeats in DM1 protein kinase (DMPK) transcripts form labile aggregates and are annotated with 'VariO:0364 effect on RNA aggregation' [83] . Many mRNAs that are directed to compartments contain one or multiple localization signal sequences (zipcodes), which are recognized by zipcode binding proteins. Diaphanous-related formin 1 (DIAPH) mRNA localizes to endoplasmic reticulum in fibroblasts independent of zipcodes. Frameshift-causing variant in DIAPH loses perinuclear localization of the transcript [84] , hence 'VariO:0363 effect on RNA localization'. HHSCcausing FTL double variant r.18c>u, r.22u>g (Pavia2) has 'VariO:0301 effect on RNA stability' since it reduces the thermal stability of the IRE-containing RNA. RNA structure and architecture have several levels and there are large differences in the structures as there are different forms of RNA and of widely different sizes ranging from short polynucleotides, like siRNAs, to long noncoding RNAs and RNA genomes i.e. from less than 20 nucleotides to molecules of millions of nucleotides. In addition to the single-stranded form there are double and multiple stranded RNA forms. r.22u>c transition at the D-stem in mitochondrial MT-TL1 for tRNA-Leu(UUR) is related to hypertension because of 'VariO:0308 variation affecting RNA structure' [85] . Genomes in many organisms are pervasively transcribed to large spectrum of RNA forms. There are two major types of affected RNA type, namely 'VariO:0350 non-protein coding RNA' and 'VariO:0351 protein coding RNA', see Fig. 3 . HGNC provides official gene symbols. Now they are working also on non-coding RNAs, currently there are systematic names for more than 7000 RNA genes in 9 categories [86] . Classification principles have been presented for long noncoding RNAs [87, 88] , however, these schemes have included concepts and levels that are not readily comparable and thus a new systematic classification is introduced for all non-coding RNAs as well as for coding RNAs, see Fig. 3 . There are six categories for non-coding RNAs including antisense and sense RNA, as well as untranslated region of protein-coding mRNA. Functional non-coding RNAs contain several groups, and classification based on location or origin of the RNA allows further details. The length of the RNA is an important factor, thus for their grouping there are terms based on the size of the polynucleotide. The goal of the classification is to include all types of ncRNAs, however, not to have terms to name all of them, because the field is rapidly developing and for annotation can be used other systematics, as well. Further, the terminology has not yet been established for all the transcripts. Antisense RNA molecules are complementary to mRNA region, while sense RNA overlaps with mRNA, but is not involved in protein coding. Both are single-stranded molecules. 'VariO:0463 Antisense RNA' can block translation by hybridizing with mRNA. These transcripts are important regulators of protein expression and have biotechnological and therapeutic applications. The 3ʹ UTR region of DM1 protein kinase (DMPK) gene contains CTG repeats. Healthy people have few tens of repeats while patients with myotonic dystrophy type 1 (DM1) have more than 100 repeats, often even thousands of them. The repeats are part of CTCF insulator that regulates the expression of DMPK. An antisense transcript extends from the regulatory region of the adjacent gene to the CTF repeats. Extensive number of repeats affects the length of the antisense transcript and thereby gene regulation [89] . The regulation of DMPK expression is complex and involves also sense RNA, which is single-chain RNA that overlaps mRNA but is non-coding and annotated as 'VariO:0464 sense RNA' [89] . Congenital myotonic dystrophy (CDM) is the most severe form of the DMPK-related diseases. Analysis of a mouse model indicated that sense RNA, which contains the extended CTG repeats and surrounding regions, forms foci and co-localizes with muscleblind like splicing regulator 1 and 2 (MBNL1 and MBNL2) proteins [90] . The produced sense RNAs have unusual structures and aggregate together with the proteins and thereby affect numerous regulatory processes. 'VariO:0353 non-coding region of mRNA' in the mature mRNA contains the 3ʹ and 5ʹ flanking regions, which are essential, e.g., for regulation and translation. Functional non-coding RNA 'VariO:0465 Functional non-coding RNA' has several subclasses (Fig. 3) . Several computational tools have been developed for identifying functional RNAs, however not discussed in here because of being outside the topic, i.e., variations in the molecules. Ribozymes catalyse sequence-specific intramolecular cleavage. Variants like r.28a>u transversion in hammerhead ribozyme [76] appear in 'VariO:0476 catalytic RNA'. Genome editing, most notably with systems developed from the bacterial antiviral defence system with CRISPR/ Cas9, is now widely used in research laboratories and biotechnological applications are under development. Editing is facilitated by guide RNA, a form of 'VariO:0484 DNA editingrelated RNA' that directs the Cas9 nuclease to digest and remove or add new genetic material. The guide RNA detects a specific DNA location to be edited based on sequence complementarity. One of the early examples is the correction of a β-thalassaemia-causing double variant in human haemoglobin subunit beta (HBB) gene [91] . This variant is common among Chinese patients. The first variant is c.-78A>G in the promoter and the other c.126_129del deletion of 4 nucleotides coding for codons 41 and 42. mRNA editing in humans and many other organisms is not RNA guided. Small nucleolar RNAs (snoRNAs) in many organisms and guide RNA (gRNA) in kinetoplastid protists are involved in RNA editing as 'VariO:0483 RNA editing-related RNA'. snoRNAs guide methylation and pseudouridylation especially of rRNAs. Deletion of two bp (TT) from the small nucleolar RNA, C/D box 50A (SNORD50A) gene appears frequently in prostate [92] and breast cancer [93] cells. The variant is homozygous in prostate cancer but often heterozygous in breast cancer. SNORD50A expression is reduced due to the variation. The expression of wild-type form inhibits cancer cell growth. Retrotransposons are genetic elements that can amplify themselves via ribonucleoprotein complex where the RNA transcript is reverse transcribed and integrated into a new position in the genome. The RNA transcript is a 'VariO:0466 retrotransposon RNA intermediate'. Only a very small number of retrotransposons in a genome is transposable after insertion [94] . Several transcripts act as 'VariO:0477 regulatory RNA'. miRNAs are typically about 22 nucleotides long RNA molecules that have a distinctive stem-loop structure. They regulate and silence gene expression. MIR140 regulates expression of many chondrocyte genes. Seed region recognizes target mRNAs. Variation r.24a>g in MIR140, annotated as 'VariO:0478 microRNA', causes human skeletal dysplasia [77] . Several methods are available for miRNA target prediction and some for predictions of effects of variants in miRNA sequences [95, 96] . Other line of tools addresses miRNAdisease associations [97] [98] [99] 'VariO:0479 small intervening RNA', abbreviated as siRNA, is 20-25 bp long double-stranded RNA. It functions in RNA interference (RNAi) pathway to regulate gene expression by directing targeted mRNAs for degradation and thus preventing protein production. Although siRNAs can be designed to silence disease-related variants, it is a daunting task where various aspects and predictions have to be taken into account [100] . This article reviewed also available computational tools. The performance of methods for bacterial small RNA target prediction has been benchmarked [101] . For siRNA silencing efficacy prediction there are many tools [102] [103] [104] [105] 'VariO:0480 Piwi-interacting RNA', piRNA, interacts with piwi-subfamily Argonaute proteins that are mainly involved in post-transcriptional silencing of transcripts for repeat sequences, especially of transposable elements. mRNA is spliced in a large protein-RNA complex that contains 'VariO:0485 spliceosomal RNA'. Homozygous g.55G>A variation in RNU4ATAC gene for RNA, U4atac small nuclear (U12-dependent splicing) caused microcephalic osteodysplastic primordial dwarfism type I (MOPD I) with associated pigmentary disorder [106] . The encoded U4atac is a minor spliceosomal RNA. Signal recognition particles are evolutionarily conserved protein-RNA complexes in cytoplasm. They recognize and target specific proteins to plasma membrane in prokaryotes during translation and to endoplasmic reticulum in eukaryotes. They contain 'VariO:0481 signal recognition particle RNA'. Xist is a lncRNA and 'VariO:0482 X-chromosome inactivation center RNA' functional in X-chromosome inactivation (XCI) where one copy of the X-chromosomal genes is inactivated in females to compensate for the presence of only one copy in males. Deletion of one of the highly conserved Xistspecific repeat elements, repeat E, increases the expression of a number of XCI escape genes [107] . Telomeres are repetitive regions in the termini of chromosomes. They protect the chromosomes and are shortened gradually during chromosome replication as the Okazaki fragment binding region is not copied. Telomerase is a ribonucleoprotein complex that adds telomere repeats to the 3ʹ ends of telomeres to compensate for loss of sequence during replication. 'VariO:0475 telomerase RNA' is used as the template for repeats. Variations in telomerase RNA are associated for example to dyskeratosis congenita and aplastic anaemia. Cellular experiments show that disease-associated point variants in the pseudoknot and template regions of telomerase RNA lead to lower level of the RNA and much shorter telomeres [108] . Several RNA components act as 'VariO:0468 translationrelated non-coding RNA'. Proteins are synthesized at ribosomes that consist in addition to several protein components also 'VariO:0493 ribosomal RNA' in the subunits that are annotated as 'VariO:0469 large subunit ribosomal RNA' and 'VariO:0470 small subunit ribosomal RNA'. m.1555A>G substitution in the small mitochondrial subunit 12S rRNA is associated with non-syndromic deafness [109] . The patients have also increased susceptibility to the ototoxic effects of aminoglycosides since several antibiotics target ribosomes and rRNAs. 'VariO:0471 intron' is a non-coding region located between exons in a pre-mRNA and are cleaved during maturation. Introns appear also in many non-protein coding RNAs from which they are cleaved during maturation. 'VariO:0473 spliceosomal intron' is cleaved at spliceosome. tRNA intron is cleaved by a tRNA splicing endonuclease, while 'VariO:0472 self-splicing intron' is removed autocatalytically. r.777_839del in BTK intron 3 position −1 causes exon skipping and is a spliceosomal intron variation [64] . 'VariO:0403 Group I intron' and 'VariO:0404 group II intron' are self-splicing introns that have catalytic activity to cleave and join the RNA chain. Group I introns appear in rRNA, mRNA and tRNA genes in bacterial genomes, in lower eukaryotes in mitochondrial and chloroplast genomes as well as in rRNAs. Group II introns can be found in all domains of life. 'VariO:0468 transfer RNA' molecules contain anticodon sequence that recognizes mRNA triplets. Loaded aminoacyl-tRNAs bring amino acids to ribosomes to be added to the elongated protein chain. Variants in these RNA molecules impair protein synthesis as an r.1616a>g substitution in tRNA-Val that causes MELAS syndrome [75] . Some 600 tRNA genes are coded by the human genome. Human mitochondrial genome codes for 22 tRNAs, the other tRNAs are imported nuclear-origin molecules. Several diseases are caused by variants in mitochondrial tRNA genes. PON-mt-tRNA is a predictor for disease relevance of mitochondrial tRNA variants [110] . Predictions for all the possible substitutions in all positions are available for all the 22 human mitochondrial tRNAs. Certain RNA molecules are located to special compartments or cellular regions, there are also nuclear and mitochondrial RNAs, described by 'VariO:0486 location or origin of noncoding RNA'. DM1-causing CTG repeats in the antisense RNA molecule [89] are in 'VariO:0487 nuclear non-coding RNA'. MELAS syndrome-associated variations in tRNA-Val [75] are in 'VariO:0488 mitochondrial non-coding RNA'. The expression of 'VariO:0490 centromeric RNA' is linked to chromosome segregation [111] . Some lncRNAs act at chromatin-modifying complexes as 'VariO:0489 chromatin-related non-coding RNA' to regulate gene expression [112] . 'VariO:0501 RNA fragment' describes RNA molecules that are products to RNA degradation. For example, tRNAderived fragments are functional in some cellular responses and in cancers [113, 114] . 'VariO:0491 size of non-coding RNA' can be classified into three categories. 'VariO:0492 short non-coding RNA' molecules are shorter than 200 nucleotides, often substantially shorter than the threshold. miR-140 which contains variants in skeletal dysplasia miRNA [77] is a short non-coding RNA. 'VariO:0495 very long non-coding RNA' chains are longer than 10 kb and ranging up to 1 Mbp. These molecules regulate expression on many genes, for a review see [115] . 'VariO:0494 long noncoding RNA' is a category for molecules between the two classes. Many methods have been developed to predict lncRNA interactions and disease-association. Studies for variation effects have started to emerge [116] . There are several forms also of 'VariO:0351 protein-coding RNA'. Alternative splicing is a common mechanism regulating gene expression [117] and increasing proteome complexity and can be detailed as 'VariO:0411 alternatively spliced mRNA' of 'VariO:0331 alternative form of mRNA'. Exons are transcript regions that are part of mature mRNA. Exons occur in most eukaryotes. The number of exons varies greatly between genes. In humans, genes for histones contain just one single exon, they are not spliced at all. In the other end of the spectrum, TTN for titin contains 363 exons [118] . The shortest known human exon is just 2 bp, while the longest one is 27,303 bp long. The corresponding numbers for introns are 26 and 1,160,411 bp. 'VariO:0460 pre-mRNA' is matured to 'VariO:0461 mRNA' via several processing steps. pre-mRNAs constitute substantial part of heterogeneous nuclear RNA (hnRNA) and contain many very long RNA molecules. The longest human gene is for RNAbinding fox-1 homolog 1 RBFOX1 of 2,473,592 bp, the longest mRNA is for TTN, 109,224 bp. Variations in 'VariO:0352 coding region of mRNA' lead to many types of protein variants, discussed in [4] . mRNA is formed by joining exons during splicing. Note that 'VariO:0496 exon' may contain non-coding RNA in the termini. These regions are annotated with 'VariO:0353: noncoding region of mRNA'. One of the mRNA maturation steps is excision of introns during splicing. Alternative splicing is common, some 95% of multiexon genes could undergo alternative splicing [119] [120] [121] , but it is unclear how many forms are biologically relevant as many of them are extremely rare, restrained to a few cell types and may thus not be the explanation for the majority of complexity of proteome [122] . Combined RNA sequencing and proteomics data along with bioinformatic predictions indicated 72% of human genes to have alternative splice forms that could be translated to proteins [123] . Analysis of functionally distinct splice forms in over 700 human and mouse genes, biased towards literature notions of alternative splicing, indicated that just a small fraction of the transcripts was functionally distinct [124] . Depending on the criteria, 5% to 13% of human genes were shown to include such transcripts. Although alternative splicing produces large numbers of variant proteins, alternative start and termination produce even wider range of variation [125] . They detected tissuedependent transcripts for about half of the 18 000 investigated protein-coding genes and mainly due to alternative transcription start and termination. 'VariO:0437 alternatively initiated mRNA' of ALK receptor tyrosine kinase (ALK) is frequent in melanomas and appears also in some other cancer types [126] . The novel initiation codon appears in intron 19 and codes for three proteins of different sizes. Similarly, 'VariO:0497 alternatively terminated mRNA' affects produced protein product. Addition of polyadenylation signals to 3ʹ end is one of mRNA maturation processes. Heterogeneity can appear at the polyadenylation tails and this can lead even to diseases as the poly-A tails are targets for miRNA regulation. 'VariO:0356 alternatively polyadenylated mRNA' due to single nucleotide variations can affect transcript length and gene expression [127] . Variations in 'VariO:0352 coding region of mRNA' lead to many types of protein variants, discussed in [4] . 'VariO:0462 circular RNA' is a covalently closed singlestranded RNA ring. circRNAs are formed via splicing and can code for proteins. These molecules are formed when an upstream splicing acceptor joins with a downstream splice donor by back-splicing mechanism. circRNAs are common and have roles both in diseases and development [128] . hsa_-circ_0124644 can be used as a biomarker for cardiovascular artery disease [129] . This circular RNA is thought also to be involved in disease pathogenesis. Although DNA is the most common polynucleotide for genomic information in nature, 'VariO:0456 RNA genome' contains the genetic material in many viruses. 'VariO:0457 RNA virus genome' is usually single-stranded. According to the International Committee on Taxonomy of Viruses (ICTV) RNA viruses are classified to Group III, Group IV or Group V in the Baltimore classification system [130] . As RNA viruses are considered those which do not have a DNA intermediate during replication. Many common disease-causing viruses have RNA genome including SARS-CoV-2, influenza, hepatitis C, Ebola, and rabies viruses. Human immunodeficiency virus and some others have RNA genomes and replicative DNA intermediates and therefore are called retroviruses. Viroids are the smallest known pathogens, they appear in plants. 'VariO:0458 viroid genome' contains just a singlestranded, circular RNA without any protein coating or other molecules. Viroids are classified as subviral agents by ICTV. Coleus blumei viroid 1 (CbVd-1) variants clone 1 (accession number MG767212) and clone B (DQ178395) differ at position 25 and have different seed-transmission frequencies, 30% to 0% [131] . Satellites are another group of subviral agents. They mainly affect plants. Their 'VariO:0459 satellite genome' contains genes for protein shell but they require helper virus to replicate. Variants leading to p.D35A and p.M98R substitution in the Satellite panicum mosaic virus coat protein Kansas isolate (SPMV-KS) affect interaction with the helper Panicum mosaic virus [132] . Effect on post-transcriptional RNA modification includes variations that can be described either with 'VariO:0498 effect on RNA modification' or 'VariO:0362 effect on RNA splicing', see Fig. 4 . Both noncoding, including tRNAs, rRNAs, spliceosomal small RNAs, etc., and coding RNAs (mRNAs) frequently contain nucleotide modifications. Collectively the different RNA forms are known to have more than 100 distinct modifications, see MODOMICS database [38] . m.14692A>G in the mitochondrially encoded tRNA-Glu (GAA/G) gene MT-TE replaces highly conserved uridine r.55u>c in TΨC loop that is modified to pseudouridine and affects the conformation and stability of the tRNA molecule leading to maternally inherited diabetes and deafness [133] . The variant has 'VariO:0354 effect on posttranscriptional RNA modification' specifically on 'VariO:0498 effect on RNA modification'. In addition to mRNAs, e.g., tRNAs, rRNAs, lncRNAs, ribozymes and circRNAs contain introns or spacers that are removed during maturation before ligating the ends of the chains. Variations at canonical and noncanonical splice sites and those introducing cryptic splice sites can alter mRNA structure and have 'VariO:0362 effect on RNA splicing'. The 'VariO:0509 effect on mode of splicing' has three subterms that describe the type of splicing process affected (Fig. 5) . Splicing of many transcripts occurs simultaneously with transcription [134] and variant can have a 'VariO:0510 effect on co-transcriptional splicing'. Dystrophin (DMD) is the largest human gene, it contains 79 exons. Some of the introns are subject to non-sequential and recursive splicing [135] , where variation can have 'VariO:0512 effect on recursive splicing'. Recursive splicing means stepwise removal of an intron by several splicing events. Variation at intron eight donor site position +1 in collagen type I alpha 1 chain (COL1A1) causes osteogenesis imperfecta due to splicing defect [136] . The variation leads to production of five distinct splice forms, which are defective and have 'VariO:0511 effect on nonsequential splicing' by affecting the order at which introns are removed. Fig. 5 depicts various mRNA splicing forms and mechanisms. A change that introduces a new splice site can cause 'VariO:0505 inclusion of intron fragment'. c.801+2_801 +3insT variation in GLA gene for galactosidase alpha leads to two aberrant transcripts [137] . In one, a novel donor splice site is created causing inclusion of 37 bp from intron to the mRNA. The patient has Fabry disease because complex glycosphingolipids are stored inside lysosomes resulting in a progressive multisystem disease. 'VariO:0474 intron retention' contains the entire intron sequence in the processed mRNA. A silent heterozygous substitution c.7464C>T in exon 44 of the von Willebrand factor (VWF) gene causes type 1 von Willebrand disease [138] . Intron 44 is retained in the mRNA. As the transcript contains premature stop codon, it is likely degraded and no protein is produced. c.1029+384A>G transition to human serpin family G member 1 (SERPING1) gene creates a donor splice site in intron 6 and causes 'VariO:0504 inclusion of cryptic exon' leading to hereditary angioedema (HAE) type I [139] . This variant can be described more precisely with terms 'VariO:0373 cryptic splice site activation' and 'VariO:0374 cryptic splice site donor activation'. This kind of inserted regions are typically transposed elements, most often Alu sequences. Formation of a new exon from intronic DNA sequence has been called in literature for exonization. If the variant had generated acceptor site, then 'VariO:0375 cryptic splice acceptor activation' would be used for annotation. 'VariO:0513 mutually exclusive exon' annotates situations where only one of two exons is included into mRNA. C>G substitution at position +19 in exon 10 of microtubuleassociated protein tau MAPT gene affects splicing of mutually exclusive exons and causes frontotemporal dementia with Parkinsonism linked to chromosome 17 (FTDP-17) [140] . Mutually exclusive exons are also called for cassette exons. They are typically about the same size and are evolutionarily conserved [141] . Variants in ribosomal protein S6 kinase A3 (RPS6KA3) cause Coffin-Lowry syndrome (CLS) with variable phenotypes, e.g., with digital and facial anomalies as well as syndromic intellectual disability. c.613G>C in RPS6KA3 causes partial exon skipping 'VariO:0502 loss of exon fragment' and leads to a premature termination codon [142] . The patient has also another transcript that codes for amino acid substitution in an important amino acid residue. Substitution c.839+5G>A in BTK gene causes XLA due to 'VariO:0502 exon skipping' and deletion of 21 residues [64] . The deletion is in-frame and thus the protein sequence is retained after the deleted segment. This variant is classified also as a 'VariO:0367 variation at five prime splice site' of type 'VariO:0369 variation at non canonical five prime splice site'. c.392-2A>C in BTK causing r.392_520del [65] is a 'VariO:0370 variation at three prime splice site' of type 'VariO:0372 variation at canonical three prime splice site'. Introns can be very large, the longest in human is over 1 million bp. Large introns can contain cryptic recursive splice sites which facilitate stepwise removal of introns [143] . Alteration to such site is a 'VariO:0366 variation at recursive splice site'. All the examples above are of type 'VariO:0365 effect on cis splicing'. 'VariO:0376 effect on trans splicing' is used to describe splicing and ligation of two mRNA molecules producing a chimeric molecule. Trans splicing is actively investigated as a gene therapy modality to correct errors in mRNAs by generating RNA chimeras [144] . Lots of computer predictors have been released for various aspects of splicing. Tools for the effects of variants on splicing, including acceptor and donor splice sites, exonic sequences, exonic and intronic splicing silencers and enhancers, branch point sequences and polypyrimidine tracts were reviewed in [145] . There are several tools for 5ʹ splice sites, but very limited amount for branch point sequences [145] . Predictors for splice site identification have been around for two decades. Predictions for all possible single nucleotide substitutions in positions −3 to +8 at 5ʹ splice site and in positions −12 to +2 at 3ʹ slice site are available from dbscSNV [146] . Several methods have been developed for RNA modification site [147] , usually specific tools for each type of modifications, including N 6 -methyladenosine (m 6 A) [148] , 5-methylcytosine (m 5 C) [149] , pseudouridine [150, 151] and others. Performance of methods for N 1 -methyladenosine (m 1 A) and m 6 A modifications have been benchmarked [152] . tRNAmodpred is an example of an RNA typespecific modification predictor [153] . The primary RNA structure (sequence) forms secondary structural elements that are central components of the threedimensional tertiary structure. The folded chain can then form quaternary structures together with RNA and other molecules. There are six types of RNA secondary structures (Fig. 4) . Stem is formed by complementary bases binding together into a double-stranded structure (Fig. 6A) . The stabilizing hydrogen bonds in the stems are similar to those in double-stranded DNA. Hairpin loops appear between stemforming regions and do not contain stabilizing interactions (Fig. 6A) . In a bulge one of the strands in a stem region contains a base or bases that do not form a pair with the other strand (Fig. 6B) . Internal loop contains mismatching bases in both strands (Fig. 6C) . The mismatches can of the same or different lengths. Pseudoknot is a special structure where three strands (parts of the same chain) come together (Fig. 6D) . Multiloop is the most complicated of the secondary structures. Two or more double-stranded stems meet in a multiloop (Fig. 6E) . Variations in the secondary structural elements have various effects. C>G substitution in MAPT gene exon modifies the structure of stem in the mRNA and increases its stability [140] . FTDP-17 is caused by changing the ratio of alternative proteoforms containing either three or four microtubule-binding repeat domains due to 'VariO:0137 effect on RNA tertiary structure' of 'VariO:0386 effect on RNA stem' [140] . Some variations within hairpin loops cause conformational alterations [161] and have 'VariO:0387 effect on RNA hairpin loop'. RNA bulges show a linear correlation between the size of the bulged loop and its stability [162] and have 'VariO:0384 effect on RNA bulge'. Yeast Saccharomyces cerevisiae ribosomal protein L30 represses its own splicing and translation. Single and multiple variations affect both protein affinity and repression [163] having 'VariO:0385 effect on RNA interior loop'. Asymmetric loop positions show differential tolerance for substitutions. Positions 55 and 57 in L30 transcript tolerate alterations, while changes at sites 10, 11, 12, 58 or 59 have marked effect on binding and regulation [163] . Telomerase complex maintains chromosome telomere length and stability. Telomerase RNA component has a highly conserved pseudoknot, variations in which disrupt the structure and abolish telomerase activity due to 'VariO:0500 effect [154] ). (B) Bulge (pink) in non-coding prohead RNA from GA1 bacteriophage, which is involved in metal ion binding (2nci [155] ). (C) Asymmetric internal loops A (yellow) and B (pink) in SL1 domain in human immunodeficiency virus HIV1 packaging signal (1m5l [156] ). HIV is an RNA virus. (D) Pseudoknot in human telomerase RNA (2k96 [157] . The two stems are indicated in yellow and cyan, and the two loops in pink and dark blue, respectively. (E) Multiloop structure in RNA tertiary domain essential to hepatitis C virus (HCV) internal ribosome entry site (IRES) -mediated translation initiation (1kh6 [158] ). The four stems are indicated in cyan, red, green and yellow. In the case of ensemble of structures, the representative chain was selected. The 2D structures were drawn with forna based on force-directed graph layout [159] and 3D structures were drawn with UCSF Chimera [160] . on RNA pseudoknot' [164] . Even 'VariO:0383 effect on RNA multiloop' can be induced by variants. VariO:0137 effect on RNA tertiary structure RNA molecules fold into three-dimensional and quaternary structures and form a large number of structural forms (Fig. 4) . RNA double helix is formed when different parts of a RNA molecule hybridize and fold together or when two chains bind complementarily. C>G substitution in exon 10 of MAPT gene [140] is an example of 'VariO:0381 effect on RNA double helix' (Fig. 7A) . Short triple-helical RNA stretches have been found from a number of proteins [165] . These regions are structurally and functionally important, e.g., in telomerase TER RNA component (Fig. 7B) where variants have 'VariO:0425 effect on RNA triple helix'. Similar to DNA, RNA can form also four-stranded structures, such as G-quadruplex [166] (Fig. 7C ). Changes to these have 'VariO:0426 effect on four-stranded RNA' of type 'VariO:0173 effect on nucleic acid G-quadruplex'. DNA and RNA chains can bind complementarily and form hybrids. R loop is formed during transcription, it consists of a DNA:RNA hybrid and a displaced singlestranded DNA (Fig. 7D) . These loops are unstable and targets for nuclease cleavage [168] . They are linked to human diseases, including trinucleotide repeat-associated diseases [169] . Changes to these hybrids can have a 'VariO:0431 effect on R loop' [170] which is a form of 'VariO:0424 effect on DNA-RNA hybrid'. R-loop DB [171] is a resource for both predicted and detected R loops in 8 organisms, including humans. RNA sugar component is modified in a number of instances. Position 34 in the anticodon wobble position is modified in mammalian tRNAs. The modifications include queuosine in tRNA-Asn and tRNA-His, mannosylqueuosine in tRNA-Asp, and galactosyl-queuosine in tRNA-Tyr [172] . These kinds of variations are annotated to have 'VariO:0361 RNA sugar variation'. 'VariO:0438 epigenetic RNA modification' has been included to describe potential epigenetic RNA changes. Epigenetic changes are heritable traits that do not change the DNA sequence. Inherited epigenetic changes are known in DNA and protein. In RNA field epigenetics is used in a misleading and non-systematic way, i.e., for RNA modifications ('epitranscriptomics'). These changes are not inherited and are thus not epigenetic. Gene expression regulation, e.g., by non-coding RNAs is not an epigenetic trait either, it is one form of regulation. These changes are annotated with 'VariO:0354 effect on post-transcriptional RNA modification' and 'VariO:0498 effect on RNA modification', similar to post- translational modifications in proteins. There are a few verified examples where short RNA molecules are involved in epigenetics, however even these are not 'VariO:0438 epigenetic RNA modification', as the epigenetic effect is not on RNA level.@story_separate@RNA related research is advancing at long paces. New RNA forms are reported frequently and novel insights are obtained on the function and importance of the various transcripts. Thus, there is immediate need for systematic description of RNA related information to facilitate data mining, integration and analysis also from several sources. Comprehensive conceptualization of RNA variations was implemented into VariO to facilitate detailed description of all kinds of RNA variants, effects, consequences and mechanisms. Consistent annotations can be made with VariOtator tool [6] . RNA terms can be used together with terms from other systematics to enrich the information content. VariO terms at several levels can be combined for this purpose.","Systematics is described for annotation of variations in RNA molecules. The conceptual framework is part of Variation Ontology (VariO) and facilitates depiction of types of variations, their functional and structural effects and other consequences in any RNA molecule in any organism. There are more than 150 RNA related VariO terms in seven levels, which can be further combined to generate even more complicated and detailed annotations. The terms are described together with examples, usually for variations and effects in human and in diseases. RNA variation type has two subcategories: variation classification and origin with subterms. Altogether six terms are available for function description. Several terms are available for affected RNA properties. The ontology contains also terms for structural description for affected RNA type, post-transcriptional RNA modifications, secondary and tertiary structure effects and RNA sugar variations. Together with the DNA and protein concepts and annotations, RNA terms allow comprehensive description of variations of genetic and non-genetic origin at all possible levels. The VariO annotations are readable both for humans and computer programs for advanced data integration and mining."
"A disaster is defined as a sudden, calamitous event that seriously disrupts the functioning of a community or society and causes human, material, and economic or environmental losses that exceed the community's or society's ability to cope using its own resources, according to the International Federation of Red Cross and Red Crescent Societies (IFRC). Though often caused by nature, disasters can have human origins 0. According to the International Disaster Database [2] , an ongoing trend exists regarding lower death tolls from previous years continuing into 2019 . However, the frequency of disaster occurrence demonstrates an almost exponential growth, attributed to the worsening climate change and rapidly growing population and urbanization [3] . As the world's second-largest economy, China is vulnerable to many natural and man-made disasters. In 2018, natural disasters affected approximately 1. 3 billion people, killing 589 while having 524.5 million urgently relocated [4] . In addition, as a new form of operation organization, large-scale chemical industry parks have been established in many cities, potentially exposing people to various hazards. Furthermore, human also face the potential threat of public health events, such as the COVID-19 in early 2020. In China, emergency field hospitals and temporary quarantine centers were established immediately based on the stadiums and industrial plants. Hence, emergency shelters/open spaces, as well as quarantine centers, play dual roles in providing places for temporary accommodation and rescue activities. It is paramount to scientifically design shelters and quarantine centers during the early stages, which empowers emergency management and mitigates the damage of passive measures. Issues pertaining to emergency shelters as well as the use of open spaces have been discussed globally, including the optimization of shelter location [5] [6] , selection criteria of open spaces [7] and so forth. Despite their proposals, these models and experiences are rarely fully understood in the context of being lessons for actual construction projects. China's current main problems in construction for emergency shelters, for example, include extensive selection of shelter locations, mismatch of refuge demand and supply, lack of maintenance during the operation period and defence deficiency of multiple disasters. Further clarity regarding the location and planning of emergency shelters and open spaces are necessary to support response and J o u r n a l P r e -p r o o f recovery efforts when disasters occur. Accordingly, numerous objects are analyzed in this study. First, surveys regarding emergency shelters and open spaces in China were reviewed and compared, which analyzed the current issues in this field thus far. Second, existing literature was consolidated on research pertaining to experiences and strategies in shelters/open space construction projects in Japan, the U.S. and Europe. Finally, guidelines and suggestions for emergency shelters and open spaces were outlined to better incorporate comprehensive factors into site selection and the planning of shelters. Overall, this paper not only highlights the issues surrounding actual implementations of emergency shelters and open spaces in China, but also summarizes instructions for future construction projects based on experiences from other countries. This study is structured according to the following sections. In Section 2, the research review methods are introduced, while Section 3 reports on the emergent challenges and issues regarding emergency shelters and open spaces in China. In light of these issues, global experiences and strategies for the construction of emergency shelters/open spaces are summarized in Section 4. This section also outlines global progress regarding shelters, explaining details in the design and site selection of shelters and open spaces. In Section 5, suggestions and guidelines for the construction of emergency shelters and open spaces in disaster management are put forward. Finally, Section 6 provides the conclusions reached by this study.@story_separate@State of art designs and construction of emergency shelters are usually summarized in conventional literature reviews. However, in this manuscript, challenges pertaining to emergency shelters/open spaces in China are identified and suggestions are provided for the construction of emergency shelters/open spaces according to global experiences. Hence, the relevant literature was comprehensively scoped, and reviewing both qualitative and quantitative studies was necessary in order to obtain extensive and comprehensive results. Figure 1 illustrates the study selection and filtering process. Both academic literature and grey literature were included in the systematic review. Grey literature refers to government documents and design standards or guidance, which presents important information concerning potential insights from the government as well as experts. In the document identification step, grey literature was collected by searching the government websites. Furthermore, six databases were considered: Web of Science, Science Direct, Taylor & Francis Online, Google Scholar, VIP (in Chinese) and CNKI (in Chinese). Search keywords include""emergency shelter""AND""risk assessment"" OR""site location""OR""data-driven algorithm""OR""natural disaster"" OR""urban resilience""OR""daily preparedness""in the title, abstract, and keywords. The inclusion criteria of the reviewed literature were: Possessing design standards and survey reports for emergency shelters in China. Containing construction standards for shelter settlement in the UK, Canada, the U.S. and European countries. Published guidelines for evacuation and emergency shelters from international humanitarian communities. Having construction standards and operating experiences in open spaces in Japan. Put forward optimization models for shelter location and site selection. J o u r n a l P r e -p r o o f Normalized construction of emergency shelters in China started in 2006. The Ministry of Construction issued the Urban Planning Methodology, which includes ""construction guidelines of disaster prevention system"" as part of urban planning [8] . In 2007, disaster prevention and mitigation became a compulsory component of overall planning from the locality to the center. In October of the same year, the ""Eleventh Five-Year Plan"" for Comprehensive Disaster Prevention in Urban Construction was issued by the Ministry of Housing and Urban-Rural Development of China, emphasizing that the objective of disaster prevention planning in China is to basically form a comprehensive disaster prevention framework in order to significantly enhance disaster prevention [9] . The principle of ""multiple use of shelters, prevention-based mechanism, quick response and effective strategies"" are gradually being added into government documents by many cities. In 2008, the national standard Site Selection and Supporting Facilities for Earthquake Emergency Evacuation was issued, especially considering earthquake disasters [10] . In January 2017, the Standards for the Construction of Emergency Shelter in Urban Communities was proposed, which divided emergency shelters into three levels according to the community's population. Additionally, the construction standards and requirements for each level were provided [11] . In early 2020, the deployment of emergency field hospitals/quarantine centers was regarded as the most effective solution in keeping COVID-19 from spreading. Technical guidance for construction and operation of emergency hospitals based on industrial buildings [12] was, in turn, immediately issued by the government. Basic requirements like site selection and ventilation systems were identified in this official document. Overall, the design of emergency shelters/open spaces is of paramount concern and should be primarily considered within prospective plans. However, in actual implementations, land use and site selection of emergency shelters/open spaces are roughly determined, which fails to fully consider actual demand. Hence, shelter construction and site selection suffer from certain issues, which are discussed in Section 3. 2. J o u r n a l P r e -p r o o f One limitation found in existing guidelines is that most guidelines proposed specific to the single earthquake disaster do not clearly explain site selection for emergency shelters/open spaces. The emergency shelters were usually established based on the existed open spaces. Hence, site selection done in the early stages does not take into account multiple aspects including demographics, community governance unit, various types of disaster, as well as the accessibility of emergency shelters. The technical indexes that stakeholders mostly adhere to are the site's size, per capita area and equipped facilities, rather than the actual performance of the emergency shelter during a disaster. In most cases, the scale and location of the emergency shelter/open space are designed using rough estimations of the population density in city blocks. However, actual demands of the shelter vary greatly in regard to spatial and temporal dimensions according to different disasters. Although relevant standards and guidance concerning the construction of emergency shelters/open spaces were proposed in most regions, publicity and evacuation drills were rarely carried out routinely. For example, in Qiqihar city, the number and location of emergency shelters in the electronic map were not consistent with the government's announcement [13] . Furthermore, according to the survey and investigation of the current status of emergency shelters in Wuhai city [14] , only 10 out of 180 citizens had some knowledge regarding the disaster prevention park. The aftermath of such inadequate levels of publicity is that many citizens would not know which shelter to go to when a disaster occurs. Hence, ""safety culture"" would be an unreachable goal if public awareness and performing drills are lacking. In principle, the distributions of emergency shelters should be arranged according to the dispersion of the disaster as well as population density. According to the survey, most emergency shelters are located within developed cities in eastern China, while the disaster prevention system in rural areas are often disregarded, especially in western China [15] . According to W. Li et al. [15] , as shown in Fig. 2 , the top 20% of developed cities provide 16780 emergency shelters/open spaces, which account for the 88 .6% of the total. In contrast, the proportion of emergency shelters/open spaces of the remaining 270 less developed cities is only 11.4%. Furthermore, compared to new communities, emergency shelters/open spaces are scarcer in old towns and undeveloped area [16] . In fact, the damage due to building collapse or fire would be more serious in poorer regions or old towns, which lack safety designs including wide laneway, emergency equipment and aseismatic structure of building. Hence, economics accounts for the unbalanced safety rates among different regions.  Numerous mismatches exist among the construction of emergency shelters with the actual demands of the evacuees. China suffers from various natural and man-made disasters every year; requirements for post-disaster resettlement and evacuation are different depending on the disaster type and spatiotemporal distribution of population. However, risk assessment ascertaining threats from hazards prior to a disaster is rarely carried out in the shelter designing phase [17] . Moreover, emergency shelters/open spaces are usually regarded as welfare rather than being rooted in developmental issues. For example, governments in most cities prefer to establish high-level emergency shelters, which provide short-term and long-term relocation. The construction of medium and small shelters is rarely integrated into disaster prevention planning [18] . However, the function of small shelters cannot be neglected-in regions that have difficulties in accessing high-level shelters, any small and medium emergency shelters/open spaces may be necessary as the transfer sites in case of a disaster. J o u r n a l P r e -p r o o f [19] , is the first regulation concerning the operation. Another challenge is the lack of financial support provided by the government and developers. Multiple financing channels including loans, fundraising and bonds should be activated to facilitate the construction of disaster prevention systems.  Japan has paid attention to disaster prevention systems from early on as they continue to face great challenges pertaining to natural disasters and high-density population for several decades [20] . The Great Kanto Earthquake in 1923 changed the spatial distribution of building/dwellings [21] . Green spaces, parks, squares and basements in the cities have been regarded as the most important parts of emergency shelters and evacuation sites. After World War II, the government enacted a series of standards and laws to integrate the public space into the disaster prevention system. For example, the Law of Urban Park [22] and the Preservation Law of Urban Green Space [23] combined the protection of green areas within the disaster prevention system. In 1999, the Guidelines for planning and design of disaster prevention parks [24] was proposed to demonstrate the design of the disaster prevention park. Since 1977, The National Earthquake Hazard Reduction Program (NEHRP) supported research, practices and policies that identified earthquake hazards and mitigated earthquake risks by improving the performance of emergency shelters [25] . After the catastrophic events of Hurricane Katrina [26] in 2005, the government actively promoted ""disaster prevention communities"", which took on responsibilities in disaster prevention, disaster response and post-disaster recovery. At present, the United States has established a disaster prevention and mitigation system comprised of federal, state and local entities. In addition, various standards concerning emergency shelters have been proposed for different types of disasters such as hurricanes, biochemicals, earthquakes, fires, explosions and so forth. [  In Europe, construction of emergency shelters could be retraced to the Renaissance [30] . For example, the 1963 January earthquake struck the southern parts of Italy and caused extensive amounts of damage [31] . In the recovery phase, narrow and curved roads were replaced by wide and straight avenues, ensuring quick and safe evacuation for its citizens. In the UK, according to the Evacuation and shelter guidance [32] proposed by the UK government, large stadiums, schools and other public buildings were appointed as emergency shelters. The responsibility of the government, voluntary organizations, enterprises and citizens have been identified in several standards and laws [33] . As previously mentioned, enormous damages are usually caused by various disasters in Japan. An open space called the disaster-prevention park has been divided into seven types according to their functions and scales, as shown in Table 2 . Table 2 Seven types of disaster prevention parks in Japan.  Moreover, a hierarchical network was formed by roads connecting these seven types of parks. For example, a community park of 500 m 2 was set up to provide facilities like a rest area and lavatory to its citizens. The distribution of these open spaces should cover all communities within the urban city. Acting as temporary shelters, neighboring parks were equipped with a water supply system and warehouse, providing shelter with a service radius of 500m. In addition, the minimum width of the road connecting temporary shelters and emergency shelters was 10m. Notably, the service radius of the emergency shelter and disaster-prevention stronghold at the district level was 2 kilometers. The disaster-prevention stronghold for large areas should be located near to the expressway, which takes on responsibilities in logistics transport and effective communication. Aside from the construction of open spaces, an evacuation route was designed and optimized combined for the disaster-prevention park. For example, in the Planning handbook for countermeasure of the earthquake, developed by the Japan Fire Administration, the density of the apartment, storage of the hazardous chemicals and distance of the evacuation route were adequately outlined. Since 1980, flame retardant projects have been carried out in urban systems, where both green plants and building materials were optimized to be fireproof. Due to this project, the safety level surrounding the disaster-prevention park as well as the evacuation route was improved [34] . Fig . 3 . The effect of the number of shelters (p value) and the level of tolerance on total evacuation time [37] . Other than car-based evacuation (self-evacuation), bus-based evacuation (supported evacuation) combined with shelter location was first addressed by M. Goerigk et al. [39] . Since the Comprehensive Evacuation Problem was a multi-objective and complicated issue, a genetic algorithm was established as an optimization method. Both the evacuation time as well as the risk for evacuees were taken into account in the macroscopic model. Additionally, in 2016 [40] , a multi-objective integer programming model was proposed for decision-making in regard to shelter location and evacuation routine in different conditions of bushfires. In the corresponding model, sensitivity analysis of evacuation plans according to the number of functioning shelters, disruption of shelter availability and major roads were taken into account. A summary of the aforementioned studies is given in Table 3 . Table 3 Combinations of shelter location with evacuation route.  The emergency management cycle is an open-ended process. The four phases (preparedness, response, mitigation and recovery) indicate the ongoing attempt to prevent the effects of a disaster Error! Reference source not found.. In this section, the role of the emergency shelter in different stages of the emergency management cycle is identified. As shown in Table 4 , the emergency shelter/open space mainly plays an important role in mitigation and recovery. In view of densely populated communities or old towns, strengthening urban system resilience may improve the mitigation of risks in various disasters. According to six case studies performed in Italy [43] , the main critical issues of strategic urban structures include: (1) lack of public space in the old town; (2) lack of a differentiated network of public spaces and functions (lack of redundancy); and (3) Many researchers have also investigated the function of the emergency shelter/open space during a city's recovery. As put forward by Brand and Nicholson [44] , public spaces and urban structures of Christchurch, New Zealand following the 2010 and 2011 earthquakes were evaluated and compared. As shown in Fig. 4 , the employee density in the commercial districts was dispersed along with the urban structure. Ref  According to the Council's Draft Central City Plan (Draft CCP), which was developed by the government after the earthquake, several projects were proposed to animate shelter construction to balance the political, individual and organizational lives of those within the territory for participatory planning. Several key lessons regarding resilience were summarized: (1) laneway construction encouraging public access and supporting retail use; (2) decentralized water supply system in order for parts of the system to continue to function independently; (3) minimized use of restrictions for public access; (4) retreating or protecting flood prone areas; (5) developing low rise cities that are greener and more people-friendly; and (6) outlined the Transitional City Project, which provided opportunities for individuals and small groups of citizens to occupy the spaces left on grounds/walls. The role of community gardens in relation to post-disaster recovery has been previously discussed by J. Chan et al [48] . Here, the authors conducted an exploratory multi-case study of several hurricane-impacted community gardens in order to understand its role in New York City. The results demonstrated that community gardens have long served as verdant refuges and community hubs in NYC neighborhoods [50] . The evacuees preferred to gather during the chaotic days that immediately followed hurricanes. Unlike other shelters and open spaces, such as parking lots and commercial buildings, community gardens were regarded as restorative places, supporting the resilience of an urban social-ecological system and J o u r n a l P r e -p r o o f inspiring their community members. In Chile, two different reconstruction programs were supported by the government; one was focused on providing houses for affected people, while the other included comprehensive plans for urban design and managing investments, which was phrased as the 'cross-sectoral reconstruction approach' [47] . According to the comparative results, a more significant improvement in resilience may be observed in the cross-sectoral reconstruction approach. Specifically, its projects included anti-tsunami measures, improvement in accessibility and connectivity, more comprehensive plans in design, and multi-functionality of open space systems. To the best of our knowledge, few cases combined emergency shelters/open spaces with the preparedness phase in the emergency management cycle. The Golden Gate Park polo field in San Francisco is a multifunctional open space [51] [52] that acts as a leading component in emergency management. After the 1906 earthquake, these parks played a critical role in the entire emergency management process. The Hyphae Design Laboratory and CMG landscape architecture settled a displaced population of 12,000 individuals in case of disaster. In addition, daily infrastructural ""lifelines"" like food, water, energy, shelter and waste management were promised for three days [49] . Tokyo is another international metropolis suffering from various natural disasters and over-utilization problems. In 2015, the Disaster Prevention in Tokyo handbook [53] was edited by the government with comprehensive consideration of local features, urban construction, lifestyle of its citizens and so forth. The handbook mentions drills to be enacted for disasters during the preparedness phase, which tells people how to take adequate action during evacuations and in emergency shelters. In addition, a trend exists in the integration of small and medium open public spaces for disaster J o u r n a l P r e -p r o o f prevention planning, such as evacuation planning of densely populated communities. According to the survey (Fig. 5) , however, only 40% of small and medium open spaces were integrated into disaster prevention planning. These spaces were considered to function in evacuation assistance, which were often associated with firefighting by permitting water access; its function also stemmed from its morphology by interrupting densely built fabric [46] . Typically, public open spaces and emergency shelters cannot accommodate large numbers of people, hence, integration of the role of privately owned spaces with the disaster prevention system is proposed in this study. In Japan, the standards Deployment of Emergency Shelters [54] was proposed in 2017, which put forward that companies and personal places could be adopted as emergency shelters whenever they were deemed necessary. Enterprises located in disaster-prone regions usually have strong backgrounds in disaster prevention. Accordingly, temporary shelters could be successfully assigned as the local government may share a long-term cooperative relationship with such enterprises. A similar measure was adopted by other counties. In the UK, Evacuation and Shelter Guidance [32] addressed that suitable buildings, such as schools, leisure centers or community halls, may be used as rest centers. Undoubtably, an agreement should be reached with the building operators or owners, and basic utilities (gas, electricity or water) should be identified as well. Notably, the additional disruption was probably caused by activation of the shelter; planners should consider minimizing the impact of the assignment while bolstering protection for the confidential department. The guidance also recommended that it may be more cost-effective to use hotels rather than opening commercial buildings as emergency shelters. Hence, planners should keep a list of local hotels that can accommodate residents and sign a contract with the corresponding accommodation providers. Underground spaces like parking lots, train stations and underground air raid shelters have been deployed as emergency shelters in many counties, such as Korea and Germany [55] . Subsurface and underground spaces play specific roles in that they promote urban resilience and disaster prevention [56] . In order to protect people from disasters, additional utilities like water supply systems and ventilation have been installed [57] . The decision on whether to leave or stay in buildings/bunkers is also important. Several American cities acquired lessons in hurricane prevention, where indoor shelters were mostly utilized compared to public open spaces. In terms of official standards and guidance, shelters that provide safety during storms/hurricanes are J o u r n a l P r e -p r o o f termed ""safe rooms"" by FEMA and ""storm shelters"" by ICC 500. A safe room is an interior room, a space within a building, or an entirely separate building, which is designed and constructed to provide near absolute life-safety protection for its occupants from tornadoes or hurricanes [58] . Table 5 depicts shelter categories classified by different purposes, capacities, locations and levels of protection. Furthermore, the American Red Cross proposed ARC 4496 [58] , which identified the minimum requirements for shelters in terms of location selection. The hazards associated with hurricanes were also considered, including surge inundation, rainfall flooding, high winds and hazardous materials. Other than private enterprises and personal places, neighborhood cities or regions may be used to relocate citizens. In the case of Dichato, Chile, the interconnection among neighborhoods facing the sea has been clearly encouraged. If one collapses, other towns could take on responsibilities in evacuation and accessibility to emergency shelters and open spaces. Hence, the resiliency of the city was improved to a certain degree [47] . Table 5 Commercial shelter categories [58] . Expected capacity 1-100 1-10 1-100 100-1000 Level of Protection Blast (medium) Blast (low) All All Basement or sub-basement area without windows and semi-hardened walls and ceiling. Interior space without windows and semi-hardened walls and ceiling. Conference room, data center, bathroom, stairwell and elevator core. School, church, mall and government building. Difficult to site; build in high water table and rocky areas. Annual or semi-annual inspection and rotation of supplies. Multiple areas in large buildings; plan to prevent overcrowding. Plan for multi-lingual, elderly and special needs populations. The  One particular goal of the United Nations' 2030 Agenda for Sustainable Development is to ""make cities and human settlements inclusive, safe, resilient and sustainable"" [59] . According to the UN International Strategy for Disaster Reduction [61] , resilience is defined as the ability of a system, community or society exposed to hazards to resist, absorb, accommodate and recover from the effects of a hazard in a timely and efficient manner, which encompass the preservation and restoration of its essential basic structures and functions [62] . As previously discussed, combining the design and planning of emergency shelters and public open spaces into the construction of urban resilience may improve disaster management. 6 . Factors attributed to a proposed building could be evaluated according to its vulnerability, non-structural parameters and spatial features. Namely, neighborhood density and design are essential to emergency response services during disasters [65] [66]. Moreover, urban infrastructures consist of many components, where the road transportation network serves as the most important characteristic related to emergency shelters. Furthermore, the community is made up of people and organizations, which is important for urban resilience in protecting against extreme events or disasters Error! Reference source not found.. In addition, the total area, distribution, quality and site conditions of an open space influence urban resilience to a disaster. In order to acquire a more detailed and reliable analysis of urban systems, the Building Information Modelling (BIM) and Geographic Information System (GIS) approaches may provide complete data and documents in regard to site selection and modelling for emergency shelters and public open spaces [67] [69] . Furthermore, it is one of the key tasks to establish databases based on BIM and GIS to provide guidance for shelter construction. For example, user feedback could be integrated into the database of post-occupancy evaluation, which is significant to optimize the site location and facility of emergency shelter and open space with the consideration of residents' need. In addition, layout-planning database gathering spatial function and area ratio of national emergency shelters is beneficial for designers and planners to carry out risk assessment and construction optimization in the early stage. Not only the living comfort and site selection problem could be optimized based on the databases, applicable construction standards and technology can be improved in the long term. J o u r n a l P r e -p r o o f Although decentralization is at the macro level in urban resilience systems, emergency shelters/open spaces should operate as a self-contained model, providing protection for the vulnerable in a centralized system [70] . Hence, four requirements exist concerning the selection of the location as well as the surrounding amenities including alternative connections of road networks [71] [72] , avoiding congestion in densely-populated areas, hierarchical scales of shelter (such as region, metro, city, section and site [73] ) and diversity in a spatial distribution [74] . Especially, risk assessment should be conducted in the multi-risk coupling area, such as city-industry integration zone. Beside open space, diversiform facilities are recommended to be integrated into the emergency shelter system, including the safe room, movable rescue capsule and refuge chamber [75] [76]. As previously mentioned, preparedness in the emergency management cycle is only emphasized by a few developed countries like Japan and the U.S. However, preparedness in disaster management not only reduces the time it takes to reach emergency shelters/open spaces in the wake of an incident, but also facilitates the optimization of the site's location as well as the operating schedule of emergency shelters/open spaces. Hence, it is recommended to embed daily preparedness and practice to existing community-based disaster risk reduction systems. Evacuation drills and other related J o u r n a l P r e -p r o o f activities should be carried out within communities, schools, and companies. To operate the emergency shelter in the school area, surrounding residents, parents and teachers are suggested to be trained on communication skill [77] . Essentially, everyday preparedness should not be a burden for citizens. Organization leaders, government officials, academic researchers and emergency nurses must begin by understanding the lifestyles, interests and actual needs of the households in cities and rural regions [78] . Accordingly, the element of preparedness in disaster management should be added into its initial activities. For example, cherry-blossom viewing is a popular cultural event, though its anecdotal purpose is to foster preparedness. The soil could be maintained by public gatherings, mitigating the risk of bank collisions [79] and flooding. Lack of prepared awareness is the deficiency in many countries. Questionnaire [80] , return on investment (ROI) analysis [81] and risk assessment [82] are usually adopted to analyze the capability of disaster preparedness. In view of the experiences acquired by Australia, most people prefer to stay indoors rather than evacuate to the emergency shelters/open spaces during bushfires [83] and flooding [84] . The reason why they decide to stay and protect their property and pets is primarily due to lack of awareness. In China, however, most evacuations are generally compulsory and are ordered by the local governments [85] . Other than passive and compulsory evacuations to shelters/open spaces, residents are educated to plan and prepare for active sheltering as it is more significant for effective evacuation. In addition, thanks to opportunities in evacuation drills and other activities, it is possible to optimize shelter locations as well as their designs. Specifically, the planner could check whether congestion would occur during the evacuation period and design a route from an apartment to a corresponding emergency shelter/open space. In order to maintain communal engagement and sustainability in regard to preparedness, activities should adhere to the principles outlined in daily life, through participation and collaboration, repeatedly, multiple purposes and locality [85] . Many Chinese cities have recently tackled high risks in disasters due to rapid urbanization, economic development, centralized population and wealth, especially in Beijing and Shanghai. In most regions, the scale and location of emergency shelters/open spaces are designed using rough and static estimations of population density in the area of city blocks. However, the actual demands of the shelter and population distribution vary greatly in both the spatial and temporal dimensions. As shown in Fig. 7 (a) and (b) , the total daytime and nighttime populations in Haidian District of Beijing was 3.24 million and 3.3 million, respectively. Although the ratio is nearly 1, there are obvious differences in the spatial population density Error! Reference source not found.. According to GIS, high-density employment centers and business districts would be considered serious blocks as they have high deaths and injuries during the daytime. In parallel with daytime, most of the population would be mainly concentrated in residential areas during nighttime. In addition, long-distance commuting and dispersed distribution are the lifestyles adopted by most people in developed cities. Hence, developing dynamic modelling in regard to the spatiotemporal distribution of the population based on the GIS platform can offer guidance for a shelter's capacity and location [86] . Furthermore, it can also provide prospective and adequate strategies to be used in disaster prevention, emergency response and relief distribution. Furthermore, the demand for emergency shelters would vary following a disaster. For example, Huiyong et al. [88] extracted the population sizes of residential communities from the Economic and Social Development Statistics Yearbook Error! Reference source not found. of Shanghai and identified the population density from hundreds of communities. When an earthquake occurred, nearly 100% of the residents had to seek temporary shelter/open space immediately, while 40% and 20% of residents had demands for higher-level shelters one day following the disaster and ten days following the disaster, respectively. Hence, it is necessary to establish hierarchical emergency shelters/open spaces combined with a dynamic program for evacuation. Specifically, temporal variation in regard to necessities should be taken into consideration when planning and designing an emergency shelter system. In addition, people's demand and refuge requirements should be considered as the important factor for site selection and shelter design. Sometimes, the ""in place"" shelter can accelerate recovery and reinforce communal bonding [90] [91]. However, larger emergency shelters that require long-distance traveling is also necessary when the urban area is too dense [92] . The hierarchical shelter system provides different levels of service after the disaster, such as long-term shelter, short-time shelter and immediate shelter. A number of multi-criteria location models have been proposed to meet various requirements in complex applications, including p-median model, location set-covering problem (LSCP) and maximal covering location problem (MCLP). Total number of emergency shelters is fixed in p-mediam model [93] . The objective of LSCP model is to find the minimum number of shelters and cover all demand points in specified regions [94] . MCLP model aims to determine the number of shelters and optimal locations to cover more demand points [95] [96] . In these models, data-driven approaches have been adopted to solve the optimal location problem, such as ant colony optimization (ACO) [97] , genetic algorithm (GA) [98] , greedy algorithms and Tabu search algorithms [99] . In other words, the construction cost of the shelter/open space could be reduced by optimizing resident allocation through a dynamic refuge demand estimation. Underdeveloped areas also face problems in mismatch between refuge demand and shelter accommodation. The distribution of residential houses is dispersed in the countryside, especially in mountainous areas. After the 2008 earthquake in Sichuan province, a top-down approach was applied for post-disaster recovery and reconstruction in Wenchuan Town. Guangzhou, a developed city, was assigned by the government as the corresponding city to support this project Error! Reference source not found.. Both the long-term and short-term developments were promoted along with public spaces including a memorial system, Hence, experts and researchers have called for a shift from a top-down approach to a bottom-up approach, which encompasses the participation of households, community-based organizations (CBOs), Non-Governmental Organizations (NGOs), private enterprises, international agencies, local government agencies and the national government, as shown in Fig. 8 . Responsibilities in construction and fundraising as well as program planning are delegated to multiple participants. For example, questionnaire survey could be conducted to identify the best measurement to generate income and facilitate normal time uses of the shelters [103] . Importantly, the community administrator should know more about the need, lifestyle and age distribution of residents and strengthen the coordination among various CBOs. The generalized CBOs not only means the households, but also includes the schools, restaurants and commercial complex nearby. Furthermore, cross-regional and cross-sector cooperation should be strengthened to comprehensively improve professionalism when constructing emergency shelters. Volunteered geographic information, such like Twitter, can capture people's preferences for planning flood evacuation shelters in a much faster and more comprehensive method than questionnaires and surveys [104] . The spatial distribution of a disaster varies across China, especially in the southeast, which possesses multi-disaster characteristics. [112] . Cities with high earthquake/hurricane hazard and inundation areas, such like Istanbul, Logan, north Carolina, are adopted as the instances. Since the flood and storm would approach in uncertain paths, shelters located in at-risk regions are assumed as not suitable for use. Hence, planners are suggested to adopt an algorithm as well as GIS to develop a deterministic model for site selection for emergency shelters/open spaces and logistics during shelter operations. Fig . 9 . Elevated ground in the disaster prevention park (also referred to as a vertical emergency shelter) [114] . Japan has valuable experiences in resisting the destruction of various disasters, including earthquake, tsunamis and typhoons. In addition to structural countermeasures such as sea dikes and offshore breakwaters, non-structural approaches were adopted to mitigate the effects of tsunamis. Green embankments, coastal forests and disaster prevention parks were established to minimize damages caused by disasters, as shown in Fig.9 . For example, in the ""Morino Project"" [112] proposed in 2011, a special layout of man-made hills and connecting elevated pathways was promoted, which acted to reduce tsunami impact and served as an evacuation function. Six park units connected with a long network and high land could resettle hundreds of evacuees. In addition, green embankments and coastal forests are able to absorb the tsunami energy while saving human lives during the tsunami [114] . Furthermore, an individual's decision-making behavior and companionship should also be integrated into the design and site selection of emergency shelters for different disasters. An agent-based model could be conducted in conjunction with GIS, where the topography of the site, population distribution, evacuating speed, and inundation features are all considered [115] . According to Fig.10 , a certain number of people prefer to evacuate vertically when the shelter is located in the center of the population, decreasing the mortality rate. However, risk and threat that residents facing would be different in a tsunami-triggered oil spill from industrial parks. Emergency shelters and evacuating routes should be planned with the consideration of different individual behaviors [115] . Fig . 10 . The proportion of people who consider vertical evacuation in coastal cities [114] . In China, the number of public open spaces outnumbers that of indoor shelters. School buildings, leisure malls, hotels and enterprise facilities could be assigned as alternatives for indoor emergency shelters. In addition, the cooperation between governments and enterprises should be strengthened. For example, training exercises and drills should be carried out by enterprises to enhance the ability of disaster prevention and emergency management. The pilot projects designating enterprises as emergency shelters should also ensure that all departments actively coordinate with each other and work together. Fortunately, when the COVID-19 broke out, the government and experts have carried out effective measures to establish the emergency field hospitals and temporary quarantine centers immediately. Parts of the functional sites were remodeled based on the school, hotels, convention center, stadiums and industrial plants. Therefore, significant experience should be adopted to guide the assignment of the indoor emergency shelters in the future.@story_separate@Designing emergency shelters and open spaces takes into account optimized issues that both governments and researchers have profoundly discussed. In this study, we critically analyzed the current limitations of emergency shelters and open spaces in China. According to design standards and surveys, the following issues were ascertained: (1) guidelines/standards concerning site selection is vague, and the scale and location of emergency shelters/open spaces are determined by a rough estimation of population density in city blocks; (2) publicity and evacuation drills are rarely carried out in daily life, where problems like congestion and disorder affect evacuation in the wake of an incident; (3) most emergency shelters are located in developed cities and new communities rather than in disaster-prone regions or old towns; (4) mismatch of accommodation and demand is present, which varies depending on the disaster, attributed to population density and temporal variation; and (5) responsibilities of government departments, social groups, enterprises and institutions during the planning, construction, maintaining and emergency stages were not identified. In order to better incorporate comprehensive factors into planning and site selection for emergency shelters/open spaces, global experiences and strategies pertaining to shelter construction were analyzed. From the reviewed literature concerning shelter construction, the present authors summarized past experiences regarding the planning and location of emergency shelters and open spaces as: (1) tackling shelter location and evacuation routing problems in an integrated manner, such as disaster prevention parks, car-based evacuation and bus-based evacuation; (2) embedding shelters/open spaces into the emergency management cycle, comprised of preparedness, response, mitigation and recovery, as well as multifunctionality of public shelters and open spaces, which would contribute to improving urban resilience; and (3) integrating privately owned spaces and public facilities into the disaster prevention system in order to safeguard people when open spaces are deemed inadequate to accommodate evacuees. The results from the corresponding documents and articles provided further clarity for prospective planning and site selection for emergency shelters/open spaces, which can significantly impact the safety of people as well as the city. Additionally, instructions for future construction including five potential directions are proposed. In order to solve current issues within shelter construction, the following suggestions are offered: Combine the design and planning of emergency shelters and public open spaces into the construction of urban systems; Embed everyday-life preparedness and practice to existing community-based disaster risk reduction systems; Estimate the refuge demands, and select adequate locations for shelters via spatiotemporal patterns and hierarchical planning; Shift from a top-down approach to a bottom-up approach by including private enterprises, non-governmental organizations and community institutions; Establish emergency shelters/open spaces by considering different disasters, while incorporating diverse spaces as alternatives. There is growing urgency to develop novel designs for the fortification of shelter construction and open spaces as rapid population growth and climate change would introduce numerous natural and man-made disasters in the future. The corresponding literature review serves as the authors' initial step toward the development of a framework for the holistic assessment and optimization of emergency shelters and open spaces. Notably, the archival documents and academic articles cited in this study possess certain limitations. Only the full text in English, Japanese and Chinese was included. In order to gain a wider and deeper knowledge of existing literature, future research should include other languages to ensure impartiality of the study's results. In addition, man-made disasters should be included in future analyses.","Emergency shelters and open spaces play dual roles in providing locations for temporary accommodation and rescue activities during disaster situations. Over time, research has attempted to optimize site selection and design for emergency shelters and open spaces, though they rarely offered lessons to guide actual projects. In this regard, it is paramount to design emergency shelters or open spaces in a forward-looking and dynamic manner, especially when the country faces challenges due to extreme events (e.g. earthquake and floods) and large populations. The aim of this paper is to analyze the issues of this field in China while summarizing instructions for future construction based on the experience and expertise of other countries. Specific suggestions include: (1) combining the designs and plans of emergency shelters/open spaces into the construction of a resilient urban system; (2) embedding routine preparedness into disaster risk reduction approaches; (3) optimizing issues in site-selection using spatiotemporal patterns in refuge demand while designing the shelter system into a hierarchical structure; (4) shifting from a top-down approach to a bottom-up approach, which includes the participation of multiple aspects of shelter construction; and (5) designing and establishing emergency shelters/open spaces to fight against different types of disasters."
"The human circulatory system is made up of the heart, blood, and over 100 000 kilometers of veins, arteries, and capillaries. No other biofluid has an intimacy with the body like blood has and therefore it is not surprising that it possesses such a richness of information concerning the overall pathophysiology of the patient. Unlike specific cell types, however, blood does not contain its own genome. Its genome can be considered as a compilation of the organism's genetic material, containing all of the variations (i.e., mutations, single nucleotide polymorphisms, gene duplications, etc.) that are found in particular cells. Since it lacks a specific genome, it follows that blood does not have its own transcriptome. Rather it can potentially contain any portion of a transcript that is transcribed within any cell in the body. Likewise, the proteome of blood potentially contains portions of any of the proteins found within the organism's cell complement. A recent study comparing N-linked glycopeptides within cultured cells and solid tissues with plasma showed that numerous proteins from different cells and tissues are indeed present within this biofluid ( Fig. 1 ) [1] . This study confirmed the prevailing hypothesis that blood contains proteins from a variety of different cells and tissues within the body and also substantiates the continued need for research into biofluid proteomics as a source of novel biomarkers. The movement of substances to and from cells is critical for survival. In the human body, this transport function is carried out at a macroscopic level by the circulatory and lymphatic systems. The human circulatory system circulates approximately five liters of blood continuously throughout Figure 1 . Identification of N-linked glycosites within various cells, solid tissues, and plasma. A combination of enrichment methods and MS was used to identify glycopeptides in plasma, B and T lymphocytes, a metastatic liver tumor, breast, bladder, and prostate cancer cells. A total of 1105 glycosites were identified in plasma alone. The unbracketed number inside the ring and within the gray circle represents the number of glycopeptides that were found within both plasma and the individual sample. The number outside the ring and within the gray circle represents the number of glycopeptides found within the individual sample but not within plasma. The number in parentheses represents the number of glycopeptides that were unique to the tumor tissue or cultured cell type [1] . the body of an average adult [2] . As blood enters capillaries, plasma leaks out to fill spaces between individual cells of tissue, becoming interstitial fluid. The delivery of nutrients to the cell is balanced by the transport of other components (i.e., waste products, signaling hormones, etc.) back into the circulatory system. The balance between capillary oncotic pressure and hydrostatic pressure causes a slow increase in the volume of interstitial fluid. Approximately 90% of the fluid that enters the interstitial space enters back into the circulatory system by osmosis. The remaining 10% of the excess interstitial fluid diffuses into lymph capillaries and is returned back to circulation, after it has been processed within the lymph nodes, via the lymphatic system [3] . Once within the lymphatic system, the interstitial fluid is called lymph. Most lymph rejoins the circulatory system through the thoracic duct, the largest lymphatic vessel in the body. Great effort has recently been exerted into characterizing the proteomes of various biofluids. The dominating reason is the hope that biomarkers indicative of a prevailing disease condition can be discovered. Much of the effort has focused on components of the circulatory system because no other biofluids possess such a broad range of characteristics that make it optimal for the discovery of protein biomarkers. First, as mentioned previously, no other fluid has complete intimacy with the body in the way blood has. In general, no cell in the human body is outside the diffusion distance of the circulatory system. Therefore, blood contains valuable information reflecting the specific physiological and pathological state of the whole human body. The extent of this information is reflected in the complexity of the proteomes of serum, and plasma. Second, blood is obtained through a relatively noninvasive procedure (i.e., venipuncture). Third, the protein content of blood is very high. While other biofluids may better fulfill one of the above criteria (e.g., urine is collected in an even less invasive manner), they do not completely satisfy the above characteristics in the manner that blood does. Because of the relatively noninvasive nature for its acquisition and the amount of information it can potentially provide, blood is routinely collected for biomarker screening and for monitoring the condition of the patient over long periods of time. Even though lymph does not encompass all of the same valuable characteristics, its close relationship with blood and the body's immune response makes it a potentially valuable source of novel disease-specific biomarkers.@story_separate@In most biomarker-driven proteomic studies, it is the plasma or serum portion of blood, rather than whole blood itself, that is analyzed. Plasma refers to the liquid component of blood and makes up about 45-55% of the total blood volume [4] . Blood cells, such as red and white blood cells (RBCs and WBCs) and platelets, are suspended within plasma. Plasma is collected by withdrawing blood in the presence of an anticoagulant (e.g., heparin, sodium citrate, EDTA, etc.) and promptly centrifuged to remove the cellular elements. Serum is prepared by withdrawing blood in the absence of any anticoagulant, allowing for the formation of a fibrin clot [4] . Centrifugation is then used to remove blood cells and a large portion of the fibrinogen content via the fibrin clot. The process of coagulation makes serum qualitatively different from plasma. At a macroscopic level, the protein concentration of serum is less than that of plasma; however, the differences have been shown to be on the order of only 3-4% [5] . This difference is largely a result of the removal of a large portion of the fibrinogen content of plasma in the form of the fibrin clot. Other proteins, however, are also removed by specific or nonspecific interactions within the fibrin clot. Conventional thinking would surmise that many coagulation factors are also removed in the preparation of serum. Actually factors IX, X, XI, and VII/VIIa are found within serum [6] . While its primary effect is the removal of the fibrin clot, coagulation involves platelet activation and coagulation cascades with many reactions occuring in the process. One study showed that the levels of platelet-secreted vascular endothelial growth factor (VEGF) are 230 6 63 and 38 6 8 pg/mL in serum and plasma of normal individuals, respectively [7] . In studies of patients suffering from thrombocytosis, in which their platelet count is substantially increased compared to matched healthy controls, VEGF levels were also found to be much higher in serum than in plasma [8] . These results show that serum and plasma VEGF levels are affected by platelets, but more markedly so in the serum. One of the most commonly asked questions is whether to use serum or plasma for biomarker discovery. The HUPO recommended based on its pilot phase of the Plasma Proteome Project (PPP) of 2002 the use of plasma over serum [9] . This recommendation was put forth because of the lower degree of ex vivo degradation observed in plasma samples that were analyzed using a variety of proteomic platforms. In addition, the recommended anticoagulants for plasma were citrate and EDTA, and not heparin. This recommendation is pretty obvious considering the molecular size and difficulty in removing heparin compared to citrate and EDTA. Heparin is highly charged and could interfere with the subsequent MS analysis, especially when a profiling method such as SELDI-TOF is being used. The choice of a sample type and preparation method has to be targeted to the specific biomarker discovery needs with a closely planned and controlled procedure. It has been reported that not only the sample choice (e.g., serum or plasma) but also the sample-collection protocol (e.g., type of collection tube) and the sample-processing procedure (e.g., coagulation temperature, time allowed for coagulation, and anticoagulant used) could all bias the final results [10, 11] . Care must be taken when the archived samples are analyzed, as a recent HUPO study clearly showed how sample processing has a significant impact on the obtained results [9] . An important step in biomarker discovery will be the development of standardized methods that allow cross comparison of different studies. While lymph is closely related to both plasma and serum it is not prepared from either of the two. As mentioned previously, lymph is made up of approximately 10% of the interstitial fluid that does not reenter circulation and is captured by the lymphatic system [3] . The lymphatic system is a major component of immune response system and is made up of a network of organs, lymph nodes, lymph ducts, and lymph vessels that transport lymph from tissues to the bloodstream. The tonsils, adenoids, spleen, and thymus are all considered a part of the lymphatic system. Lymph ranges from clear-to-white and contains RBCs, WBCs (primarily lymphocytes), as well as proteins and fats. Lymph is acquired from the patient by insertion of a cannula into the thoracic duct. The process of acquiring lymph is by no means routine and probably explains why it has not been the subject of as many proteomic investigations as serum and plasma. The realization that MS-based technologies had the capability of identifying large numbers of proteins within complex proteomes was first shown through the combination of 2-DE and MS. During the mid 1990s, several laboratories combined the high-resolution separation capabilities of 2-DE with the high-throughput identification of MS to characterize a number of complex proteomes [12] . It was several years later that John Yates' laboratory showed the ability to circumvent 2-DE and use a combination of multidimensional fractionation and MS/MS analysis to identify almost 1500 proteins from Saccharomyces cerevisiae [13] . With the capability of identifying large numbers of proteins in a comparatively rapid manner realized, proteomics turned its efforts to the characterization of complex proteomes from a variety of different organisms and cell types. While most of the early analytical focus in proteomics was on cultured cells and simple prokaryotic and eukaryotic organisms, in the early part of this decade a number of researchers including George Wright Jr., Daniel Chan, Sam Hanash and William Hancock amongst others, initiated clinical studies examining human biofluids. In 2002, a paper showing the ability to correctly diagnose serum samples obtained from women with ovarian cancer using simple TOF spectra obtained using a low-resolution mass spectrometer was published by Lance Liotta and Emanuel Petricoin [14] . While this study did not focus on broad-scale protein identification, it was limited to examining the low molecular weight fraction of the serum proteome, and the results remain extremely controversial, it created such a frenzy in the scientific community that many high-resolution MS/MSbased proteomic laboratories began focusing on methods for analyzing clinically important biofluids. It was recognized early on, particularly in the analysis of serum and plasma, that the high dynamic range of protein concentrations found in these two fluids was going to be problematic for downstream MS analysis [15] . On the surface, serum and plasma seem to be the ideal clinical samples for MS-based proteomic analysis. They are relatively easy to obtain from the patient and have a very high protein concentration (e.g., on the orders of tens of mg/mL). The protein concentration, however, is deceiving. Twenty-two proteins make up approximately 99% of the protein content of serum and plasma (Fig. 2) . It is estimated that the protein concentrations in these samples span ten orders of magnitude and the prevailing thought is that specific disease biomarkers for diagnostic and prognostic purposes are most likely to remain within the very low concentration range. Considering that the dynamic range of a mass spectrometer is on the order of two orders of magnitude, it is easy to figure out that a straightforward LC-MS/MS analysis will result in the characterization of only the highest abundance, and probably least interesting, proteins. While strong cation exchange (SCX) prefractionation prior to a RP LC-MS/MS analysis has been shown to increase the ability to identify low-abundant proteins in many proteomic studies [16, 17] , this strategy alone is not sufficient to gain comprehensive coverage of the low-abundant proteins within biofluids. It was quickly recognized that to effectively characterize serum or plasma was going to require methods to remove the high-abundant proteins prior to downstream analysis. One of the earliest approaches used to deplete high-abundant proteins was to pass a serum/plasma sample over Cibracon blue, a dye with a high affinity for albumin [18] . Albumin, as shown in Fig. 2 , comprises approximately 50% of the protein content of serum/plasma. Recently, Agilent introduced the multiple affinity removal system (MARS) for the immunodepletion of six high-abundant proteins (i.e., albumin, IgG, IgA, transferrin, haptoglobin, and alpha-1antitrypsin) in serum/plasma [19] . Similar products have been developed, including a ProteoPrep 20 plasma immunodepletion kit from Sigma, and the Seppro ™ MIXED12 IgYbased affinity LC column, for the depletion of the 12 highest abundant plasma proteins manufactured by GenWay Biotech [20] . The reproducibility and effectiveness of these products to deplete major proteins in serum/plasma samples have always been a concern. In fact, a recent study published the results of the reproducibility of a MARS column across serum samples from patients with prostate cancer. They found that the depletion of high-abundant proteins from all 250 serum samples was complete and reproducible, with a RSD below 7%, over a six week period [19] . A recent study comparing a series of sample preparation methods has also confirmed the effectiveness and robustness of immunoaffinity subtraction methods for simplifying the serum proteome prior to MS analysis [21] . Depletion of high-abundant proteins is now considered an essential sample-handling step in any serum/plasma study regardless of subsequent analytical strategies. There are always concerns, however, when using affinity based depletion strategies that potentially important biomarkers will be lost either through the possible ""sponge"" effect of the high-abundant proteins or by the nonspecific binding to the affinity column used. Indeed, studies have shown that proteins remain bound to the targeted high-abundant proteins during their depletion [22, 23] . Moreover, a major protein depletion alone certainly was not enough to deal with the dynamic range problem. Besides the affinity depletion approaches, alternative approaches have been applied to target and isolate a subproteome of the serum/plasma in order to reduce the sample complexity and improve low-abundant protein characterization. One such approach has utilized hydrazide chemistry to capture and enrich glycoproteins onto a solid support and eventually release N-linked glycosylated peptides using Nglycosidase [1, 24] . Glycosylation plays a significant role in modulating the function and physiology of body and aberrant glycosylation has been implicated in many diseases. Since most secreted proteins are glycosylated, enriching for this class of peptides not only reduces serum/plasma sample complexity but also provides a targeted approach for biomarker discovery. This glycopeptide-targeted approach is capable of identifying hundreds of glycopeptides in a single analysis. Another approach is to apply a reversible capture release cysteinyl-peptide enrichment method using thiopropyl-sepharose 6B thiol affinity resin to reduce serum/plasma sample complexity [25] . This method is most effective, however, when used in combination with albumin depletion as this protein contains a large number of cysteinyl residues. This technology has shown the capability of identifying and quantitating over 600 proteins in a single LC-MS/MS run. To achieve the dynamic range measurements needed for serum/plasma samples, it has become a common practice to use a combination of strategies of depletion and fractionation strategies. Fittingly, one of the first large scale studies that showed the ability to identify hundreds of proteins within a biofluid, in this case serum, incorporated 2-DE with Figure 3 . Characterization of the human serum proteome using immunodepletion/chromatographic/2-DE fractionation strategy followed by MS identification. Serum, in which the high-abundant proteins had been immunodepleted, was fractionated using anion-exchange and size-exclusion chromatography resulting in a total of 74 fractions that were separated and visualized on 2-DE gels. Raw and immunodepleted serum were directly separated on two other gels. Analysis of the accumulative 20 000 spots resulted in the identification of 350 unique proteins [26] . MS identification. (Fig. 3 ) [26] . Recognizing its large dynamic range of protein concentration, the serum sample was immunodepleted to remove the most abundant proteins (i.e., albumin, IgG, haptoglobins, transferrins, transthyretin, a-1antitrypsin, a-1-acid glycoprotein, hemopexin, and a-2-macroglobulin). The remaining proteins were separated into 74 fractions using sequential anion-exchange and size-exclusion chromatography. Each of these fractions was run individually on a 2-DE gel. Coomassie staining of the gels resulted in approximately 20 000 individual protein spots. Removal of redundant spots by the analysis of the visual images still left 3700 unique spots. Analysis of these spots by MALDI-TOF and/or LC-MS/MS resulted in the identification of 1800 of these spots, which could be correlated to 325 unique proteins. So what did they find in serum? Almost 39% of the proteins identified were known to be localized within the circulatory system, while 35% represented intracellular proteins that are hypothesized to leak into circulation. Proteins that are known to reside on the cell surface made up just over 6% of the total number of unique protein identifications. Not surprisingly, considering the amount of fractionation that was conducted, several proteins with known serum concentrations less than 10 ng/mL (e.g., interleukin-6, metallothionein II, cathepsins, and various peptide hormones) were identified. Almost concurrent with the above study, the laboratories of Richard D. Smith and Joel Pounds at Pacific Northwest National Laboratories were investigating the human serum proteome using multidimensional fractionation of a serum tryptic digestate combined with MS/MS identification (Fig. 4 ) [27] . As done with the above 2-DE-based study, serum was immunodepleted, however, only for Igs and not several of the other major high-abundant proteins. This immunodepleted sample was fractionated into 60 aliquots using a SCX LC. Each of these aliquots was analyzed using a microcapillary RP LC coupled on-line with tandem MS. This solution-based (or ""shotgun"") method resulted in the identification of 490 unique proteins (cf. 325 in the 2-DE analysis). As with the 2-DE study described above, many of the expected circulatory proteins were identified as well as those originating from cells and tissues throughout the body. Several very lowabundant proteins, such as prostate-specific antigen (PSA), which are believed to be present at concentration in ng/mL range in the serum sample, were identified using this method. Both of these studies illustrated that the current technology is sensitive enough to detect low-abundant proteins in serum and plasma. Further developments in technology to deal with the serum and plasma sample dynamic range without losing the low-abundant proteins or a more targeted sample analysis are required for biomarker discovery. A comparison between these two serum analyses presents some obvious advantages/disadvantages for either strategy. The 2-DE strategy appears to be extremely laborious, requiring significant prefractionation prior to running 74 gels. This fractionation is followed by the selection of 3700 protein spots that are required in-gel tryptic digestion and MS analysis by MALDI-TOF, with additional LC-MS/MS analysis in cases where peptide mapping was unsuccessful. The 2-DE method, however, does provide an inherent protein quantitation if comparative studies are conducted, through the staining of the proteins fractionated within the gel. In addition, isoforms originating from differential PTMs such as glycosylation can be observed for individual proteins. The multidimensional LC approach is less laborious, requiring a single tryptic digest and tens of LC-MS/MS analyses to identify the proteins present within the serum. This method appears to be more sensitive than the 2-DE method for the detection of low-abundant proteins. The solution-based fractionation method does not, however, provide the individual protein coverage capable through the analysis of individual proteins fractionated by 2-DE. In addition, the solution-based fractionation strategy lacks direct quantitative comparison capabilities, however, algorithms such as spectral counting or an exponentially modified protein abundance index (emPAI) [28] have been employed recently to quantify protein data acquired using such a method. After the publication of these two studies, many other laboratories began their own exploration of the serum proteome. A recent study in human plasma proteome HUPO pilot phase project applied a Hi-D separation strategy using major protein depletion, IEF, one-dimensional gel electrophoresis (1D-SDS) to fractionate the proteins before they were digested and analyzed by RPLC-MS/MS on a linear IT mass spectrometer (LTQ) [29] . A total of 159 samples were analyzed by RPLC-MS/MS. A detection dynamic range of nine orders of magnitude with 575 and 2890 proteins iden-tified from plasma and serum, respectively, were reported. Of these identified proteins, 16 are known to be present in the pg/mL to ng/mL range. A number of subsequent studies utilized the SCX-LC with RPLC-MS/MS approach for both plasma and serum and the technology rapidly matures to a point that it was not uncommon to identify thousands of proteins within these biofluids. Obviously there will be differences in the number of proteins identified in studies, which is generally an effect of the sample processing methods used, but can also be dependent on the criteria and stringency used for MS/MS identification. When using SEQUEST to analyze raw MSW/MS data, a number of different parameters such as enzyme constraint, crosscorrelation (X corr ), and delta correlation (DC n ) score have a significant impact on the number of peptides that are considered as ""confident"" identifications. An excellent example is presented in a study conducted in Dr. Richard Smith's laboratory [30] . The same dataset of MS/MS spectra obtained from human plasma was analyzed using a variety of different enzyme constraints and X corr and DC n values used in other publications. Their results showed that the number of peptide and protein identifications ranged from 2912 to 3935 and 880 to 1682, respectively, depending on the criteria chosen. Recently, an extensive reference plasma proteome database from trauma patient has been established using the combination of major protein depletion, target protein enrichment, and multidimensional LC [31] . The crude plasma was processed and analyzed as illustrated in Fig. 5 . After removal of 12 high-abundant proteins, the sample was split into two aliquots. One of the aliquots was digested with trypsin, and a thiol affinity resin was added to this mixture allowing for the enrichment of cysteinyl-containing peptides. The other aliquot was oxidized by period and the glycoproteins were covalently coupled to hydrazide beads. These proteins were then digested with trypsin and the released peptides were isolated. The N-glycopeptides that remained bound to the beads were released using PNGase F. All four of the fractions (i.e., noncysteinyl peptides, cysteinyl-containing peptides, nonglycopeptides, and N-glycopeptides) were then individually separated using SCX into 30 fractions. Each fraction was analyzed using RPLC-MS/MS. Gene Ontology (GO) analysis revealed the identification of a large number of inflammation and immune response-related proteins in this sample. There were a total of 22 267 unique peptides identified in this extensive study corresponding to 3654 nonredundant proteins. The various fractionation strategies afforded the identification of proteins over a dynamic range of protein concentration greater than seven orders of magnitude. Many low-abundant proteins including 78 cytokines and cytokine receptors (such as tumor necrosis factor receptor, interleukin, vascular endothelial growth factor, and transforming growth factor-b, etc.) as well as 136 cell differentiation molecules were also identified using this method. While this method is obviously laborious and quite sophisticated, it provides an effective illustration into the importance of being able to observe lowabundant proteins if we anticipate using proteomic technology to identify novel biomarkers. Although most multidimensional LC methods that characterize biofluids are based on the separation of peptides, a recent study fractionated intact proteins according to their pIs (8.5 to 4.0) followed by their hydrophobicity. This sequential chromato-focusing/RP chromatography system is commercially available from Beckman Coulter, under the brand name ProteomeLab PF2D system (http://www.beckmancoulter.com/products/instrument/protein/proteome-lab_pf2d_dcr.asp). The intact protein fractions are then digested using trypsin and analyzed by LC-ESI-MS/MS or MALDI-MS for protein identification. Application of this method using an albumin-and Ig-depleted serum sample from a healthy individual resulted in the identification of 150 proteins [32] . Included in these identifications were proteins spanning three orders of magnitude in concentration (e.g., coagulation factor XIII and troponin T, which are present at mg/mL and ng/mL quantities, respectively). This strategy of fractionating intact proteins offers a greater opportunity of identifying various protein isoforms compared to methods that predigest the proteome samples into peptides prior to separation. While most of the developments in finding more effective ways to characterize the proteomes of complex biofluids have focused on sample preparation, novel MS instrumentation methods are also being pursued. A recent application using ion mobility-MS (IMS-MS) in combination with multidimensional LC to characterize the plasma proteome was able to yield 731 highly confident peptide identifications in merely 3.3 h without the need for high-abundant protein depletion [33] . Even though there are limitations to this emerging technology, IMS-MS undoubtedly shows a great potential as a new MS-based approach for high-throughput serum/plasma proteome analysis in the future. Lymph has been analyzed in only a few reports. In one of the first reports, normal ovine lymph was compared to plasma [34] . Proteins from both samples were analyzed using 2-DE. Both the lymph and plasma gels were dominated by albumin. Other plasma proteins that were observed in lymph included fibrinogen aand b-chains, IgG heavy chain, serotransferrin precursor, lactoferrin, and apolipoprotein A-1. Two proteins, glial fibrillary astrocyte acidic protein and neutrophil cytosol factor-1, were found to be differentially abundant in lymph, showing that this biofluid is simply more than just an ultrafiltrate of plasma. Even though the process of acquiring lymph is not trivial, it contains a lower concentration of large proteins since bigger plasma proteins, including albumin, do not readily pass through capillary walls into the interstitial fluid. This feature along with its close relationship to the immune response makes lymph a very interesting and informative proteome. The popular strategies that have been applied to serum/plasma sample studies described above could certainly help to better define proteomic differences between lymph and plasma/serum. Unfortunately these studies did not provide the ""biomarker goldmine"" that was anticipated; however, they did reveal the complexity that the proteomic community was facing as it moved forward in the search for diagnostic and therapeutic biomarkers. While the proteins identified in these many studies could be grouped using a number of classification categories (e.g., localization, molecular function, etc.), one thing was obvious, serum and plasma not only contained the expected circulatory proteins, but also proteins from every conceivable source (i.e., cell surface, cell nucleus, cell cytoplasm, mitochondria, etc.) in the body. There has probably been no more active field in proteomics over the past few years than the search for biomarkers. A simple search of PubMed using the terms ""serum"" and ""proteomics"" gives 673 citations since the year 2000, with approximately one-third of these being published since the beginning of 2006. Within these citations are studies that have a variety of different aims and use a number of different technologies. A detailed description of every different technology would fill this entire journal edition by itself. The relative importance of each study highlighted below is left up to the individual reader; however, we have endeavored to select examples that illustrate the breadth of techniques used to identify biomarkers within serum, plasma, or lymph. Obviously, the comprehensive identification of proteins in a single clinical sample is not going to reveal useful biomarkers. Such studies require some type of comparisons to be made between samples obtained from different populations (e.g., healthy versus disease-affected patients). As with any proteomic comparison, 2-DE remains a stalwart in quantitative comparison of biofluids. A 2-DE approach was recently applied to compare plasma samples obtained from patients with severe acute respiratory syndrome (SARS) and healthy individuals [35] . Twenty-two plasma samples from four different SARS patients were separated by 2-DE using a narrow range IPG strip (pH 4-7) and the resulting profiles compared to those obtained from six healthy plasma samples. Seven proteins were exclusively present in the 22 SARS samples. Eight additional spots were up-regulated in all 22 SARS patients compared to the healthy controls. Many of the proteins up-regulated in plasma from SARS patients can be classified as acute phase proteins (APP) that are produced as a consequence of serial cascades initiated by the SARS-coronavirus infection. Interestingly, the intracellular, antioxidant protein peroxiredoxin II was found to be up-regulated in all of the 22 SARS plasma samples. In a separate validation study, peroxiredoxin II was found in the plasma of approximately 36% of SARS patients, but only 10% of patients with fever. This rate of detection is higher than that found in human immunodeficiency virus (HIV) patients, suggesting that peroxiredoxin II may function as a useful serum biomarker for SARS infection. In addition to changes in protein abundance, differences in specific PTMs are also critical in disease pathology. A recent study evaluated total protein, glycoprotein, and phosphorylated protein difference between ovarian cancer (OVC) patients and healthy controls [36] . Plasma samples from five OVC patients and five healthy controls were used for the study. Each pooled plasma sample was first depleted of the top six proteins and then analyzed by 2-DE in triplicates to be stained with stains specific for total protein, phosphorylation, and glycosylation respectively. A phosphorylated isoform of fibrinogen-a-chain was found up-regulated in this study, which agrees with an early low-molecular weight serum study of OVC patients from the same group. Nongel based approaches for conducting comparative proteomic analyses of samples such as cell and tissue lysates have been widely applied, however, many of these methods that require stable-isotope labeling are not inherently useful for a comparative analysis of human biofluid samples. For instance, metabolic labeling with heavy isotopes while not impossible, as shown by the creation of a heavy-isotope labeled rat [37] , may not be practical for this type of analysis in which many samples from different subjects need to be compared. In addition, isotope-coded affinity tag (ICAT)-labeling, which has been used in numerous studies comparing proteomes of cell cultures and tissues, has been used on a limited basis in the comparison of biofluids, except for CSF. A recent study, however, used ICAT-labeling to measure changes in protein abundance observed in serum obtained from pediatric patients with severe traumatic brain injury (TBI) [38] . Samples from six patients (heavy ICAT-labeled) were compared to a pooled sample of healthy adults (light ICAT-labeled). All samples were depleted of albumin and IgG prior to ICATlabeling. A total of 95 proteins were found to be differentially abundant in the TBI serum samples compared to the pooled control. Most of the identified differentially expressed proteins are known to be involved in inflammation, innate immunity, and early stress/defense response. These proteins included several low-abundant proteins such as Toll receptors, signaling kinases, serine/threonine-protein kinases, transcription factors (serum response factor, golgin 45, myocyte-specific enhancer factor 2B), proteases (pappalysin-2 precursor, MMP-9), and proteins involved in response to oxidative-stress. The global changes in serum protein expression in TBI patients indicated a massive defense response with the most prominent response being the recruitment of proteins involved in inflammatory and immune pathways. Several proteins that can potentially be localized to the brain were quantitatively measured in this study. Many of these, such as g-enolase, amyloid b4 precursor, a-spectrin, and cleaved microtubule-associated protein tau, which have been previously detected in serum or CSF from TBI, or other types of brain injury, were found at increased levels in pediatric TBI patients. This study showed that ICAT-labeling can be useful in the comparative analysis of biofluid samples. While there have also been studies utilizing 18 O/ 16 O trypsin-mediated isotopic labeling, comparative proteomics of biofluids have typically been limited to spectral counting studies where the number of identified peptides, or a peptide's peak area, is used as a measure of a protein's relative abundance compared to another sample. An excellent example of these methods was the study published by Richard Smith's lab in which peptide peak areas and the number of peptide identifications from 2D-LC-MS/MS analyses were used to garner a quantitative comparison of protein abundances between plasma samples obtained from a human subject prior to (untreated) and 9 h after lipopolysaccharide (LPS) administration (treated) [39] . LPS is an endotoxin released by Gram-negative bacteria that is known to induce inflammatory reactions, such as cytokine production, cell migration, and production of acute-phase proteins. This study sought to quantitate changes in the acute phase plasma proteome in response to the LPS administration. The untreated and LPS-treated plasma samples were digested with trypsin and each sample was fractionated using a SCX chromatography. A total of 50 fractions were collected for each sample and each of these was analyzed by RPLC-MS/MS. Some of the SCX fractions that had a high peptide content were run twice, resulting in a total of 148 RPLC-MS/MS analyses. Combining both analyses (i.e., treated and nontreated) resulted in a total of 804 unique plasma proteins (not including IgGs) being identified from 5176 unique peptides. Of these, 83% (669 proteins) were identified by at least two unique tryptic peptides. To determine if the number of peptide identifications for each protein could be used in a quantitative manner, the group plotted the number of peptides identified for 74 specific proteins against their literature-documented concentration in plasma (Fig. 6) [39] . In general, the correlation was quite good suggesting peptide hit number is at least semiquantitative. The group also compared the peak areas for peptides that were identified in both samples and used this ratio, along with the number of peptide hits, to identify proteins that were differentially abundant in LPS-treated plasma. A number of proteins were found to be significantly increased in concentration following LPS administration. Amongst these included several inflammatory or acutephase response proteins such as LPS-binding protein, LPSresponsive and beige-like anchor protein, C-reactive protein, serum amyloid A and A2, hepatocyte growth factor activator, Figure 6 . Correlation between the number of peptides identified for specific proteins during a multidimensional fractionation/MS/ MS analysis of plasma compared to their documented concentration [39] . and von Willebrand factor. As shown in Table 1 , eight out of the nine proteins listed for which a protein abundance ratio was determined showed an increase in concentration following LPS administration by both the protein abundance ratios and the ratios of peptide hits. The two computational approaches, however, are generally complementary as many of the up-regulated proteins were identified in only one of the two methods. This study was one of the first to show that signal intensity and peptide hit count could be used to quantitatively compare protein abundances in biofluids analyzed by LC-MS/MS. Presently, most nongel based comparative studies of serum and plasma are conducted using either of these two computational approaches to measure the relative quantitation of proteins in two or more samples.@story_separate@A simple search of the literature will reveal the enormity of resources that have been spent to search for biomarkers, particularly in serum and plasma. If success is gauged by the number of validated biomarkers identified using MS-based methods, the result looks pretty bleak. However, if we examine the steps in the identification of clinically useful biomarkers, the impact of proteomic developments is readily obvious. The identification of novel biomarkers begins with discovery. The purpose of the discovery phase is to identify proteins that possess some characteristic (e.g., abundance difference) that is different between samples obtained from disease-afflicted patients and healthy controls. This phase is usually conducted through the comparison of only a few (e.g., 10-100) clinical samples. A useful discovery study will provide many potentially useful biomarkers. In general, it will not be possible to move all of the potential biomarker candidates forward into the validation phase, where it may be necessary to examine thousands of samples. Therefore, key decisions need to be made to determine which biomarker candidates identified in the discovery phase will be interrogated in the validation phase. Those proteins that survive validation then become targets for assay development to specifically measure their presence in clinical samples. Proteomic studies, as described above, have not had a major impact in the validation phase. Their real impact at the discovery phase, however, is without question. Biofluid proteomics as it is practiced today is a science in its infancy, yet the rate at which it has grown is astounding. It was not more than 5 years ago that only a handful of laboratories were capable of identifying a few hundred proteins in serum or plasma. Today the ability to identify thousands of proteins, as well as hundreds of differences between disease-afflicted and control samples are almost commonplace. There are two major challenges facing clinical proteomics as it attempts to discover novel biomarkers. A simple reading of the literature, including this review, shows that many of the approaches that utilize MS/MS to characterize biofluids lack the throughput necessary to survey the number of clinical samples required to make any type of solid conclusion about the potential of any protein becoming a useful biomarker. The challenge will be to develop high throughput MS platforms that enable direct protein identification while having the capability of analyzing hundreds of clinical samples in a reasonable time frame. Fortunately, the speed at which current mass spectrometers are capable of performing MS/MS experiments makes this challenge attainable. The next challenge in proteomics will be to refine the decision point to increase the success rate at which candidates are validated as useful biomarkers. To produce a clinically useful biomarker requires four phases; discovery, qualification, verification, and validation with assay development [40] . These stages require the analysis of increasing number of samples, but fewer analytes within each sample. MS fits well within the discovery phase as it is able to meas-ure hundreds of differences between clinical samples. Although the discovery phase only requires on the order of tens of samples to be analyzed, this number is still time consuming with current MS-based proteomic studies. As the throughput of MS instrumentation continues to increase, the ability to survey a greater number of samples, and repeatedly observe specific differences in protein abundances, will increase the confidence in selecting which differentially abundant proteins have the greatest chance of eventually becoming validated for clinical use.","Probably no topic has generated more excitement in the world of proteomics than the search for biomarkers. This excitement has been generated by two realities: the constant need for better biomarkers that can be used for disease diagnosis and prognosis, and the recent developments in proteomic technologies that are capable of scanning the individual proteins within varying complex clinical samples. Ideally a biomarker would be assayable from a noninvasively collected sample, therefore, much of the focus in proteomics has been on the analysis of biofluids such as serum, plasma, urine, cerebrospinal fluid, lymph, etc. While the discovery of biomarkers has been elusive, there have been many advances made in the understanding of the proteome content of various biofluids, and in the technologies used for their analysis, that continues to point the research community toward new methods for achieving the ultimate goal of identifying novel disease‐specific biomarkers. In this review, we will describe and discuss many of the proteomic approaches taken in an attempt to find novel biomarkers in serum, plasma, and lymph."
"In 2012, a new fatal viral disease causing pneumonia and death was identified in Saudi Arabia (Zaki et al., 2012) . The newly emerged virus was termed as Middle East Respiratory Syndrome coronavirus (MERS CoV) (de Groot et al., 2013) . The infection range comprises the Arabian Peninsula and several countries worldwide (Banik et al., 2015; Choe et al., 2017) . The danger of MERS CoV is aggravated by fatal outbreaks documented in South Korea and China (Seong et al., 2016) . Despite several years of MERS CoV circulation, there are still many secrets of virus replication and fusion with host membranes that need more study. The structural approach to revealing changes in virus substructures can be of unique importance in determining viral structural dynamics. However, few molecular dynamics (MD) simulations have been carried out to investigate MERS CoV structural changes and the dynamical aspects of MERS CoV molecular domains (Alfuwaires et al., 2017) . The viral membrane fusion protein is a rational target for drug discovery, as inhibition of the viral membrane fusion function can lead to cessation of the replication cycle Yao and Compans, 1996; Vanderlinden et al., 2010) . This approach proved good efficiency against several viral infections as HIV (Carravilla and Nieva, 2018) , SARS CoV (Liu et al., 2009) and respiratory syncytial virus (Mackman et al., 2015) . Viral membrane fusion can be accomplished by fusion of the virus spike with a host cell receptor target (Bosch et al., 2003) . In most enveloped viruses, the spike protein is composed of two cleavable protein domains that can be cleaved by proteases. This property was recorded with SARS CoV, MERS CoV and mouse hepatitis virus (MHV) (Xu et al., 2004) . However, they show considerable structural differences including the size, composition of fusion proteins and the sites of protein cleavage (Yuan et al., 2017; Wicht et al., 2014) . The CoV spike is composed of two proteins, S1 and S2. There are two consecutive events that occur at the start of cell infection. The first step is virus attachment, in which S1 comes into contact with the host receptor. For MERS CoV, dipeptidyl peptidase-4 (DPP4) is the target for binding with host cells Kandeel et al., 2014) . Soon after attachment, S1 is cleaved by proteolytic enzymes to expose a highly hydrophobic membrane binding domain of S2 (Kirchdoerfer et al., 2016) . S2 is the fusion protein that integrates with the host cell membrane; its integration is followed by fusion of the viral and host cell membranes. In MERS CoV and the highly related SARS CoV, S2 is associated with protein fusion process Forni et al., 2015) . During fusion, major conformational changes occur in S2, forming a six-helical bundle (6HB) of three-stranded coiled coils . Each S2 subdomain contains two motifs, heptad repeat domain 1 (HR1) and heptad repeat domain 2 (HR2). HR1 forms a homotrimer exposing three hydrophobic pockets on its surface (Xia et al., 2014) . S2 HR domains pass through three conformational changes during viral membrane fusion. The first is pre-fusion state, in which both HR1 and HR2 are not bound together. The second is pre-hairpin intermediate state in which 6HB is formed. HR2 packs into the three major hydrophobic grooves of HR1. The last stage is stable hairpin formation, thus bringing the viral and cell membranes into proximity, forming membrane bilayer and starting viral membrane fusion (Gao et al., 2013) . When three HR1 motifs align together, the central core is predominantly composed of hydrophobic residues. A HR domain is composed of tandem repeat motifs of seven residues, named from a to g. Of the seven residues, the first (a) and fourth (d) are predominantly hydrophobic or bulky (Gao et al., 2013) . This feature is the main forerunner in coiled coil formation and becomes stabilized by the long hydrophobic interface. Previous reports showed that CoV spike is assembled in the form of trimers . It was reported that there are many unassembled monomers found in the cells as well as on the virion surface (Delmas and Laude, 1990) . Trimers are the accepted form of completing the fusion process. The functional and dynamical aspects of discrete spike monomers in virions are still not well understood. In this work, we carried out a comparison of structural dynamics of S2 monomer and trimer from MERS CoV. Molecular dynamics is a gold standard in the evaluation of protein structural changes and stability (Alfuwaires et al., 2017; Perilla et al., 2015) . Quantitative assessment of the changes in protein structure using MD simulation will help in understanding the global and local changes of protein domains or subdomains and support the future design of suitable compounds to modulate protein function. Classical tools such as root mean square deviation (RMSD) and more recent algorithms using global distance test (GDT_TS) and contact area difference (CAD) scores are used to evaluate and compare different structures (Olechnovic et al., 2013) . To date, only a few studies have been carried out to investigate the MD of viral membrane fusion in general, and specific studies for MERS CoV are scarce. In this work, we used MD simulation to reveal changes in MERS CoV HR structure during fusion and factors affecting HR stability. MD simulation, energy system stability, RMSD, hydrogen bonding, contact mapping of inter-residue and inter-HR interactions, GDT_TS, and CAD scores were used to evaluate HR stabilization mechanisms. For this purpose, we simulated the MERS CoV S2 protein in the YASARA structure software followed by comprehensive analysis with YASARA built-in analysis macros and webservers for the calculation of global and local changes in distance and contact change measures.@story_separate@In order to assess the changes of S2 monomer and trimer structures, two softwares with distinct force fields were used. 2.1.1. MD simulation using YASARA and AMBER force field Structures of the MERS CoV HRs were retrieved from the Protein Data Bank. Two structures were used in this study, 4MOD and 4NJL. Both structures are similar in sequence and well aligned except for 6 additional residues at the N-terminal region in 4NJL. The software YASARA Structure (version 14.12.2) was used for all MD simulations by opting the use of AMBER14 as a force field. The simulation cell was allowed to include 20 Å surrounding the protein and filled with water at a density of 0.997 g/ml. Initial energy minimization was carried out under relaxed constraints using steepest descent minimization. Simulations were performed in water at constant pressure with temperature at 298 K. In order to mimic physiological conditions, counter ions were added to neutralize the system; Na or Cl was added in replacement of water to give a total NaCl concentration of 0.9%. pH was maintained at 7.4. The simulation was run at constant pressure and temperature (NPT ensemble). All simulation steps were run by a preinstalled macro (md_runfast.mcr) within the YASARA package. Data were collected every 250 ps. A molecular dynamics simulation was performed using the CHARMM force field (MacKerell et al., 1998) (version 27) in NAMD (Kalé et al., 1999) with a non-bonded van der Waals cut-off of 12 Å. The monomer and trimer protein were solvated in a cubic TIP3 water box (20 Å water layer). Sixteen Na + and 12 Cl − (26 Na + and 14 Cl − ) ions were included in the monomer (trimer) case to neutralize the system. Periodic boundary conditions (Jorgensen et al., 1983) , a constant temperature of 298 K (controlled by Langevin temperature piston), the NVT canonical ensemble, and the particle-mesh Ewald summation for long range interactions were used. After a steepest-descent energy minimization to remove atomic overlaps, the systems were equilibrated for 0.5 ns, followed by a 50 ns production run with data collection every 2 ps. All simulations were run with SHAKE (Ryckaert et al., 1977) using a 2 fs time step. The contact between HR1 and HR2 residues before and after MD simulation was calculated by YASARA Contact Analyzer. The range of analysed residues included all amino acids of HR2 (L1259-Y1280). During calculation, two sets of results were collected based on the calculated free energy. At first, all contacts were calculated without energy restrictions; then contacts were reanalysed based on a −1.6 kJ/ mol (0.38 kcal/mol) contact energy cut-off (Pande and Rokhsar, 1998) . The changes in H-bonds before and after MD were analysed for HR monomer and trimer by YASARA. The ranges of analysed residues were I997-Q1031 for HR1 (residues in direct contact with HR2 without the linker region) and L1259-Y1280 for HR2. The secondary structure contents of HR monomer and trimer were analysed before and after MD simulation using the YASARA secondary structure analysis wizard. Comparisons were made based on the percentages of helix, sheet, turn, and coil content. GDT_TS is a common measure of global changes in protein structure. GDT_TS is used to compare the structure similarities between two proteins with identical sequence. In comparison with RMSD, GDT_TS is more accurate in measuring movement of small fragments and changes in flexible termini (Kufareva and Abagyan, 2012) . The structures of MERS CoV S2 monomer or trimer were imported to YASARA Structure. The initial structures and those after MD simulation were superimposed. The Critical Assessment of protein Structure Prediction GDT_TS score was calculated over a distance of 1, 2, 4, or 8 Å by the global distance test implemented in YASARA software. The CAD score is an important measure for structural changes, providing a measure of change in the contact area between two structures (Grzybkowska et al., 2016; Abagyan and Totrov, 1997) . For this analysis, contact MD simulation files were submitted to the CAD score webserver (Olechnovič and Venclovas, 2014) . The analysed structures output included all atoms-all atoms (A-A), all atoms-side chains (A-S), and side chains-side chains (S-S). The differences in contacts between two similar proteins can be quantitatively measured and inspected by colour display. The colour coding for superimposed contacts in the structures before and after simulation were red and green colours. Therefore, the changes in contacts between the structures in both S2 monomer and trimer can be visually assessed. Furthermore, local contact area differences can be assessed by evaluation of changes in colour output from CAD server contacts-area plot, where red and blue colour indicates lower or higher contact area differences, respectively. Bioinformatics and computational tools are widely used for understanding the functional and structural aspects of microbial proteins Kandeel et al., 2009; Alnazawi et al., 2017) . MD simulation is a widely used technique for understanding structural protein changes in response to different effectors (Alfuwaires et al., 2017; Moore et al., 1998; Shen et al., 2003; Kandeel and Kitade, 2018) . In this study, MD simulation was run in a system comprising monomer or trimer of MERS CoV S2 HR. The stability of each system was evaluated by changes in RMSD as well as changes in the system energy. In order to get maximal precision, the MD simulation results were compared from two different software programs by implementing two different force fields, AMBER14 and CHARMM. All MD simulations showed rapid energy stabilization for both HR monomer and trimer. Fig. 1 shows the changes in RMSD for each structure in relation to time in ps. HR trimer showed rapid stabilization at less than 5 ns, having constant low fluctuations in RMSD and remaining around 3 Å over the entire recorded simulation. In contrast, S2 monomer from two structures was less stable, showing high fluctuations in RMSD with major drifts at 25-30 ns (Fig. 1A) . Despite the lower RMSD observed for the monomer in 3MOD structure, it shows high fluctuations in RMSD. This indicates that monomer of S2 bears high flexibility and instability, while trimer constitutes the more or less rigid state of S2. This agrees with the prediction models and resolved structures indicating that S2 of SARS CoV (Deng et al., 2006; Bernini et al., 2004) and of MERS CoV could arrange into trimers (Gao et al., 2013) . Additionally, the results from NAMD CHARMM run (Fig. 1B) was highly comparable with YA-SARA AMBER14, indicating conserved features of trimer stability and monomer dynamic nature. Fig. 1C shows the energy during MD simulation and indicates the stability of the trimer at lower energy level. The changes in RMSD for every amino acid in MERS CoV HR were estimated for trimer ( Fig. 2A) and monomer (Fig. 2B) . The crystal structures of monomer (4MOD and 4NJL) showed more or less similar profiles, albeit with some differences in RMSD (Fig. 2B) . In S2 monomer, there was more generalized change in RMSD with clear differences at a) the N and C-termini of the HR complex, b) in the middle of the HR1 helix, and c) in the linker between HR1 and HR2. In contrast, the trimeric structures showed different profiles, with major changes in the linker and C-terminal regions and little or no change in other HR regions ( Fig. 2A) . In addition, most residues in trimer showed low RMSDs of around 1 Å, with a maximum value at 6.2 Å. A large increase in RMSD values was observed for residues in the range from GLY1250 to ASN1256 (RMSD 3-5 Å). S2 monomer showed more dynamic changes, with a peak RMSD exceeding 10 Å and generalized changes of 2-4 Å along the HR residues. Alignments of pre-and post-MD simulation structures for both monomer and trimer are represented in Fig. 2C . The alignment reveals greater stability for trimer (represented by one chain in the lower panel), compared with more dynamic changes in monomer (upper panel), especially in the middle of HR1, the linker region, and at the protein termini. The obtained results from NAMD software and CHARMM force field (Fig. 3) were almost similar that estimated by YASARA software. This confirms the finding that residues in monomer are highly mobile either within the linker region or within the backbone of HR1 and HR2. The higher RMSD scale (x-axis) in monomer implies generally higher changes in residues in comparison with trimer. RMSD is a common measure of global changes in protein structure. However, several concerns and uncertainties in using RMSD have been previously raised (Kufareva and Abagyan, 2012) . Of special interest are major dynamical changes at the termini of HR domains with large RMSDs, which might result in misestimation of dynamical changes across the whole system. For more accurate consideration of global changes and accurate inclusion of flexible or terminal highly mobile loops, analysis was also performed using GDT_TS. In agreement with RMSD, GDT_TS revealed the stability of MERS CoV S2 trimer (Table 1 ). The percentages of superimposable residues within 1, 2, 4, or 8 Å in trimer were 2-to 3-fold higher than in monomer. This reflects the more dynamic nature of S2 monomer during MD simulation. The GDT_TS scores for monomer and trimer were 40.6 and 74.2, respectively. Therefore, the greater global changes in S2 monomer are decomposed by trimerization. In addition to global changes in HR, specific residue changes were also investigated. The helical component of HR is composed of several repeats of seven residues. The position of residues in these repeats can be termed a, b, c, d, e, f and g. Of special interest are the residues at positions a and d; which are located almost in the center of the HR, are predominantly bulky and hydrophobic, and share in establishing the hydrophobic core of HR. Position a is represented by residues F1012, F1019, V1026, and L1033, while position d comprises residues F1001, M1008, T1015, V1022, and L1036. MD simulation revealed that residues at a and d positions are most stable, having the least changes in RMSD in comparison with the initial structure. The average RMSD after MD simulation for residues at a and d positions (RMSDad) was found to be smaller than the average of all residues. For HR1, the average RMSDad was 4.37 Å and 0.93 Å for HR monomer and trimer, respectively. Similarly, the RMSDad for HR2 was 3.9 Å for monomer and 1.01 Å for trimer. These values are much lower than the general RMSD averages of 4.98 Å for monomer and 1.49 Å for trimer (Table 2) . To determine the key factors governing the stabilization of viral HR, the inter-HR1-HR2 contacts were analysed. Residue-residue contacts were also analysed for their energy contributions to HR stabilization. During residue-residue contact calculations, the contact interaction could be significant if the interaction energy was below −1.26 kJ/mol. For the identification of key residues in contacts between HR1 and HR2, the contact value and number of residues were calculated. In all of the analysed data, there was no positive or repulsive energy. After MD simulation, the total number of contacts was increased for the trimer and to a lesser extent in monomer (Table 3) . Analysis of every HR residueresidue contact revealed three different levels of interaction energy: a) high interaction energy above 10 kJ/mol, b) medium interaction energy of 4-10 kJ/mol, and c) low interaction energy of 1-4 kJ/mol. The high interaction residue contacts occurred at two positions: first, just in proximity to the N-terminal of HR2, between K1021and Q1023 of HR1 and D1261 and L1262 of HR2; second, distal to the C-terminal of HR2, including the interactions between Q994, K1000, D1282, and E1285 (Fig. 4A) . Parallel to the high interaction residues, several lines of medium interaction energy residues were observed (Fig. 4B) . These medium interaction residues were distributed at almost regular intervals starting at the end of the linker between HR1 and HR2 (residues E1039 and L1252) and at residues T1257, L1259, L1269, and D1282. Weak interaction energy contacts fill the gaps between the previously described high and medium interaction contacts. This described profile applies to both monomer and trimer. However, in trimer there was an additional high-energy interaction at the start of the linker region. Therefore, it is suggested to consider the interaction energy of residues during the design of new antiviral membrane fusion agents based on Table 3 The secondary structure content, total surface area and residues contacts of MERS CoV S2 monomer and trimer before or after MD simulation. M. Kandeel et al. Computational Biology and Chemistry 75 (2018) 205-212 short peptides. CAD scores were used to assess the changes in structures after MD simulation, compared to the initial conformation. Similarities and differences in contact areas were plotted on a colour scale of blue, white, and red corresponding to the range from agreement to difference between structures. The superimposed contact map revealed more changes in contacts for monomer of MERS CoV S2 (Fig. 5) . In monomer, the residues with major changes in contact were ASP1053, ASP1059, GLU1062, SER1064, ARG1067, and GLY1068. In trimer, major contact changes were observed in ARG1067, GLY1068, I1070, and ASN1111. Analysis of A-A contacts revealed a wider area of contact changes in monomer from GLY1045 to LEU1085, while in trimer more restricted distances were seen from GLN1063 to PHE1073. The CAD score was higher for trimer than monomer (Table 4 ). This agrees with the lower RMSD for trimer and indicates low perturbations in trimer and high perturbations in monomer. The residues showing red spots on CAD superimposition plots also had the highest RMSDs in both monomer and trimer. This indicates the feasibility of using RMSD for evaluation of structural changes. Fig. 6 shows local contact area differences plotted on a colour scale for both MERS CoV S2 monomer and trimer before and after MD. Monomer showed more dispersed red spots, indicating larger changes in contact areas. While A-A analysis shows small areas of contact changes, A-S and S-S determinations show larger contact area changes. The secondary structure content of HR is shown in Table 3 . Helices and coils are the major constituents of HR. After MD simulation, the helix% was increased in both monomer and trimer on the expense of coils. Despite the differences recorded between monomer and trimer during MD simulation, little or no significant change was observed in their secondary structure contents. This suggests that the components of HR retain their full helical or secondary structures even before trimerization.@story_separate@During viral membrane fusion with the cell membrane, the virus spike S2 protein arranges in a coiled coil with its HR2 domain packed into a deep groove on HR1. By MD simulation, we show that monomer is more dynamic and its residues have more positional fluctuation than in trimer. Furthermore, HR2 recognition by HR1 occurs through three levels of energetic interaction, with high, medium, and low energies distributed in parallel patterns along the HR. The hydrophobic residues at the a and d positions of HR helices have the smallest RMSDs. GDT_TS and CAD scores coincide well with RMSD data, supporting the finding that monomer is unstable and undergoes large fluctuations. Based on these results, the design of peptide analogues could consider the energetic and dynamic aspects of HR1 and HR2 interactions. Since discrete or unassembled monomers are found in the cell and in virions, the noticed flexibility and high dynamics of spike monomers might modulate the virus infection process. Additionally, the stable less dynamic trimer might be required in stabilizing the viral-cell membrane hairpin formation in preparation for fusion of virus and cells. Fig. 6 . Colour-coded profiles for contact area changes. The colour scale ranges from 0 (blue) to 1 (red). Blue colour indicates lower contact differences. Red colour indicates higher degree of contact area differences. Monomer showed higher differences indicating their variable and dynamic structure. In contrast, trimer was more or less stable by showing lower differences. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)","Structural studies related to Middle East Respiratory Syndrome Coronavirus (MERS CoV) infection process are so limited. In this study, molecular dynamics (MD) simulations were carried out to unravel changes in the MERS CoV heptad repeat domains (HRs) and factors affecting fusion state HR stability. Results indicated that HR trimer is more rapidly stabilized, having stable system energy and lower root mean square deviations (RMSDs). While trimers were the predominant active form of CoVs HRs, monomers were also discovered in both of viral and cellular membranes. In order to find the differences between S2 monomer and trimer molecular dynamics, S2 monomer was modelled and subjected to MD simulation. In contrast to S2 trimer, S2 monomer was unstable, having high RMSDs with major drifts above 8 Å. Fluctuation of HR residue positions revealed major changes in the C-terminal of HR2 and the linker coil between HR1 and HR2 in both monomer and trimer. Hydrophobic residues at the a and d positions of HR helices stabilize the whole system, with minimal changes in RMSD. The global distance test and contact area difference scores support instability of MERS CoV S2 monomer. Analysis of HR1-HR2 inter-residue contacts and interaction energy revealed three energy scales along HR helices. Two strong interaction energies were identified at the start of the HR2 helix and at the C-terminal of HR2. The identified critical residues by MD simulation and residues at the a and d positions of HR helix were strong stabilizers of HR recognition."
"The perspective toward the importance of the healthcare has already changed considerably in time. Globally, healthcare is accepted as the second largest sector after manufacturing businesses in terms of economy. This situation has triggered the development and growth of the healthcare sector [1] . A competitive atmosphere is created for patients to receive faster and easier health services by means of healthcare institutions. Accordingly, as the level of welfare of people increases, they demand higher quality of healthcare services from hospitals. All of these are particularly inspiring investors to promote quality assurance in healthcare systems of countries [2] . It is desirable for people to have high patient satisfaction by receiving high-quality services conveniently at healthcare institutions. Especially, in European countries, people travel to other cities or countries because of poorquality healthcare services in the city where the patients are located. With the emergence of such a situation, many researches have been conducted to provide an easy, fast, and inexpensive healthcare service for patients [3, 4] . Factors that enable patients to have an access to healthcare have been taken into consideration in such studies, but these studies have been performed for local situations rather than general conditions. In this study, a general view is presented considering the factors that form the infrastructure of the health systems of countries. A study measuring health system performance covering 191 countries was conducted by the World Health Organization (WHO) in 2000. Five main factors defined as health, health inequality, responsiveness-level, responsiveness-distribution, and fair-financing were determined, and regression equations were formed in this study. The correlation between these factors and health system performance was emphasized. These factors were limited to [0, 1] , and a weighted value was given for each factor. Thus, the overall efficiency calculation was made in all WHO member countries, and their health systems were listed. However, this study does not include any factors regarding the economic data and health infrastructures of countries. We would like to emphasize that it would not be appropriate to make a comparison between the study conducted by WHO and our study. Because of the fact that the parameters discussed were different and that approximately 18 years had elapsed, there was no consistent comparison. Many methods have been developed in terms of both engineering and management to solve health problems. Especially, management approaches have been widely applied in the field of healthcare [5] [6] [7] . Addressing managerial implications for the provision of medical devices [8] , increasing the types of services, providing improved management information systems [9] , employing skilled managers in their fields, patient case management, health quality assurance systems, and so on are important on behalf of the managers in determining the applicable strategies by taking into consideration in the field of healthcare management. Thus, the effects of healthcare problems are finally concluded with implications by the research conducted by healthcare managers [10] . However, since healthcare problems are very specific issues or unique problems, such as avoiding vital harm to patients, insufficient resources, poor healthcare services, and medication error, researchers have adopted algorithms and optimization models in order to solve these issues [11] . Therefore, optimization techniques are indispensable methods of management, engineering, and business applications. The purpose of using optimization, also named as operation research (OR), is to provide maximum benefits (such as revenue and production) and minimize loss (such as costs, expenditure, defects, and waiting time). Optimization applications vary widely according to the areas where they are used, such as energy, automotive, manufacturing, transportation, and logistic. Optimization techniques have been exploited to solve problems in healthcare systems, which are among the most trend topics of recent years [12] . Commonly, optimization models have been developed to optimize the schedule of resources of the healthcare systems. In addition, optimization models have been established for the management of healthcare materials for logistics [13] , for emergency services, to reduce waiting time for patients and to reduce expenditure or cost of treatment on healthcare facilities [14] . However, these methods have been used limitedly in the field of healthcare. The reason for this is that work or patient flows have a stochastic structure rather than a deterministic structure in the healthcare systems. Stochastic constructions are usually explained by nonlinear equations, which means the analysis of mathematical modeling is both difficult and long-lasting in parabolic situations. Statistical analysis is an alternative method for solving healthcare problems [2] . In particular, statistical methods have been used to make predictions for management of the healthcare system in the future based on past data and experience. Especially, regression analysis was most widely used among statistical methods by researchers [15] . For this reason, regression analyses are considered as a good forecasting tool for the future. Nevertheless, this tool is not sufficient to use solutions alone in healthcare area, because of the fact that the statistical analysis obtained provides only information about what the current system will achieve in the future, not the future goals of these systems. Consequently, statistical analysis was used together with optimization technique in this study. In addition to statistical analysis, calculation of GHCI values belonging to countries was provided by using desirability optimization technique. Formulations were obtained by considering the lower and upper limits of the factors' values considered on the basis of this method. Besides, it was possible to clearly show the traces of the factors on which they affect the response that was the objective function of the optimization model with the developed methodology. The problem statement which is an optimization model with the help of statistical analysis was developed to create GHCI to measure the structural and economic status of healthcare of considered countries in this research. While the economic development of the countries has been measured with the global competitiveness index (GCI) studies so far, in this study, we wanted to examine the development levels of the healthcare systems of the countries by creating a GHCI [16] . Up till now, the quality of healthcare has only been determined on the basis of patient satisfaction [17] . This criterion was measured according to questionnaire surveys, so the numerical data and analysis were disregarded in the studies [18] . Patient satisfaction and employee performance were measured using questionnaire or verbal interview method to determine the quality of healthcare [19] . However, it is obvious that the results of such methods are weak in terms of accuracy or not enough to reflect the real problems of healthcare. For this reason, different methods have to be used to obtain the quantitative and tangible results. Through this research, the results of the analysis with the numerical data by quality tools have led to touchable solutions for quality of the healthcare and allowed the detection of the future problems [20] . We believe that this study will be a good source for future studies in terms of the measuring method of the healthcare quality. Also, this study will have significance in theory as it promotes a new index to measure the competitiveness of healthcare system across different countries. This study has come to fruition in four parts. In the first section, the studies in the literature have been discussed. The methodology of the study was considered in the second section. The factors affecting the healthcare systems were determined, and methodologies of the study were constructed on this part. The statistical, optimum, and feasible results were obtained with the developed method for GHCI, and the GHCI values belonging to the countries were calculated and ranked in the third section of the study. In the last section, conclusion about the study has been provided.@story_separate@The method used in the study consists of two parts as statistical phase and optimization phase. GHCI values of the countries considered were calculated by developing nonlinear optimization models based on statistical optimization technique. As shown by the flowchart in Fig. 1 , there are eight key steps as definition of inputs as decision variables, historical data collection, obtaining descriptive statistical information about the collected data and removing decision variables that are not statistically significant, determining the limits of decision variables, the decision variables and objective functions, creating the optimization models, and succeeding the optimum results for decision variables and objective functions in order to create GHCI of the countries considered. In the statistical analysis stage, the statistical significance of the factors was analyzed to define the decision variables and the limits of these variables in the statistical analysis stage. Optimization models containing decision variables that have an impact on the objective functions were developed, and optimum values of decision variables and objective functions were obtained in the optimization stage. Finally, after calculating GHCI optimization values not included in the flowchart, an index was created to list the healthcare systems of the countries considered in this study. The resources of the healthcare system of the countries were calculated by the World Bank database as the number of beds, doctors, and nurses and midwives per 1000 persons. However, for the analysis of these data to be consistent, the total number of these sources was calculated as below: where x ij is defined as a decision variable and i denotes the resources of healthcare of the countries and j represents the names of the countries. In this equation, i only symbolizes the number of beds, doctors, and nurses and midwives per (1) x ij = total x ij 1000 1000 persons, but since there are more than one factor in the study, i notation representing the factors was used in general terms. Likewise, j notation expresses generally the names of the countries considered in the study rather than writing them separately. A total − x ij refers to the total number of factors i for beds, doctors, and nurses and midwives in country j. The factors affecting healthcare systems were evaluated in two parts that were defined as structural and economical. The factors come from the resources that build the substructure of the healthcare system in the first part. The most important assessment for measuring the performance of a country's healthcare system is the relationship between resources and outcomes [21] . Some of these factors are doctors, assistant doctors, nurses, officers, patient rooms, beds, triage rooms, laboratories fulfillments of clinical requirements, general behavior of doctors, registration and administrative procedures, infrastructure and amenities, professional performance of doctors, and facilities at reception and outpatient department area [17] . These resources must be supplied and managed properly in a healthcare system. Nevertheless, deficiencies in the management of these resources are affecting the quality of healthcare in the negative direction [22] . Generally believed, physicians, nurses, and beds construct the infrastructure of the healthcare systems [23] . Thus, the numbers of physicians, nurses, and beds [24] that were used in this study were the most employed parameters in the researches [25] . We evaluated the effects of each resource on the different levels on OF the outcomes and analyzed healthcare resources individually. The life expectancy [26] factor discussed was also considered among the structural factors in this study. Especially, this factor may be more effective in state and private healthcare systems. As a result of the previous studies, there was a strong connection between HEs and life expectancy [21] . The main reason for this deal was that people with high levels of prosperity are increasing their HEs because people want to live longer. In this case, states or private enterprises need to increase their HEs. In terms of economy, there are many factors that are influential in the healthcare system level headedly. There are two economic factors that are gross domestic product (GDP) and gross domestic product per capita (GDP PC) of the countries considered to be influential on the healthcare systems. Countries with GDP PC of ten thousand dollars or more were regarded in this study. Moreover, in the studies carried out in terms of the relation between healthcare and income level, a positive correlation appears to exist between the income per capita and life expectancy [27] . HEs were considered as responses or dependent factors/ variables influenced by the independent factors. Most of studies covered at most six and few independent factors in the calculation of index scores. Generally, scientists suggested that the four independent variables, such as the number of physicians, nurses, beds, and healthcare expenditure per capita, were effective and reliable on the healthcare systems of countries. HE data of countries [28] were calculated by the percentages of countries dependent on GDP [29] . A statistical analysis of how these factors affect HE has been shown as the result. HEs of countries were considered as response variables. It was seen that HEs in economically developed countries are higher than those in developing countries. As a result of the statistical optimization analysis, the six important factors of which were gross domestic product, gross domestic product per capita, life expectancy, the number of beds, the number of nurses and midwives, and the number of physicians considered were more influential on HEs as revealed in this study. Optimization models were developed with the help of mathematical equations of the developed desirability functions. Developed mathematical models are provided to minimize the amount of HEs besides of maximization of GHCI with objective function. The desirability analysis and optimization techniques have been merged to create the main methodology of this study. The desirability equations obtained as a result of statistical analysis and the GHCI values belonging to the countries were calculated to construct the optimization models. Contemplating types of factors among the countries with index i and j all notations are presented in Table 1 . In the optimization model developed when decision variables are created for each country or each factor, there were 318 decision variables generated in total. Likewise, a total of 319 constraints were created by considering the lower and upper limits of each factor, in addition to contemplating the nonnegative constraint in the optimization model. The method of desirability has been developed to obtain the best results for multiple reactions or factors acting as a process. (This method is widely used for multi-objective optimization models.) It produces the best response values of the factors to minimize, maximize, or reach the target value of the specifications. While statistical analysis gives mostly linear regression equations, the equilibrium found due to the weighted factor values in the desirability technique has a nonlinear characteristic. Before constructing the optimization models, it is necessary to consider the function of desirability according to the results to be obtained as a result of statistical analysis. The factors affecting the response function directly influence the desirability function [30] . In short, it is desirable that the factors affecting the main response values are at the target values which are measured by the value of desirability. The best result is gained as this value goes from zero to one. The complete desirability function includes the upper and lower bound values of the factors that have an effect on the responses. GHCI was created separately for each factor. GHCI formula was obtained by geometric mean of these factors. However, the values of GHCI and HE are converted into the following formula in order to get a meaningful and accurate result. Thus, the value of GHCI and HE was placed between 0 and 1. where D * denotes the geometric mean of the desirability indexes of the factors. d 1 , d 2 , d 3 , … , d n take a value between 0 and 1. If it is the worst and undesirable value. n indicates the number of factors and since there are six factors in this study, n = 6 is written. There were six different factors in this research, and the expansion of these factors on the desirability formulas as objective functions for the optimization models was shown as below: where l 1 , l 2, l 3 , … , l 6 and l 1 , l 2, l 3 , … , l 6 are the lower and upper specification limit of the responses, the power w 1 , w 2, w 3 , … , w 6 correspond to the weighted factor, and it is the parameter that determines the shape of d 1 , d 2 , d 3 , … , d n . . c is a multiplier that was used to have the result obtained have normal values in the equations. To find n factors' values, the value of each response value is expressed as y 1 , y 2, y 3 , … , y 6 . The following developed equations have been used as objective functions with the constraints to determine the competitiveness indexes of a country's healthcare systems. In this part, as a result of statistical analysis, finding the best solution and the results obtained in optimization models have been discussed. Furthermore, calculating the GHCI values of the countries considered, the advancement levels of the health systems of these countries have been ranked. This section provides general information about the collected data for statistical analysis and shows the accuracy of the analysis. The precision of the factors with the effects was measured on the response's variables. Decision variables named as independent factors were abbreviated as gross domestic product, GDP; gross domestic product per capita, GDP PC; life expectancy, LE; the number of beds, B ; the number of nurses and midwives, NW; and the number of physicians, MD. The statistical analysis of factors and Healthcare Expenditure for factor i the country of j in the year y ∀ y ∈ � response variables are illustrated in Table 2 . The results of statistical analysis showed that the accuracy of the collected values had high values of R 2 . The accuracy of the statistical analysis of this study was estimated as 99.68% of the R 2 value and 99.63% of the adjusted R 2 value. The effects of the determined factors on the GHCI were examined based on HEs. The importance of a factor on the response depends on the P-value of the factor as a result of the statistical analysis. Contribution ratios were calculated as percentage of contributions to the total sequential sum of squares of each source in Table 2 . Higher percentages of contributions rates indicated that it calculated more variation on the responses. The most important factor was found as GDP PC by country in the statistical analysis. (P-value of GDP PC was computed as zero.) GDP, the numbers of nurses, physicians, and beds were found to be more impressive factors on HEs and GHCI. However, these results indicated that GHCI were not influenced by the HEs of countries. Thus, this factor was excluded in the application optimization model to calculate optimum values of remained factors, which was defined as an objective function. Note that the effects of factors considered for GHCI were measured as non-interactively. Optimization mathematical models for optimizing both the factors and the objective functions (or response) were developed to compute the optimal and feasible values. These values show the necessary data to compete for a country in the field of healthcare. Considering the factors analyzed, the calculated values were higher than the mean of the data of 53 countries. As a result of the statistical analysis revealed, sixth of the important factors discussed was more effective on HEs and GHCI in this study. Figure 1 shows the variation in the optimum and feasible results obtained for GHCI and HEs according to D*. The optimum point was located between 0.52 and 0.68 of D* (with the creation of graphic for feasible solutions, the D value was calculated as 0.72 maximum and the minimum value as 0.40, respectively) shown in Fig. 2 . The optimal point located to be within the feasible region, but two different objective functions (for different directions: max-min) ensure that the solution was nonlinear. The objective functions and the factors constituting the constraints that have optimum and feasible values are demonstrated in Table 3 To determine the individual effects of the factors that could affect the response and *Statistically significant (P-value < 0.01), **At the margin of statistical provisionally significance (P-value < 0.1) Contribution: displays the percentage that each source contributes to the total variation in the response CI: Confidence Interval is an interval estimation type for the actual values of an unknown population parameter calculated from the statistics of the observed data P-Value: The P-value is a probability that measures the evidence against the null hypothesis The values that each factor takes outside of optimum values were seen to affect the objective function either in the positive or negative direction. According to Table 4 , as GDP PC values of the factors increase, the value of the objective functions defined GHCI and HE decrease. This had a negative effect on GHCI, while it had a positive effect on HEs. The bidirectional tendency in the objective functions leaded to the transformation of the developed optimization models into nonlinear mathematical equations. On the reverse side, the values of GDP increase among the factors, the value of the objective function assigned GHCI increases and the value of the objective function assigned HE decreases. We could mention GDP and GDP PC are bidirectional tendency for the healthcare system of the country with the increase in life expectancy values as well as bidirectional effects on the healthcare system of a country. In terms of the resources, healthcare had different situations for GHCI and HEs. The values of the rest of these sources, except NW, affect the objective functions to a certain extent. However, the numerical increase in these two resources reaching a certain number did not affect the value of GHCI and HEs as well. In addition, MD and B factors did not have any effect on the desirability function defined D*. The effect of the factors affecting the healthcare system on GHCI and HE is shown in Fig. 3 . The calculation of the optimum values for each factor was completed by considering the D* value. Areas formed constitute the feasible zone for the objective functions in Fig. 3a -l. The most important point to be considered in these figures was that the factors that were effective in the healthcare system have the maximum values for the GHCI value (local maximum), forcing it to be at a minimum level (local minimum) for HE. There are two different behaviors in order to get the optimum values of the factors. The factors desire to get the maximum value for GHCI and want to get the minimum values for HE based on constructed optimization models. Therefore, D* values were calculated from different ranges for each figure. In addition, although HE and GHCI were defined as two independent objective functions, GHCI was affected by HEs. Our findings inferred that HE should continue to improve GHCI so that GHCI can become a much better forecaster of the quality of healthcare. The optimization model with statistical analysis has been developed to demonstrate the competitiveness of the healthcare systems of the countries covered in this study. Country selected rankings were made by calculating the GHCI scores of the countries with this study on healthcare. We aimed to show that a country with a high GHCI score has a quality and competitive health system. Thus, the healthcare systems with high GHCI score will have the ability to offer a better service to the patients. The GHCI scores of the countries ranged from 0 to 6. The average of the GHCI values of the countries was calculated 2.4758. When the GHCI scores were examined, it was found that the highest value was in the USA (5.7490) and the lowest value was in Qatar (0.4301) (see Table 5 ). The GHCI score of many countries was below the optimum GHCI value. Only seven out of 53 countries were above the optimum GHCI value. The GHCI score of 19 countries was above the average GHCI score. The rest of these countries need to improve their GHCI scores immediately. The results of this study partially coincide with the results obtained in several studies. The increase in HEs is caused by various factors, such as aging of the population, medical technology, and developments in living standards. In the context of the investigations, as people's quality of life and their willingness to live increases, HEs increase. In terms of the data, it is seen that HEs are high in economically developed countries further than developing countries [21] . In terms of the healthcare, policies of governments are examined, and it is desirable to reduce the HE which is a burden on the country's economy. According to observations, it was determined that the values of HEs fluctuated from country to country. The levels of HEs were mainly calculated as high in USA, Japan, Germany, France, and UK, while HEs were estimated low particularly in Cyprus, Lithuania, Estonia, Latvia, and Croatia. Comparing different countries defined locational, we can advocate that in the European region, where there was a high number of developed countries, and the level of HEs was the highest compared to the other location. As a remarkable point, there are large differences in HEs among European countries. For instance, in the level of HE in Germany, France was higher than the level of HE in Finland and Greece. Obviously, we can conclude that there was an imbalance in the HEs of countries close to each other. The most noteworthy factor in countries with above average GHCI value is the amount of GDP in these countries. It should be noted that the fact that the amount of the HEs is high in a country does not mean that it has a quality healthcare system. Indeed, the patient satisfaction surveys support this outcome in most of these countries. According to the government policies in these countries, it is a common idea to reduce HEs. The countries with high HEs (such as UK, Canada, Japan, Italy, Germany, France, and USA) healthcare systems are needed to be examined in detail. It is inevitable that countries with social and nonsocial healthcare systems will have an impact on HEs. When we look at health services in a comparative way, the high proportion of elderly people from the Scandinavian countries has become a major challenge in the delivery of healthcare. Likewise, the length of waiting times due to the density of the elderly has become unacceptable for England. In this regard, there is the pressure of providing the service to be provided in the health sector to its citizens before overseas people. Despite the revised health system in the USA, there are sectoral problems due to the increasing cost of health expenditures. Although most of the citizens in the country have complementary supplementary insurance, they cannot get a comprehensive service. In this regard, in order to receive full-scale healthcare, out of the country are mostly exported from the USA. However, the USA is still at the top of the list of competitive health sectors because it maintains its position as the most advanced country in terms of private healthcare management and innovation. This study has also its limitations. For example, the indices such as the number of private and public hospitals of a country were not able to be considered in this study because of lack of information about them. However, our independent variable as the number of beds or exam rooms instead of the number of hospitals was considered in the optimization models. Another limitation was about the data being relatively short-term. We recommend that we use decision variables to predict long-term global healthcare index five years and beyond. We have taken a small step to compare several factors that may impact the GHCI. We found that two economic dimensions can be better indicators than HE even though HE is also a good predicting variable for high-quality healthcare service. We encourage for further refinement of the GHCI by including healthcare resources (types of hospitals, technicians, technology, etc.) and the components of healthcare (governments as a rule/law maker, pharmaceutics sector, healthcare insurance companies, etc.) so that it can become a more reliable index. Thus, it may be able to better predict future GHCI. Since there are limited studies on this subject (being the first study on this subject), this study is very important for future studies in order to calculate the index of healthcare systems for cities or countries.@story_separate@Healthcare dates back ancient times depending on the development of the world and humanity. Applications of the politics in terms of health system have been determined, and adapting it to the changing world scheme, the process of drawing up different approaches for each term have been considered in a methodology in relation to the approaches of national and international elements affecting that period. The rise in the cost and expenditure of healthcare during this process is now determined by many variables, on the one hand, with the use of high technology to provide healthcare, the existence of expensive treatment methods in the form of supply, on the other hand, rise in income, improvements in the living standards, demographic changes, etc., in the form of demand. The existence of these items necessitates the application of changes. When health sector is considered socioeconomically, the main reason why developments in health sector should be mostly financed: a) by public sector and b) by private sector is regarded as a problem to be dealt with is related to finance and carrying out this organization. As the Corona Virus (Covid- 19) , which emerged in Chine and spread around the world, bringing out global health and economic problems along with, the importance of public sector has once been shown to be highly important in the field of healthcare. As a consequence, it is necessary that public should carry out an effective and leading service regarding the evaluation of health system and healthcare because the most important reason of this situation is the conversion of a health system, which has a social structure, to a nonsocial structure. The basis of this study was to develop optimization models to use the healthcare economics of the countries more efficiently. In these models, the constraints were derived from the factors that affect the healthcare economics besides healthcare systems. Thus, the optimum and feasible values of the factors as well as the GHCI and HE data were calculated in this study. According to the results of the study, we proved that allocating too much HE budgets (that can be regarded as waste) does not guarantee to have a high-quality healthcare system in a country. Index of healthcare competitiveness shows how good the quality service is in the healthcare field of countries. This index determines the ease or difficulty of receiving services of patients from hospitals or other health institutions. Taking the results obtained into consideration in this paper, the competitiveness of a country's healthcare system will only be possible if it carries optimum or feasible values. We have concluded that countries under these values have low quality of healthcare systems.","The aim of this research is to enhance desirability optimization models to create a global healthcare competitiveness index (GHCI) covering 53 countries with gross domestic product per capita (GDP PC) of over $10,000. The GHCI is defined as an index that reveals the progress and quality of the healthcare systems in countries providing their patients with easier access opportunities to healthcare services within the scope of this work. Methods of statistical analysis have been adopted together with optimization models and techniques in this research. The optimum and feasible values of the factors considered influential on objective functions have been determined as the basis of healthcare expenditure (HE) and GHCI in those relevant countries. Those released optimum outcomes are displayed between 0.64 and 0.66 in terms of desirability value. The GHCI values of those aforementioned countries range from 0 to 6. The computed average of the GHCI values of those countries is estimated as 2.4758. Finally, GHCI values of 53 countries have been calculated to set the current basis of desirability optimization models. These findings will be deemed as the basic essence of those prospective theories to be established for the future researches to constitute a new index to measure the competitiveness of healthcare systems in various countries all over the world."
"COVID-19 is a life-threatening disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoX-2). 1, 2 Since the COVID-19 outbreak in China at the end of 2019 and until the 7 July 2020, the pandemic has infected more than 11 million people worldwide. Due to routine anti-cancer treatments and examination in the hospital, patients with cancer are at higher risk of contracting COVID-19. [3] [4] [5] Besides, patients with cancer patients are more susceptible to infections due to overall poor health status and systemic immunosuppressive states, which has attracted increasing attention from clinicians. [6] [7] [8] [9] [10] Currently, a key question in oncology practice amidst the COVID-19 pandemic is whether anti-cancer therapy affects COVID-19 severity and mortality. Patients with cancer recently received anti-cancer treatments have been generally assumed to be at a higher risk of exacerbation. 11, 12 Early studies suggested that anti-tumor treatment within 14 d before COVID-19 diagnosis increased the risk of developing severe events. 13 However, recently published studies have reached different or even opposite conclusions. [14] [15] [16] Oncologists hold different notions and continue to receive mixed messages regarding whether cancer patients should receive anti-tumor treatments as usual. [17] [18] [19] In light of this, a systematic review is needed to summarize the best available evidence of the impact of anti-cancer therapy in cancer patients with COVID-19. Knowledge of anti-cancer therapy's role is essential for risk assessment, monitoring, disease prevention, and control in cancer patients with COVID-19. To date, there has been no systematic review that comprehensively explores the role of anti-cancer treatment in cancer patients with COVID-19 to guide clinical practice better. Therefore, we performed a metaanalysis of the available studies to explore whether COVID-19 patients with recent immunotherapy or other anti-cancer treatments had more severe symptoms and higher mortality.@story_separate@This study followed the recommendations of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. 20 PubMed, Embase, Web of Science, and the Cochrane Library were systematically searched for relevant articles published before June 28, 2020. The search strategy included the following specific terms: ""2019-nCoV"" or ""Coronavirus"" or ""COVID-19"" or ""SARS-CoV-2"" or ""2019-nCoV"" or ""Wuhan Coronavirus"" and ""cancer"" or""tumor"" or ""carcinoma"" or "" malignancy"" and ""anti-cancer treatment"" or ""anti-tumor treatment"" or ""surgery"" or ""chemotherapy"" or ""targeted therapy"" or ""radiotherapy"" or ""immunotherapy."" Besides, the references of the relevant reviews and original articles were manually searched to find out more potential eligible studies. The above process was performed independently by two reviewers. The inclusion criteria of the study are as follows: (1) Types of Studies: published studies reported the relationship between anti-tumor treatment and cancer patients infected by SARS-CoV-2; (2) Exposure intervention: COVID-19 patients received anti-cancer treatment (surgery, chemotherapy, targeted therapy, radiotherapy) within 40 d, and the time interval of immunotherapy was relaxed to six months; 21, 22 (3) Outcome indicator: the odds ratios (OR) with 95% confidence intervals (CI) for each type of anti-cancer therapy. The exclusion criteria: (1) Reviews, summaries of meetings or discussions; (2) Insufficient data information provided; (3) Duplicate publications; (4) The sample size was less than 25; (5) Patients received immunotherapy more than 180 d or other anti-tumor therapies more than 40 d before diagnosed as COVID-19. Two investigators independently and separately conducted the data extraction and quality assessment, and any discrepancies were resolved by discussion or by consulting a third reviewer. The extracted data included as follows: (1) publication data were encompassing the first author surname, the year of publication, country of the population, sample size, study design, and population size; (2) clinicopathological data such as age, gender, severe events, death events; (3) statistical data including OR and corresponding 95% CI. If the univariate and multivariate analysis were both reported, we selected the multivariate analysis. If the OR was not presented directly, available data from original articles were used to estimate the OR. The quality of the included studies was assessed using the Newcastle-Ottawa Scale (NOS) tool. 23 The NOS consists of three parts: selection, comparability, and exposure. A study was considered of high quality if it had a NOS score of ≥7.Higher NOS scores indicated higher literature quality. The meta-analysis was performed using Stata12.0 software (Stata Corp, College Station, Texas). We calculated odds ratios (OR) and corresponding 95% confidence intervals (CI) in each included study. Then, the OR and 95% CI were used to measure the association between anti-tumor treatment and severe and death events. Heterogeneity was assessed with the Cochran Q statistic and the I 2 statistic. For moderate heterogeneity (I 2 ≥ 50%), random rather than fixed-effects models were used. Results were considered significant statistically when the p-value was less than 0.05. We also tested for interaction between the subgroups and considered the interaction test significant when its p-value was <0.05. 24 Publication bias was assessed using the Begg funnel plot and Egger test linear regression test (where at least five studies were available). If P < .05 indicates obvious publication bias. Figure 1 illustrates the search process and the final selection of relevant studies. Our search found 1,106 studies from PubMed, Cochrane Library, Web of Science, and Embase databases, and 2 studies were retrieved from reference lists. After removing duplicates, 208 references were screened for titles and abstracts. Of these, 168 articles were excluded after the first screening based on abstracts or titles, leaving 40 articles for full-text review. After a full-text review, 23 articles were excluded due to the violation of inclusion criteria and the remaining 17 articles were eligible for inclusion in the current analysis. 3,13,15,16,25-37 Seventeen relevant studies were retrieved, including 15 retrospective studies and 2 prospective studies, comprising 3,581 cancer patients infected by SARS-CoV-2. 3, 13, 15, 16, [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] Detailed clinical characteristics of the study patients are reported in Table 1 . Seven studies were performed in China, 3, 9, 13, 15, 25, 26, 32 five in the USA, 16,27-30 two in Spain, 33, 37 one in France, 36 and the other two in Italy. 31, 35 Four of the included studies reported data on patients who diagnosis with COVID-19 while they received anti-cancer therapy. 31, 34, 35, 37 Additionally, patients who recently received anti-cancer treatment before the diagnosis of COVID-19 within 4 weeks and 30 d were reported in four studies and six studies, respectively. The anti-cancer treatment characteristics are summarized in Supplementary Tables 1 and 2. All articles are of high quality because of NOS score no less than 7. Five studies provided the data in terms of anti-cancer therapy and the risk of exacerbation in cancer patients with COVID-19. 3, 9, 13, 25, 26 With no obvious heterogeneity (I 2 = 22.3%, P = .266) among these studies, so a fixed-effect pattern was used for assessment. No correlations were observed between anti-cancer therapy and the risk of exacerbation (OR 1.54, 95% CI 0.96-2.49, P = .074). Nine studies were evaluated for anticancer therapy and the risk of death events. 16, 26, 27, 31, 32, [34] [35] [36] [37] A random-effects model was used since the heterogeneity test suggested obvious heterogeneity (I 2 = 68.3%, P = .001). The result showed no significant correlation between anti-cancer therapy and the risk of mortality in cancer patients with COVID-19 (OR 1.33, 95% CI 0.84-2.10, P = .229) (Figure 2 ). Five anti-cancer treatments, including surgery, chemotherapy, targeted therapy, immunotherapy, and radiotherapy, were analyzed to determine the relationship between different anticancer therapy and the risk of exacerbation. As shown in Figure 3 , surgery, chemotherapy, and immunotherapy are not associated with severe events in cancer patients infected by SARS-CoV-2 (All P-value >0.05). Only one study reported on targeted therapy and radiotherapy, and we do not observe patients who have recently received targeted therapy or radiotherapy at higher risk of exacerbation (Table 2) . 26  Associations between different anti-cancer therapies and the risk of mortality are shown in Figure 4 . No statistically significant correlation was shown between anti-cancer therapy (including surgery, chemotherapy, targeted therapy, immunotherapy, and radiotherapy) and the risk of death events in cancer patients with COVID-19 (All P-value >0.05). 16  To further verify the correlation of different anti-cancer therapies and the risk of exacerbation and mortality, subgroup analysis was conducted. The results of the subgroup analysis are presented in Table 2 . The subgroup analysis results further support the results of surgery, targeted therapy, and radiotherapy. In the result of subgroup analysis, we further observed that chemotherapy within 28 d increased the risk of death events (OR 1.45, 95% CI 1.10-1.91, P = .008, p-value = 0.015 for test of interaction), and immunotherapy within 90 d increased the risk of exacerbation (OR 2.53,95%1.30-4.91, P = .006, p-value = 0.170 for test of interaction). Subgroup analyses showed no statistically significant tests of interaction, except for the subgroup of the association between chemotherapy and the risk of mortality. Table 2 shows the results of publication bias, which were evaluated by funnel plots and Eggers test. The publication bias showed no significant publication bias in the included studies (All p > .05). Oncology patients are considered more susceptible to SARS-CoV-2 infections due to their immunocompromised status caused by both cancer and various anti-cancer treatments such as chemotherapy, immunotherapy, and radiotherapy. [38] [39] [40] [41] Given this, with the increasing risk of the COVID-19 pandemic, the management of oncology patients undergoing anticancer therapy must be adequately balanced. 42 Currently, there is a lack of knowledge to evaluate the risks and benefits of anticancer treatment in cancer patients with COVID-19. 43, 44 Several previous studies have shown that SARS-CoV -2-infected cancer patients who underwent recent anti-cancer treatment had a higher risk of clinically severe events than those not receiving treatment. 13 Liang first reported that patients who underwent chemotherapy or surgery in the past month had a numerically higher risk of clinically severe events than did those not receiving chemotherapy or surgery by analyzed data from 18 cancer patients with COVID-19. 5 Other scholars only found that patients receiving chemotherapy within 4 weeks before symptom onset were risk factors for death during admission to hospital. 32 However, some recent studies have argued that anti-cancer therapy did not affect the severity of COVID-19 among these cancer patients. 25 Due to the different study endpoints, experimental design, and sample size, the real impact of anti-cancer treatment on cancer patients with COVID-19 is still unclear. Therefore, we conducted a systematic review and comprehensive meta-analysis to evaluate the relationship between anti-cancer therapy and the risk of exacerbation and mortality in cancer patients with COVID-19. Compared with patients who had not received anti-cancer therapy within 40 d of testing positive for COVID-19, those who had received anti-cancer therapy did not suffer increased COVID −19 severity and mortality when analyzed by our meta-analysis. To elucidate this relationship in greater detail, we analyzed the role of different anti-cancer treatments, including surgery, targeted therapy, chemotherapy, immunotherapy, and radiotherapy. The result of different anticancer treatments obtained further support the above research findings. The time effect should be kept in consideration when interpreting the results. According to the current data, we divided the time interval between recently anti-cancer treatment (surgery, targeted therapy, chemotherapy, and radiotherapy) and diagnosis of COVID-19 into within 28 d and 40 d for subgroup analysis. Subgroup analysis showed that different anti-cancer treatments (surgery, targeted therapy, and radiotherapy) were not associated with increased risk of exacerbation and mortality, in addition to chemotherapy. Contrary to early reports, receipt of chemotherapy within 40 d before COVID-19 diagnosis was not associated with a higher risk of death and severe events from our study. 13, 34 Nevertheless, in our subgroup analysis, we observed an increased risk of death events related to chemotherapy within 28 d only. Interaction test results indicated that the association between chemotherapy and the risk of mortality was significantly modified by time interval. These differences might be 15 Tian et al. found that risk of COVID-19 severity and death was highest for patients with last chemotherapy treatment within 2 weeks of admission, and decreased as the time interval since last chemotherapy increased, with significantly reduced risk when the last treatment was at least 3 weeks before hospital admission. 15 Therefore, the time interval between the last chemotherapy and the diagnosis of COVID-19 should be more accurate analysis and fully considered in future studies, especially time interval of fewer than 21 d. A recent study by Luo et al. suggested that lung cancer patients' immunotherapy were not more likely to develop severe COVID-19 and death than those who were not received. 28 Our meta-analysis show suggested that immunotherapy not associated with increased risk of exacerbation and mortality in cancer patients with COVID-19, similar to the study by Luo 48 These findings may explain our results on immunotherapy, and future research should be considered the number of immunotherapy cycles received. To the best of our knowledge, this is the first meta-analysis evaluating the effect of recent anti-cancer treatment before diagnosed with COVID-19 for cancer patients. For the pooled result, we found that cancer patients recently under anti-cancer treatment days before diagnosed with COVID-19, including surgery, targeted therapy, immunotherapy, and radiotherapy, were not associated with increased risk of severe and death events. We also found that chemotherapy within 28 d increased the risk of mortality, and chemotherapy was not associated with increased risk of severe COVID-19. Because of the limited number of patients and potential confounding factors, whether immunotherapy and chemotherapy will be harmful to the patient during the epidemic deserve further evaluation. The results of this study should be interpreted with caution due to several limitations. Firstly, the major limitation of the current study is the small sample size, which may render the results underpowered. Secondly, the pooled results of anti-cancer therapy only come from one study. Thirdly, due to insufficient information in the included study, the different time interval delimitations are not precise and uniform. Finally, the quality of different studies was different, which might lead to bias. Concept and design: Bolin Wang. Acquisition, analysis, or interpretation of data: All authors. Drafting of the manuscript: All authors. Critical revision of the manuscript: All authors. Administrative, technical, or material support: All authors. Supervision: Yan Huang. No potential conflicts of interest were disclosed. No sources of funding were used to conduct this study. http://orcid.org/0000-0002-1599-9086@story_separate@Cancer patients recently under anti-cancer treatment before diagnosed with COVID-19, including surgery, targeted therapy, immunotherapy, and radiotherapy, were not associated with increased risk of exacerbation and mortality. Chemotherapy within 28 d increased the risk of mortality, and chemotherapy was not associated with increased risk of severe COVID-19. Given the limitations of the current study, these findings should be interpreted cautiously but deserve further evaluation in subsequent studies, especially chemotherapy and immunotherapy.","BACKGROUND: This study was designed to investigate whether COVID-19 patients with recently received immunotherapy or other anti-cancer treatments had more severe symptoms and higher mortality. METHODS: A literature search was performed using the electronic platforms to obtain relevant research studies published up to June 28, 2020. Odds ratio (OR) and 95% confidence intervals (CI) of research endpoints in each study were calculated and merged. Statistical analyses were performed with Stata 12.0 (Stata Corp LP, College Station, TX). RESULTS: A total of 17 studies comprising 3581 cancer patients with COVID-19 were included in this meta-analysis. SARS-CoV-2-infected cancer patients who recently received anti-cancer treatment did not observe a higher risk of exacerbation and mortality (All p-value >0.05). We also found that surgery, targeted therapy, chemotherapy, immunotherapy, and radiotherapy were not associated with increased risk of exacerbation and mortality (All p-value >0.05). Chemotherapy within 28 d increased the risk of death events (OR 1.45, 95% CI 1.10–1.91, P = .008, p-value = 0.015 for test of interaction), and immunotherapy within 90 d increased the risk of exacerbation (OR 2.53,95%1.30–4.91, P = .006, p-value = 0.170 for test of interaction). CONCLUSION: Cancer patients recently under anti-cancer treatment before diagnosed with COVID-19, including surgery, targeted therapy, immunotherapy, and radiotherapy, were not associated with increased risk of exacerbation and mortality. Chemotherapy within 28 d increased the risk of mortality, and chemotherapy was not associated with increased risk of severe COVID-19. The role of anti-cancer therapy in cancer patients with COVID-19 still needs further exploration, especially chemotherapy and immunotherapy."
"In 2020, the novel coronavirus disease (COVID-19) outbreak occurred worldwide. This serious pandemic not only affected the burden of coronavirus disease but also impacted psychological health, reduced physical activity (PA), and caused weight gain because of the COVID-19 lockdown [1] . During the pandemic, PA has been important for enhancing the immune system and maintaining physical and mental health [2] . However, group PA in confined spaces should be restricted due to the possibility of increasing the infection rate [3, 4] . Instead of indoor group PA, PA at home or outdoor PA is recommended [2] . In fact, according to up-to-date studies on COVID-19, outdoor PA, such as bicycling and walking, is increasing and is expected to be an alternative to public transportation that allows for social distancing [5, 6] . The benefits of outdoor PA are not only PA itself but also increased sun exposure, which improves vitamin D status. Both benefits are effective in improving musculoskeletal Int. J. Environ. Res. Public Health 2021, 18, 4437 2 of 12 conditions, especially bone mineral density (BMD) [7] . Osteoporosis is characterized by decreased BMD [8] . Preventing osteoporosis by increasing BMD is important because osteoporosis can cause fractures that lead to death in elderly individuals [9, 10] . Osteoporosis is closely related to lifestyle factors such as PA, dietary intake, vitamin D status, obesity, smoking, and alcohol consumption [7] . Increasing evidence has shown that PA could improve BMD, thereby preventing osteoporosis and fractures [11] [12] [13] . Furthermore, previous studies, including our previous study, have investigated the combined effect of PA and other lifestyle factors, such as body mass index (BMI, kg/m 2 ) or serum vitamin D, on BMD [14, 15] . However, studies on the combined association of PA and other lifestyle factors, such as smoking and alcohol consumption, with bone loss are rare. Sun exposure supplies vitamin D, which enhances bone health. Other sources of vitamin D are source foods and vitamin D supplements. Elevated serum 25-hydroxyvitamin D (25[OH]D), the form of vitamin D in the body, could increase the absorption of calcium and phosphorus from the intestine by reducing parathyroid hormone (PTH) secretion to increase BMD [16, 17] . An increasing number of studies are being conducted on the association between vitamin D and BMD. However, the intake of vitamin D alone is not evident to prevent fractures [18, 19] ; instead, review studies have suggested that combining vitamin D with calcium intake is associated with a lower risk of osteoporosis and fractures [20, 21] . On the other hand, an experimental study demonstrated that sunlight exposure is more effective for improving bone structure than vitamin D supplementation in rats [22] . One study confirmed that fracture prevalence was inversely associated with lifetime ultraviolet (UV) radiation exposure according to the Beagley-Gibson (BG) grade in women [23] . Another study from South Korea reported that the prevalence of vitamin D deficiency was higher in northern cities, which are relatively far from the equator, than in southern cities [24] . Accordingly, daily sunshine duration (SD) could increase vitamin D levels to improve bone health. However, the association between daily SD and osteoporosis in humans is still questionable. One of our previous studies investigated whether the combination of PA and serum 25(OH)D is inversely associated with osteopenia or osteoporosis using national population survey data with a cross-sectional study design [15] . Our study also examined the association between the intensity of PA and various causes of death using longitudinal national cohort data [25] . Furthermore, the purpose of our current study was to identify the association between PA and osteoporosis and between SD and osteoporosis using the same national cohort data that were used in our previous study [25] . In addition, we performed various subgroup analyses in our study, including age/sex, SD (or intensity of PA), obesity, smoking, and alcohol consumption status.@story_separate@This study was approved by the ethics committee of Hallym University (HALLYM 2019-08-029). The requirement for written consent was waived by the Institutional Review Board. All analyses followed the guidelines and regulations of the ethics committee of Hallym University. Health screening cohort data from the Korean National Health Insurance Service-National Sample Cohort (NHIS-NSC) were used in this study. The details of the Korean NHIS-NSC have been described elsewhere [26] . A total of 514,866 participants who were randomly selected from 5,150,000 health insurance holders and who had undergone a health screening by NHIS from 2002 to 2003 in Korea were included. The follow-up period of the data was from 2002 to 2015. Among 514,866 participants, 94,932 were included in the osteoporosis group according to the osteoporosis definition, and 419,934 were included in the control group. Participants who did not have PA records were excluded. As the PA information was different from 2002 to 2008, those participants were also excluded (n = 75,531 in the osteoporosis group, n = 56,732 in the control group). Control participants who were diagnosed with Interna-tional Classification of Diseases, 10th version (ICD-10) codes M80-M82 without BMD test data were excluded (n = 55,779). The matching method that was previously used in our studies was conducted [27, 28] . The osteoporosis and control groups were matched in a 1:2 ratio by age, sex, income, and region of residence. Control participants were sorted by random number to minimize selection bias. The index date of each osteoporosis participant was defined as the date of the onset of osteoporosis. Each control participant was assigned the same index date as each matched osteoporosis participant. During the matching process, 50 osteoporosis participants and 268,721 control participants were excluded. A total of 19,351 osteoporosis participants and 38,702 control participants were included in the final analysis ( Figure 1 ). Among 514,866 participants, 94,932 were included in the osteoporosis group according to the osteoporosis definition, and 419,934 were included in the control group. Participants who did not have PA records were excluded. As the PA information was different from 2002 to 2008, those participants were also excluded (n = 75,531 in the osteoporosis group, n = 56,732 in the control group). Control participants who were diagnosed with International Classification of Diseases, 10th version (ICD-10) codes M80-M82 without BMD test data were excluded (n = 55,779). The matching method that was previously used in our studies was conducted [27, 28] . The osteoporosis and control groups were matched in a 1:2 ratio by age, sex, income, and region of residence. Control participants were sorted by random number to minimize selection bias. The index date of each osteoporosis participant was defined as the date of the onset of osteoporosis. Each control participant was assigned the same index date as each matched osteoporosis participant. During the matching process, 50 osteoporosis participants and 268,721 control participants were excluded. A total of 19,351 osteoporosis participants and 38,702 control participants were included in the final analysis ( Figure 1 ). A schematic illustration of the participant selection process that was used in the present study. Of a total of 514,866 participants, 19,351 osteoporosis patients were matched with 38,702 control participants for age, sex, income, and region of residence. Osteoporosis patients were defined as participants diagnosed with M80 (osteoporosis with pathological fracture), M81 (osteoporosis without pathological fracture), or M82 (osteoporosis in diseases classified elsewhere) using ICD-10 codes ≥ 2 times and whose BMD was tested using dual energy X-ray absorptiometry (DEXA) or computed tomography (CT) scans (Claim codes: E7001-E7004, HC341-HC345). PA information was collected using a modified International Physical Activity Questionnaire (IPAQ) [29] . The questionnaire asked the number of days the participants Figure 1 . A schematic illustration of the participant selection process that was used in the present study. Of a total of 514,866 participants, 19,351 osteoporosis patients were matched with 38,702 control participants for age, sex, income, and region of residence. Osteoporosis patients were defined as participants diagnosed with M80 (osteoporosis with pathological fracture), M81 (osteoporosis without pathological fracture), or M82 (osteoporosis in diseases classified elsewhere) using ICD-10 codes ≥ 2 times and whose BMD was tested using dual energy X-ray absorptiometry (DEXA) or computed tomography (CT) scans (Claim codes: E7001-E7004, HC341-HC345). PA information was collected using a modified International Physical Activity Questionnaire (IPAQ) [29] . The questionnaire asked the number of days the participants walked ≥30 min, performed moderate-intensity activity for ≥30 min, or performed vigorousintensity activity for ≥20 min in a week. We used PA information from the first record of health screening. 'Moderate-to high-intensity PA (MHPA)' was defined as walking ≥5 days, performing a moderate-intensity activity ≥ 5 days, performing a vigorous-intensity activity ≥3 days, or any combination of walking, moderate-intensity activity, or vigorous-intensity activity ≥5 days with ≥600 metabolic equivalent (MET)-min/week based on the IPAQ. Other participants were classified in the 'low-intensity PA (LPA)' group. SD was defined as a measure of the hours of sunshine hours per day at each given location, excluding the duration of time it was cloudy or foggy. SD data were collected by the Korea Meteorological Administration (KMA) measured hourly by an automated synoptic observing system (ASOS) and manual measurement at 94 locations [30] . We merged SD data and NHIS-NSC data by residential area and index year. The residential areas were classified as Seoul, Busan, Daegu, Incheon, Gwangju, Daejeon, Ulsan, Gyeonggi, Gangwon, Chungcheongbuk, Chungcheongnam, Jeollabuk, Jeollanam, Gyeongsangbuk, Gyeongsangnam, and Jeju (16 areas). As NHIS-NSC data are annual data, mean values of 1 year (365 days) for SD were calculated based on the index year of participants. SD was classified into short SD (≤6 h) and long SD (>6 h) based on the median value of SD (6.0 h). The classification of age, income, and region of residence was performed as described in our previous study [25, 31] . Age was collected from participants who were ≥ 40 years old in 2002 and 2003 and classified into 9 groups at 5-year intervals. The income groups were classified from 1 (low) to 5 (high) level, and region of residence was divided into urban and rural. Tobacco smoking status (nonsmoker, past smoker, current smoker), alcohol consumption (<1 time a week, ≥1 time a week), and obesity based on BMI (<18.5 for underweight, ≥18.5 to <23 for normal weight, ≥23 to <25 for overweight, ≥25 to <30 for obese I, ≥30 for obese II) were classified in the same way as in the previous study [25, 31] . To evaluate the burden of comorbidities, the Charlson Comorbidity Index (CCI) score was used. The CCI includes 17 comorbidities. Higher CCI scores indicate more severe and varied comorbidities. In our study, the CCI was scored as 0 (no comorbidities) to 15 (7 comorbidities) as a continuous variable [32] . The general characteristics were compared between the osteoporosis and control groups using the chi-square test. Conditional logistic regression was used to analyze the odds ratios (ORs) with 95% confidence intervals (CIs) for osteoporosis in the MHPA group compared to the LPA (control) group and in the long SD group compared to the short SD (control) group. In this analysis, crude and adjusted models were calculated. In the PA groups, SD, obesity, smoking status, alcohol consumption, and CCI scores were adjusted in the adjusted model. In the SD group, PA instead of SD and other covariates were adjusted in the adjusted model. The analysis was stratified by age, sex, income, and region of residence. For the subgroup analyses, we regrouped participants by age and sex (<60 years old and ≥60 years old; males and females) and analyzed the crude and adjusted models with a conditional logistic regression model. Additionally, subgroup analyses according to SD (short and long SD)/PA (LPA and MHPA), obesity (underweight, normal weight, overweight, obese), smoking status (nonsmoker and past/current smoker), and alcohol consumption (<1 time a week and ≥1 time a week) were performed using model 1 (adjusted for age, sex, income, and region of residence) and model 2 (adjusted for model 1 plus smoking status, alcohol consumption, CCI scores, and SD (or PA)). In these analyses, we used an unconditional logistic regression. Two-tailed analyses were performed, and significance was indicated by a p-value < 0.05. We used SAS version 9.4 (SAS Institute Inc. Cary, NC, USA) for statistical analyses. The general characteristics of the participants are shown in Table 1 . The prevalence of MHPA in the osteoporosis group was significantly lower than the prevalence in the control group (42.1% (n = 8,142/19,351) vs. 44 .6% (n = 17,240/38,702), p < 0.001). The prevalence of long SD in the osteoporosis group was significantly lower than the prevalence in the control group (50.3% (9,726/19,351) vs. 51.5% (19,930/38,702) , p = 0.005). The adjusted OR for osteoporosis in the MHPA group was 0.90 (95% CI = 0.87-0.94, p < 0.001). In subgroup analyses according to age and sex, the findings were consistent with the main results in <60-year-old females and in ≥60-year-old males and females (p < 0.05, Table 2 ). The adjusted OR for osteoporosis in the long SD group was 0.96 (95% CI = 0.93-1.00, p = 0.027). In subgroup analyses according to age and sex, the findings were consistent with the above finding only in < 60-year-old females (p < 0.05, Table 3 ). In subgroup analyses conducted in the PA groups, the ORs of MHPA for osteoporosis were significantly lower in the short and long SD, normal weight, overweight, obese, nonsmoker, past and current smoker, and <1 time a week and ≥1 time a week of alcohol consumption subgroups (p < 0.05, Figure 2 and Table S1 ). In subgroup analyses conducted in the SD groups, the ORs of long SD for osteoporosis were significantly lower in the LPA, obese, nonsmoker, and < 1 time a week alcohol consumption subgroups (p < 0.05, Figure 3 and Table S2 ). In subgroup analyses conducted in the SD groups, the ORs of long SD for osteoporosis were significantly lower in the LPA, obese, nonsmoker, and <1 time a week alcohol consumption subgroups (p < 0.05, Figure 3 and Table S2 ). In subgroup analyses conducted in the SD groups, the ORs of long SD for osteoporosis were significantly lower in the LPA, obese, nonsmoker, and < 1 time a week alcohol consumption subgroups (p < 0.05, Figure 3 and Table S2 ).  We evaluated whether PA and osteoporosis, and sunshine duration and osteoporosis were associated using data from a large national cohort with 7-year follow-up periods. To the best of our knowledge, this is the first study to identify the association between SD and osteoporosis using meteorological data. We found that a lower occurrence of osteoporosis was associated with MHPA and long SD using Korean national cohort data with a matching method. Specifically, MHPA was associated with a lower occurrence of osteoporosis in various subgroups except >60-year-old male and underweight subgroups. On the other hand, long SD was associated with a lower occurrence of osteoporosis in females in the <60-year-old, LPA, obese, nonsmoker, or lower alcohol consumption subgroups. Increasing evidence has demonstrated that PA is positively associated with BMD; hence, PA can prevent osteoporosis and fractures. One review study summarized 43 randomized controlled trial studies by conducting a meta-analysis and found that non-weightbearing high-force exercise was the most effective exercise intervention for BMD of the femur neck (mean difference = 1.03, 95% CI = 0.24-1.82), and combination exercise programs were effective at improving spine BMD (mean difference = 3.22, 95% CI = 1.80-4.64) in postmenopausal women [12] . In one longitudinal study, men who engaged in weightbearing exercise during their lifetime had a reduced risk of low bone strength in old age. In detail, the bone mineral content and inertia were 6.5-14.2% higher in the group that performed high-weight-bearing exercise in their lifetime than in the group that performed low-weight-bearing exercise in their lifetime [11] . One cohort study reported that 5-year mean PA with adjustment for BMI was associated with increased lumbar spine BMD in men (0.010 (95% CI = 0.004-0.017) g/cm 2 ) and increased total hip BMD in women (0.006 (95% CI = 0.003-0.009) g/cm 2 ) [13] . The results of our study that was conducted using a national sample cohort also support the above study findings. Moreover, the study findings were consistent in detailed and varied subgroups such as the obesity status, smoking, and alcohol consumption subgroups, but the same was not true for the young male and underweight subgroups. To our knowledge, this is the first study to suggest that long SD is associated with a lower occurrence of osteoporosis using national cohort data. Although SD is a broad range of vitamin D indicator, the long SD group was associated with a lower occurrence of osteoporosis in our study. This finding is consistent with previous studies exploring the links of vitamin D obtained via dietary intake or supplementation, serum 25(OH)D, and sunlight exposure with BMD, osteoporosis, or fractures [20, 21, 23, 33] . One experimental study using rats reported that PTH was significantly reduced in the sun-exposed vitamin D-deficient group (67.69 ± 13.18 pg/ml) compared to the levels in the vitamin D-supplemented rats (78.93 ± 8.31 pg/ml) and vitamin D-deficient rats (86.05 ± 9.67 pg/ml) [22] . Regarding sunlight exposure in humans, one study demonstrated that high BG grade as a biomarker of lifetime UV radiation had beneficial associations with reduced fracture prevalence in females (fewer major fractures in women with higher BG grade than lower BG grade, relative risk (RR) = 0.75, 95% CI = 0.60-0.97) [23] . Based on the above studies and our study findings, daily SD could be considered one of the factors affecting bone health. Although PA × SD interaction was not significant (p = 282), the OR for osteoporosis was the lowest in the MHPA with long SD group compared to other combined PA and SD groups (reference = LPA with short SD, OR = 0.87, 95% CI = 0.83-0.91, Table S3 ). In other words, combining a higher intensity of PA with a longer SD could help to lower the risk of osteoporosis. PA could improve bone strength through multiple mechanisms, such as gravitational loads and muscle contraction forces [34] . Specifically, gravitational loads occur when the weighted body makes contact with the surface. Without gravitational loads, for instance, astronauts in space could have increased risk of bone loss because of the absence of gravity [35] . In addition, muscle contraction forces could function in a particular region where a contraction was forced. For example, resistance training in athletes and cyclists was positively associated with BMD of the legs [36] . Furthermore, our study findings showed an association with various characteristics, such as female sex, older male sex, and other lifestyle factors, including SD, smoking status, and alcohol consumption. Based on these findings, MHPA could be recommended for the general population to improve bone health. However, the association between PA and osteoporosis was not shown in the <60-year-old male subgroup or the underweight subgroup in our study. The common reason for these findings might be the lack of participants in each subgroup (the number of age < 60-year-old male group = 1314; the number of underweight group = 1655 out of a total of 58,053 participants). Another possible reason for the lack of an association in the <60-year-old male group is that the PA questionnaire did not classify PA into occupational PA and leisure PA. The retirement age in South Korea is usually 60 years old. A previous study using the same cohort data for PA and mortality analyses demonstrated that PA did not reduce mortality in males <60 years old. The PA in the study was not classified into occupational PA and leisure PA in the questionnaire, as described in our previous study [25] . In previous studies, in men with occupational PA, PA was not associated with BMD or was even associated with lower bone mineral content [37, 38] . Another possible reason is that BMD is usually preserved or increased until <60 years old in men [37] . In summary, in young males, MHPA was not associated with the occurrence of osteoporosis in our study because occupational PA was not separated from leisure PA in the MHPA group, and osteoporosis among young males was rare. Regarding the outcome of the underweight subgroup in our study, the previous study reported that active women were consistently at a lower risk of hip fracture than inactive women in each BMI category. However, the study finding shows that the risk for hip fracture was significantly higher in individuals with a low BMI than in individuals with a high BMI regardless of the intensity of PA [14] . Based on these findings and our study findings, both lower BMI and MHPA might affect BMD, making the association seem to disappear. On the other hand, previous studies emphasized that bone loss was more directly associated with loss of muscle mass than loss of fat mass. Moreover, weight loss resulting from exercising was not associated with bone loss [39, 40] . Therefore, reduced BMI due to PA may not be the cause of bone loss. The mechanism of vitamin D synthesis resulting from sun exposure in the body is as follows. When the skin is exposed to sunlight, UVB radiation interacts with 7dehydrocholesterol and forms cholecalciferol (vitamin D3), which is the primary source of endogenous vitamin D [17, 41] . Elevated serum 25(OH)D levels increase BMD by increasing the absorption of calcium and phosphorus from the intestine by reducing PTH, preventing osteoporosis [16, 17] . These mechanisms could explain our study findings regarding the association between SD and the low occurrence of osteoporosis. Moreover, females in the <60-year-old, LPA, obese, nonsmoker, and lower alcohol consumption subgroups showed the same results. Regarding the outcome in the < 60-year-old female subgroup in our study, a previous cross-sectional study reported that compared to the LPA with low serum 25(OH)D group, the low PA with high serum 25(OH)D group had a lower prevalence of osteoporosis among females [15] . Females who were ≥ 60 years old did not exhibit this association in our study possibly because bone loss may have already started due to the effect of estrogen deficiency regarding menopause [42, 43] . Another possible reason is that the SD did not accurately represent the real effect of vitamin D so that other subgroups of age and sex might not have exhibited the associations. Regarding LPA and the association between SD and osteoporosis, a cross-sectional study also reported that the odds ratio of the low PA with high 25(OH)D group for osteoporosis was significantly lower than that of the low PA with low 25(OH)D group [15] . In other words, the LPA group could have improved their bone health by increasing their vitamin D status through increased sun exposure even though PA is not available. In addition, long SD was inversely associated with the risk of osteoporosis in the obese, nonsmoker, and lower alcohol consumption (<1 time a week) subgroups in our study. Obesity has a positive effect on bone health [14] , whereas smoking and alcohol consumption negatively affect bone health [44, 45] . In this regard, long SD may be more effective at decreasing the occurrence of osteoporosis when individuals have other positive characteristics associated with increased BMD. Further studies are needed to elucidate the effect of SD on bone health when participants have positive characteristics associated with improved bone health. The main strength of our study was the use of a large cohort with a 7-year follow-up period. In addition, we matched the osteoporosis and control groups at a 1:2 ratios for age, sex, income, and region of residence to identify the independent association between osteoporosis and the intensity of PA and between osteoporosis and SD. Furthermore, subgroup analyses according to age, sex, PA, SD, obesity, smoking status, and alcohol consumption status were performed to evaluate whether the findings were consistent in individuals with different characteristics. Several limitations remained in our study. The major limitation is that the study used secondary data. Accordingly, we could not consider the intake of vitamin D or calcium. Moreover, the METs could not be correctly calculated because the questionnaire did not ask for a specific PA duration, which is similar to the data used in a previous study [25] . Instead, PA groups were classified into only LPA and MHPA. In addition, PA was not specifically classified according to whether it was performed during leisure or occupational time. Another limitation is that SD did not reflect the data of individual sunlight exposure. Finally, the causal relationship between PA/SD and osteoporosis is not definite because of the limitation of the observational study design. Restrictions apply to the availability of these data. Data were obtained from the Korean National Health Insurance Sharing Service (NHISS) and are available at https: //nhiss.nhis.or.kr (accessed on 21 April 2021) with the permission of NHIS.@story_separate@We suggest that both MHPA and long SD are inversely associated with osteoporosis. Specifically, MHPA might decrease the occurrence of osteoporosis in individuals with various characteristics or lifestyles, such as age/sex background, SD, obesity status, smoking status, and alcohol consumption status, but this might not be true for young males or underweight individuals. Long SD might decrease the occurrence of osteoporosis in young females, obese individuals, nonsmokers, and individuals with lower alcohol consumption. Therefore, we concluded that outdoor PA, which could not only increase the intensity of PA but also expose people to the sun for a long time, might be beneficial for preventing osteoporosis. Supplementary Materials: The following are available online at https://www.mdpi.com/article/ 10.3390/ijerph18094437/s1, Table S1 : Subgroup analyses of crude and adjusted odds ratios (95% confidence intervals) of moderate-to high-intensity physical activity for osteoporosis according to sunshine duration, obesity, smoking, and alcohol consumption. Table S2 : Subgroup analyses of crude and adjusted odds ratios (95% confidence intervals) of long sunshine duration for osteoporosis according to physical activity, obesity, smoking, and alcohol consumption. Table S3 : Crude and adjusted odds ratios (95% confidence intervals) for osteoporosis in PA × sunshine duration interaction, and combined PA and sunshine duration group. Informed Consent Statement: Patient consent was waived due to the fact that the study utilized secondary data.","(1) Background: The purpose of the study was to evaluate the associations between physical activity (PA), sunshine duration (SD) and the occurrence of osteoporosis according to lifestyle status. (2) Methods: Data from the Korean National Health Insurance Service–National Sample Cohort (NHIS-NSC) collected from 2009 to 2015 were used. Osteoporosis (n = 19,351) and control (n = 38,702) participants were matched in a 1:2 ratio according to age, sex, income, and region of residence. PA was classified as moderate- to high-intensity PA (MHPA) or low-intensity PA (LPA) based on the International Physical Activity Questionnaire (IPAQ). SD was classified as short (≤6 h) or long (>6 h). Conditional logistic regression was used to calculate the odds ratios (ORs) with 95% confidence intervals (CIs) of MHPA and long SD for the occurrence of osteoporosis. Subgroup analyses were performed according to SD (or PA), obesity, smoking, and alcohol consumption. (3) The adjusted OR of MHPA for osteoporosis was 0.90 (95% CI = 0.87–0.94). The results were consistent in the age/sex, SD, obesity, smoking, and alcohol consumption subgroups, but not the <60-year-old male and underweight subgroups. The adjusted OR of long SD for osteoporosis was 0.96 (95% CI = 0.93–1.00). The findings were consistent in the <60-year-old female, obese, nonsmoker, and <1 time a week alcohol consumption subgroups. (4) Conclusions: We suggest that both higher intensity of PA and long SD could decrease the risk of osteoporosis. Specifically, PA could decrease the risk of osteoporosis in individuals with most characteristics except male sex or underweight. Long SD could decrease the risk of osteoporosis in young females, obese individuals, nonsmokers, and individuals with lower alcohol consumption."
"More than 650,000 people died worldwide from the coronavirus-disease 2019 caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The percentage of COVID-19 patients requiring non-invasive (NIV) or invasive mechanical ventilation (IMV) is unclear and strongly affected by the hospital organization and the availability of resources. Three studies from China, US, and Gemany reported the use of IMV among hospitalized COVID-19 patients to be 2.3%, 12%, and 17%, respectively. 1-3 ICU-mortality among mechanically ventilated COVID-19 patients varies from 25% to 97%. [2] [3] [4] [5] [6] [7] [8] [9] [10] Such wide variations may have different explanations. First, few studies included all patients hospitalized for COVID-19 able to provide a complete overview of characteristics and outcomes of COVID-19 patients that required hospitalization. Second, information regarding the respiratory management of COVID-19 patients has been mainly described in the setting of the intensive care units (ICUs). 4, 8, 9, 11 Thus, the number of hospitalized COVID-19 patients treated with oxygen supplementation and NIV has been markedly underreported leading to inaccurate information regarding the overall use of the different respiratory supports and outcomes. Third, most previous reports included a percentage of patients still admitted at the ICU at the end of the follow-up ranging from 2.3% 8 to 71% 2 and this may have led to different degrees of inaccuracy in estimating ICU-mortality. Finally, ICU beds and human resources availability is likely to vary across different areas and it has not been generally described in clinical studies, affecting generalizability of the results. 12 The Province of Rimini in Northern Italy was one of the areas more severely affected by the COVID-19 outbreak. On March 7 th , Rimini province was declared ""Red Zone"" due to the high number of infected people and, therefore, it was isolated with no possibility of entry or exit. Using clinical and demographics information routinely collected in a unique database including all residents in the entire province, we performed the present population-based cohort study with the following aims: 1) to describe the characteristics of hospitalized COVID-19 patients, 2) to examine patient outcomes overall and stratified by the adopted respiratory support, 3) to describe the organization of local healthcare system.@story_separate@Setting COVID-19 patients admitted to the Rimini hospital were retrieved in the present observational population-based cohort study, following the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) reporting guideline. The Italian National Public Healthcare System provides homogeneous and free access to any level of appropriate treatment to all the people including irregular immigrants through the hospital network, family doctors, and District Health Systems. The province of Rimini consists of approximately 340,000 inhabitants and is served by a network of five public hospitals, with Rimini Hospital being the largest and providing up to 600 beds. Since the beginning of the outbreak and for the entire duration of this study, Rimini Hospital was identified as reference hospital for all COVID-19 positive or suspected patients. With the increase of admitted patients, 340 hospital bedsincluding 80 in two newly opened wards-were progressively dedicated to infected subjects and 28 negative pressure rooms -eight set up at Emergency Department (ED) and 20 already available in the Infectious Diseases ward-were dedicated to patients requiring NIV. During the first decade of March, the number of ICU beds was progressively increased from 20 to 53, of which 48 dedicated to COVID-19 patients and five to non-COVID-19 patients. Non-COVID-19 patients exceeding local availability were transferred to another nearby hospital equipped with 10 ICU beds. All patients evaluated at one of the five EDs of the province from February, 26 to April, 18 2020 presenting symptoms suspicious for COVID-19 infection (i.e. fever and/or respiratory symptoms) and tested for the SARS-CoV-2 (real-time RT-PCR) 13 were considered for inclusion. Patients with positive swab, as well those with chest X-ray or CT scan evidence of COVID-19-related pneumonia despite a negative swab were included. The more stable patients were discharged home and entrusted to the primary care, while patients identified to be at high risk for complications based on symptoms severity and associated comorbidities were admitted at the hospital and represented the study population. All included patients with a first negative swab had at least a positive subsequent swab during the same hospitalization except those patients that died at the ED before a second swab. A daily meeting -always attended by the heads of the Emergency, Infectious Disease, Pneumology, Radiology, and Intensive Care Departments-was planned regardless of the holidays or Sundays respecting airborne and contact transmission precautions, with the aim to ensure a homogeneous and standardized treatment to all COVID-19 patients. Every day the relevant clinical information (e.g., comorbidities, respiratory status, medical treatments, and active clinical conditions) of the critically ill cases were updated and the overall therapeutic stewardship to be adopted (e.g., off-label medications, change in the respiratory support) was collegially discussed and agreed. Moreover, for each patient the appropriate treatment to adopt in the event of worsening conditions was planned, taking into account the limitation of the available resources. All decisions were recorded and promptly communicated to physicians and nurses working at all COVID-19 wards, comprising the ICU. Oxygen administration via ventimask or mask with reservoir was considered the standard of care for moderate/severe patients, while NIV (comprising continuous positive airway pressure, CPAP) and IMV following tracheal intubation were chosen for the most critical cases. To avoid airborne viral transmission, high flow nasal cannulae were used only for respiratory weaning in patients become negative and helmet was identified as the only interface to be used for NIV. Conditions leading to the decision to start IMV were: respiratory fatigue, new hemodynamic instability, or worsening of gas exchange notwithstanding oxygen/NIV, also considering prognostic criteria and resources availability (e.g., ICU beds and mechanical ventilators). According to the level of the respiratory support, the study population was divided in the following subgroups: Oxygen (patients receiving no more than oxygen supplementation); NIV-only (patients receiving no more than NIV); IMV-after-NIV (patients undergoing IMV after a failed NIV trial); IMV-only (patients starting IMV at hospital admission or after a trial with oxygen). For the whole population, demographic data were retrieved by an administrative and clinical database (Maria DB, Log 80, Forlì-Italy) and information about administered off-label medications related to the COVID-19 treatment (i.e. hydroxichloroquine, antivirals, steroids, canakinumab, and tocilizumab) were collected by clinical documentation. For patients treated with NIV and/or IMV the Charlson comorbidity index was computed, 14 and the SpO2 at the hospital admission, the PaO2/FiO2 (P/F) ratio at the inception of NIV and IMV, and the duration of ventilatory supports were collected. For those patients the extent of lung damage was estimated from the chest radiogram using the Brixia score: each lung was transversally divided into three sectors and to each obtained sector a score ranging from zero (no alteration) to three (interstitial-alveolar infiltrates) was assigned (total score range: 0-18). 15, 16 For patients admitted to the ICU (i.e., IMV-after-NIV and IMV-only), the Simplified Acute Physiology Score II (SAPS 2) 17 and the Sequential Organ Failure Assessment (SOFA) score 18 at ICU admission were computed and the possible implementation of tracheostomy and renal replacement therapy were documented. All patients were followed up to 60 days from hospital admission. The condition of being dead or alive at this time constituted the main study endpoint. Accordingly, data collection was concluded on June 18, 2020 to ensure at least 60 days of observation to the patients included last. The investigation conforms with the principles outlined in the Declaration of Helsinki. The AUSL della Romagna Institutional Review Board approved the project (registration number NCT04348448) with a waiver of informed consent. No additional procedure or investigation potentially related to the study was requested or provided. Continuous variables were described as mean ± standard deviation. The differences between the means were analysed by a paired or unpaired Student's t-test, as appropriate, after considering whether the subgroups had equal variance using Levene's test. One-way analysis of variance (ANOVA) was applied for all comparisons between the subgroups. The nominal variables were presented as numbers and percentages and compared either through  test or Fisher's exact test, as appropriate. The ability of the P/F measured before a NIV trial to predict NIV failure (i.e., death or need of IMV) was tested by calculating the area under the receiver operating characteristics curve (AUC). The maximum Youden index (J) was considered as the optimal P/F cut-off value. Sixty-day mortality was computed using Kaplan-Meier technique overall and among groups. The Mantel-Cox log-rank test was adopted to assess differences among the survival rates. A multivariate Cox proportional hazard analysis was used to estimate 60-day mortality risk among the study groups in comparison to the Oxygen group (reference group), adjusted by age, sex, and administration of steroids, canakinumab, and tocilizumab. The results were presented as a proportional hazard ratio (HR) with 95% CI and adjusted cumulative survival curves. Finally, in order to examine the potential impact of survival bias among patients treated with IMV, especially among patients intubated after NIV (a patient had to survive until endotracheal intubation), we computed the adjusted HRs for 10-day and 11 to 60-day mortality, separately, as sensitivity analysis. Moreover, minimally to fully adjusted HRs were reported in the Supplemenatry material. For all tests, statistical significance was set at an alpha level of p = 0.05. All statistical analyses were performed using the software IBM SPSS Statistics, version 24.0 (Armonk, NY, US: IBM Corp.). During the study period, 1,424 symptomatic patients were evaluated at the EDs in the province and had a positive swab and/or chest imaging suspicious for the COVID-19. Nine-hundred and four (63.5%) were treated at home without further ED accesses, while the remaining 520 (36.5%) were hospitalized and constituted the study population (males 350, 67.3%; mean age 70.7 ± 14.1, range 19-98). Among hospitalized patients, 440 (84.6%) were treated with oxygen supplementation at the time of hospital admission, while the remaining received either NIV or IMV. During the subsequent days, 57 patients (11.0%) required an upgrade of the ventilatory support. Almost all patients were treated with hydroxichloroquine and antiviral drugs. Table 1 describes the demographic characteristics and the administered medications of the study population overall and by groups. Figure 1 shows a complete synthesis of the respiratory support adopted and the 60-days mortality for each group. The main clinical characteristics of the 112 (21.5%) patients who received any ventilatory support (NIV and/or IMV, mean age 66.9 ± 9.4 years) are reported in Table 2 . Among those, 71 (63.4%) patients received at least one trial of NIV (mean age 65.1 ± 11.8) with a mean duration of 3.8 ± 2.2 days (range 1-10, Figure 2a ) and a mean P/F ratio of 105.9 ± 40.6 at the beginning of NIV. Thirty-eight (53.5%) of the patients treated with NIV improved and were transferred to a COVID-19 ward, while 25 (35.2%) were intubated and admitted to the ICU to undergo IMV. The P/F ratio before starting NIV differed significantly between patients with a successful trial and those that failed (successful: 119.4 ± 46.2; failing: 92.1 ± 23.8; p = 0.003). For patients who needed IMV a statistically significant but clinically irrelevant improvement in P/F ratio was documented (before NIV: 93.1± 23.8; before IMV: 113.8 ± 41.5; p = 0.041). The remaining eight patients (11.3%) died without being intubated (P/F ratio before NIV 85.3 ± 36.5). The length of NIV did not differ among patients with successful or failing trial ( Table 2 ). The ability of the P/F ratio obtained before NIV to predict the failure of a NIV trial showed an AUC of 0.71 (95% CI 0.59-0.83; p-value 0.002) and provided a best cut-off of 115.5 (sensitivity 52.6%, specificity 81.8%; J 0.353). Overall, 66 patients (Groups IMV-after-NIV and IMV-only, mean age 66.9 ± 9.4 years) were admitted to the ICU and treated with IMV ( Figure 1 ). The mean interval between hospital admission and the onset of IMV was 5.0 ± 5.9 days (range 0-26) ( Figure 2b ). Eleven (15.2%) patients were intubated after 10 or more days from hospital admission (P/F before intubation: 113.0 ± 46.7). No difference in P/F ratio at the time of the definitive ventilation support was found among the three study groups (p = 0.993). Patients who failed the NIV trial (n = 21) had a Brixia score of 10.6 ± 3.4 before NIV which worsened to 13.4 ± 2.5 (p = 0.002) before IMV. Similar Brixia score was obtain at the time of endotracheal intubation among patients that failed a NIV trial and among patients that were treated with IMV without a NIV trial ( Table 2) . Among the 66 patients admitted to the ICU, mean duration of IMV was 22.6 ± 19.0 days (range 2-87). Forty-six ICU patients (69.7%) received percutaneous tracheostomy (time from intubation: 9.8 ± 3.5 days). Renal replacement therapy was performed in 19 (28.8%) patients, with a mean duration of 21.7 ± 18.7 days. None of the 904 patients treated at home died during the follow-up. The overall 60-days mortality for hospitalized patients was 24.2% (n = 126), and was 23.0% (n = 94), 19.6% (n = 9), 32.0% (n = 8) and 36.6% (n = 15) for Group Oxygen, NIVonly, IMV-after-NIV, and IMV-only, respectively (p = 0.165). Mortality among the 112 patients receiving any ventilatory support (NIV and/or IMV) was 27.7%. No between-groups difference in mortality was found by comparing the crude Kaplan-Meier curves (Log-rank test: p = 0.343, Figure 3a ). Age was a risk factor for death in Groups Oxygen and NIV-only . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted August 15, 2020. . but not for patients undergoing IMV. The relationships between some characteristics of the study groups and mortality are described in Table 3 . The mean duration of IMV was 26.5 ± 21.1 days for the 43 patients alive at the end of the follow-up (range: 6-70) and 15.3 ± 11.5 days for the 23 non-survivors (range: 2-51). Among the 43 survived, 14 patients (32.6%) underwent IMV for more than four weeks (Figure 2c) . Two patients (one belonging to the IMV-after-NIV group and one to the IMV-only group) were still on IMV at the end of follow-up, with an IMV duration of 70 and 63 days, respectively. Compared with Oxygen group, the multivariate Cox regression analysis showed a 60-day mortality risk progressively increasing among the other groups being statistically significant for the IMV-after-NIV group (HR 2.776; 95% CI 1.144-6.734; p = 0.024) and the IMV-only group (HR 2.966; 95% CI 1.557-5.652; p=0.001) but not for Group NIV-only (HR 1.778; 95% CI 0.794-3.980; p = 0.162) (Figure 3b ). Among the explored covariates, older age (HR 1.097; 95% CI 1.073-1.120; p < 0.001) and male sex (HR 1.597; 95% CI 1.072-2.379; p = 0.021) showed a statistically significant association with the mortality. As expected, the sensitivity analysis showed a higher risk for 10-day mortality in the only-NIV group but not in patients receiving IMV compared to patients treated with oxygen, while similar results to the main analysis were provided for the 11 to 60-day mortality (Table 4 and Figure 4 ). The present study showed how an integrated multidisciplinary clinical organization allowed to optimize the allocation of the available resources among 520 hospitalized COVID-19 patients. The overall 60-day mortality was 24.2%. Approximately 21% of patients were mechanically ventilated, with a mortality ranging from 19.6% in patients treated with NIV-only to 36.6% in patients undergoing IMV without a NIV trial. To our knowledge, this is the first study reporting 60-day mortality in a cohort of hospitalized patients diagnosed with COVID-19 overall and according to all adopted ventilatory strategies. A recent Chinese multicentric study 19 enrolling ICU 258 patients reported an overall 60-day mortality of 64.3%, with 19 patients deceased within 48h after ICU admission. Among 165 mechanically ventilated patients, the 60-day mortality was 83%, 56%, and 94% for those treated with IMV, with NIV, and receiving both treatments, respectively. Median P/F ratio in the Chinese population was 91 (IQR 67-134) and SOFA score 6 (IQR 5-7). It should be noted that in our study we documented for patients treated with NIV and/or IMV -despite a similar P/F ratio (median 98.0; IQR 84.0-124.5) and a higher SOFA score (median 8; IQR 6-10)-a considerably lower 60-day mortality rate, overall and in individual groups. A study from six COVID ICUs from US 4 enrolled 217 patients, 165 of which (76.0%) received IMV, with a median IMV length of 9 days (IQR 4-13). Among IMV subjects, ICU mortality was 33.9% without any difference in IMV days between deceased and survived. At the end of follow-up (median observation time 15 [IQR: 9-24] days), hospital mortality was 35.7% (59/165), being 8 patients still in ICU on IMV. However, no information was provided neither about the adopted respiratory support, nor the outcome, for patients not undergoing IMV. Despite mortality reported by Auld et al. is similar to the mortality reported in our study in the IMV-only group, they reported a much shorter follow-up (maximum follow-up 60 days) and 4.8% of patients were still admitted to the ICU. Therefore, 60-day mortality among those patients is likely to be higher that the reported hospital mortality. Recently, a nationwide study from Germany including more than 10,000 hospitalized COVID-19 patients was published. Interestingly, despite the Germany health-care system has not been overwhelmed by the pandemic the reported inhospital mortality was markedly higher among patients treated with NIV (45% in patients with successfully NIV, 50% in patients that failed NIV) and IMV (53% in patients treated with IMV), while it was lower in patients without mechanical ventilation (16%). 3 Two further studies considering ICU patients reported information about all adopted breathing support strategies. An Italian multicentric study 8 enrolling a cohort of 3,355 critically ill patients (median follow-up: 69 days; ventilatory support: IMV 87%, NIV 10%, CPAP/oxygen 2.3%) reported a mortality rate at the censoring (median observation time 70 [range, 38-112] days) of 17%, 36%, and 52% among patients treated with oxygen, NIV and IMV, respectively. Accordingly, NIV and IMV were associated with an increased risk of death compared with patients treated only with oxygen (HR 2.36, 95% CI 1.33-4.17 and HR 3.77, 95% CI 2.19-6.51, respectively). Again, mortality reported in the present study is likely to underestimate 60-day mortality and mortality in the NIV and IMV groups is higher than the reported in our study population. It should be noted that in all above cited studies the reported mortality for patients treated with standard oxygen and NIV was clearly conditioned by the reduced size of these subgroups, related to the study settings limited to ICU. Compared with the previously reported literature, mortality reported in our study is generally lower. The centralized multidisciplinary approach adopted at Rimini hospital may partially explain the difference with the existing literature. The . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted August 15, 2020. . sensitivity analysis and the differences between the crude and the adjusted HRs reported in the Figure 4 and Table 4 may help with the interpretation of the study findings. Interestingly, in the present investigation, mortality was higher in the oxygen and NIV-only groups in the first 10 days compared to the other two groups. This is partially explained by the survival bias among the mechanically ventilated patients, especially among patients that were intubated after failing a NIV trial. Indeed, although HRs for the 10-day mortality risk did not reach the statistical significance, mortality risk was lower for patients intubated after a failed NIV trial only in the first 10 days of follow-up and not in the 11 to 60-day mortality. On the other side, the fact that the oldest patients and those with severe coexisting disease were treated only with oxygen carrying a higher early mortality was clearly described by the difference between the crude HRs and the HRs adjusted only be age. This finding together with the information of a mean IMV length of almost 20 days may support the idea that those patients would have not survived anyway to the IMV and, therefore, endorse the decisions taken by the multidisciplinary team. Moreover, our results suggest that initial management of severe hypoxemia by oxygen or NIV might be a valuable alternative to immediate IMV in the occurrence of limited available resources. The decision about the best breathing support to be provided to COVID-19 patients is anything but simple. Although often severely hypoxic, they tend to present less severe dyspnea than expected, probably because many patients, at least in the early stages of the disease, have normal pulmonary compliance and therefore exert limited inspiratory efforts. In patients whose lung compliance tends to progressively decrease, the inspiratory effort increases and vigorous inspiratory effort can contribute to lung injury (Patient Self-Inflicted Lung Injury-P-SILI). 20 This feature has been supposed to rise morbidity and mortality. Therefore, early mechanical ventilatory support has been advocated for COVID-19 associated respiratory distress. 21 Unfortunately, criteria to intubate COVID-19 patients are controversial and the decision may locally reflect the available resources. 22 Older age and comorbidities burden have been largely reported as the two main conditions associated with increased mortality risk, 4, 8, 23 so that prioritization of younger patients has been advised in case of shortage of resources. 24 Notably, in the present investigation -although age was higher among non-survivors than survivors in each of the four groups and each calendar year was associated with a 10% increased risk for mortality-the age difference was smaller and not statistically significant among survivors and non-survivors undergoing IMV. Moreover, the median age of patients submitted to IMV in our study (69 years) was slightly higher than reported by other authors (59 to 64 years), 2, 4, 8 suggesting that the adopted criteria at our Institution were less restrictive in term of age. Furthermore, once a patient was considered as potentially salvageable by ICU admission and IMV, the length of IMV was not a criterion to withdrawing treatment. We strongly think that this ethically crucial decision could be widely considered. Another interesting finding of our study was that similar P/F ratio was found among patients treated with NIV-only or with IMV, either preceded by a NIV trial or not. Therefore, we speculated that a low P/F ratio should not be the only criterion to decide which patient would benefit from IMV. It should be noted that among patients intubated after a NIV trial or receiving immediate IMV, the latter group had a higher SOFA score and SAPS (Table 2 ). These findings highlight the fundamental role played by our organizational strategy, allowing to ensure a tailored treatment to each patient by taking into account the level of care that would have more benefited for her/him, and to daily re-discuss each decision at the light of new clinical reasons or changes in the resources availability. For example, during the very early phase of the outbreak not all patients with appropriate indications were treated with NIV due to the scarce availability of helmets, while their subsequent increase in availability allowed more targeted choices in the following days. Moreover, this strategy contributed to create a more collaborative way to approach difficult decisions, supporting healthcare professionals, especially the younger ones, during such ethically and emotionally demanding decisions. 22 The main strength of the study consisted in having described the impact of the SARS-CoV-2 epidemic in an entire province. Since all patients with moderate to severe COVID-19 were managed at the same hospital, a shared and homogeneous standard of care was guaranteed. Moreover, follow-up was established at 60 days, taking into account all respiratory supports and without patients' loss. As many COVID-19 patients require prolonged IMV, a short follow-up time is an important limitation for the majority of the reports published so far. The number of patients requiring ventilatory support was smaller compared to other studies. 2, 3, 10 Consequently, the generalizability of our findings should be considered with caution. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted August 15, 2020. . The copyright holder for this preprint this version posted August 15, 2020. . . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted August 15, 2020. . Figure 4 . Adjusted Kaplan-Meier curves for the risk of 10-and 11 to 60-day mortality in patients belonging to the study groups according to provided respiratory support. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted August 15, 2020. . Measurements are reported as mean and standard deviation or absolute number and percentages. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted August 15, 2020. . https://doi.org/10.1101/2020.08.13.20174615 doi: medRxiv preprint Table 2 . Characteristics of patients receiving either invasive or non-invasive mechanical ventilation. Group IMV-after-NIV Group IMV-only p-value n; mean ± SD n; mean ± SD n; mean ± SD . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) The copyright holder for this preprint this version posted August 15, 2020. . @story_separate@The COVID-19 outbreak has strongly challenged the healthcare systems of many countries. A multidisciplinary panel in charge of the decision of the individualized breathing approach to adopt with hospitalized COVID-19 patients maybe be a valuable option to maximize 60-day survival, dealing with the imbalance between the available resources and the clinical needs. Our findings highlight the need of high quality follow-up data that could support the decision-making for the appropriate ventilatory support strategy for COVID-19 patients. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.","Background: Among COVID-19 patients, the decision of which ventilation strategy to adopt is crucial and not guided by existing outcome evidence. We described the clinical characteristics and outcomes of hospitalized COVID-19 patients according to the adopted respiratory strategy. Methods: Population-based cohort study including all COVID-19 patients (26/02/2020-18/04/2020) within Rimini Italian province. Hospitalized patients were classified according to the maximum level of respiratory support: oxygen supplementation (group Oxygen), NIV (group NIV-only), IMV (group IMV-only), and IMV after a NIV trial (group IMV-after-NIV). Sixty-day mortality risk was estimated with a Cox proportional hazard analysis adjusted by age, sex, and administration of steroids, canakinumab, and tocilizumab. Findings: We identified 1,424 symptomatic patients: 520 (36.5%) were hospitalized, the remaining 904 (63.5%) were treated at home with no 60-days death. According to the respiratory support, 408 (78.5%) were assigned to Oxygen, 46 (8.8%) to NIV-only, 25 (4.8%) to IMV-after-NIV, and 41 (7.9%) to IMV-only groups. There was no significant difference in the P/F at IMV inception among IMV-after-NIV and IMV-only groups (p=0.9). Overall 60-day mortality was 24.2% (Oxygen: 23.0%; NIV-only: 19.6%; IMV-after-NIV: 32.0%; IMV-only: 36.6%; p = 0.165). Compared with Oxygen group, the 60-day mortality risk significantly increased for IMV-after-NIV (HR 2.776; p=0.024) and IMV-only group (HR 2.966; p=0.001). Conclusions: This study provides a population-based figure of the impact of the COVID-19 epidemic. A similar 60-days mortality risk was found for patients undergoing immediate IMV and those intubated after a NIV trial. Many patients had a favorable outcome after prolonged IMV."
"n 2020, the spread of the novel coronavirus (COVID- 19) completely changed human life across the whole world [1, 2] . In order to control the COVID-19 epidemic, the city of Wuhan firstly implemented a stringent lockdown policy [3, 4] , which appears to be the largest quarantine to date and was unprecedented in human public health history [5, 6] . As the global COVID-19 epidemic worsened, many other cities around the world subsequently implemented similar lockdown policies, including limiting public transportation, closing schools and non-essential shops, ordering residents to stay and work at home, prohibiting public events and gatherings, etc., to control transmission [7] [8] [9] [10] [11] [12] [13] [14] . Numerous studies have also indicated that such control measures can mitigate the growth of the COVID-19 epidemic [6, [15] [16] [17] [18] [19] [20] . Since the details of lockdown policies and their degrees of stringency have varied across different cities [21] , it is difficult to quantitatively evaluate their effects and draw fair global comparisons. Transportation density is a good indicator of intracity population flow, as reductions in this metric have certainly been felt by the inhabitants of the cities under lockdown. However, it is difficult to obtain accurate and quantitative transportation data from various cities in different countries. High-resolution remote sensing data can provide objective and consistent observations among different cities and between two periods (before and after city lockdown) [22] [23] [24] [25] , which offers a unique opportunity to evaluate the transportation density reduction caused by city lockdowns during the COVID-19 epidemic for a fair comparison [26] [27] [28] . Although a recent work addressed an analysis of traffic pattern during COVID-19 epidemic, it used the images with the resolution of 3m, which is hard to identify normal vehicles accurately with such a resolution [29] . For most existing studies focusing on vehicle detection on remote sensing imagery, they are based on the aerial images with limited experimental areas, since their resolutions are smaller than 0.3m and have enough spatial details to identify cars [30] [31] [32] [33] [34] [35] [36] [37] . The problem is, only a few satellite sensors can reach the resolution of 0.3m, and they are not just in the right pathway to acquire the necessary images for studying city lockdown. Hence, it is also necessary to develop a robust vehicle extraction model for remote sensing images with a lower and more common resolution, which can be employed in the complex scenes of practical applications covering the whole city. In the present study, we use the number of vehicles extracted from multi-temporal high-resolution remote sensing (RS) images before and after lockdown in six cities around the world, with the goal of conducting an objective and quantitative study of transportation density reduction caused by city lockdown during the COVID-19 epidemic. A novel vehicle detection model that combines a morphology filter and multi-branch deep learning identification network was developed to quantify the impact of city lockdown on transportation density. We further aim to analyze the correlation between transportation density reduction and lockdown stringency, as well as to provide an interpretation of the geographic view within the cities. We choose six large and famous cities for analysis: Wuhan, Milan, Madrid, Paris, New York, and London, all of which were epicenters in the early COVID-19 epidemic [38] and subsequently implemented stringent lockdown measures [4, 7, 9, 10, 13, 14] . Multi-temporal remote sensing images with resolution of 0.5m covering the core areas of these cities, acquired before and after the implementation of city lockdown policy, were collected for a total study area of 475 at one time. Open Street Map was used to extract road regions [39] , while a COVID stringency index [21] was utilized to evaluate the city lockdown policy. References for a total area of 63 and 35770 labeled vehicles indicated that our vehicle detection results obtain a total accuracy of 69.63%, which is satisfactory at this resolution in practical applications. Following analysis of the vehicle density changes, our results indicate that the transportation densities reduced by an average of approximately 50% in these six cities after city lockdown, with a decline sometimes as high as 75.96%. The influence on transportation density reduction rates is highly correlated with policy stringency, with an exceeding 0.83. In terms of the view inside the city, transportation density changes also varied according to city land-use patterns. Considering that public transport was either limited or forbidden during these periods [4, [40] [41] [42] , city lockdown policy can be deemed effective in controlling intracity human transmission.@story_separate@In order to objectively compare the transportation density variations before and after lockdown in the six large cities selected, we collected multi-temporal high-resolution RS images to extract the numbers of vehicles on the road. The metadata of all RS images used in this study is presented in Table 1 . Almost all RS images were acquired by the Pleiades satellite, with a resolution of nearly 0.5 m and four spectral bands (Blue, Green, Red, and Near-Infrared). The only exception is the RS image covering Wuhan before lockdown. The nearest available and clear Pleiades image covering the study area with low cloud coverage was acquired in 2016, which is too long ago compared with the other image pairs. Accordingly, we selected the RS images from WorldView-3 instead, then resampled them to a resolution of 0.5 m and four spectral bands. Figure 1 : Remote sensing images acquired before and after city lockdown. The images shown in each line represent Wuhan, Milan, Madrid, Paris, New York, and London respectively. The first column presents the images before city lockdown, while the second column shows the images after city lockdown. Moreover, the third column shows the road networks from OSM; here, yellow represents arterial roads, red represents collector roads and blue represents local roads. The city lockdown dates and subsequent remote sensing image acquisition time are presented alongside the cumulative confirmed cases [43] [44] [45] [46] [47] [48] and COVID-19 government response stringency index developed by Oxford University [21] in Figure 2 . It can be observed that the RS images were all acquired in the rising phases of the COVID-19 epidemic when the governments were implementing the most stringent lockdown measures. These collected remote sensing data will therefore be effective for evaluating the influence of lockdown policy on transportation density. Since these six cities differ markedly in terms of size and area, we only selected their core regions with obvious landmarks. (1) For the city of Wuhan, the study area covers the region inside the second ring road; (2) For the city of Milan, the study area was bounded by SS11 road; (3) For the city of Madrid, we selected the region contained by the M-30 road as the study site; (4) For the city of Paris, the study area contains all arrondissements except for two parks; (5) For New York City, the study area covers the borough of Manhattan; (6) For the city of London, we selected the region covered by zone 1. The study areas and multi-temporal high-resolution remote sensing images are presented in Figure 1 . The total observation area at one time is 475 . In order to identify vehicles on the roads only, we selected Open Street Map (OSM) data for the six cities. The OSM data for ""motorway"", ""trunk"", ""primary"" and their corresponding ""link"" were classified as arterial roads with a buffer of 20 m, while those for ""secondary"", ""tertiary"", ""unclassified"" and their corresponding ""link"" were classified as collector roads with a buffer of 20 m; moreover, those for ""residential"" and ""living_street"" were classified as local roads with a buffer of 15 m [49] . Only the vehicles within these buffers were extracted and counted. The OSM data are presented in Figure 1 . All of these images were processed using the GS pan-sharpening method and co-registered to the OSM maps to facilitate better alignment. Since the vehicles identified in the remote sensing images with resolution of 0.5m cannot provide enough shape information to facilitate direct detection, we decide to combine unsupervised vehicle candidate extraction with deep learning identification in our model. The goal of the unsupervised vehicle candidate extraction is to find possible vehicle targets and remove interference on the road, while the deep learning identification is used to distinguish between vehicles and non-vehicles. During the process of vehicle candidate extraction, the basic concept is that vehicles in remote sensing images contrast with the road background, enabling them to be captured by human interpreters. Accordingly, we first separate the bright and dark objects by segmenting the contrast density images using the TopHat and BottomHat morphology process [50] ; subsequently, the connected objects are filtered according to their areas, shapes, and compactness, as shown in Figure 3 . In more detail, the procedure is as follows: In this process, only the road regions masked by OSM data are considered. TopHat and BottomHat morphology processes are utilized for each spectral band of the remote sensing image to highlight bright and dark objects against the road background. Two contrast density images are generated by the Euclidean distance of the multi-band morphology results, after which two binary images are obtained by separately thresholding TopHat and BottomHat density images in accordance with visual interpretation. The corresponding opening binary images are produced by means of a opening morphology process on the binary TopHat and BottomHat images and used as auxiliary information. Here, the opening process is used to avoid mis-detection caused by the connection of multiple objects. Moreover, a vegetation mask is generated using the manual thresholding of the NDVI feature image to avoid false detection of tree shadows. We now have five types of input data: binary TopHat image, binary BottomHat image, binary Opening TopHat image, binary Opening BottomHat image, and NDVI mask. In the first step, bright and dark objects are extracted separately by connecting the binary TopHat image and binary BottomHat image with the vegetation mask. For dark objects, shadows always accompany bright vehicles and can thus easily be falsely detected as dark vehicles. Thus, dark objects are taken out by means of shadow removal if a certain proportion (e.g. 30%) of the contours of a dark object are adjacent to bright objects by one or two pixels. Notably, dark objects with fairly good vehicle shapes are retained. In the next step, bright objects and dark objects are filtered according to their areas, shapes and compactness. There are three criteria: 1) the area of objects should be neither too large or too small; 2) the height and width of the minimum bounding rectangle should be smaller than the thresholds, while the aspect ratio should also not be too large; 3) the proportions of an object occupying its contour hull or minimum bounding box should be larger than a given threshold. Moreover, objects filtered by the third criterion may be falsely removed due to the connection of two nearby vehicles in the image with such a resolution. Therefore, for these removed objects, their corresponding objects in the binary opening image will be added instead, provided that the alternative opening objects satisfy the stricter criterion discussed above. Finally, the bright objects and dark objects are combined to make up the vehicle candidates. However, since the pixels in these candidates may not be especially accurate, while some vehicles may be divided into two objects for a number of reasons (e.g. a dark sunroof), we generate anchors that center the vehicle candidates. The bounding boxes of vehicle candidates are expanded with a height zoom ratio of [1, 1.5, 2] and width zoom ratio of [1, 1.25, 1.5], after which the anchor ratios are rotated by . Normally, for each vehicle candidate, we will obtain 18 anchors; however, if the vehicle candidates have an obvious direction with a large aspect ratio, the anchor expansion will only be generated in this direction. After the vehicle anchors are obtained, we develop a multi-branch CNN model to identify and distinguish between vehicles and non-vehicles. Since vehicles on remote sensing images with such a resolution do not contain detailed shape information, we fuse multi-scale patches with R, G, B and NIR bands that center the anchors to be the input of the deep learning model. The first input is a window patch of size 64 64 , which is clearly larger than the vehicle, to facilitate extracting the background information; the second input is a sub-window patch of size of 16 , aligned with the direction of the anchor; moreover, the third input is the aligned anchor patch, which is resized to 16 for convenience. The multi-branch deep learning model is shown in Figure 4 . The structure of this model derives from VGG-16 [51] ; it contains convolution layers, max pooling layers, ROI pooling layers, and fully connected layers. ROI pooling layers are used here to guarantee a fixed size for the output vector in each branch. The final layer uses a sigmoid function to output the probability that the detected object is a vehicle. When training the deep learning model, we train each branch separately by connecting the features after ROI pooling to three new fully connected layers, as in the multi-branch model. The three trained branches are then connected, as in Figure 4 , with new fully connected layers, then re-trained with the same training samples. Since we have extracted numerous vehicle candidates, it is much easier to label the training samples from the candidates rather than drawing boxes directly from the images. The number of training samples selected in each image is presented in Table 2 . All training and testing samples are normalized with reference to the statistics of their source images. Random flipping (both vertically and horizontally) is adopted as a data augmentation method during the training process. In the optimization procedure, binary cross-entropy is used as the loss function. Since a quantity imbalance exists between the vehicle and non-vehicle training samples, we use a simple weighting approach in the loss function, which assigns to vehicle samples and to nonvehicle samples; here, and are the sample amounts of vehicles and non-vehicles. This simple approach is able to balance the weights of vehicle and non-vehicle samples during the calculation of loss. The optimizations of each single branch and the multi-branch model are all implemented 100 times, and the Adam optimizer is applied [52] ; the batch size is set to 200, and the shuffle process is also used. More specifically, we use a warmup learning rate scheduler strategy [53] , where the start rate is 1 −4 , the max rate is 1 − , the min rate is 1 −6 , the warmup epoch is 20, the sustaining epoch is 0, and the decaying parameter is 0.8. For the training of the first branch with window patch, the start rate and max rate are changed to be 1 −5 and 1 −4 . In the testing procedure, the testing samples are determined to be vehicles if the output probability of the deep learning model exceeds 0.5. In the post-processing phase, there are two steps: the first is Non-Maximum Suppression (NMS), while the second is the removal of vehicles under temporal shadow coverage. Since numerous anchors have been generated that center the vehicle candidates, several of them will be retained after deep learning refining is complete. Therefore, we utilize NMS to choose the most probable detection box. Unlike the normal NMS, we apply some special processes here. First, if the heights, widths, and aspect ratios of the anchors do not satisfy the shape filter criterion, they will be suppressed. Second, in addition to the Intersection over Union (IoU), the Intersection over Area (IoA) of the anchor with a maximum probability is also used to filter repetitive anchors. Third, if a suppressed anchor shows a probability that is only 0.05 less than the remaining one, but has the minimum area, it will be retained instead. After the NMS process is complete, the detection results for vehicles on the high-resolution remote sensing images are obtained. However, since some of these images were acquired in winter, the solar altitudes are still low even when the acquisition time is midday. This phenomenon leads to more shadow coverage appearing in only one image of the multi-temporal pair, which will result in unfair comparison of vehicle numbers on the road. We therefore utilize a Tasi's ratio image to find and extract shadow coverages [54] . Shadow areas are segmented by means of manual thresholding. A 5 5 Closing morphology process and Opening morphology process are subsequently implemented so that only large-area shadow regions remain. The union of shadow areas in a multi-temporal image pair is used to remove the covering vehicles in both images. All results and statistics presented in this paper are based on the detection maps following multi-temporal shadow coverage removal. The examples of vehicle detection results are shown in Figure  5 . It can be observed that after city lockdown, the vehicle densities in these images obviously reduced. Most vehicles in these images were successfully detected, and the vehicle numbers were accurately estimated. In order to demonstrate the credibility of our method, we selected nine regions for each RS image (for a total of 108 reference regions) and manually labelled all vehicles. The total size of the reference area extends to 63 with 35770 labeled vehicles. Since the objective of this study is to count the number of vehicles, a detection will be judged to be true if the detection box covers the labeled box. The total accuracy of the proposed model, comparison methods and the corresponding ablation experiments are presented in Table 3 ; here, ""All Image Training"" represents the proposed model. The comparison method selected is the well-known faster RCNN with rotating box. Since the training sample set for our proposed model is not suitable for faster RCNN, we attempt two ways of implementing this model. The first way involves downsampling the DOTA dataset to a resolution similar to that of our data, then training the faster RCNN model for vehicle detection [55] . In the second way, we conduct fine-tuning with the pre-trained model, as in the first way, based on half of the references. As Table 3 indicates, the classical faster RCNN obtains far lower accuracies than the proposed model; this is because the spatial shape information is lacking in images with a resolution of 0.5m, which are unsuitable for direct object detection. As shown in Table 3 , vehicle candidate extraction obtains a recall rate of 85.42%, meaning that it has found most vehicles in the image. The objective of deep learning identification is to increase the precision rate while simultaneously maintaining the recall rate. After DL refining is complete, it can be observed that the precision rate increases very substantially (11.66% to 69.86%), while the recall rate is kept relatively steady (85.42% to 69.90%). Single Image Training means that the DL model for each image is trained only with the training samples themselves. The accuracy results show that more training samples can yield more robust performance, where the best model is trained using all training samples. Moreover, we also test the result of the DL model that is trained with all samples and fine-tuned specific to the testing image. We find that the fine-tuned model identifies vehicles more accurately, but also misses more targets; this may be because the fine-tuning increases the model's identification ability, but will also falsely remove some vehicles if they are not covered in the training samples of the testing image. The proposed weighting and anchor generation processes are also evaluated by ablation. The experiments show that anchor generation increases the total accuracy (F1 score) by 3%, while the weighting process increases the accuracy by less than 1%; since the imbalance between vehicle and non-vehicle samples is not severe, the weighting process improves the performance only slightly. Moreover, as the proposed model consists of three branches, we test various combinations of different branches. The ablation experiments reveal that the proposed method with all three branches outperforms those with only one or two branches. The detailed accuracies for the six cities are listed in Table 5 It can be observed from the table that almost all results have accuracies higher than 65%. These accuracies are all satisfactory in practical applications, especially considering the lack of detailed shape information, the complex acquisition environments, and the large covering areas. The vehicle number statistics before and after lockdown, along with their change ratios, are presented in Table 4 . It can be observed that in the entire study area, the cities of Wuhan, Milan and Paris exhibited obvious reductions, at the rate of approximately -50%, while the other three cities showed a slight increase. It is worth noting that, in these cities, stopped vehicles may be parked along the roadside, meaning that it is difficult to distinguish between parked vehicles and running vehicles on the same road. Therefore, in order to better evaluate the real influence of city lockdown on transportation, we create statistics for vehicles on the arterial roads in each city, which are shown in the second column of Table 4 . As the table indicates, city lockdown obviously decreases the vehicle densities in the main traffic routes of all six cities. Among these six cities, transportation in Wuhan was influenced by city lockdown to the greatest extent, i.e. by 75.96%. Considering that Wuhan completely forbid public transport within the city [4] , this significant reduction of vehicle density proved that the stringent implementation of lockdown policy evidently limited human transmission. Paris and Milan show similar influences on transportation, with reductions of about 65%, while Madrid exhibits a comparatively lower reduction of 47.49% that still approximates to 50%. Finally, London's lockdown policy reduced the vehicle density by 28.66%, while New York City showed the least effect with a reduction of 17.75%. It can be seen that the change ratios of vehicle numbers on the arterial roads of these six cities can be ranked as follows: Wuhan>Paris≈Milan>Madrid>London>New York (see Table  4 ). These rankings are quite similar to and in accordance with the countries' COVID stringency index [21] on the RS acquisition days (as shown in Figure 2 (b)), with the city of Wuhan being the only outlier. According to the computation methodology of the COVID stringency index, the index value is determined by a sub-index based on the lockdown policies and their implementation range [21] . Since the stringent Wuhan lockdown limited the spread of the COVID epidemic throughout the whole country of China [3, 6, 17, 56] , all of the most stringent policies were implemented in the city of Wuhan, and China's overall COVID stringency index was decreased through the local implementation of stringent policy. A similar situation occurred for the stringency index of New York, which was analyzed separately in the reference [21] . Accordingly, in order to evaluate the correlation between COVID stringency index and transportation density change ratio, we correct these indices to be city-wise, as shown in Figure 6 (a). The policy values were retained and the flag for each sub-index was assigned to be ""general"" in the city-wise index calculation, since all of these six cities were COVID epicenters in the corresponding countries on those days (the index of New York was used directly, since it is the only city to have a separate index [21] ). Using the corrected city-wise COVID stringency index, we conducted a regression analysis between the transportation density change ratios and the indices by means of quadratic polynomial fitting, as shown in Figure 6 (b). It can be observed from these results that the values are both as high as 0.83, which indicates that the reduction in transportation density is highly correlated with the stringency of lockdown policy.  The statistics of transportation density distribution are presented in Figure 7 . From the perspective of the cities' core regions, the transportation density colors become obviously dim in Wuhan, Milan, and Paris; this decline can also be proven with reference to the reduction of vehicle numbers for the whole city in Table 4 . In the other three cities, some parts become brighter while others become darker, which can be explained by the fact that more vehicles were stopped in the residential regions. Examining the transportation density changes, moreover, reveals that in all six cities, the arterial roads exhibit obvious vehicle reduction, since the deep blue mostly appears along the arterial roads. For the three cities that show increasing vehicle numbers in the entire cities (Madrid, New York, and London), the obvious decrease in vehicle numbers is mainly concentrated on the arterial roads, while increases may represent residential regions. For example, Uptown Manhattan in New York City shows a significant increase in vehicle density, which proves that more vehicles were stopped near their homes due to the stay-at-home order; moreover, the center of London's Zone 1 shows vehicle density reduction, while its edges become brighter in the density slice, which may be because there are more apartments in the city center.@story_separate@Our results demonstrate that the lockdown policies during the COVID-19 epidemic clearly reduced transportation density by an average of 49.59%, and as much as 75.96%, by comparing the vehicle numbers before and after lockdown in six large cities (Wuhan, Milan, Madrid, Paris, New York, and London) that were the epicenters of the early global COVID-19 outbreak. The influence on transportation density change ratios can be ranked in size order as follows: Wuhan Paris Milan Madrid London New York; these rankings exhibit extremely high correlations with the COVID lockdown policy stringencies (the is higher than 0.83). We also find that transportation density reductions were spatially relevant to the land-use distribution within the cities. In order to mitigate the COVID-19 outbreak, similar lockdown policies were implemented in many cities all over the world [4, [7] [8] [9] [10] [11] [12] [13] [14] 57] . Studies have indicated that intercity travel bans can delay the progression of the epidemic to other cities by several days [6, 15, 17] . At the same time, control measures within cities, including stay-at-home orders, public transport limitations, closures of schools and workplaces, and bans on public gatherings, have been proven to be effective in decreasing the transmissibility and reducing the case incidence of COVID-19 [6, 18, 20, 58] . One key aspect of these lockdown measures within the city is the control of intracity population transmission [3] . At the moment, most corresponding studies are based on the policy interpretation [6, 15, 17] , mathematical modelling [20, 58] , and the intercity population flow data [6, 15, 17] ; however, there has still been no research on quantitatively evaluating the intracity human transmission affected by city lockdown measures around the world. As we know, control measures during city lockdown will evidently affect the transportation situation [57] . Accordingly, by quantitatively analyzing the transportation density variations in six epicenters of the early COVID-19 epidemic, we find that the transportation densities undergo a significant decrease after city lockdown, and are also strongly positively correlated with the stringency of control measures. Considering that public transport in these cities was largely limited and sometimes even forbidden [4, [40] [41] [42] , this study indicates that stringent lockdown policy is effective in controlling human transmission. Our study provides new insight into these cities during the epidemic, and can thus help researchers and policy analysts better understand and develop epidemic containment measures to control COVID-19 outbreaks. Figure 7 : Statistics of vehicle numbers in blocks in the six cities under analysis. The first column presents the vehicle distributions before lockdown, while the second indicates the vehicle distributions after lockdown. The third column shows the transportation density changes caused by lockdown. The left two columns are presented with equal interval slices, while the last column is shown with Jenks natural break slices for better illustration [59] . The color slices are shown under the transportation density maps; here, the densities in the first two columns of each line are sliced with the same range to facilitate comparison. The rows from top to bottom indicate the statistics for Wuhan, Milan, Madrid, Paris, New York, and London. Our results may also help in the development of scientific recommendations for lockdown policy implementation. From Figure 6 , we may infer that if the lockdown policy is not sufficiently stringent, its effect on controlling transmission will be slight. According to the regression, when the COVID-19 policy stringency is below 74, the numbers of vehicles on the arterial roads may not even reduce. After city lockdown, the transportation density reductions within the cities vary, which may reflect some potential land-use patterns for future city planning. A recent work also addressed a study on the traffic pattern of cities under lockdown during the COVID-19 epidemic [29] . However, the remote sensing images used in this work is acquired by Planet with the resolution of 3m. For most normal vehicles in cities, they are smaller than 5m 2m, which only occupy less than 2 1 pixels in the image. Besides, this study utilized tophat process to separate vehicles from the roads, which is only sensitive to bright vehicles. So, this previous work may not identify all the vehicles very accurately. It worth noting that this previous work also indicated the obvious reduction of transportation density, and its correlation with lockdown stringency, which shows another evidence for the conclusion of our study. Our study is also affected by the following limitations. In addition to the detection rate, it is difficult for the automatic vehicle detection algorithm to distinguish between running vehicles on the road and stopped vehicles parked at the roadside. To solve this problem, we also present the statistics of numbers of vehicles on arterial roads, whereas this phenomenon cannot be avoided. Considering that there may be more vehicles parked at the roadside after a city lockdown has been put in place, the transportation density reduction in our study may be underestimated. The second problem is that, for fair comparison, we use OSM data for six cities across the world to extract road regions and define arterial roads. However, OSM data is a free geographic data source that is provided by individuals [39] , meaning that we cannot guarantee the accuracy of road type information; this may also affect the fairness when comparing variations in transportation density.","As the COVID-19 epidemic began to worsen in the first months of 2020, stringent lockdown policies were implemented in numerous cities throughout the world to control human transmission and mitigate its spread. Although transportation density reduction inside the city was felt subjectively, there has thus far been no objective and quantitative study of its variation to reflect the intracity population flows and their corresponding relationship with lockdown policy stringency from the view of remote sensing images with the high resolution under 1m. Accordingly, we here provide a quantitative investigation of the transportation density reduction before and after lockdown was implemented in six epicenter cities (Wuhan, Milan, Madrid, Paris, New York, and London) around the world during the COVID-19 epidemic, which is accomplished by extracting vehicles from the multi-temporal high-resolution remote sensing images. A novel vehicle detection model combining unsupervised vehicle candidate extraction and deep learning identification was specifically proposed for the images with the resolution of 0.5m. Our results indicate that transportation densities were reduced by an average of approximately 50% (and as much as 75.96%) in these six cities following lockdown. The influences on transportation density reduction rates are also highly correlated with policy stringency, with an R^2 value exceeding 0.83. Even within a specific city, the transportation density changes differed and tended to be distributed in accordance with the city's land-use patterns. Considering that public transportation was mostly reduced or even forbidden, our results indicate that city lockdown policies are effective at limiting human transmission within cities."
"The coronavirus disease 2019 (COVID-19) pandemic is growing rapidly worldwide and is on course to affect the largest number of people globally since the 1918-1919 influenza pandemic ('Spanish flu'). 1, 2 In addition to the direct morbidity and mortality of COVID-19, there is increasing concern regarding the pandemic's social and psychological sequelae. 3 Evidence-based literature on previous pandemics, for example, the 2003 Severe Acute Respiratory Syndrome (SARS) pandemic, suggest their impact on individuals' mental health. [4] [5] [6] [7] Pervasive depression, anxiety and post-traumatic stress disorder (PTSD) were found among survivors of SARS upon their recovery from their acute illness. 6 In a longitudinal cohort study based in Hong Kong, depressive and PTSD symptoms were found to persist at 30-month follow-up. 7 Since the outbreak of COVID-19, researchers and clinicians from the first endemic countries, including China and Italy, raised concerns that individuals with preexisting mental disorders may be the most vulnerable to psychiatric consequences among a pandemic. 8, 9 Identified at-risk patient populations include those with major depression, anxiety disorders, and schizophrenia. 9,10 Bipolar disorder (BD) is a severe and persistent mental disorder that affects approximately 2% of the population worldwide. 11 Preliminary survey findings from Australia reported that patients with mood disorders, particularly BD, experience higher levels of depression and general distress during the COVID-19 pandemic. 12 Given the substantial psychological and physical health burden associated with BD, we believe that COVID-19 poses both general and specific challenges to patients with BD, and to the clinicians and health systems providing care for them. We hypothesize that without targeted interventions, patients with BD are more likely to be infected and to experience poorer physical outcomes compared with the general population. We also expect that the social and economic consequences of COVID-19 will adversely affect individuals with BD and extend to wider society. In this narrative review and commentary, we combine our collective clinical experience with evidence from a selective survey of relevant literature, including commentaries, reviews, cohort studies and cross-sectional studies on bipolar disorder and COVID-19 through literature database search. We aim to raise awareness of the interplay of patient, provider and social factors that increase the risk of adverse outcomes for individuals with BD in the context of COVID-19. We also identify and propose potential mitigation strategies. Finally, we consider current and future implications of the COVID-19 pandemic for researchers studying BD.@story_separate@In this scoping review, PubMed/MEDLINE was searched from inception to 7 April 2020 and updated on 10 November 2020 using the following search terms: Following the search, one author (S.X.) reviewed all the titles and abstracts to identify relevant articles. Two authors (S.X. and M.I.H.) decided which of these articles to include. They preferentially selected high-level evidence from metaanalyses or prospective cohort studies of adults with BD. Reference lists of included articles were also screened to identify other relevant studies, including seminal reviews and commentaries. There are many reasons to believe that individuals with BD are more susceptible to contracting COVID-19 and experiencing a more severe course of illness. This is because individuals with BD have poorer physical health compared with the general population: they experience premature mortality; 13 they are less likely to prioritize their physical health; 14 and they commonly suffer from a variety of chronic health conditions, including obesity, 15,16 diabetes 16 and cardiovascular disease. 16, 17 The primary contributor to premature mortality in BD is cardiovascular disease, and individuals with BD have up to a fivefold higher risk of cardiovascular disease compared with healthy controls. 17 Individuals with BD also have higher mortality rates from infectious and respiratory causes compared with the general population with standard mortality ratios (SMRs) of 3.4 and 3.1, respectively. 13 Available data regarding COVID-19 show that age and the presence of preexisting physical disease (in particular, hypertension, diabetes and cardiovascular disease) are the strongest risk factors for experiencing poor outcomes of COVID-19 infection. [18] [19] [20] For critically ill patients with COVID-19 pneumonia, age and comorbid chronic conditions remain major predictors of mortality. 21  Anxiety is a common comorbidity among individuals with BD, half of whom may develop an anxiety disorder in their lifetime. 22 Anxious temperament, which is known to be associated with BD, is identified as a significant risk factor for moderate-to-severe psychological distress during the COVID-19 pandemic in Italy. 23, 24 Anxious and irritable behaviour may predispose the individuals with BD to interpersonal conflicts and domestic violence. 25 Moreover, high anxiety level is associated with low mood in BD. 26 In other words, individuals with BD, even during euthymia, who might be experiencing anxiety, will also experience low mood -albeit through a different but synergistic mechanism. Furthermore, in the recent U.S. National Epidemiologic Survey on Alcohol and Related Conditions (NESARC-III), the lifetime prevalence of substance use disorder was 2.3 higher among individuals with BD after adjusting for other psychiatric comorbidities. 27 Social distancing may lead individuals with BD to use recreational substances to cope with isolation, and this may exacerbate impulsivity and impair decision-making. Public health measures are being promoted to decrease the risk of COVID-19 transmission, for example, hand washing and social distancing. 28 However, BD itself may interfere with the implementation of these measures. Individuals with BD experiencing a manic or hypomanic episode typically exhibit impulsivity, disinhibition and impaired judgement. 29 They are more likely to engage in risk-taking behaviours that would contravene personal hygiene and social distancing protocols. Social cues ( 'zeitgebers') such as meals, exercises, and entertainment play an important role in preserving biological rhythms and maintaining mental wellness in individuals with BD. 30 Widespread confinement ( 'lockdowns'), closure of schools and universities, restaurants and gyms, and travel restrictions are contributing to major disruptions in social rhythms. Individuals with BD will struggle setting up a routine, with negative consequences for their sleep and energy regulation. Confinement during a pandemic has also been associated with low mood and depression, which may affect individuals with BD disproportionally. 3 Individuals with BD who already have a small social network may find this restricted further. Existing evidence suggests that a reduced social network in BD is associated with greater impairment in functioning and longer duration of time to recovery. 31 BD is considered one of the most difficult psychiatric disorders to manage; it often requires close monitoring of symptoms and medications. 25 To curtail COVID-19 transmission, many mental health providers are being asked to defer 'nonessential' services and to offer more urgent services through telephone or virtual platforms. For example, assertive community treatment (ACT) teams have reduced the number of home/shelter visits they offer. The most underserved and vulnerable individuals with BD may be without a phone or stable Internet connection. These people will be facing greater barriers to accessing care, potentially contributing further to symptom exacerbation, risk of relapse and increased risk of suicide. 32 Challenges facing individuals with BD will be compounded by difficulties in visiting pharmacies, attending community laboratories for therapeutic drug monitoring required for medications like lithium or divalproex, and in receiving maintenance electroconvulsive therapy (ECT) due to imposed self-isolation guidelines. In the initial phase of the pandemic, many psychiatric outpatient clinics and emergency departments have experienced a marked reduction in routine and emergency visits. 33 Apprehension about potential COVID-19 exposure to individuals with BD and their families is likely contributing to delays in seeking care. It is possible that this decrease in preventive and maintenance care will result in symptom exacerbations, relapses and suicidality, with a rebound in demand for acute psychiatric care and hospitalization. Evidence suggests that suicide rates in BD are the highest among all mood disorders. 34 The early economic consequences of the pandemic include a rapid increase in unemployment and financial stress, two well-known risk factors for suicide attempts and suicide completions in BD. 35, 36 In the context of a recession precipitated by the pandemic and the lockdown required to control its spread, the unemployment rate is expected to continue to increase for many months to come. A high proportion of individuals with BD are young to middle-aged and already face difficulties in securing and maintaining employment; 37 the economic uncertainty in the face of the COVID-19 pandemic is likely to exacerbate these challenges. The COVID-19 pandemic is affecting medical research worldwide. Given the paramount concern for the safety of study participants, governing agencies such as the US Food and Drug Administration has asked investigators to limit study visits to those needed for participant safety or clinical care; they have suggested that other visits be conducting virtually and that safety assessments (e.g. laboratory tests) be arranged through local providers. 38 Some institutions have mandated the suspension of all 'non-essential' research including many pivotal clinical trials. 39 This may lead to the disbanding of skilled research teams and disruption of hardto-establish relationship between researchers and clinicians, that is, sources of referrals for clinical studies. Moreover, research funds will need to be diverted to support pandemicrelated research proposals. This is likely to disproportionally affect BD research, which is already underfunded compared with research for physical conditions that are associated with a similar or lower degree of disability. 40 COVID-19 has disrupted health systems worldwide. Experience from previous pandemics and emerging evidence from the current pandemic suggest the widespread psychiatric sequelae, particularly for those with preexisting mental disorders. [7] [8] [9] In this review and commentary, we examine the individual, health care provider and societal factors that, when compounded, would likely negatively influence clinical picture of individuals with BD in the pandemic. Our suggestions are based on a selective narrative review and combined experience of providing direct clinical care for and conducting research on BD in tertiary psychiatric facilities in Toronto, Canada. However, we recognize that response to COVID-19 drastically differs across time and geographical locations, and that some factors we identified may not be generalized. As the pandemic continues to evolve, more data would allow future studies to evaluate the actual impact of COVID-19 on individuals with BD. We have an opportunity to critically examine and adapt our current practices. In particular, this pandemic has highlighted the urgent need for scaling up digital psychiatry and online interventions. 41 For example, Canada has implemented free online mental health and substance use support services, and similar efforts are seen internationally. [42] [43] [44] We outline below, further recommendations for clinicians working in mental health and more specifically those providing care for individuals with BD: 1. Work with allied health professionals and support patients with BD to obtain affordable phone and Internet access. 2. Ensure that clinicians have access to virtual platforms and that they are appropriately trained to use them. 3. Develop and deliver digital psychoeducation and psychosocial interventions, including electronic cognitive behaviour therapy (eCBT) for management of mood episodes. 4. Apply the use of 'chatbots' for wellness, check-ins, and adverse effect monitoring, with concerning results flagged to clinicians. 5. Modify clinical and research protocols to include frequent remote contact by phone, email, or video conferencing platforms to keep patients and participants engaged and informed. 6. Shift to digital tools and assessment scales enabling clinical care and research studies to be conducted remotely through virtual visits. 7. Prepare for protocol deviations and display flexibility in close cooperation with research sponsors, institutional research boards, and data and safety management boards. The author(s) received no financial support for the research, authorship, and/or publication of this article. M Ishrat Husain https://orcid.org/0000-0001-5771-5750 M Omair Husain https://orcid.org/0000-0002-8575-3364@story_separate@Telemedicine has seen an exponential uptake in the context of the pandemic. Appropriate training is essential for clinicians to deliver virtual care safely and securely, particularly for those caring for BD patients, who can often present in crisis and who may be more susceptible to the mental health impacts of the COVID-19 pandemic. While it is important to carefully evaluate digital platforms to ensure that the data obtained are secure, it is also important for providers and research funding agencies to recognize the benefits of using and assessing remote monitoring in BD in real-world settings. Implementing such steps may reduce the impact of the COVID-19 pandemic on patients with BD, but potentially also improve the quality and accessibility of clinical services for this vulnerable population in the long-term.","The COVID-19 pandemic has posed significant challenges to health care globally, and individuals with bipolar disorder are likely disproportionally affected. Based on review of literature and collective clinical experience, we discuss that without special intervention, individuals with bipolar disorder will experience poorer physical and mental health outcomes due to interplay of patient, provider and societal factors. Some risk factors associated with bipolar disorder, including irregular social rhythms, risk-taking behaviours, substantial medical comorbidities, and prevalent substance use, may be compounded by lockdowns, social isolation and decrease in preventive and maintenance care. We further discuss implications for clinical research of bipolar disorders during the pandemic. Finally, we propose mitigation strategies on working with individuals with bipolar disorder in a clinical and research context, focusing on digital medicine strategies to improve quality of and accessibility to service."
"The ongoing COVID-19 pandemic placed significant stress on healthcare systems worldwide, and in particular, the massive numbers of COVID-19 patients experiencing acute kidney failure far exceeded the capacity to provide optimal kidney replacement therapy (KRT) in patients with kidney failure (1) (2) (3) . Further exacerbating the surge of patients in need of KRT, there was an acute shortage of dialysis machines, both hemodialysis (HD) and continuous renal replacement therapy (CRRT), consumable supplies, personnel, and the emergence of unexpected complications such as a hypercoagulable state leading to frequent clotting and wastage of CRRT cartridges. We previously described the strategies to deal with the surge at one of our hospitals (4, 5) . We now address the tactics employed throughout our hospital network-The New York-Presbyterian (NYP) hospital system. NYP is comprised of seven hospitals and three regional hospitals with over 3,800 beds and 421 intensive care unit (ICU) beds. The number of available ICU beds more than doubled during the COVID-19 response to 901 by converting operating rooms, medical and surgical floor beds, cardiac catheterization suites, and meeting spaces to accommodate patients with ICU level needs. At the peak of the surge, 83% of our ICU beds were occupied with COVID−19 patients and with over 140 patients needing CRRT compared to the usual 30-40 across all campuses. In response to this unprecedented surge of patients with kidney failure, we formed the New-York Presbyterian COVID-19 Kidney Replacement Therapy Task Force to ensure optimal care of patients requiring KRT. In this article, we report our experience with the formation and organization of this task force, and the strategies and innovative tools we developed which could inform nephrology programs faced with similar challenges ( Table 1) .@story_separate@In the initial phase of the surge, much attention was focused on the hospital's supply of mechanical ventilators. However, after reviewing data from other countries, hospital leadership recognized it would need to ramp up capacity for KRT exponentially in a matter of weeks. An attempt to secure additional KRT machines and supplies, described later in this report, would not provide a timely solution to the impending demand. Thus, a nephrology leader and a hospital administrator were tasked with convening a group of nephrologists and nurses from across NYP hospitals to develop a strategy to combat the impending shortage of resources. The group met daily via an online call to share local patient volume, status of CRRT machines, staffing and consumables in the face of a rapidly escalating patient census at each hospital. We used a color-based system (green, yellow, orange, and red) to indicate the status for each of these variables (Figure 1) . It was immediately obvious that the hospital system was critically short on nearly all aspects of KRT support: personnel and KRT machines/disposables, and patient volume only beginning to increase. Based on daily analysis of data from a tracking tool (see section KRT Trackers and Dashboards), the group adopted a unified response plan in real time by which all KRT machines and consumables would be distributed proportionally across campuses. Existing machines and supplies were moved within 24 h between hospitals based on relative proportions of machines/disposables to patients on KRT. There would also be proportional distribution of newly acquired KRT machines. Because of this collaborative effort, all campuses were able to provide KRT proportionally during the surge period and no patient suffered from a lack of treatment. We developed a data-driven approach to analyze and guide our response to the COVID-19 surge using web-based spreadsheets. These tools are accessible to the reader (see links below). In order to identify develop an enterprise-wide plan to support patients requiring KRT during the surge, it was necessary to know exactly the CRRT and HD censuses, machine and supply availability and staffing. To centralize this information, we developed an Enterprise-wide KRT Trackers and Dashboards for CRRT and HD. Each hospital entered the details of their own CRRT and HD programs (Figures 1A,C, respectively) , which then automatically populated the Enterprise-wide KRT Dashboards (Figures 1B,D, respectively) . The CRRT dashboard also included the current count of HD and CRRT disposables available at each site and at the central warehouse. The projected number of days of supply remaining for each site and for the entire enterprise based on the size of the CRRT census was automatically calculated and color-coded. The dashboard allowed for identification of hospitals that were disproportionally affected and aided in enterprise-level decisions to shift resources between hospitals or from central storage. For example, Figure 1B shows that 2 extra CRRT machines at Hospital 1 are available to be transferred to Hospitals 2 and 5 which are most in need and therapy fluid needs to be delivered to Hospital 5 from the Central Depot). Similarly, the HD dashboard allowed for quick, visual reporting of the status of working machines, census sizes, nurses, and HD technicians at each site. Both tools were critical in decision-making that occurred nightly on the crosscampus conference calls and allowed for the equitable sharing of resources across a large, complex healthcare system. In anticipation of the upcoming surge of KRT patients, we had placed large orders for CRRT machines and consumables (fluids and cartridges) during February 2020. Despite early planning, we were acutely aware that we would not receive an adequate supply in time. Additionally, we were facing newly imposed allocations of consumables from our industry partners since many hospitals were vying for the same pool of supplies. a. Early (pre-emptive) Action: as part of our pandemic planning in early March we reviewed all of our key KRT supplies and successfully ordered a quantity equivalent to the previous 1 month's usage plus a 10% buffer. This buffer gave us extra time to adjust usage protocols, find alternative products on the market and work with our suppliers to determine an order release schedule. Using the same approach with KRT machines we were able to fill approximately 30% of our order in addition to rental machines. b. ""Self "" Distribution: Instead of using a third-party distributor to manage our dialysis supplies, we instead ordered all supplies directly to hospitals or to an off-site warehouse that was managed 24/7. from alternate vendors. However, we kept these supplies as backup since we needed to make significant practice changes prior to implementation. tested an in-house CRRT fluid generation strategy utilizing HD machines. The method was adapted from a previously reported protocol (6). d. Therapy Fluid Conservation Nomogram: At the peak of the surge, our CRRT fluid reserve was down to 9 days (from the usual 30 days). Partially used fluid bags at the end of the therapy were discarded, adding to the critical shortage of treatment fluids. We developed a nomogram to avoid discarded bags is described in detail in a previous publication (5). e. Anti-coagulation Protocol: A significant proportion of our COVID-19 patients were experiencing multiple clotting episodes while on CRRT despite therapeutic doses of systemic heparin. In collaboration with our hematologists and pharmacists, we developed an anticoagulation protocol based on anti-Factor Xa activity in patients with recurrent clotting (Supplementary Figure 1) . f. Personnel: The rapid escalation of critically ill patients CRRT/PIRRT significantly added to the burden on the already depleted ICU nursing staff. We deployed perfusionists to and a color-coded status was generated for filters/cartridges, therapy fluid, and machines, which allowed for identification of hospitals that were disproportionally affected by the surge. Enterprise-wide HD tracker (C) and dashboard (D). Each hospital entered the details of their HD program (C), which then automatically populated the Enterprise-wide HD dashboard (D). This allowed for quick, visual reporting of the status of working machines, census sizes, nurses, and HD technicians at each site. assist with CRRT after receiving essential training. They were responsible for mobilizing equipment, and providing adjunct support to the ICU nurses. Furthermore, a large dialysis organization provided us with home HD nurses and technicians who assisted the ICU nurses and perfusionists. Several problems needed to be addressed for hospitalized HD patients: (1) the need for isolating COVID-19 patients, (2) the unknown but anticipated increase in patient load, (3) the risks to our non-COVID-19 population, and (4) potential staffing and supply issues (7, 8) . Early in the surge, per CDC recommendations (9), we placed COVID-19-positive patients in unused Hepatitis-B isolation rooms. Subsequently, we added additional COVID-19-dedicated HD beds with partitioning shields and appropriate distancing, and eventually, we created COVID-19-only shifts. We dialyzed non-COVID-19 patients on earlier shifts, leaving the later shifts for the COVID-19 patients to allow for terminal cleaning. Twice-weekly treatments were used in stable patients at the peak of the surge. Lastly, we transferred stable COVID-19 patients to offsite outpatient COVID-dedicated HD units designated in the Emergency Management Plan. These factors made room for more COVID-19 inpatient HD treatments. All HD patients were successfully accommodated using these strategies. In regional Hospitals without CRRT, we transferred some critically ill patients to tertiary care hospitals in the network. In the remaining patients, we performed HD on alternate days, with additional treatments for fluid and electrolyte control. Stable patients received twice-weekly HD with the occasional use of potassium binders to treat hyperkalemia. In preparation for the anticipated surge of HD patients, we requested additional staffing including HD nurses and HD technicians during the early stages of the pandemic. We redeployed nurses with prior HD after receiving a week of refresher training on HD. Additionally, we were able to utilize volunteer research coordinators and laboratory staff for machine preparation and assisting the nursing staff. To ensure safety, all non-traditional personnel needed to be compliant with a ""job profile competency task list."" To limit prolonged exposure of nursing staff we employed video monitoring in COVID-19 HD patients where treatment monitoring could be performed from outside the patient's room. These steps allowed the increase of nurse/patient ratios during the peak of the pandemic, significantly relieving the burden on our nursing staff. Additionally, because of this support, the nursing personnel reported a considerable alleviation of anxiety and stress during the surge. Comprehensive KRT disaster planning also included the need to establish/expand the use of peritoneal dialysis (PD) (10) . Placement of PD access and the availability of PD-trained personnel were significant barriers to the widespread use of PD. We employed a multidisciplinary strategy to establish acute PD. Each site identified one key nephrologist with PD experience to coordinate urgent start PD. We identified general or transplant surgeons who could perform bedside PD catheter insertion in accordance with International Society of Peritoneal Dialysis (ISPD) guidelines (11) (12) (13) . To ensure effective PD delivery, we excluded patients with extensive surgical history, ascites, active gastrointestinal issues, who required prone ventilation or had high oxygen/PEEP requirements. We treated hyperkalemic patients on PD with hybrid CRRT temporarily, or with high frequency PD prescriptions using automated PD. We also converted patients with repeated CRRT filter clotting to PD (14) . A cumulative total of 22 patients received PD. At the Cornell campus, the acute PD population represented approximately 20% of the total census of KRT patients at the peak. The redeployment of PD nurses to other areas of the hospital posed a challenge. A combination of resources to gather personnel who could deliver PD. The creation of adult ICU beds in pediatric units allowed the use of PD since pediatric nurses were trained in PD. Additionally, at one NYP site, pediatric nurses formed a rotating ""Acute PD team"" which allowed patients at any location to receive treatment. Additionally, training of cardiac perfusionists and bedside ICU nurses by outpatient PD nurses and from industry resources allowed expansion of the PD provider pool. The use of telehealth technology allowed these educational resources to provide ""virtual hands on"" training and around the clock troubleshooting support for staff at the bedside. The original contributions presented in the study are included in the article/Supplementary Materials, further inquiries can be directed to the corresponding author/s. DOI for Enterprise-wide KRT Tracker and Dashboard: https:// doi.org/10.7916/d8-cvhb-4667.@story_separate@The unprecedented surge of nephrology inpatients during the COVID-19 pandemic in New York City required us to form the New-York Presbyterian COVID-19 Kidney Replacement Therapy Task Force with intercampus physician, nursing, and supply chain representation. We organized daily meetings around novel dashboards that were developed to track supply/demand of resources and were able to make projections about the burn rate of our supplies on hand. This facilitated equitable sharing of resources across a complex healthcaresystem and allowed for the rapid implementation of standardized protocols at each hospital. We have made these tools available for use given their potential benefit for nephrologists at other institutions.","The unprecedented surge of nephrology inpatients needing kidney replacement therapy placed hospital systems under extreme stress during the COVID-19 pandemic. In this article, we describe the formation of a cross campus “New-York Presbyterian COVID-19 Kidney Replacement Therapy Task Force” with intercampus physician, nursing, and supply chain representation. We describe several strategies including the development of novel dashboards to track supply/demand of resources, urgent start peritoneal dialysis, in-house preparation of kidney replacement fluid, the use of unconventional personnel resources to ensure the safe and continued provision of kidney replacement therapy in the face of the unanticipated surge. These approaches facilitated equitable sharing of resources across a complex healthcare-system and allowed for the rapid implementation of standardized protocols at each hospital."
"Acute lung injury (ALI) or its aggravated stage acute respiratory distress syndrome (ARDS) is a common cause of respiratory failure in severely ill patients. Despite substantial progress in intensive care therapy and organ supportive technology, the mortality of ALI remains high at 30%-40% in most studies. 1, 2 The most common cause of ALI is a bacterial or viral infection. For example, patients who are infected with SARS-CoV-2 can present with pneumonia and hypoxemia, even progressing to ALI/ ARDS. 3 Sepsis, aspiration of chemical agents and gastric contents, and shock are other common causes of ALI. 4 The pathogenesis and pathophysiology of ALI are characterized by the destruction of alveolar-capillary integrity and by increased permeability, resulting in fluid, proteins, inflammatory agents and red blood cells accumulating in the alveolar space, 2 and the clearance ability of the lung also being impaired. 5 Clinical treatment of ALI/ARDS focuses on early diagnosis, control of infections, supportive ventilation, careful fluid management, and general supportive measures. 2 Until now, clinical short-term or long-term mortality has not been able to be reduced by pharmaceutical drugs, 6 but some pharmacologic agents have been proved to be effective in ameliorating ALI/ARDS. Glucocorticoids, such as dexamethasone, when administrated at an early stage, may decrease the duration of supportive ventilation, as well as the overall mortality, 7 but they are harmful at approximately 2 weeks after ARDS has been diagnosed. 8 Neutrophil elastase increases the permeability of the alveolar-capillary barrier and causes proteolytic lung tissue damage. 2 Neutrophil elastase inhibitors, such as sivelestat, could be optional for the treatment of ALI; however, a multi-national clinical trial proved that sivelestat therapy was unsuccessful. 9 Simvastatin has been reported to prevent organ dysfunction experimentally in ALI by decreasing vascular inflammation and leakage. 10 However, a clinical trial reported that, although its safety was guaranteed and its adverse effects were minimal, simvastatin did not show distinct clinical benefits. 11 Inhaled nitric oxide can improve oxygenation and lung function but may have side effects or even be harmful when applied improperly. 12 Gene silencing, via short-interfering RNA (siRNA) to protect the integrity of the epithelial-endothelial barrier and prevent lung cell death, is a promising therapeutic option, but its application has been hampered by delivery technology challenges and safeties. 13 Surfactant replacements, anticoagulation, and antioxidants have shown some effects in experiments but have failed in clinical trials 14 (Table 1) . The limited success of pharmacological therapies forces us to develop new agents to combat ALI. The rapid development of nanomedicine might shed new light on this issue. Nanomedicines that possess active or passive targeting abilities have shown therapeutic advantages in various diseases. For example, it is recommended that nano-formulating dexamethasone be used to improve the efficacy in treating COVID-19, due to its ability to target hyper-activated immune cells, as well as its anti-edema activity. 15 Therapeutic agents such as drugs, siRNA, and proteins can be conjugated or encapsulated inside nanomedicines. 16, 17 In the presence of ALI, there arose enhanced permeability of blood vessels, along with alterations in oxidants, pH and enzymes in the microenvironment, as well as regulation of the expression of various cell surface receptors. 18 The above characteristics offer targets for site-specific delivery and the microenvironment for responsive drug release. To this end, the nanomedicine for ALI has broad alternative therapeutic strategies, including the delivery of anti-inflammatory agents to the disease site, [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] the direct scavenging of inflammatory factors, [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] the regulating of inflammatory cell activities, [48] [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] or the modulating of inflammatory signaling pathways. [60] [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] [71] [72] [73] Various nanomedicines act on different cells or pathophysiology processes to achieve therapeutic effects, many of which have demonstrated satisfying in vitro and in vivo effects. In this review, we will focus on a systematic overview of the state-of-the-art and advances in therapeutic nanomedicines for ALI. Firstly, a brief profile of essential cellular targets of nanomedicine for enhanced therapeutic effects will be presented. Subsequently, the diverse nanomedicines will be categorized into four groups and their applications in the treatment of ALI will be shown in elaborate detail. Lastly, we will analyze both the ongoing chances and challenges of nanomedicine-based therapy for ALI, especially presenting some of the innovative technologies that will navigate the future direction of nanomedicine, such as nanorobotics, machine learning and artificial intelligence ( Figure 1 ).@story_separate@During ALI, the damage of the alveolar-capillary barrier increases vascular permeability and fluid accumulation. 2, 4 This process is mediated by macrophages, neutrophils, and epithelial and endothelial cells through the innate immune response. 74 Alveolar macrophages (AMs) are resident cells in the alveoli that use a variety of mechanisms as a defense against the invasion of foreign particles and pathogens at the first line. 75 Upon stimulation, resting macrophages (M0) are activated through classical and alternative pathways that are polarized and mainly classified into pro-inflammatory phenotype (M1) and anti-inflammatory phenotype (M2). 76 The M1 macrophages can release various potent pro-inflammatory cytokines including IL-1β, IL-6, and TNF-α. 77 Modulation of AMs has been found to mitigate lung injury by attenuating neutrophil accumulation and reducing pro-inflammatory cytokines. 78 The mannosylated nanomedicine can target to macrophages by mannose receptors, 25, 52, 61 and sialic acid bound to the E-selectin on the macrophages 29,67 can also be utilized for active targeting. Because nanoparticles can be easily phagocytosed by macrophages, they possess a superior ability to control the inflammatory responses mediated by macrophages. 37, [45] [46] [47] 62, 64, 67, 68, 71, 73, 79 Among the leukocytes at the sites of inflammation, neutrophils are the first to be recruited in response to chemotactic factors. 80 They migrate across the endothelium and through the epithelium into the alveoli, then release histotoxic mediators to damage lung tissue, such as reactive oxygen species (ROS), neutrophil extracellular submit your manuscript | www.dovepress.com International Journal of Nanomedicine 2021:16 81 as well as disrupting the endothelial-epithelial barrier. 82 Nanomedicine can suppress neutrophil function by inhibiting upregulated superoxide anions and elastase, 48, 49 repressing activity of neutrophil overactivation by inhibiting phosphodiesterase 4 (PDE4) activity 50, 51 and detaching neutrophil adherence by blocking integrin signaling, 58 or using neutrophils as vehicles 54, 56, 57 for targeted ALI therapy. Restoring the integrity of the endothelial-epithelial barrier is critical in ALI therapy. 83, 84 As the primary injured structure, the pulmonary epithelium is subject to dissociation of intercellular junctions 85 and cell death, 31 but is more resistant to injury than the endothelium. 86 The endothelial junctions' breakdown or the endothelial cells' death will increase the lung's vascular permeability, thus resulting in excessive fluid and protein leakage into the alveoli. 2 To target the endothelium and epithelium, intercellular adhesion molecule-1 (ICAM-1), 87 plateletendothelial cell adhesion molecule (PECAM-1) 88 and surfactant protein (SP) 89 can be bio-conjugated by corresponding antibodies for active targeting nanotherapy. [20] [21] [22] 24, 28, 35, 56, 60, 63 Nanomedicine can restore barrier integrity and prevent cell death by influencing inflammatory pathways 20, 21, 44, 60, 63, 65, [70] [71] [72] and alleviating oxidative stress. 28, 35, 36, 42, 54, 56 The influence of nanomedicine on cellular architecture can be assessed by a pragmatic optimized air-liquid interface system, which showed comparable results as those in an in vivo study. 90 For ALI treatment, nanomedicines are mainly administrated through the intrapulmonary or the intravenous route. For the intrapulmonary route, the agents should be sufficiently potent to penetrate the mucus layer and pass through the cell membrane; two major barriers that affect pulmonary delivery efficiency. 91 As the mucus layer is rich in negatively charged glycoproteins and phospholipid pulmonary surfactant that can trap cationic agents, it is reasonable to modify the nanomedicines to enhance penetration. 62, 92 For the intravenous route, target delivery to the inflammatory site ensures satisfactory therapeutic efficacy. Targeting strategies include the passive targeting effect called ELVIS (extravasation through leaky vasculature and the subsequent inflammatory cell-mediated sequestration) 18 and the conjugating of active targeting moieties to the backbone of the nanomedicine. Next, some of the nanomedicine applications in treating ALI through different mechanisms and the therapeutic efficacy are introduced. Liposomes are a well-established drug delivery system in the clinical context that are composed of single or multiple concentric lipid bilayers and aqueous compartments. The lipophilic agents are embedded within the phospholipid bilayer, while the aqueous core can encapsulate hydrophilic agents. 93 Nanostructured lipid carriers (NLCs) or oilloaded solid lipid carriers are the second generation of lipid carriers. The oil core of NLCs offers a variety of fascinating properties, including increased loading capacity, excellent biocompatibility, controlled release compared to the rapid release of liposomes, and feasibility of large-scale production. 20, 94 Anti-inflammatory drugs, [20] [21] [22] [24] [25] [26] 49, 60, 61 anti-oxidant agents, 27, 35 and neutrophil function inhibitors 48, 50, 51 were transformed into nanomedicines to improve their effectiveness and decrease their side effects ( Table 2) . The lipophilic antioxidant (α-tocopherol) and the hydrophilic anti-inflammatory agent (Glutathione, GSH or DEX) were encapsulated into liposomes, which showed an advantageous effect in ameliorating lung injury over the free drug. 26,27 PDE4 inhibitors, rolipram or cilomilast, can repress the activity of neutrophil overactivation, 95 however, the brain penetration side effects and narrow therapeutic index have restricted their application. 50 Employing phosphatiosomes to deliver PDE4 inhibitor showed enhance pulmonary surfactant affinity and reduced penetration into the brain; additionally, neutrophil activation was repressed by decreasing the O 2 •− , Ca 2+ content and increasing the cyclic adenosine monophosphate (cAMP) production. 50, 51 Oleic acid (OA) can also inhibit inflammation of activated neutrophils at a certain dose. The changing amount of mineral oil in OA-loaded nanocarriers enabled mean diameters to vary among 105, 153, and 225 nm. Smaller sizes exhibited greater neutrophil uptake to decrease the cell viability and the intracellular calcium level, while larger sizes exhibited greater lung targeting ability than the smaller ones 48 (Figure 2A ). Overproduction of NETs promoted inflammatory pathologies, 96 and neutrophil elastase (NE) participated in the formation of NETs. 97 Sivelestat is an NE inhibitor that is clinically used in patients with ALI who develop a systemic inflammatory response. Interbilayer-crosslinked multilamellar vesicles (ICMVs) loading sivelestat (ICMVsivelestat) were readily taken up by neutrophils and inhibited the formation of NETs effectively in vitro. ICMVsivelestat alleviated lung injury by reducing NE and production of other pro-inflammatory cytokines. 49 To improve the targeting ability of liposomes, mannosylated, 25, 61 and antibody-modified 20-22,24,35,60 liposomes were developed. Mannosylated (Man) liposomes can target AMs, inhaled liposomes encapsulating dexamethasone palmitate, and Man-cationic liposome/NFκB decoy reduced pro-inflammatory cytokines, and suppress neutrophil infiltration. 25, 61 Surfactant protein A (SP-A) is a type of pulmonary SP that is mostly expressed in type II alveolar epithelial cells but is rarely expressed in extrapulmonary tissues and organs. 24 SP-A nanobodyconjugated immune-liposomes delivering glucocorticoids showed good lung-targeting specificity and decreased the Lung protection↑ [35] submit your manuscript | www.dovepress.com International Journal of Nanomedicine 2021:16 cytokine level in bronchoalveolar lavage fluid (BALF). 22, 24 NLCs can be incorporated into lung endothelial cells via caveolar vesicles, and thus possess endothelial-protective effects. 98 Conjugating ICAM-1 antibody to NLCs endowed its active targeting ability to lung endothelium, and the size and zeta potential of the liposomes were correlated with the therapeutic effects. 20, 21, 60 The larger NLCs (337.8 nm) loading simvastatin exhibited ideal lungtargeting characteristics 21 ( Figure 2B ). Other lung-targeted ternary NLCs loaded with simvastatin, protamine (Pro), and the angiopoietin-1 (Ang-1) gene with a larger size (357.1 nm) also showed better improvements. 60 The anionic NLCs exhibited higher cellular uptake and stronger pulmonary distribution, showing significant antiinflammatory efficacy. 20 PECAM-1 binds to the endothelium and is internalized via the noncanonical cell adhesion molecule (CAM)-mediated endocytic pathway 99 and can be adopted to endow lung-targeting ability. PEGylated liposomes conjugated with anti-PECAM-1 loaded with EUK-134 accumulated in the lungs after i.v. administration, inhibited cytokine-induced inflammatory activation, and provided >60% protection against lung edema in the endotoxin-stimulated mouse model. 35 Recently, a soft nanobot composed of double micellar microemulsions has been developed, which possess the capability of active nanodrug delivery to strictures of air-liquid interface, and this could become a promising technology for therapeutic carriers and targeted delivery to ALI/ARDS. 100 Polymeric nanomedicines can be engineered from natural or synthetic polymers, 101 (Table 3) . Poly-lactic-co-glycolic acid (PLGA) is the most commonly used polymer with good biodegradability and biocompatibility. 103 The PLGA nanoparticle-containing ruthenium red was used in the ventilator-induced lung injury (VILI) model via inhalation. This nanomedicine reacted through the alveolar macrophages and the capillary endothelial cells, blocked calcium signaling, and inhibited vascular permeability in ex vivo ventilation-perfusion IL-6, TNF-α↓; H&E↑ [68] submit your manuscript | www.dovepress.com International Journal of Nanomedicine 2021:16 manner. 67 Oxidative stress plays an important role in ALI 104 and mitochondria are the main source of ROS production. Sialic acid (SA)-functionalized PEG−PLGA microspheres loaded with triphenylphosphonium (TPP) cation-modified curcumin (Cur) were utilized as mitochondria-targeting ALI therapy. The microsphere's size was larger than 800nm, thus enabling good lung distribution, and the SA modification exhibited an ideal lung-targeted characteristic. 29 Using poly(ε-caprolactone) to construct lipid-core nano-capsules (LNCs) and encapsulated αbisabolol (α-bis) or resveratrol (RSV) into LNCs can minimize drug oxidation, improving internal absorption and showing satisfactory therapeutic effects. 19, 23 The inflammatory microenvironment of ALI has the feature of a low pH; poly(β-amino esters) have a sharp acid-sensitive segment; bio-conjugating anti-ICAM-1 antibodies enable satisfying lung targeting and extended circulation. The pHresponsive nanoparticles can load anti-inflammatory agent TPCA-1 at a high content (24%, w/w). The accumulative release of TPCA-1 increases from less than 20% 24 h at pH 7.4 to approximately 90% 15 h at pH 6.5 28 ( Figure  3A-D) . Polymeric nanoparticles are also attractive gene carriers for ALI therapy. 91 Nebulized PLGA bearing erythropoietin receptor (EpoR) complementary DNA (cDNA) nanoparticles upregulated pulmonary EpoR expression and downstream signal transduction to counteract the inflammation in hyperoxia-induced lung injury in rats. 65 Modifying low molecular weight polyethyleneimine (PEI) with dexamethasone improves its translocation into the nucleus and its gene transfection efficiency. 33 PEI carries β2-adrenergic receptor (β2AR) gene, which regulates alveolar ion and fluid transport, 105 dramatically improving alveolar clearance and decreasing lung fluid content without major adverse effects. 31 Merckx et al used Curosurf ® , a clinically used pulmonary surfactant (PS), as the shell and siRNA-loaded nanosized dextran nanogels as the core to form hybrid nanoparticles for inhalation therapy. The PS shell improved the particle stability, and the intracellular siRNA delivery was enhanced by inserting SP-B into the phospholipid shell. 63 High mobility group box-1 box A (HMGB1A) may be captured in the mucus layer due to its positive charges when administered intratracheally; heparin has negative charges and an anti-inflammatory effect. The HMGB1A/heparin complex was obtained using electrostatic interactions, and reduced proinflammatory cytokines synergically. 64 Fluoropolymers in the form of perfluorocarbon (PFC) nano-emulsions could improve cellular siRNA delivery. 106 Wang et al reported a PFC emulsion polyplex as a gene carrier, containing fluorinated polymeric CX-C chemokine receptor type 4 (CXCR4) antagonist and delivered plasminogen activator inhibitor-1 (PAI-1) siRNA to inhibit CXCR4 and PAI-1 for combined therapy. 53 Another way to enhance mucuspenetrating ability is to develop bifunctional guanidineand fluorine-decorated helical polypeptides. The fluorinated polypeptides dramatically enhanced mucus permeation capability by approximately 240-fold, while the guanidine domain and the α-helix structure facilitated trans-membrane siRNA delivery. Using the topperforming polypeptide, P7F7, to administer TNF-α siRNA intratracheally produced highly efficient (~96%) gene knockdown 62 (Figure 3E and F) . Dendrimers are regularly branched macromolecules that are usually developed as forming dendrimer-drug conjugates or as gene carriers. 101 Inspired by Mycobacterium tuberculosis, Blattes et al designed manno-dendrimers that mimicked the bioactive supramolecular structure of mannose-capped lipoarabinomannan. The manno-dendrimers could target the C-type lectin receptor DC-specific intercellular adhesion molecule 3-grabbing nonintegrin (DC-SIGN), thus inhibiting neutrophil recruitment significantly. 52 Adiponectin (APN) is an anti-inflammatory and cytoprotective adipokine. 107 Delivery of APN using dexamethasone conjugated polyamidoamine (PAM-D) upregulated APN expression. 72 In a subsequent study, the RAGE-antagonist peptide (RAP) increased the gene delivery efficiency of PAM-D, and the RAP inhibited the RAGE-signal to show antiinflammatory effects. 69 Cholesterol-conjugated polyamidoamine micelles could deliver pHO-1 (heme-oxygenase -1 plasmid) along with RSV or Cur, in which pHO-1 induced HO-1 expression to decrease pro-inflammatory cytokines, and RSV or Cur inhibited the inflammatory reactions synergically. 70, 71 The phosphorus dendrimers have been shown to have more efficiency in the cellular delivery of siRNA 108 and exhibited anti-inflammatory properties simultaneously. 109 Compared with morpholinium-containing dendriplexes, pyrrolidinium-decorating dendriplexes demonstrated a stronger siRNA complexation, and the higher cellular uptake enabled an enhanced silencing efficiency of TNF-α 73 ( Figure 3G ). The biocompatibility, biodegradability, and the lack of immune response properties of self-assembling peptides make them ideal drug carriers and regenerative medicines. 110 Self-assembling R3V6 peptides with submit your manuscript | www.dovepress.com International Journal of Nanomedicine 2021:16 a positive charge and membrane-penetrating properties were suitable for gene delivery. The siS1PLyase/ HMGB1A/R3V6 delivering a siRNA ternary complex, in which siS1PLyase down-regulates the S1PLyase (sphingosine-1-phosphate lyase) and S1P on alveolar macrophages to block the NF-κB signaling pathway, demonstrated synergistic anti-inflammatory effects. 68 Self-assembling peptide EAK16-II carrying Src tyrosine kinase inhibitor (PP2) demonstrated a lower toxicity, and a satisfactory anti-inflammatory effect against the lung ischemia-reperfusion (IR) model in rats. 34 Recently, a self-assemble DNA origami nanorobotic delivery platform is available with nanoscale precision sensing, movement and manipulation properties, which may provide a new strategy for nanomedicine-based gene therapy. 111, 112 Inorganic Nanomedicine Inorganic nanomedicines are generally composed of inert and biocompatible metals, which endow them with stable characteristics and smaller diameters. Most inorganic nanomedicines are highly efficient and exabit multiple effects during biological applications. Gold, cerium dioxide (CeO 2 ) and selenium (Se) have been used to treat ALI. The biggest challenge that limits their application is their elimination from the body, as repeated administration can result in toxicity by accumulation effects. 101 The strategies to tackle this problem include biogenic route of synthesis, conjugating peptides on metallic nanoparticles or immobilizing inorganic nanoparticles on silica nanoparticles. The main mechanisms of inorganic nanomedicine to treat ALI include inhibiting the inflammatory signal [37] [38] [39] [40] [41] 44 and scavenging oxidants 36, 42, 43 (Table 4) . Gold nanoparticles (GNPs) could reduce the acute inflammatory response and excessive ROS production, protecting lung tissue from LPS-induced morphological changes. 38 However, they tend to be trapped in the liver and spleen and are nonbiodegradable; so, the biosafety concerns still exist. The biogenic route of GNPs synthesis offers an efficient way to tackle the biosafety problems, which can fabricate spherical, 113 anisotropic, and high aspect ratio gold nanomaterials. 114 The molecular dynamic simulation, supported with experimental photothermal therapy, has shown the excellent application of these GNPs in nanomedicine for clearing biofilm and promoting the grown of fibroblast. 115, 116 These biomineralized nanomaterials proved excellent imaging agents 117 and are drug carriers with enhanced bioavailability in vitro 118 and in vivo. 119 Modifying the GNPs with peptides is another way to enhance efficiency and safety. Peptide-modified GNPs could modulate the process of endosomal acidification and inhibit multiple Toll-like receptor (TLR) intratracheally, only approximately 8.49% ± 0.7% of the injected dose remained in all the tested organs/ tissues. At both 6 and 26 h post-intratracheal injection, a significant amount of P12 was detected in the feces and the amount in the intestine was much higher than that in the liver, suggesting that P12 was cleared through a hepatobiliary route 41 ( Figure 4) . Furthermore, the 20-nm hybrid P12 (G20) was more potent than the 13-nm hybrid P12 (G13) and 5-nm hybrid P12 (G5) in inhibiting TLR4 activation and its downstream cytokine production. The P12 (G20) exhibited a higher cellular uptake and a stronger endosomal pH buffering capacity, endowing it with enhanced inhibitory effects. 39 Cigarette smoke extract (CSE, 1%) was able to be adsorbed onto the GNP hybrids and largely increased their cellular uptake. CSE-P12 inhibited TLR4 activation through endosomal acidification and contributed to autophagy induction and subsequent antioxidant protein expression. 40 P12 could also increase the alveolar anti-inflammatory M2 phenotype macrophages by polarization in the BALF and lung tissues, and decrease M1 macrophages in the alveolar and interstitial spaces. 37 CeO 2 is a promising oxidant-scavenging nanoparticle, but its slow elimination induces concern for its toxic effect. By immobilizing it on the surface of silica, the toxicity of the cerium nanoparticles was reduced. The CeO 2 nanoparticles showed anti-inflammatory and antioxidant effects, as well as stimulating oxygen consumption in healthy rats and those with pneumonia. 43 The same strategy was adopted to fabricate porous Se@SiO 2 PVP coated nano-spheres. In a paraquat-induced rat model, the nano-spheres could attenuate oxidative stress, eliminate ROS, and reduce inflammatory cytokines. 42 The nano- DovePress spheres could also modulate mitochondrial function, activity, and dynamics, significantly increasing the epithelial cells' resistance to oxidative injury. 36  Polydopamine is a natural biopolymer that can selfassemble or be a film coating. Enriched phenol groups enable it to act as a nano-enzyme to scavenge H 2 O 2 directly or to catalyze the decomposition of H 2 O 2 . Polydopamine alleviated lung tissue damage by diminishing ROS generation. 46 β-cyclodextrin (β-CD) is a cyclic oligosaccharide that mimics enzyme conformation with a hydrophilic rim and a hydrophobic cavity. Two ROS eliminating agents, Tempol and PBAP, were simultaneously conjugated to β-CD to construct a superoxidase dismutase/catalase mimetic material (TPCD). TPCD nanoparticles eliminated a broad spectrum of ROS, protected macrophages from apoptosis, attenuated inflammatory responses and oxidative stress 47 ( Figure 5 ). In another study, luminol-conjugating β-CD (LCD) nanoparticles could act on both neutrophils and macrophages, effectively inhibiting the inflammatory response, oxidative stress and cell migration, demonstrating desirable efficacy in treating ALI with biosafety 45 (Table 5) . Inflamed vasculature targeting ability was achieved by conjugating anti-ICAM-1 antibody or peptides to nanoparticles. 121 However, this strategy might impair their specificity and affinity, especially when administrated in vivo. 122 During inflammation, neutrophils abundantly express integrin β2; this integrin interacts with the ICAM-1 molecules on the endothelial cells. 123 This interaction could be blocked to inhibit the accumulation of neutrophils, or use this interaction for targeting drug delivery. Piceatannol blocks the ""outside-in"" integrin signaling in neutrophils. Albumin nanoparticles loading piceatannol were taken up by neutrophils, detaching neutrophils' adherence and eliciting their release into the circulation. 58 Inspired by the study above, it is promising to design nanoparticles that hitchhike activated neutrophils in situ; then, neutrophils could deliver nanoparticles to the inflammatory site by adhering and migrating across the blood vessel endothelium into the inflammatory tissues. Using bovine serum albumin to deliver TPCA-1, this nanoparticle dramatically ameliorated inflammation and decreased permeability in the lung. 57 Nitrogen cavitation was initially employed to isolate neutrophil plasma membrane as sealed vesicles, which minimizes lysosomal and nuclear rupture. Neutrophils were placed in the cell disruption bomb with optimum pressure and duration of equilibration, then the pressure was quickly released to disrupt cells, and the vesicles were obtained followed by a series of centrifuge. 124 The cell membrane nanovesicles, which are made from activated neutrophils using nitrogen DovePress cavitation, possess intact targeting molecules of integrin β2, and can selectively bind to inflamed vasculature. Human neutrophils are abundant in the blood; thus, this strategy could be utilized to develop personalized nanomedicines. 56 In another study, piceatannol was remotely loaded in nitrogen cavitation nanovesicles via a pH gradient. The piceatannol-loading nanovesicles dramatically alleviated ALI and sepsis induced by LPS. 54 Platelet-derived extracellular vesicles (PEVs) are another cell-based drug-delivery system. The platelets intrinsically have inflammation-site affinity and are suitable for targeting ALI treatment. When loaded with TPCA-1, they significantly inhibited pulmonary inflammatory cell infiltration and calmed regional cytokine storm syndromes. This system is also suitable for the treatment of chronic atherosclerotic plaque, rheumatoid arthritis, and wounds associated with the skin. 79 Symbols: ""↑"", increase or improve; ""↓"", decrease or deteriorate; ""-"", no significant difference; NG, not given; pre, before stimulation; post, after stimulation; Administrations: i.g, intra gastric; i.n, intranasal; inh, inhalation; i.p, intraperitoneal; i.t, intratracheal; i.v, intravenous; p.o, per os; Others: ACE, angiotensin-converting enzyme; AKP, alkaline phosphatase; ALI, acute lung injury; ARDS, acute respiratory distress syndrome; CAT, catalase; Chol, cholesterol; CINC-1, cytokine induced neutrophil chemoattractan; CoV, coronavirus; COVID-19, coronavirus disease 2019; DCFH, 2.7-dichloro dihydro-fluoresce indiacetate; DiD, 1.1′-Dioctadecyl-3,3,3′,3′-Tetramethylindodicarbocyanine, 4-Chlorobenzenesulfonate Salt; DiR, 1.1′-dioctadecyl -3,3,3′,3′-tetramethylindotricarbocyanine iodide; DOPC, 1. @story_separate@To date, no pharmacological therapy has been proved to be completely effective in treating ALI. Although many therapies have been proved to be effective in experiments, clinical translation is small. The advent of nanomedicine could open new avenues to address current limitations in the field of traditional pharmacological therapies, but challenges still remain to improve their clinical translatability. The toxicity and safety concerns are great challenges for nanomedicine clinical translation. Recent advances in machine learning and artificial intelligence immensely decoded and empowered the cell-nanomaterial interaction, which gifted the computational tool for the prediction process 125, 126 and in-silico methods 127, 128 to potentially decipher the quantitative nanostructure activity-relationship (Nano-QSAR) for nanotoxicology and nanotherapeutics ( Figure 6 ). Similar to the enhanced permeability and retention (EPR) effect in a solid tumor, the inflammation-specific retention is called ELVIS. 18 The role of EPR in the cancer barrier is somewhat oversold considering that less than 5% of nanomedicine formulations accumulate at the site of tumor; 129 and the heterogeneous outcomes of clinical trials of nanomedicine can be explained by the inter-and intraindividual heterogeneity in EPR-mediated targeting. Biological nanomedicine which employs bacterial, 100, 130, 131 human cells and tissue 59, 132 and DNAs 111 as carriers seems promising ways to improve and individualize nanomedicine treatments. A future direction to improve nanomedicine clinical translatability is about to integrate nanomedicines and/ or nanorobots with biological cells, which do not need sophisticated instruments, space, chemicals, acoustic and magnetic setup to deliver agents inside the body. [130] [131] [132] [133] Bacteria-driven microparticle swimmers possess actuation and sensing capabilities, which make them promising active carriers with high efficiency of tissue cells. 100, 130, 131 Sperm cell-driven microrobots are biocompatible microrobots, which are fast microswimmers in stagnant fluids without the need for toxic media or fuel, might have an impact on the development of assisted reproductive technologies. 132 These emerging strategies are a promising way to realize personalized pharmacological therapy. The increasing use of nanodiagnostics and nanomedicine for personalized and targeting therapy raises potential social and ethical conundrums. Nanomedicine commercialization requires a large investment, 134 and the cost-effective benefit is an inevitable issue. 135 Extreme profitability concept leads to concerns that global equality in access to health care might be even further compromised. Continuous efforts to cultivate cost-effective nanomedicine with more security are mandatory to make better use of nanotechnologies for global welfare.","Acute lung injury (ALI) or its aggravated stage acute respiratory distress syndrome (ARDS) may lead to a life-threatening form of respiratory failure, resulting in high mortality of up to 30–40% in most studies. Although there have been decades of research since ALI was first described in 1967, the clinical therapeutic alternatives for ALI are still in a state of limited availability. Supportive treatment and mechanical ventilation still have priority. Despite some preclinical studies demonstrating the benefit of pharmacological interventions, none of these has been proved completely effective to date. Recent advances in nanotechnology may shed new light on the pharmacotherapy of ALI. Nanomedicine possesses targeting and synergistic therapeutic capability, thus boosting pharmaceutical efficacy and mitigating the side effects. Currently, a variety of nanomedicine with diverse frameworks and functional groups have been elaborately developed, in accordance with their lung targeting ability and the pathophysiology of ALI. The in-depth review of the current literature reveals that liposomes, polymers, inorganic materials, cell membranes, platelets, and other nanomedicine approaches have conferred attractive therapeutic benefits for ALI treatment. In this review, we explore the recent progress in the study of the nanomedicine-based therapy of ALI, presenting various nanomedical approaches, drug choices, therapeutic strategies, and outcomes, thereby providing insight into the trends."
"Amid the coronavirus disease 2019 (COVID-19) pandemic, vulnerable populations are the most affected medically, resulting in limited access to care and worsened health outcomes, including death [1] . Identified vulnerable populations include children with developmental disabilities, mature adults, pregnant women, sexual and gender minorities, and Black, Indigenous and People of Color communities [2] [3] [4] [5] [6] . Health outcomes differ in vulnerable populations than the general public since there is a disproportionate exposure to risk factors that cause disease [7] . Among these vulnerable groups, patients with pre-existing psychiatric conditions pose an increased risk of severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) infection, which causes the respiratory state known as COVID-19 illness [8] . Psychiatric patients tend to have higher comorbid medical conditions, such as diabetes mellitus and hypertension, which predispose to disease [8, 9] . Isolation and small social networks limit support from friends and families during prolonged periods of quarantine [10, 11] . Psychiatric patients may display inadequate social space and limited adherence to personal protection strategies such as handwashing or the use of a mask on a consistent basis [12] . Unprecedented financial stressors due to loss of jobs, limited access to adequate housing, and a higher risk of homelessness can further affect mental health care access [13] . In an ongoing study by the Center for Disease Control and Prevention in the United States, up to 12% of individuals in the general population responded that they needed counseling and medication for mental health but could not receive it during the pandemic [14] . Delays in treatment affected primarily unemployed individuals due to fears of contamination with COVID-19 when assisting appointments [15] . Most importantly, the COVID-19 pandemic disrupted mental health services globally. The World Health Organization surveyed 130 countries and over 60% reported impacts in mental health care services, including children (72%), older adults (70%), and women requiring prenatal and postnatal services (61%) [16] . Psychiatric patients in vulnerable populations are also at increased risk for mitochondrial stress, which potentially can result in an altered immune response to COVID-19 illness [17] . Mitochondria are particularly susceptible to dysfunction affected by nutrition and viral disease exposure [17] . It is posited that mitochondrial stress can eventually widen the health disparity since many vulnerable populations live in urban environments affected by toxic metal pollution [17] . Overcrowded housing, inadequate nutrition, socioeconomic stressors, age, and metabolic syndrome can further potentiate the effect of mitochondrial stress in vulnerable populations affected by COVID-19 illness [17, 18] . The disparity of vulnerable populations can further be attributed to immune perturbations associated with stress that precede pro-inflammatory responses, especially in Black, Indigenous and People of Color [19] . Before the COVID-19 pandemic, these vulnerable populations lived in a heightened state of physiological stress and awareness complicated by psychological stress due to socioeconomic and environmental stressors [19] . The mechanism in which SARS-CoV-2 extends to the central nervous system is binding to peripheral nerve terminals and through retrograde transport reaching the brain [12, 20, 21] . The SARS-CoV-2 can also impact the central nervous system through a systemic cytokine response resulting in further chronic inflammation and, in some cases, brain damage and even death [20, 21] . Among psychiatric patients, the immune response triggered by COVID-19 may lead to depressed mood, severe anxiety, and suicidal ideations [9] . Apart from COVID-19 causing neurotropism and disarray of reactive immune responses, there is an increased risk for collective trauma in vulnerable populations [12, 22] . COVID-19 is most likely to impact the mental health of psychiatric patients compared to healthy controls with a higher incidence of posttraumatic stress disorder (PTSD) (31.6% vs 13.8% controls) [9] . The collective trauma response is multilayered and consists of three components: Further fears of infection, prevalent economic difficulties, and disruption of established routines with isolation [22, 23] . COVID-19 disproportionately affects vulnerable ethnic populations contributing to an accumulation of trauma through experiences of racism, discrimination, and social inequalities [22, 24] . The COVID-19 pandemic forms a feedback loop between viral illness and disparities, leading to the accumulation of further inequities comprising a collective trauma [22] . Preceding the COVID-19 pandemic, the average untreated global rates of schizophrenia, major depressive disorder, and alcohol use disorder were 32.2%, 56.3%, and 78.1%, respectively [25] . These untreated psychiatric illnesses could potentially overburden an existing overwhelmed health care system, further increasing the inequalities in health between groups of people in countries, thus causing global health disparities [26] . Although there is a growing body of evidence of the mental health impact on the global population during the COVID-19 pandemic [27] , little is known about the effects of COVID-19 in vulnerable psychiatric patients. In this minireview, we highlight the global health disparities affecting vulnerable psychiatric populations (Table 1) and examine the social barriers influencing global mental health care during the COVID-19 pandemic.@story_separate@Search engines through PubMed, Google Scholar, and PsychINFO were used with the following keywords: COVID-19, global health disparities, global mental health disparities, social barriers, social determinants of health, ethnic health disparities, sexual gender minorities, pregnant women, psychiatric patients, children, mature adults, developmental disabilities, urban and rural population. Articles in English and articles translated in English were reviewed from January 2020 to March 2021. Psychiatric patients from ethnic minorities display worsened mental health outcomes during the COVID-19 pandemic. In the United Kingdom, people of color (Bangladeshi, Indian, and Pakistani men) show a more significant increase in mental health distress than White British men [28] . In the United States, ethnic minority respondents who reported prior treatment for depression, anxiety, and PTSD presented with higher prevalence of these symptoms and subsequent adverse mental health outcomes than those who did not receive treatment [29] . It is speculated that Blacks may have fewer physiological and psychiatric complications from COVID-19 based on a reduced molecular expression of angiotensinconverting enzyme-2, the binding site for SARS-CoV-2 present in the lung, kidneys, and brain [30] . However, there is a notable difference in COVID-19 outcomes since studies have shown more complications to the viral illness, including higher psychological distress and increased mortality in Blacks compared to Whites [31, 32] . In the United States, a third of hospitalized COVID-19 patients are Black, although they make up only 13% of the total population [33] . In the United Kingdom, Blacks were found to have a four-fold increased risk for COVID-19 infection [31] . Black individuals diagnosed with COVID-19 (34.6%) showed psychological distress, but only a minimal (8.6%) sought out psychiatric care [31] . Poor access to services, stigma, cultural insensitivity and lack of awareness of their symptoms might prevent Blacks from obtaining mental health care during traumatic events [34] . Structural racism or the ""ways in which societies foster discrimination through mutually reinforcing inequitable systems [35] "" can complicate mental health disparities. A general mistrust of health systems is based on the historical mistreatment of Blacks in psychiatric care [36] . To this extent, the American Psychiatric Association issued a public apology about their role in Blacks' historical injustices acknowledging their Nadler et al [65] , 2021 Children with autism and developmental disorders Case study United States Case study of child with autism and behavioral health concerns examines limited psychosocial support and availability during the COVID-19 pandemic Bishop [69] , 2020 Sexual gender minorities Qualitative study Global Interviews with 59 SGM from 38 countries; majority showed increased isolation and anxiety Suen et al [73] , 2020 Sexual gender minorities Hong Kong (China) SGM are particularly vulnerable to poor mental health outcomes during the COVID-19 pandemic Peterson et al [74] , 2020 Cross sectional, convenience sample survey United States Greater psychological distress in SGM. A large number of bisexual individuals in the sample may have magnified the differences between SGM and sexual-majority groups Durankuş and Aksu [77] , 2020 Pregnant women Cross sectional survey Turkey Higher depression scores on Edinburgh Postpartum Depression Scale in pregnant women than control group during COVID-19 pandemic Liu et al [80] , 2021 Pregnant women Cross sectional survey United States Study on pregnant women and women who recently gave birth. Women with self-reported psychiatric diagnoses were 1.6-to-3.7 more likely to score at clinically significant levels of depression, generalized anxiety, and PTSD Corbett et al [81] , 2020 Pregnant women Preliminary crosssectional survey Ireland Pregnant women surveyed described heightened anxiety over COVID-19 affecting older adults, their children and their unborn baby Saccone et al [82] , 2020 Pregnant women Cross sectional survey Italy More than half of pregnant respondents rated psychological impact of COVID-19 as severe Wang et al [83] , 2020 Female gender Cross sectional survey China Increased psychological impact on female gender and having a poor self-rated health status Liu et al [85] , 2020 Female gender Cross sectional survey China Female respondents had higher negative cognitions on posttraumatic stress symptoms than males Berthelot et al [87] , 2020 Pregnant women Longitudinal cohort survey Canada Pregnant women assessed during the COVID-19 pandemic reported more prenatal stress and psychiatric symptoms than pre-pandemic cohort Lebel et al [88] , 2020 Pregnant women Cross sectional survey Canada Higher symptoms of depression and anxiety in pregnant women were associated with greater concern about COVID-19 threatening the life of the mother and baby Taquet et al [113] , 2021 Psychiatric vs non psychiatric population influence in inequitable practices, including the misdiagnosis of schizophrenia in this population [37] . Indigenous people also face similar mental health disparities. There are presently 45 million indigenous people living in Latin America, 5.2 million in the United States, 2 million in Canada, and 798363 Aboriginal or Torres Strait Islander living in Australia [38] . In a controlled variable study, being Aboriginal or Torres Strait Islander was a strong predictor for higher anxiety, suggesting that indigenous people in Australia might be more vulnerable to poorer mental health outcomes [39] . During the COVID-19 pandemic, many indigenous people tend to handle stress independently of others by relying on spirituality to guide them, leading to a decline in seeking mental health care services [40] . Limited access to indigenous villages due to isolation in remote geographical areas can prevent obtaining mental health care for indigenous people. In Brazil, many indigenous people cannot access health services due to geographical distance and unavailability of care [41] . Restrictive bans placed by governments to prevent the pandemic's spread can further complicate travel by health care workers to these remote places [38] . In specific indigenous communities such as the First Nations, Inuit, and Métis in Canada, there have been lower rates of COVID-19 compared to nonindigenous people due to protective measures taken by their leaders [42] . Despite lower infection and mortality in Canadian indigenous cultures, 38% of indigenous people reported fair to poor mental health on a governmental online survey during the pandemic compared to 16% in prior years [43] . Public health messaging, which is essential during a pandemic, is hindered by poor access to broadband and internet service in indigenous villages. In Mexico, only 10% of indigenous villagers in Oaxaca have adequate internet service, mostly concentrated in city centers [44] . Limited access to internet service hampers the growth of telepsychiatry modalities in these communities, which could be beneficial for patient treatment. Although the scientific literature focuses mainly on Blacks and their experience with discrimination during the COVID-19 pandemic, Asians have been significantly affected, displaying in some cases worsened mental health outcomes [45] . In the United Kingdom, 41.7% of hospitalized Asians with COVID-19 proved to have higher depression screening symptoms on Patient Health Questionnaire 4 than Blacks [31] . In the United States, many Asian Americans are targeted by their ethnicity in response to reports of the emerging virus [45] . Compared to pre-pandemic percentages, a United States cross-sectional study showed that 41% of Asians and Asian Americans reported an increase in anxiety and that 53% reported an increase in depressive symptoms [45] . A higher level of community support mitigates the impact of discrimination on depressive symptoms in Asians and Asian Americans [45] . Among other ethnic minority groups, Asians showed a lower incidence of suicide (6%) in the United States [29] . During the start of the pandemic, Hispanics displayed a higher incidence of suicidal thoughts (18.6%), higher anxiety and depressive symptoms compared to other ethnic groups in the United States [29] . In general, Hispanics have lower utilization of mental health services with a strong Spanish language social preference and higher levels of ethnic identity, which are strong predictors for mood and anxiety disorders [46] . Hispanics also have the lowest health insurance coverage rate and twice the poverty level than Whites in the United States [47] . This ethnic group is overrepresented in critical essential workers, which, during the pandemic, mainly worked in the food industry, laboring in overcrowded meatpacking factories with poor ventilation, thus increasing exposure to COVID-19 illness [29, 48] . Among other ethnic minorities, cultural-religious minorities have shown worsened mental health outcomes during the COVID-19 pandemic. For instance, in India, an increased relapse of pre-existing psychiatric illness was observed during the quarantine period in Kashmir Muslims already affected by trauma and civil unrest in the area [49] . Moreover, members of the religious majority discriminated against the Muslim community in social media posts as being spreaders of viral illness [49] . Children, adolescents, and adults with intellectual disability (ID) and neurodevelo-pmental disorders like autism spectrum disorder (ASD) are vulnerable to significant impacts by pandemic-related changes such as social distancing standards, stay-athome orders, and closures of nonessential services. Furthermore, many individuals with ID and ASD experience significant communication challenges in processing information [50] , afflicting their ability to respond effectively and efficiently to social changes implemented secondary to the COVID-19 pandemic. Given expressive communication challenges, individuals with ID or ASD may have difficulty communicating emotional distress, pain, or illness symptoms [51] . As a result, many individuals with ID and ASD have to rely on their families or caregivers to communicate important information about the pandemic or observe for symptoms of a potential viral illness. Caregiver stress and depression increased during the COVID-19 pandemic compared to the pre-pandemic outbreak furthering the mental health disparity [52] . Furthermore, individuals with ID and neurodevelopmental disorders face various challenges secondary to the pandemic, including limited access to behavioral health services, changes in health services delivery, disruption of educational services, and vocational changes [53] . During the lockdown period in Spain, there were increased psychiatric emergency admissions in individuals with ASD, most likely due to an observed disruption of family daily routines [54] . Individuals with ASD share a physiological and genetic vulnerability for COVID-19. There are documented increased levels of pro-inflammatory cytokines present in individuals with ASD [55] . The tendency towards a pro-inflammatory state among individuals with ASD may place them at higher risk for even more severe symptoms once this virus is contracted [56] , including a rare multisystemic inflammatory syndrome evident in children [57] . Focal brain inflammation in the amygdala with activation of mast cells may potentially result in a higher prevalence of ASD diagnoses due to COVID-19 infection [58] . Furthermore, individuals with ID and ASD have a greater risk for overall poor health compared to the general population [59] , type 2 diabetes [60] , along with sensory impairments and physical disabilities [61] ; all of these have been documented as poor prognostic risk factors from COVID-19 infection [62] . Many individuals with ID and ASD share documented health disparities both in the pre-pandemic and pandemic period. This health disparity might be due in part to the higher rates of ASD among transgender and gender-diverse individuals vs cis-gender individuals [63] . In the pre-pandemic period, transgender and gender-diverse groups with ASD experienced increasing rates of comorbid mental illness and higher rates of being refused health services [64] . In the United States, access to existing therapeutic services, including Applied Behavioral Analysis in rural regions, has been curtailed due to job shortages [65] . In addition to disparities in medical access and services contributing to this disproportion, there is evidence about potential genetic or biological susceptibilities among various ethnic groups [66] . It could be expected that individuals with ID and ASD within these groups carry even higher risk given the vulnerabilities outlined above. Sexual and gender minorities (SGM) include Lesbian, Gay, Bisexual, Transgender, Two-Spirit, Queer, Questioning, Intersex, and Asexual (LGBT2SQIA+) individuals [67] . SGM are a diverse group whose history of stigma, systemic discrimination, and structural violence leaves them particularly vulnerable to disproportionate impact by COVID-19 given inequitable conditions that have reduced their access to vital resources [67] . SGM share a heightened psychological vulnerability associated with minority stress, which increases their susceptibility to underlying health conditions [22, 68] . Historically, SGM have been excluded from disaster research, thereby preventing identification of their unique mental health needs [69, 70] . It is, therefore, no surprise that SGM experience health disparities, including higher rates of mental health diagnoses, substance use, and suicidality compared to their sexual-majority counterparts [71] . Due to discrimination, SGM are overrepresented in essential services and sex work, experience higher poverty rates than the general population, and are less likely to have adequate healthcare and are more vulnerable to poor mental health outcomes [72, 73] . When SGM intersect with oppressed intersections of identity, such as race, ethnicity, socioeconomic status, or chronic illness, the compounding nature of these vulnerabilities creates even wider health disparities for communities of SGM [68, 70, 72] . For instance, a Hong Kong community sample found stressors beyond those of the COVID-19 pandemic, including family conflict due to sexual orientation and April 19, 2021 Volume 11 Issue 4 disconnection to the SGM community, likely leading to a combined effect on the oneweek prevalence of clinical depression (31.5%) and generalized anxiety disorder (27.9%) [73] . Given the social disadvantages and mental and physical health disparities faced by SGM, it is likely that the pandemic trauma and social isolation measures to mitigate COVID-19 transmission are exacerbating these inequalities [68] . Discrimination, oppression, and violence occur at the familial level, making isolation and quarantine dangerous for SGM who are forced to cohabitate with unsupportive and abusive family members or partners [69] . Many SGM rely upon community supports for safety and acceptance, yet due to COVID-related restrictions, SGM have been unable to connect with their supportive community safe spaces [69] . SGM experience more anxiety, depression, and peritraumatic stress responses while doubling the odds of meeting future criteria for COVID-related PTSD when compared to sexual majorities [74] . Furthermore, given the higher rates of human immunodeficiency virus (HIV), mental health diagnoses, substance use, and trauma among SGM, disruptions in mental healthcare access during COVID-19 have been particularly concerning [69, 73, 75] . SGM reported having difficulty obtaining HIV medications and mental health care in case studies in 59 countries [69] . Further validating this report, a convenience sample (n = 2732) of cisgender gay men and other men who have sex with men across 103 countries displayed moderate to severe psychological distress with a prevalence of 31% in depression [75] . Globally, SGM have reported that they would delay or avoid seeking care for COVID-19 symptoms due to anticipated stigma and discrimination [69] . Pregnant women are a vulnerable group for COVID-19 due to an increased risk of physical and mental health complications, such as depression and anxiety [76, 77] . Homeschooling children and taking care of elderly family members in addition to working from home has been a challenge for most women, which generally increases caregiver stress [78] . COVID-19-related health anxiety experiences may also increase the likelihood of mental health symptoms among those pregnant women without a previous psychiatric diagnosis [79] . Pregnant women in Ireland reported excessive worries regarding older relatives and their unborn baby [80] , while Italian pregnant women reported high rates of anxiety regarding vertical transmission [81] . Studies from China, when COVID-19 emerged, have found that the female gender is significantly associated with higher self-reported anxiety, depression, and posttraumatic stress, causing a severe overall psychological impact [82] [83] [84] [85] . Similarly, studies from Canada reported a higher rate of depressive and anxiety disorders, and substance use disorder [86, 87] . The outcomes of psychiatric complications during the COVID-19 pandemic are complex for pregnant women since mental health problems are associated with adverse outcomes, including suicidal ideations and an unstable mother-infant bond [88] . Perinatal depression is also related to fetal complications, including fetal growth restriction and increased odds of premature delivery [89] . Maternal complications of depression are also associated with preeclampsia and gestational diabetes [77] . For many pregnant psychiatric patients, telehealth proves to be the most convenient way to obtain care. Due to governmental restrictions to halt the spread of COVID-19, many pregnant women cannot attend appointments in person. For socioeconomically disadvantaged women, telehealth can further the disparity of favoring those populations with resources to access technology vs those without, thus causing a gap in digital literacy [90] . This socioeconomic and educational disparity becomes apparent in developing countries where the prevalence of depression can reach around 15.6% during pregnancy and 19.8% after childbirth [91] . In Turkey, a recent online survey revealed that years of education and knowledge of COVID-19 protected against worsening of depression in pregnant women [77] . Another challenge complicating the mental health of pregnant women is dealing with the surge of domestic violence. For some pregnant women, more time at home may mean more time spent with an abusive partner [6] . According to the United Nations, domestic violence rates against women are increasing worldwide during the pandemic [92] . This increased rate is indicated by the number of calls received by emergency support lines in China, Italy, France, Brazil, and Spain during their lockdowns [85] . Psychiatric patients who are mature adults are particularly vulnerable to COVID-19 because of a susceptibility to disease precipitated by a normal age-related decline in the immune system and comorbid conditions [8, 93] . Comorbid conditions such as associated polypharmacy, hypertension, diabetes mellitus, chronic renal failure, and chronic obstructive pulmonary disease can contribute to an increased vulnerability to illness with poor health outcomes in this population [8, 93, 94] . Early identification of COVID-19 is key to mitigating illness in mature adults but can be challenging due to physical health factors. Unique challenges are encountered specifically in mature adults with major neurocognitive disorder due to an atypical presentation of symptoms, such as low fevers, increased confusion, increased agitation, and sudden mood changes [95] . Upon the onset of the COVID-19 outbreak in China, the first psychiatric patient identified with the viral illness was a man hospitalized with a major neurocognitive disorder who supposedly ate outside food brought by family members and unknowingly infected others [8] . Masking protocols and appropriate handwashing can prove challenging to implement in the cognitively impaired within hospitals and long-term care facilities [8, 96] . Mature adults with severe neurocognitive deficits may wander off, causing problems in implementing social distancing [96] . Another health inequity posited to affect mature adults more than other populations is self-isolation. Most of the daily social activities and contacts of many mature adults occur outside their homes [97] . In Canada, as the pandemic spread, leading to the cancellation of multiple community-based day programs for seniors [98] . During the early stages of the pandemic in China, self-isolation became a struggle, especially with mature adults without children or support, increasing the propensity to depressive symptoms [99] . Loneliness may also result from self-isolation. In Turkey, the pandemic has led to increased demand for home health aides to assist and accompany the elderly [94] . In the Caribbean, including Cuba, Puerto Rico, and the Dominican Republic, many mature adults with neurocognitive disorders live with extended family who can be of comfort and assistance [100] . Although social and physical distancing proves to be a difficult mitigating strategy, finding ways to connect and belong can help overcome the perceived loneliness of isolation. Many mature patients also struggle with technology due to being unfamiliar with electronic devices, have difficulty navigating the internet or using platforms like those intended for telehealth appointments. There is a higher rate of illiteracy among the geriatric psychiatric population in the Middle East and North African countries [5] . Cognitive decline and lower education can add to the slow adaptation of technology and internet use in mature adults [94, 101] . It is important to note that those mature adults who have higher education and seek social connectedness tend to adapt more rapidly to technology to overcome isolation [101] . For some mature adults who are marginalized, frequent phone calls and telephone calls from peer supports can help mitigate selfisolation [97] . Around 22% of Africa's population could become infected with COVID-19, a continent with a propensity to collective trauma due to the after-effects of the Ebola epidemic [101] [102] [103] . From 2014 to 2016, the Ebola virus outbreak caused psychosocial stressors such as discrimination, stigma, anxiety, and depression in Sub-Saharan communities [102] . In a cross-sectional study in Sierra Leone, knowing someone quarantined increased anxiety and depression, while PTSD was evident in 76% of Ebola virus survivors [104] . In many villages, healers and cultural treatment traditions were invalidated in preference for Westernized methods and treatment [105] . The government banned gatherings, and fear about the virus increased, causing heightened distress [105] . Due to overwhelming fears, many of the usual prevention methods of infectious spread were not practical during the Ebola epidemic [102, 105] . Prevention methods to limit the spread of the Ebola virus were found to be poorly effective in those individuals with depression and trauma symptoms due to poor insight and impaired cognition [106] . Although it is unknown if the same prevention pattern with COVID-19 will result in individuals with depression, it is anticipated that such a similar pattern could occur since it has already been observed with other infectious diseases such as in the spread of HIV [106] . The inequity between urban and rural environments in mental health care becomes April 19, 2021 Volume 11 Issue 4 evident in the villages' communal life in Africa. In these communities, there is a reliance on family members to provide for each other economically and to provide support to relatives with psychiatric symptoms [102] . The pandemic has led to income losses as many villagers can no longer sell farming goods or artisan items in urban centers [107] . Food security is an issue for many Africans during the pandemic due to economic availability [107] . Due to the governmental lockdown restrictions, the rural villagers can be further stressed, potentially increasing depression and anxiety [102] . When Africans display psychiatric symptoms, communal methods are usually exhausted first before sharing their symptoms directly to health care providers [102, 105] . Plans to provide trauma-informed care by community health workers are primordial for the survivors of COVID-19 to prevent further re-traumatization due to discrimination, which was an issue with the Ebola virus [103] . Also, the ease of telehealth services is generally limited in rural environments compared to urban centers due to the availability of internet and poor signal reception [102] . The urban environment mental health disparity, which becomes apparent in densely populated cities, might be related to mitochondrial stress. Toxic metal pollutants in high dense areas can cause mitochondrial stress, increasing the vulnerability in psychiatric patients who already have mitochondrial dysfunction due to psychiatric illness, namely schizophrenia, bipolar disorder, and major depressive disorder, and who become infected and develop COVID-19 illness [17, 108] . In Kuwait, a multi-ethnic country, South East Asians mostly live in highly populated dense areas where overcrowding and pollutants lead to higher exposure to COVID-19 illness [2] . This higher exposure can increase transmission of COVID-19, possibly leading to worsened mental health outcomes due to the potentiating effects of mitochondrial stress. A similar pattern occurs in Israel in which the Orthodox Jewish community populated in high dense cities contributes to an increased rate of transmission through religious rituals and gatherings [109] . The minireview is limited mainly to observational studies due to the emergency state of the COVID-19 pandemic (Table 1) . Articles in English and articles translated in English were reviewed, restricting the scope of the search. A lack of substantiated research in global health disparities, especially in ethnic minority groups, was appreciated in countries like Japan and South Korea. Although significant, other disparities involving prisoners, homeless persons, and persons with low educational attainment with psychiatric diagnoses during the COVID-19 pandemic were not addressed due to the narrow body of research. It is essential to consider the short span since the pandemic's outbreak since the sequelae of COVID-19 are currently being studied in psychiatric and non-psychiatric patients [110, 111] . The sequelae of COVID-19 may affect individuals with and without a history of psychiatric illness: A recent retrospective study showed that COVID-19 is associated with a higher incidence of a first psychiatric diagnosis following 14 to 90 days from exposure in non-psychiatric patients [110] . It is imperative to examine the impact of the COVID-19 pandemic on vulnerable populations without psychiatric illness. There is also a need to evaluate the impact on health care workers who face daily traumatic psychological stressors due to their job occupation and high exposure. Health care workers are confronted by an increased rate of mortality and increased health care demands [112] .@story_separate@The narrative minireview yielded evidence for worsened global mental health outcomes among vulnerable psychiatric patients during the COVID-19 pandemic. Psychiatric patients among vulnerable populations appear to be at higher risk for depression, anxiety, and posttraumatic stress symptoms. Among these vulnerable groups, collective trauma impacts marginalized individuals and communities. Expanding access to internet services and technical assistance in underserved areas can provide more effective delivery of psychiatric services through telehealth to vulnerable populations. Peer community supports for psychiatric patients can reduce social isolation. Programs promoting internet literacy for mature adults can support connectedness with family and relatives during prolonged periods of isolation. Providing programs that include psychiatric services with prenatal care can","The coronavirus disease 2019 pandemic affects psychiatric patients disproportionately compared to the general population. In this narrative review, we examine the impact of the pandemic on significant global health disparities affecting vulnerable populations of psychiatric patients: People of diverse ethnic background and color, children with disabilities, sexual and gender minorities, pregnant women, mature adults, and those patients living in urban and rural communities. The identified disparities cause worsened mental health outcomes placing psychiatric patients at higher risk for depression, anxiety and posttraumatic stress disorder symptoms. Those psychiatric patients who are ethnic minorities display barriers to care, including collective trauma and structural racism. Sexual and gender minorities with mental illness face discrimination and limited access to treatment. Pregnant women with psychiatric diagnoses show higher exposure to domestic violence. Children with disabilities face a higher risk of worsening behavior. Mature adults with psychiatric problems show depression due to social isolation. Psychiatric patients who live in urban communities face pollutants and overcrowding compared to those living in rural communities, which face limited access to telehealth services. We suggest that social programs that decrease discrimination, enhance communal resilience, and help overcome systemic barriers of care should be developed to decrease global health disparities in vulnerable population."
"During coronavirus disease 2019 (COVID- 19) , telemedicine (real-time videoconferencing between clinicians and patients) has skyrocketed across the United States as a method of providing medical care while limiting virus exposure. 1 Even before COVID-19, telemedicine was an attractive technology in the palliative care (PC) community. For seriously ill patients with functional, time, and/or financial limitations and those who lack local PC services, telemedicine may increase PC access and reduce the burden of traveling to PC appointments. 2, 3 A growing body of literature suggests PC video consultation is associated with high patient satisfaction, improved symptom burden, and in some cases, lower health care utilization. 4, 5 However, even as telemedicine has become increasingly important in the provision of health care, our understanding of its ideal use and limitations is incomplete. 2 In 2014, our quaternary urban academic medical center established a ''Telehealth Resource Center'' and began providing work relative value units (wRVU) (clinician productivity credits) for all telemedicine encounters regardless of insurance reimbursement to support telemedicine expansion. Our outpatient PC program began offering telemedicine visits in 2016. In 2019, we completed 4840 outpatient PC visits, half of which occurred by telemedicine. Informal feedback on telemedicine in our practice has been overwhelmingly positive. To improve the quality of our telemedicine offerings, we aimed to survey patients and caregivers to characterize their experience with PC telemedicine visits, understand their preferences regarding the timing of telemedicine visits, and their preferred settings to discuss sensitive topics (e.g., discussing difficult news, prognosis, and what to expect near the end of life). At the time of our survey, there were very limited data on these issues. 8, 9 Since our survey's data collection period, understanding these preferences has become even more important because of the rapid growth of telemedicine during the COVID-19 pandemic. Currently, both PC and non-PC clinicians alike are meeting patients for the first time by video and are needing to have sensitive conversations by telemedicine. We hypothesized satisfaction with telemedicine visits as well as preferences regarding the timing and content of telemedicine visits would vary by age-with older patients responding less favorably to telemedicine-and by distance from our medical center-with those living farther away more strongly preferring telemedicine. Differences in telemedicine acceptability and satisfaction by distance from the medical center has been shown in several studies. 10, 11 Methods Subjects Patients were eligible for the survey if they spoke English and completed at least one visit by telemedicine with one of the two outpatient PC practices (cancer and noncancer). For our cancer PC clinic, we included patients seen from June 15 to 30, 2019 (Refs. 12, 13 ). To ensure a similar number of surveyed patients from our smaller noncancer clinic, we included patients seen by telemedicine from January to June 2019. We set a goal of surveying 35 patients as we felt this number would be sufficient to identify clinically significant trends and would be a feasible number of interviews for our summer research assistant. Potential caregiver participants were recommended by patients who had completed the survey and by the treating PC team. A trained research assistant used a script to recruit patients and conduct the phone survey. Participation was voluntary and without compensation.@story_separate@We developed a multiple-item telephone survey synthesizing validated telehealth satisfaction surveys 14-16 with the National Quality Forum's Palliative Care and End-of-Life Domains. 17 The survey included questions requiring either quantitative or qualitative responses and was peer-reviewed by our investigator team. Preliminary versions of the survey were administered to three volunteer patients and two volunteer caregivers to assess survey feasibility and obtain feedback that was incorporated into the survey's final version. The final survey contained 22 items for patients and 23 items for caregivers (see Supplementary Appendix SA1 for survey instrument). The UCSF Institutional Review Board approved the project (No. 19-28351); all survey participants provided verbal consent. Demographic data on patient age, gender, driving distance from their residence to our academic medical center, primary diagnosis, and number of in-person and telemedicine visits with PC were obtained through review of the electronic health record. Caregiver age and gender were self-reported during the survey. Descriptive statistics, including frequencies, means, and standard deviations, were used to examine the distribution of measures. Chi-square analysis was undertaken to examine bivariate associations between categorical variables and analysis of variance was undertaken to examine associations between categorical and continuous variables. Subgroup analyses by age ( ‡65 vs. <65 years, driving distance ‡80 km vs. <50 km, cancer vs. noncancer diagnosis, and whether a patient had their first PC visit in person) were conducted with patient data only. The Statistical Package for the Social Sciences version 26 for Mac was used to conduct all analyses. For the three open-ended questions, thematic analysis based on the framework by Boyatzis 18 was performed to identify themes and subthemes. After an initial meeting to discuss organization of the thematic analysis, two investigators from our team (W.P.S. and B.C.) individually coded all of the comments for each free-response question, creating their own themes and subthemes. The analysis was considered complete when there was redundancy and saturation of theme identification. The investigators met to discuss the themes and subthemes they each identified and reach agreement on these categories; a third investigator (K.E.B.) mediated any differences of opinion. The investigators categorized the data based on the agreed upon themes and subthemes and met a final time to obtain inter-rater agreement. Frequencies of themes and subthemes were calculated. A total of 126 patients (50 patients with cancer and 76 patients without cancer) were initially identified using the data ranges described. Thirty-four patients were excluded by their treating PC team because they were emotionally distressed, dying, or deemed to have communication difficulties. Our telephone-based survey was conducted from July to August 2019. Sixty-five patients were contacted, 25 patients were not reached after three attempts, 5 declined participation, and 35 were surveyed. Survey recruitment stopped after our target of 35 patients was reached (therefore, 27 eligible patients were never contacted). Twenty-two caregivers were contacted, 5 could not be reached after three attempts, 2 declined participation, and 15 were surveyed. The overall response rate among patients and caregivers was 57.5%. There were no significant differences between survey participants and nonparticipants by age ( p = 0.09), gender ( p = 0.99), patients' primary diagnosis ( p = 0.06), or race ( p = 0.21), or ethnicity ( p = 0.23). There were no significant differences between patients who participated and those who did not in terms of number of office visits ( p = 0.11) or the number of video visits they received ( p = 0.9). Patient mean age was 61 years, half had cancer (49%), 43% were women, and most were Caucasian (86%) ( Table 1) . Forty-nine percent of patients had an in-person visit before a telemedicine visit. No patients had an in-person visit after a telemedicine visit. At the time they completed the survey, patients had completed an average of 4.3 telemedicine visits (range 1-21). Caregiver mean age was 62 years, 80% were women, one-quarter (27%) were caring for a patient with cancer. We compared demographic data for all PC patients who were seen at least once by telemedicine (n = 488) with PC patients who only received in-person visits (n = 560) between January and June 2019. Patients seen by telemedicine were younger (mean age 61.1 vs. 64.6 years, p < 0.0001) and more likely to be white (70.9% vs. 63%, p = 0.05) and English speaking (95.1% vs. 89.5%, p = 0.001) than patients only seen in person. There were no statistically significant differences by gender ( p = 0.33) or ethnicity ( p = 0.94). Mean telemedicine satisfaction score on a 10-point scale (0 = ''not satisfied at all'' to 10 = ''completely satisfied'') was 8.9 for patients (95% confidence interval [CI]: 8.5-9.3) and 8.8 for caregivers (95% CI: 8.0-9.6). There were no differences in telemedicine visit satisfaction between patients and caregivers ( p = 0.88) or by patients' age ( p = 0.07), distance from the medical center ( p = 0.84), cancer diagnosis ( p = 0.69), or whether the first visit was in person ( p = 0.20). Nearly all participants reported that they would have another PC telemedicine visit if it was offered with no difference between patients (97%, n = 34) and caregivers (100%, n = 15; p = 0.51) ( Table 2 ). There were no significant differences between patients and caregivers in recommending having a PC visit by telemedicine to others (86%, n = 30 vs. 93%, n = 14; p = 0.45). All patients and 93% of caregivers reported that it is easy to communicate with the PC team over telemedicine and 86% (n = 30) of patients, and 93% (n = 14) of caregivers reported the telemedicine technology was easy to use. There were no significant differences by patient subgroups. Telemedicine visit timing A greater percentage of caregivers (60%, n = 9 of 15) than patients (25%, n = 9 of 35) agreed the first PC appointment should be in person in clinic ( p = 0.02). Forty-four percent of patients (n = 7 of 16) living closer than 50 miles from the medical center agreed an initial in-person visit was important versus only 11% of patients living farther than 50 miles (n = 2 of 19; p = 0.03). Furthermore, patients who had their first PC visit in person (n = 10, 59%) were more likely to feel an initial in-person visit was important than patients who were seen exclusively by telemedicine (n = 7, 41%, p = 0.04). Most patients (71%) and caregivers (67%) reported feeling comfortable having all of their PC appointments by telemedicine ( p = 0.51). Ninety-seven percent of patients and 100% of caregivers felt comfortable discussing sensitive topics by telemedicine ( p = 0.51). These results were consistent across patient subgroups. Participants felt telemedicine was an acceptable, and often preferable, format to discuss most sensitive topics (Fig. 1) . For patients, the only topic for which more than one-quarter of patients said they would want an in-person visit was receiving bad news (34%); for caregivers, the topics were receiving bad news (53%), advance care planning (27%), and what to expect in the future (27%). There were 116 comments to the open-ended question asking patients and caregivers what they liked about receiving PC by telemedicine ( Table 3 ). The most common themes were convenience (53 comments, 46%), that patients and caregivers felt telemedicine visits were equivalent or better than other forms of communication, including telephone and in-person visits (32 comments, 28%), and enhanced access to care (21 comments, 18%). Regarding convenience, respondents appreciated the time-savings of telemedicine visits (33 comments, 62%) and comfort of doing a visit in one's own home (11 comments, 21%) (with several mentions of being able to attend the visit in pajamas). Describing how telemedicine visits felt equivalent or better to other types of settings, one caregiver said, ''Unless there is a compelling reason to be there in person such as physical examinations, or laboratories, a video visit is a preferred way for us to do the interaction, especially when patients are dealing with mobility.'' There were 76 comments about perceived downsides of receiving PC by telemedicine (Table 3) . A key theme that emerged was technology (26 comments, 34%). Another theme that emerged was concern about relationship and rapport building by telemedicine (21 comments, 28%). Three respondents noted limited opportunities to see body language by video. There were 60 comments by patients and caregivers on areas for improvement with PC telemedicine visits (Table 4 ). Notably 19 patients and caregivers reported they could not think of a way to improve PC telemedicine visits. Half of the comments focused on technological improvements with participants desiring more help setting up the technology (30 comments, 50%), an improved telemedicine platform quality (11 comments, 37%), and real-time technology support (4 comments, 13%). The experience of surveyed patients with serious illness and family caregivers seen by PC telemedicine at our urban academic medical center was overwhelmingly positive. Patient and caregiver satisfaction was high. The majority of patients and caregivers would do another telemedicine visit if offered and would recommend receiving PC by telemedicine to others. Consistent with past research, convenience 5, 9, 19 (both time and costsavings) and enhanced access to the medical team 10,11 (especially for those with functional limitations or who lacked specialty PC near their home) were reported as key advantages of telemedicine in our survey. Participants also frequently noted telemedicine visits felt equivalent or better than in-person visits. Survey respondents offered a range of perspectives on the effectiveness of rapport building by telemedicine. One-third of the participants' comments on telemedicine limitations acknowledged concerns about rapport building by video. One patient remarked telemedicine visits are, ''still a cold experience. It is not a warm handshake. It is not human to human contact.'' Another patient offered an alternative perspective, ''It's almost like being with the person since you can see the person on the other side and we can talk freely.'' To date, the impact of telemedicine on patient-familyclinician rapport has not been adequately characterized in the literature. 19 Although some clinicians assert an initial in-person visit is necessary to establish rapport, 8 only one-quarter of surveyed patients felt this was important. Patients living closer to the medical center or who had their first PC visit in person were significantly more likely to feel an initial in-person visit was important. Notably, the telemedicine satisfaction scores for patients seen exclusively by telemedicine were as high as those seen by the PC team in person initially. Caregivers were more likely than patients to feel an in-person visit was important. Caregivers may feel a full assessment in person is needed to provide comprehensive care or patients who are symptomatic find the burden of travelling to clinic outweigh any potential advantages of in-person visits. As our health care system increasingly relies on telemedicine during and after COVID-19, research to understand the impact of telemedicine and the timing of telemedicine visits on patient-clinician rapport and key health care outcomes is needed. In the meantime, universal clinician training on telemedicine best practices convenience (5, 9) ''This is very convenient and helpful.'' (Caregiver) Cost-savings (4, 8) ''I was astonished to find out that [the video visit] was at no cost to us. It saved significant expense because it saved a drive, food, and a hotel for 2 nights.'' (Caregiver) Comparisons with other forms of communication (32, 28) Comparisons with in-person visits (15, 47) ''Unless there is a compelling reason to be there in person such as physical exams, or labs, a video visit is a preferred way for have the interaction, especially when patients are dealing with mobility.'' (Caregiver) Unique features of video visits (12, 38) ''Gives me a lot of time to think about how I'm really feeling. I'm more reflective at home. It's easy to go grab my meds at home.'' (Patient) Comparisons with telephone encounters (5, 16) ''It feels a lot more intimate than a phone call. Seeing people face to face enhances the visit. It's like you are in the room with them.'' (Patient) Enhanced access (21, 18) For symptomatic or disabled patients (10, 48) ''My husband is in a wheelchair and on a ventilator, video appointments avoid having to transport him for a visit.'' (Caregiver) Greater frequency of communication with medical team (8, 38) ''It's easier to stay up-to-date.'' (Patient) To specialty PC services (3, 14) ''I've spent a lot of time trying to track down any local palliative care services and we've found nothing [near home].'' (Caregiver) Technology (10, 9) Ease of use (6, 60) ''It is easy to use.'' (Patient) Video platform quality (4, 40) ''You can say and hear everything you need to.'' (Patient) Technology concerns (26, 34) Video visit platform (18, 69) ''The technology didn't work and it wasted a lot of time.we had to move to a phone-call.'' (Patient) Tech-literacy (5, 19) ''Some people might be intimidated about doing it over video.'' (Patient) Video visit tech setup (3, 12) ''It requires that you have things at home to have a high quality video conference. Need good wifi, a good camera, a good screen.'' (Caregiver) Relationships and rapport (21, 28) Rapport building (12, 57) ''It's not as intimate as far as communicating 1:1. I don't feel like I get to know the doctor as well compared to in-person appointments.'' (Patient) Nonverbal communication (4, 19) ''You can see faces but sometimes you miss the body language.'' (Patient) Value in initial in-person visit (3, 14) ''Technology creates a little distance, but it's not a big deal if you've met the person [in-person first].'' (Caregiver) Discussing sensitive topics (2, 10) ''With sensitive topics you feel a little less empathy.'' (Caregiver) Limitations in scope of services (15, 20) N/A ''Losing the proximity with other human beings is a real disadvantage. such as ensuring good lighting and orienting the patient to the clinician's location 2 support rapport building by video. Ensuring telemedicine technology works smoothly (a limitation and area for improvement often cited by our survey participants) can also promote rapport building by avoiding technology-related frustration and supporting the clinician in effectively identifying patient and caregiver verbal and nonverbal cues. The most striking finding from our survey was how comfortable patients and caregivers felt discussing sensitive or emotional topics by video (97% of patients and 100% of caregivers reported feeling comfortable). With few exceptions, survey participants did not prefer inperson visits for the discussion of nine sensitive topics. The only topic for which more than one-quarter of patients wanted an in-person visit was receiving bad news (34%). For caregivers, the topics that more than one-quarter preferred discussing in person were bad news (53%), advance care planning (27%), and what to expect in the future (27%). Further study on this topic is warranted. It is unclear whether patients and caregivers are truly as comfortable discussing sensitive topics through telemedicine, or whether the convenience and timeliness of telemedicine outweigh comfort gained from an in-person visit. In a study of telemedicine at an academic medical center, some primary care patients reported a preference to receive difficult news by video because they felt they would receive the news earlier than coming to clinic and/or their home provided more comfort, social support, or privacy. 9 Our medical center's experience and anecdotal reports from PC and non-PC teams across the country suggest more difficult conversations are being held by telemedicine during COVID-19. 6, 20 Additional data to understand the quality of sensitive conversations held by telemedicine and how they may differ from in-person conversations are needed to guide telemedicine best practices and clinician trainings. 21 Future research should also investigate whether patient outcomes such as pain management and completion of advance care planning documents are different for patients cared for by telemedicine versus in person. As we await these data, general and COVID-specific communication frameworks offered by VitalTalk 22 and Ariadne Lab's Serious Illness Conversation Project 23 can support clinicians as they have serious illness conversations in person and by telemedicine. Our survey is the first to provide a detailed report on patient and caregiver perspectives on the timing of tele-medicine visits and preferences for communication about sensitive topics by video. We present quantitative and qualitative perspectives to offer a fuller picture of their experience. Our participants represented a broad range of ages and diagnoses. Our findings are tempered by the following limitations. We had a small convenience sample size-we did not survey all individuals who completed a telemedicine visit during our study's time period. Our participants were predominantly white/Caucasian and Englishspeaking, limiting generalizability to other populations. Until very recently, we were unable to include interpreters in our telemedicine visits and this in part explains our survey population's homogeneity. Our survey occurred before COVID-19. The demographics and perspectives of patients completing telemedicine visits before versus during the pandemic may differ. We did not survey a sizable number of patients who may have completed telemedicine visits but for whom their treating clinician asked us not to survey because they were emotionally distressed or dying, which may have led to selection bias. Patients were not randomized to receive PC by telemedicine, which may make our results look more positive than with a less eager or technologically savvy group of patients and caregivers. To this point, it is notable that our patients who completed at least one telemedicine visit were more likely to be white, younger, and speak English than our patients seen exclusively in person. Deepening our understanding of the perspectives of patients from diverse groups on telemedicine and developing creative strategies to address barriers that exist is critical to prevent disparities in access to care. No funding was received. No competing financial interests exist. Supplementary Appendix SA1@story_separate@Telemedicine visits were highly rated by our outpatient PC patients and caregivers. Participants appreciated the convenience telemedicine visits offer and frequently cited telemedicine visits felt equivalent to other forms of communication, including in-person visits. Patients and caregivers felt comfortable discussing a wide range of sensitive topics by telemedicine. Attention to telemedicine platform quality and technology setup is needed to optimize clinician-patient communication and rapport building. To ensure patients and families receive the best care possible, research within a large population of patients from a diversity of backgrounds is needed to understand the impact of telemedicine on rapport, patient care, and health care outcomes. This information can guide telemedicine quality improvement efforts, the development of best practices, and clinician trainings.","Background: Telemedicine visits reduce the physical and financial burdens associated with in-person appointments, especially for patients with serious illness. Little is known about patient and caregiver preferences regarding telemedicine visit timing and the discussion of sensitive topics by telemedicine. Objective: To characterize the experience of patients with serious illness and their caregivers receiving palliative care (PC) by telemedicine. Design: Mixed-methods telephone survey. Setting/Subjects: Patients and family caregivers who had at least one telemedicine visit with the outpatient PC team at our urban academic medical center. Results: A total of 35 patients and 15 caregivers were surveyed. Patient mean age was 61 years, 49% had cancer, and 86% were Caucasian. Caregiver mean age was 62 years. Mean satisfaction with PC telemedicine visits was 8.9 out of 10 for patients; 8.8 for caregivers. Patients (97%) and caregivers (100%) felt comfortable discussing sensitive topics over video. Participants felt telemedicine was an acceptable format to discuss most sensitive topics but 53% of caregivers preferred to receive bad news in person. Participants valued the convenience of telemedicine; they had concerns about rapport building and desired a more user-friendly telemedicine platform. Conclusions: Patients with serious illness and their caregivers rated telemedicine visits highly and felt comfortable discussing sensitive topics by video. Concerns included rapport building and telemedicine platform setup and quality. The rapid growth of telemedicine during coronavirus disease 2019 creates an imperative for research to understand the impact on the quality of care and mitigate any negative effects of telemedicine within a diverse population of patients."
"Although we have a draft sequence of the human genome, little is known about how the chromatin fiber is packed in three-dimensional (3D) space, or how packing affects fiinction (Jackson 2003). We know packing plays a major role; the rate of transcription of a typical gene can vary over eight orders of magnitude (Ivarie et al. 1983), but deleting local elements like promoters and enhancers reduces expression by less than 5000-fold in transient transfection assays where the 3D ""context"" is missing. Common sense suggests the fiber cannot be packed randomly, but elucidating what any underlying order might be has proved difficult. First, the foldings of the chromatin fiber have dimensions below the resolution (-200 nm) of the light microscope (LM) and so can only be seen by electron microscopy (EM), but then the fixation required can distort structure. Second, DNA is so long and packed so tightly it breaks and/or aggregates easily on isolation. Third, chromatin is poised in a metastable state so small charge alterations trigger changes in structure and fiinction, and replacing the natural environment with our buffers often promotes aggregation. Not surprisingly, biochemists minimize aggregation through the use of hypo-and hypertonic buffers, but then different isolates made in different buffers have quite different structures; for example, ""matrices"" and ""scaffolds"" contain different sets of DNA sequences associated with dif-Macromolecular structures are generated in two fundamentally different ways (Misteli 2001a ). Many virus particles ""self-assemble"" to a fixed plan to attain a true thermodynamic equilibrium; the particles are stable, and survive in the absence of a pool of unincorporated subunits when released from the host. Our houses are similar structures, although we direct their construction; when the builder has finished he removes any unused bricks but the house usually remains standing. But most cellular structures are built differently. Thus, the cytoskeleton lacks a rigid architecture. It is ""self-organizing,"" intrinsically unstable, and persists only by exchanging subunits with others in its surroundings; if those subunits are removed, it collapses and disappears. The structures discussed here fall into the latter category. They are ever-changing, with their shape at any particular moment depending on past and present environments. We cannot make pre-else predictions about the position of any particular molecule or gene within the structure; however, we should eventually be able to predict the probabilities that they will be found in one particular place rather than another. Evidence for the dynamic nature of nuclear structures is reviewed in a number of chapters in this book, and photobleaching studies of the critical components that concern us here-^transcription factors and polymerases tagged with green fluorescent protein (GFP)-reveal that they exchange continually with the soluble pool (e.g., Stenoien et al. 2001; Becker et al. 2002; Chen et al. 2002; Dundr et al. 2002; Kimura et al. 2002 ).@story_separate@The idea that the chromatin fiber is looped is one of the oldest in cell biology. Images of the lampbrush chromosomes that can be isolated from oocytes are often cited as providing the best evidence for looping. During the first meiotic division, duplicated homologues pair, and loops can be seen extending microns away from axial chromomeres. Unusually, these chromosomes are transcribed, and nascent RNA is attached to both loops and chromomeres (Snow and Callan 1969) . Note, however, that these loops only become visible on dispersing chromatin in hypotonic buffers, and none are seen in sections of whole oocytes where chromatin appears as a granular aggregate. Therefore, some transcription units may be stripped off the granules during dispersal, and possible intermediates in such a processsmall granules-can be seen scattered around loops (Mott and Callan 1975; Cook 2001) . Supercoiling provides additional evidence. Supercoils cannot be maintained in linear eukaryotic DNA without looping. However, lysing cells in >1 M NaCl releases ""nucleoids"" containing superhelical DNA (Cook and Brazell 1975; Benyajati and Worcel 1976) , and nascent transcripts are associated with attachment points, but not loops (Jackson et al. 1984a) . Nucleoids made from all active cells examined (e.g., fibroblasts, erythroblasts, epithelial cells of men, chickens, frogs, insects) contain such supercoils. In contrast, inactive chicken erythrocytes (and human sperm) yield relaxed DNA, with the supercoiling (and so looping) being lost progressively as they develop (Jackson et al. 1984a) . But this evidence is also compromised by the unphysiological conditions used. Experiments involving nuclease digestion in isotonic buffers are also consistent with looping. Cutting an unlooped fiber should release long fragments that are then shortened, but the expected long fragments are not seen; rather, kinetics fit the release of short fragments from loops (Jackson et al. 1990 (Jackson et al. , 1996 Jackson and Cook 1993) . Other in vitro evidence is also supportive; EM reveals that pure repressors like Gal, AraC, and X bind to distant sites on one molecule of DNA to loop it (reviewed by Ptashne 1986), and an enhancer can only influence a promoter on another plasmid if the two make molecular contact, which implies that when they are on the same chromosome they must also do so (Mueller-Storm et al. 1989). Contact between an enhancer on one chromosome and a target promoter on another also underlies the phenomenon of transvection seen in Drosophila larvae (Wu 1993; Cook 1997) . The Although none of this evidence is derived fi^om the analysis of living cells, taken together it provides good evidence for looping. Many models for looping involve stable interactions between a motif like a MAR with some abundant protein bound to the substructure; this molecular tie would persist from one interphase to the next, be highly conserved, and found in all cells in the population. However, genomic sequencing has 8. Dynamic Chromatin Loops and the Regulation of Gene Expression 181 failed to uncover any such motifs, and a simple experiment demonstrates that different ties are found in different cells in the population. Cells are permeabilized in an isotonic buffer, treated with a nuclease like EcoRl, detached fragments removed, and remaining fragments analyzed. If the same DNA ties were found in all cells (Fig. la) , removing all but 10% cellular DNA should leave the ties enriched tenfold. However, enrichments are never this high, implying that the same ties are not found in all cells and that they change continually (Fig. lb; Dickinson et al. 1990; Jackson et al. 1990 Jackson et al. , 1996 Jackson and Cook 1993) . Chromatin dynamics can be monitored in living cells containing lac operator arrays integrated into a chromosome if they also express the repressor tagged with GFP; the repressors bound to the array appear as a moving spot (Robinett et al. 1996; Gasser 2002; Spector 2003) . Analyses of the kinetics are consistent with the array diffiising randomly over short distances. In yeast, the array can sample a considerable fraction of nuclear volume in minutes (Heun et al. 2001 ). In larger fly and human nuclei, the local neighborhood (diameter -500 nm) is sampled roughly as rapidly, but diffusion further a field is constrained-^presumably by neighboring chromatin (Marshall et al. 1997; Vazquez et al. 2001; Chubb et al. 2002; Chubb and Bickmore 2003) . Which proteins and DNA motifs constitute the molecular ties? In experiments like that illustrated in Fig. 1 , the residual fi*agments prove to be parts of transcription units associated with engaged polymerases. For example, they hybridize with poly(A)^ RNA (Jackson and Cook 1985) and contain active rDNA cistrons (Dickinson et al. 1990 ), while cloning and sequencing over 100 randomly selected ones shows they are nearly all parts of transcription units (Jackson et al. 1996) . Moreover, they remain associated with engaged polymerases; removing most chromatin in an isotonic buffer does not reduce polymerizing activity, whether it be the total activity or due to polymerases I or II (Jackson and Cook 1985; Dickinson et al. 1990 ). These polymerases cannot be tethered to the substructure through nascent transcripts as they remain after RNase treatment. (Note that when the experiment illustrated in Fig. 1 A detailed study of a model loop confirms that engaged polymerases mediate attachments (Jackson and Cook 1993) . When a few plasmids carrying the SV40 origin of replication are transfected into monkey cells (i.e., cos 7), they replicate over 2-3 days to give hundreds of minichromosomes. After permeabilization in an isotonic buffer, nearly all these model loops (~5 kb) resist electroelution and so must be attached to the substructure. Transfecting in progressively more DNA increases the number of attached plasmids up to a maximum of-1200, but then higher inputs generate additional unattached ones. This suggests there are a saturable number of attachment sites. Only the attached population is active, as eluting the unattached fraction does not reduce plasmid-specific transcription. Cutting with Haelll and removing most of the resulting -400 bp fragments (as in Fig. 1 ) also has little effect on transcription but it leaves fragments from within one or other of the two transcription units. Quantitative analysis shows that each minichromosome is attached at 1-2 points through either one of the two promoters in the plasmid or the body of the transcription units. Support for the idea that polymerases act as the ties comes from the regeneration of loops from unlooped sperm DNA (Gall and Muq^hy 1998). When demembranated sperm heads (which are inactive and contain unlooped DNA; above) are injected into the germinal vesicle of amphibian oocytes, the heads swell, accumulate polymerase II, and begin to be transcribed. If the contents of the germinal vesicle are now dispersed in a hypotonic buffer, lampbrush loops derived from both injected sperm and host are seen. The generation of lampbrushes from sperm DNA depends on transcription, as actinomycin D prevents it. Moreover, the active form of polymerase II (marked by hyperphosphorylation of serine 5 in the heptad repeats of the C-terminal domain of its catalytic subunit) becomes concentrated in the lampbrush axis. Polymerases elongate at -1.8x10^ nucleotides/min, and take -10 min to transcribe a typical human gene (Kimura et al. 2002) . While transcription continues, an active transcription unit will remain attached, and only on termination will it detach to leave the bound polymerase that is now free to exchange with others in the soluble pool. Transcription factors bound to their (untranscribed) DNA targets probably mediate additional attachments, as those targets resist nucleolytic detachment from the substructure. The first hint that this was so came from studies on nucleoids derived from rat cells transformed by polyoma virus; when most DNA was detached with EcoRl, fragments containing viral enhancers remained (Cook et al. 1982) . These results were confirmed using isotonic buffers; thus, minichromosomes are attached as much through nontranscribed promoters as through the body of transcription units (above). These kinds of attachment are unlikely to persist for long, as most GFP-tagged transcription factors remain bound to DNA only for seconds (Misteli 2001b; Chen et al. 2002; Hoogstraten et al. 2002) . And likely to be established only when assembled into preinitiation complexes, as specific transcription factors like Spl and C/EBP are detached with the body of the loop by Haelll digestion; while in contrast basal factors like TFIIB and TFIIH remain (Kimura et al. 1999) . Another kind of tie-^not to a factory-may be essentially permanent. Histones can carry a ""code"" (Fischle et al. 2003; Lachner et al. 2003 ) that ensures they bind tightly to others in heterochromatin or to the lamina (Polioudaki et al. 2001) . These are probably the ones that GFP-tagging indicates do not exchange except when DNA is replicated (Kimura et al. 2001 ), so they can sequester a loop permanently away from a factory. For example, genes involved in immunoglobulin rearrangements are repositioned to centromeric heterochromatin during lymphocyte development coincident with their inactivation (Brown et al. 1999) . Other kinds of tie probably exist only fleetingly, and will be best analyzed in vivo (e.g., by fluorescence correlation spectroscopy; Lippincott-Schwartz et al. 2001; Weidemann et al. 2003) . These arise because chromatin presents such a huge binding surface to the nucleoplasm; then, many so-called soluble nuclear proteins probably spend much of their time bound transiently to it through low-affmity interactions. The principles and results described above lead to a general model for the organization of all genomes (Fig. 2; Cook 1995 Cook , 2002 . When genes strung along a template are transcribed, active polymerases form into clusters to loop intervening DNA. We call one of these clusters a ""factory"" as it contains several polymerizing complexes working on different transcription units. Each factory would be surrounded by a rosette (or ""cloud"") of loops, and strings of nucleosomes and factories (plus surrounding clouds) would constitute the major architectural motifs responsible for organizing the genome. As we have seen, transcription factors in factories would mediate additional attachments. Then, RNA polymerase is not only an enzyme, but a critical structural component that ties the genome in loops. Note that the 3C and RNA TRAP methods provide powerful evidence for the local contact between two active transcription units; for example, the LCR only contacts the p-globin gene that it regulates when both are transcribed (above). Textbooks tell us that active RNA polymerases track like locomotives down their templates. In contrast, in our model active polymerases are attached to the substructure, and the immobilized enzyme works by reeling in its template . Evidence for this is of three general types. First, a tracking polymerase would generate a transcript that is entangled about the template, but this problem does not arise if the polymerase is immobilized. Second, biochemical results indicated that active genes, RNA polymerases, and nascent transcripts are all so closely associated with the underlying structure that they remain when most chromatin is removed with nucleases (as in Fig. 1 ). Third, we developed a method for localizing nascent transcripts with high resolution, and used it to show that active polymerases engaged on a number of different transcription units are concentrated in a limited number of discrete sites (diameter --50 nm) firmly associated with the substructure. (Engaged polymerases are allowed to extend transcripts by a few nucleotides in Br-UTP or biotin-CTP before the resulting tagged RNA is immunolabeled [Jackson et al. 1993 ].) As there turn out to be 8-fold more active molecules of RNA polymerase in a HeLa cell than transcription sites, and as only one polymerase is typically engaged on a transcription unit, each site (diameter -50 nm) must contain -8 different polymerases active on ~8 different units. This suggests active polymerases are immobilized, but can they then work? There is good evidence they can, as tethering them to a slide or plastic bead has no effect on the rate of polymerization of nucleotide triphosphates (Schafer et al. 1991; Cook and Gove 1992) . They can be viewed as motors that haul in their templates. They are powerful ones largely due to their low gearing; for each triphosphate hydrolyzed, DNA is reeled in by '^.34 nm-one-tenth and one-hundredth the step-lengths respectively of kinesin and myosin V (Gelles and Landick 1998). We are all familiar with the prototypic mammalian transcription factorythe nucleolus-which is dedicated to the synthesis of 45S rRNA and the production of ribosomes (Grummt 2003). Active polymerase II and III are also each concentrated in their own discrete factories dedicated to the production of particular transcripts. Thus, in a HeLa nucleus polymerase II transcripts (but not polymerase III, or its transcripts) are concentrated in -8000 nucleoplasmic sites, while polymerase III transcripts (but not polymerase II, or its transcripts) are found in another -2000 sites (Pombo et al. 1999) . Some factories specialize even further, and become dedicated to the transcription of specific sets of genes. Examples include: OPT domains (transcribing genes depending on Octl and PTF; Pombo et al. 1998) , sites containing jJ-globin and LCR transcripts (above), stress granules (tran-8. Dynamic Chromatin Lx)ops and the Regulation of Gene Expression 187 scribing satellite repeats; Jolly et al. 2004), CBs (transcribing snRNAs; Callan et al. 1991; Frey et al. 1999; Jacobs et al. 1999) , and perinucleolar polymerase III factories (transcribing tRNAs; Thompson et al. 2003) . Active RNA polymerase II is part of a huge complex that carries out many-^perhaps all-of the functions (RNA synthesis, capping, splicing, polyadenylation) required to generate a mature message (Maniatis and Reed 2002; Proudfoot et al. 2002) . This complex probably also proofreads mRNAs before going on to destroy faulty ones along with any peptides generated during proofreading (Iborra et al. 2001 (Iborra et al. , 2004 Andrulis et al. 2002; Brogna et al. 2002; Libri et al. 2002; Lykke-Andersen 2002) . Proofreading involves the nonsense-mediated decay (NMD) pathway (Hilleren and Parker 1999); this probably uses ribosomes to scan mRNAs for inappropriately placed (i.e., premature) termination codons (PTCs), and-if detected-^triggers the destruction of those faulty messages (along with any misfolded peptides that might result from proofreading). Nascent peptides-which are presumably made during the scanning by the ribosomes-are found in the nucleoplasmic transcription factories, together with components of the translation and NMD machineries. As inhibiting transcription immediately inhibits this nuclear protein synthesis, the processes must be tightly coupled (Iborra et al. 2001) . Moreover, the transcriptional, translational, NMD, and degradative machineries colocalize and coimmunoprecipitate; selected components (translational initiation factor eIF4E, ribosomal subunit S6, NMD factors UPFl/2) also copurify with the catalytic subunit of the polymerase, probably by binding to its C-terminal domain (Iborra et al. 2004 ). Nuclei of HeLa cells contain a dispersed pool of RNA polymerase II present at --1 jxM, but this is unlikely to account for much transcription because the local concentration in a factory is -1000-fold higher (Cook 2001) . This is especially so when promoters are tethered close to a factory. Compare two promoters in a loop, where one lies ~4-fold further away along the DNA from the factory. The distant promoter will be confined to a 64-fold larger volume around the factory (as volume depends on radius^). This reduces its local concentration by the same amount, and so the frequency with which it contacts the factory. As a result, distant promoters are much less likely to initiate than proximal ones. The long tether has another effect; it buffers the distant promoter from transcription-driven movement, and this immobility makes it likely to acquire the histone code characteristic of ""closed"" chromatin. The context then becomes selfsustaining: productive collisions of the nearer promoter with the factory attract factors increasing the initiation frequency, and the longer the distant promoter remains inactive the more it is likely to be embedded in heterochromatin. The probability that a promoter collides productively with a factory is increased by increasing promoter mobility (by ""opening"" chromatin), increasing promoter-factory affinity (through binding of appropriate factors), and reducing promoter-factory distance (by shortening the tether; Iborra et al. 1996; Cook 2003) . It will also depend on which other promoters compete for binding sites in nearby factories. For example, a polymerase II unit might be ""silenced"" by nearby polymerase II units (because they compete too effectively for a polymerizing site) or polymerase III units (because they attach to a remote polymerase III factory distant from any polymerase II factory). Alternatively, adjacent transcription units can stimulate activity. Consider the activation of the human e-globin gene during development. We imagine that it is initially embedded deep in heterochromatin far from a factory. During erythroblast development, the concentration of critical activators rises so the LCR-which would be in open chromatin on the edge of the heterochromatin-^now has an increased affinity for the factory. Once the LCR attaches and transcription begins, a polymerase reels in the template. This transiently subdivides the long loop into two smaller ones and the associated movement reverses the histone code, to open e-globin and bring it closer to the factory; now, it is much likelier to attach. Active transcription units can also act as barriers that prevent the spread of heterochromatin down the chromatin fiber; the inevitable movement associated with activity counteracts histone aggregation and the spread of an inactive histone code. Therefore, the pattern of activity of adjacent genes on the chromosome will also determine whether or not a particular gene is active. @story_separate@This model illustrated in Fig. 2 has several advantages. First, it is general, and can easily be extended to bacteria. Thus, lysing bacteria in a detergent and 1 M NaCl releases the prototypic factory-a, cluster of still-engaged polymerases surrounded by DNA loops. Transcription maintains this structure, as inhibiting it with rifampicin or treatment with ribonuclease (RNase) unfolds it (Pettijohn 1996) . Moreover, GFP-tagging reveals that RNA polymerases are concentrated in discrete foci, each of which probably contains several different operons encoding rRNA (Lewis et al. 2000; Cabrera and Jin 2003) . Second, it is a minimalist model in which all the basic structural motifs are defined. Third, we have seen it can readily explain how gene activity is regulated. Fourth, it can be extended to mitosis. Then, the contour length of the loops and the basic shape of the chromatin clouds remain unchanged (Jackson et al. 1990; Manders et al. 1999) , and decreased transcription coupled to increased cloudxloud and factory ifactory aggregation could drive reassembly into the most compact and stable structure, a cylinder of nucleosomes around an axial core containing the remnants of the factories (Cook 1995). Despite these advantages, many questions remain. For example, we still know very little about the microarchitecture of factories, or what maintains their structure as their constituents exchange continually with others in the surroundings. Three different kinds of motion of the chromatin loop have been described: (i) the random Brownian motion of DNA segments within the loop, (ii) the directed transcription-dependent reeling-in by immobilized polymerases that continuously changes the contour length of one particular loop, and (iii) the making^reaking of the molecular ties that attach loops to the factory (so adjacent loops split/merge as polymerase initiate/terminate or transcription factors bind/dissociate). These three motions counteract the tendency of chromatin to condense into heterochromatin, with a consequential alteration in histone code.","Although we have a draft sequence of the human genome, little is known about how the chromatin fiber is packed in three-dimensional (3D) space, or how packing affects function (Jackson 2003). We know packing plays a major role; the rate of transcription of a typical gene can vary over eight orders of magnitude (Ivarie et al. 1983), but deleting local elements like promoters and enhancers reduces expression by less than 5000-fold in transient transfection assays where the 3D “context” is missing. Common sense suggests the fiber cannot be packed randomly, but elucidating what any underlying order might be has proved difficult. First, the foldings of the chromatin fiber have dimensions below the resolution (≈200 nm) of the light microscope (LM) and so can only be seen by electron microscopy (EM), but then the fixation required can distort structure. Second, DNA is so long and packed so tightly it breaks and/or aggregates easily on isolation. Third, chromatin is poised in a metastable state so small charge alterations trigger changes in structure and function, and replacing the natural environment with our buffers often promotes aggregation."
"Rising health care costs have been prioritized in the budget planning of all Western countries. Rising R&D costs of up to $800 million per marketed new pharmaceutical have dramatically reduced the approval of new chemical entities (NCEs). Globalization of diseases like AIDS and SARS has had a definite impact on the economic situation not only in the Western world, but also in developing countries, especially for AIDS in Africa and SARS in Asia. The World Health Organization (WHO) is calling for free anti-tuberculosis (TB) drugs to be made available to people living with human immunodeficiency virus (HIV). The spread between unmet medical need in large indications (e.g. Alzheimer's disease) and in niche indications (e.g. Huntington disease) and the economic burden to create a blockbuster ($1 billion sales within one year after launch) has created a marketing-driven clinical development of new chemical entities. A paradigm shift has occurred by which developing a new innovative drug by documenting shortterm efficacy, quality and safety rather than long-term efficacy and emphasizing pharmacovilliglance including considerations of health economy within the medical environment a shift that has fundamentally changed and challenged the pharmaceutical industry.@story_separate@Health costs in the Western world range as low as 6.9% of the gross domestic product in the UK to 13.1% in the USA followed by Germany with 12.3% with rising trends. In developing countries (especially in Africa) with almost no capital, health costs remain in the one-digit range and are almost negligible for medical care or prevention. Expenditures out of total health costs ranked number 3 in most countries and for medication range from as low as 11% in the USA to 13.3% in Germany to 16.3% out of the total health costs, demonstrating different policies and health environmental systems. Unmet medical needs are a universal problem, affecting however different areas differently. While tuberculosis plays an increasing role in Asia (the number two cause of death in Indonesia) still cardiovascular diseases play a primary role in the civilized world though were overtaken by cancer in 2005. The share of the registered pharmaceutical world market demonstrated that more than 60% contribute to North America (especially the USA), 25% to Europe and more than 16% to Japan. The USA, therefore, is the main driver in pharmaceutical development. In 1996, R&D costs were $16.9 billion, in 2002 at $32 billion; however, new chemical entities (NCEs) declined from annual $53 billion to $17 billion in the same time period. The mean clinical and approval phase lengths for small-molecular drugs approved have not changed in 30 years and will take -including preclinical development from CD (candidate drug) to NDA -approximately 12 years (Fig. 1 ). Demographics are changing due to life expectancy and GDP in different parts of the world (Fig. 2) . Africa, Asia and Australia, even growing by a two-digit percentage on the pharmaceutical market, contribute far less than 10% to the world market. Populationwise China with 1.3 billion people and a GDP of $4.8 billion contributes increasingly by volume not by value, but may be a main driver for new untreated population (e.g. approximately 40 million untreated asthmatic patients) and a growing pharmaceutical market, due to rising prosperity. Demographic changes due to increasing life expectancy have altered the 'target population'. The unmet medical need for a population has broadened and shifted, e.g. for antibiotics for infectious diseases like syphilis, pneumonia, etc., to cardiovascular diseases at the end of the last century, whereas the most primary focus today relates to oncology, neuroscience and infection. Pharmaceutical development has focused on the male population (due to the risk for women of childbearing potential) and on a target group of 20-60 years. Children and the growing elderly population have only poorly been investigated and off-label use is common. 1 In the elderly population the most common neurodegenerative disease, Alzheimer's disease constitutes about two thirds of cases of dementia overall with vascular causes and other neurodegenerative diseases such as Pick's disease and diffuse Lewy-body dementia. Alzheimer's disease is a progressive neurological disease that results in the irreversible loss of neurons, particularly in the cortex and hippocampus and the clinical hallmarks are progressive impairment in memory, judgement, decision-making, orientation to physical surroundings, and language. Alzheimer's disease has a prevalence of approximately 1% among those 65-69 years of age and increases with age to 40-50% among persons 95 years of age and over. 2 Parkinson's disease, the second most common neurodegenerative disorder after Alzheimer's disease, is clinically characterized by resting tremor, bradykinesia, rigidity and postural instability, and pathologically by the loss of neurons mainly in the substantia nigra in association with the presence of ubiquinated protein deposits in the cytoplasma of neurons (Lewy bodies) and threadlike proteinaceous inclusions within neuritis (Lewy neuritis). Parkinson's disease has a prevalence of approximately 1% among persons 65-69 years of age, rising to 3% among persons 80 years of age and older. 3 These diseases are predominantly idiopathic disorders of unknown pathogenesis. However, the genetic mapping and gene-isolation tools created by the Human Genome Project over the past decade have greatly accelerated the rate of identification of genes involved in the rare inherited forms of these diseases. The Western world has changed its lifestyle; the estimated lifetime risk according to a recent US trial of developing diabetes for individuals born in 2000 is 32.8% for males and 38.5% for females. Females have higher residual lifetime risks at all ages. The highest estimated lifetime risk for diabetes is among US Hispanics (males, 45.4% and females, 52.5%). Individuals diagnosed as having diabetes have large reductions in life expectancy. A recent trial demonstrates that an individual diagnosed at age 40 will lose 11.6 life-years and 18.6 quality-adjusted life-years whereas women will even lose 14.3 life-years and 22.0 quality-adjusted life-years. 4 On the other hand, the current statistics and outcome trials on cardiovascular diseases including acute myocardial infarction and ischemic heart disease have dropped continuously throughout the last decade due to better diagnostics, identifying risk factors and new medication including anti-hypertensives, anti-diabetics and thrombolytics. Health data and literacy are a prerequisite for guided clinical development and the appropriate use of medication (81.5% of the total population in China compared to 52% in India) and have a huge impact on life expectancy (71.4 years in China and 62.5 years in India). The worldwide highest rate is currently in Japan with over 81 years for women and more than 75 years for men, which is comparable to the Western world. Currently far more than 30% of patients in the USA first consult the internet before visiting a doctor and learn about web-based guidelines, trials and recommendations about the possible outcome of a disease. The growing population of affluent older people may have greater expectations of medical care, fuelled by advertising and communication (older people are likely to demand cures for wrinkles, baldness, yellow teeth and relief from symptoms of menopause or andropause). Botulinum toxin has been developed for the treatment of wrinkles, minoxidil for male pattern baldness (primarily developed as a vasodilating hypotensive drug), hormone replacement therapy, currently under heavy discussion for women, and Viagra for impotence developed as an anti-hypertensive drug. The limits to demand for health care have been widely discussed in the literature. The controversial results of the Women's Health Initiative Trial on post-menopausal hormonal treat-ment (HRT) also published in the lay press have led to a 50% decrease according to a recently published observation in New Zealand. The intrinsic prosperities of an 'ideal' drug cannot by themselves ensure appropriate utilization. An ideal treatment may enhance physician and patient comfort by providing maximum efficacy and safety in the most convenient formulation. This would entail once-daily administration of a single treatment not influenced by meals or time of day with no adverse event or monitoring required, no adjustment for age, weight or race. Therefore comprehensive leaflets or programmes of effective communication and educational tools are necessary. Huge armies of representatives distribute such information, but have been heavily criticized for the only minuteslasting information given to the doctor. However, in general licensed doctors do not have to update their knowledge on pharmacotherapy on an obligatory basis and rely on the information and education provided by the pharmaceutical industry. Almost all of the ten biggest pharmaceutical companies in the world have at least two cardiovascular drugs in their pipeline or in their portfolio. Anti-hypertensives like beta blockers, angiotensin II antagonists and calcium antagonists sales have been rising throughout the years, since better diagnostic tool awareness and communication of risk factors documented by international megatrials and huge prospective outcome studies have been communicated via lay press and internet within and without the industrialized world. Simple screening diagnostic tools like blood pressure measurements and blood sugar cholesterol have changed and have been included in the guidelines for medication. New individual or conventional prognostic factors may even build up a different understanding in the pathogenesis of a disease and may ameliorate standard treatment to be documented in outcome trials. More than 50% of patients with coronary heart disease (CHD) lack any conventional risk factors (cigarette smoking, diabetes, hyperlipidemia and/or hypertension). 5 Among patients with CHD at least 1 of 4 conventional risk factors was present in 84.6% of women and 80.6% of men. Other non-traditional factors and genetic causes have to be evaluated. Although C-reactive protein (CRP), lipoprotein (A), fibrinogen and homocysteine are associated with vascular risk, their optional use in routine screening and risk stratification remains to be demonstrated. 6 The introduction of CAD risk equivalent categories within the American ATP III Guidelines substantially increases the number of patients eligible for LDL-cholesterol reduction to less than 100 mg/dl from approximately 5 or 6 million to approximately 20 million. More intensive lipid modifying treatment is thus also clearly needed to provide the additional reduction LDL-cholesterol methods for achieving optimal levels in patients with more challenging LDL-cholesterol goals. Prospective observational studies identified low HDL cholesterol as an important independent coronary risk factor. Some rare inborn errors of HDL metabolism cause low HDL cholesterol and premature atherosclerosis. In large, controlled intervention studies, statin and fibrate treatment of patients with increased HDL cholesterol reduced the incidence of coronary events, and in some of these trials, the increase in HDL cholesterol correlated significantly with the decrease of event rates. HDL-associated proteins and lipids exert several potentially anti-atherosclerotic activities. Transgenic over-expression of human apoA-I or ABCA1 genes was shown to inhibit the development or even induce regression of atherosclerosis in atherosclerosis-susceptible animal models. Six controlled and perspective landmark studies (>50,000 patients >5 years) demonstrated that lowering of LDL cholesterol with HMA-CoA reductase inhibitors (= statins) or fibric acid derivatives (= fibrates) therapy reduces coronary event rates by 30% (fatal and non-fatal MI, as well as coronary intervention). 6-7 LDL is currently considered a causal factor (and main diagnostic tool) in atherosclerosis, but may be just the tip of the iceberg, whereas the pleiotropic effects of statins may also cover other effects, e.g. anti-inflammatory and antifibrotic potencies. Traditional progress to cancer treatment relies on the combination of surgery, radiotherapy and chemotherapy. Current cancer drugs which are mostly lethal for cells (cytotoxics) either affect DNS synthesis and the process of cell division or cause chromosomal damage. Identifying a candidate drug (DC) via computer, chemical or biotechnical models followed by in vitro/in vivo proof of concept will lead to a new chemical entity (NCE). Biotechnical engineering in the production of new drugs plays an increasing and important role (e.g. STH, insulin, TPA), the role of gene therapy is yet limited to well-defined niche indications and will not revolutionize the pharmaceutical world but will affect approximately 20% of the diseases within the next two decades. Which population will be targeted and who can pay for it or provide reimbursement will have a major impact on the success of individualized or gene therapy in the near future. The life cycle of a new pharmaceutical has a great impact on investment for the pharmaceutical industry. An innovation of changing salicylate into acetylsalicylate has been developed in the early last century, but the pathophysiological pathways have only been evaluated within the last decades including anti-thrombotic effects, stroke prevention and even anticancer potentials. Nevertheless, aspirin is out-of-patent for decades and only different formulations may protect the originator from overrun by generics or even OTC (over-the-counter) medication. The patent situation in different countries might be the key to success or a nightmare for a pharmaceutical company because development times reach up to 10-12 years before being approved by the authorities or even marketed. Despite improving clinical development but due to the increasing prerequisites guidelines to develop a new pharmaceutical, it has left normally only a couple of years for new pharmaceuticals to pay back the huge development costs of approximately $500-800 million. Different national laws allow different patent interpretations, whereas molecule patent or manufacturing patents are differently interpreted, indication patents are almost not valid outside the USA. Today the biggest market of more than $21 billion in 2002 are the statins, which have been proven to prevent coronary heart disease (CHD). [6] [7] [8] The situation that statins were claimed to have hypothetically an anti-osteoporotic effect and being patented by an American company has led to a reduced interest in the pharmaceutical industry to develop this indication because royalties would have to be paid to the appropriate patent holder and outcome trials demonstrating superiority will last more than 5 years with 10,000 patients in different populations and a high risk of failure. Generics are quite necessary to reduce costs and stimulate innovations. The current situation, however, demonstrates an increasing share of generics, especially in countries like Germany or Denmark which face a generic market of more than 50%, whereas in the USA (currently heavy discussion about import from outside especially Canada and Mexico), the UK and France this market is yet in the range of 10%. The existence of generics on the one hand has built up a whole new industry with two-digit growth taking market share and volume with no money spent on innovation or any clinical development. The pharmaceutical companies on the other hand spend up to 20% of their sales on R&D development. The different legislations within the different countries in Europe also have built up parallel trade to undermine the bright politics earning in the range of billions for a parallel trader who just repacks, refills and sends it to different countries. The development of sildenafil for erectile dysfunction, orlistat for obesity and minoxidil for male pattern baldness have been classified as ""lifestyle drugs"" in the popular imagination. 9 It is difficult to define what we mean by the term lifestyle drug since the perception of what is illness and what is within the sphere of personal responsibility rather than health care may depend on whether one is a potential patient or potential payer, thus problems at the margins of health and well-being. 10 @story_separate@The pharmaceutical industry is an important economic tool in the Western world. The increasing costs of health care systems in any country in the world have recently changed the paradigm from developing a new, efficacious and safe highquality drug for unmet medical need to identifying either potential blockbusters or individual biotechnically driven, highly specialized individual therapies. The main cost drivers are still the treatments of the most common diseases like cardiovascular, oncological or infectious diseases. In the early 1990s, the Australian government introduced formulary submission guidelines 11 and since then such guidelines have been applied in many major markets. The need to demonstrate cost-effectiveness, affordability and the benefit-risk ratio to the health system of a preferred formulary position as opposed to a restricted indication, where the product is targeted at subpopulation of patients is leading into off-label use. The efforts to develop an innovative drug to be competitive, well-tolerated and reimbursed by the different health insurance systems have become as costly as a more than $500-million jigsaw puzzle which may pay off in the case of statins and proton pump inhibitors. The failure or safety issue may delete or seriously effect a worldwide operating company as seen 30 years ago in the case of Grünenthal (thalidomide) or recently for Bayer (cerivastatin: Lipobay). The probabilities for success or forecasting are not real science but have made development of medication a risky business case for any unmet medical need.","Rising health care costs have been prioritized in the budget planning of all Western countries. Rising R&D costs of up to $800 million per marketed new pharmaceutical have dramatically reduced the approval of new chemical entities (NCEs). Globalization of diseases like AIDS and SARS has had a definite impact on the economic situation not only in the Western world, but also in developing countries, especially for AIDS in Africa and SARS in Asia. The World Health Organization (WHO) is calling for free anti-tuberculosis (TB) drugs to be made available to people living with human immunodeficiency virus (HIV). The spread between unmet medical need in large indications (e.g. Alzheimer's disease) and in niche indications (e.g. Huntington disease) and the economic burden to create a blockbuster ($1 billion sales within one year after launch) has created a marketing-driven clinical development of new chemical entities. A paradigm shift has occurred by which developing a new innovative drug by documenting shortterm efficacy, quality and safety rather than long-term efficacy and emphasizing pharma-covilliglance including considerations of health economy within the medical environment a shift that has fundamentally changed and challenged the pharmaceutical industry."
"Smallpox was once a rampant and devastating disease in several parts of the world [1] . The World Health Organization (WHO) initiated the eradication programme in 1958 and intensified it since January 1967. The WHO declared eradication of smallpox by a resolution adopted in the World Health Assembly (WHA), the resolution WHA 33.3 on 8th 2 The Scientific World Journal May 1980 [2] . The last naturally occurring case was reported in Somalia on 26th October 1977 [3] . It is assumed that about 300 million people died of smallpox in the twentieth century alone. Approximately 30% of those infected with smallpox died around the globe and those who survived lived with ugly scars. In the posteradication era, the resolution WHA 33.4 directed that all variola viruses globally were to be destroyed, except at two international centres, one in USA and another in USSR. The WHA Resolution 49.10 recommended that all existing smallpox virus stocks were to be destroyed on June 30, 1999. However, the same was not carried out, and in the resolution WHA 60.1 the WHA requested the Director General to undertake a major review in 2010 on the research previously undertaken, those that were underway, and the plans and requirements for further essential research so that the Sixty-fourth World Health Assembly in 2011 might reach a global consensus on the timing of the destruction of existing variola virus stocks [4, 5] . During the meeting of WHA in May, 2011, it was decided to postpone the review on this aspect to 67th WHA in 2014 [6] . So we still have reasons to fear smallpox as there are at present 450 isolates of variola preserved in the WHO collaborating centre in the United States [7] and 691 samples preserved in the WHO collaborating centre in Russia [8] signalling the chance of inappropriate release of live virus from these high security laboratories at any time. Therefore, the debate on the logic of retaining or destroying stocks of smallpox is haunting the minds of scientists as well as the World Health Assembly (WHA) [9] . With this background, the lead authors of this paper decided to conduct an inquiry on this aspect among the clinicians and researchers who attended a stem cell meeting in a developing Asian nation and bring out their verdict on destruction or continued preservation of variola virus stocks.@story_separate@The participants of the inquiry comprised a rare blend of research scholars, students, and clinicians from biotechnology, life sciences, medicine, veterinary, and dentistry who attended a one-day stem cell meeting conducted on 15th October in 2011. The meeting had a quiz, a symposium, and plenary lectures as various components [10] . The total number of participants were 200, out of which 23.93% were students in faculty of medicine, 61.59% were from the field of biotechnology and allied basic sciences, 11.59% were from dentistry, and 2.89% were from veterinary sciences. 74.31% of the meeting participants were students from the various fields mentioned above. Majority of the attendees (96-98%) were from India while the rest (2-4%) were from countries like Japan, Canada, and Malaysia. The inquiry was conducted by a questionnaire (Table 1 ) distributed to the participants. A brief history of smallpox, the current topic of debates whether to preserve or to destroy smallpox, its importance in terms of biological research, and the potential threats were explained by an expert (one of the coauthors of this paper-Dr. Sudhakar John) who has a teaching experience of more than a decade in community ophthalmology and also had a fellowship training in the London School of Tropical Medicine. The participants were also allotted adequate time to interact and clarify their doubts with the expert. At the end, they were given 20 minutes to fill the form (Table 1) , concurrently allowing them to interact with the peer group and also other experts who had gathered for the stem cell meeting. The questionnaire had totally 10 questions with ample space provided to express their views on the topic of whether the smallpox virus stocks should be destroyed or not. The forms were collected at the end of two hours and the results were analyzed. It is to be noted that 90% of the participants who answered the questionnaire were born in the posteradication era of smallpox. Among the 200 participants, only sixty-six participants had answered the survey. Out of 66, forty (60.6%) had answered that the virus stocks should not be destroyed and twenty-four of them (36.4%) answered that, the virus stocks should be destroyed. The remaining two (3%) had answered that they were not able to decide ( Figure 1 ). It should be noted that 75.75% of the 66 participants who had answered the questionnaire were students. The 66 participants who had answered the questionnaire fairly represented the meeting participants because 28.78% of these 66 participants were from the field of medicine, 63.63% were from biotechnology, 4.54% were from dentistry, and 3.03% were from veterinary sciences. Among the 60.6% of those participants who answered the questionnaire in favour of the virus stocks not to be destroyed, the major reason quoted (  This inquiry provides an analysis of important issues from public health, clinical, and research aspects. The verdict is quite strong with nearly 60.6% of those who have answered the question, in favour of preserving the two remaining smallpox viral stocks, and, among the various reasons given, utility of specimens for future research is predominantly strong. Some other minor reasons include historical links. Among those who have said it should be destroyed, the major concern was bioterrorism, and they argued that the availability of the full genome of the variola virus and availability of related strains of viruses for study can compensate the loss of the smallpox virus stocks. The other factors include the nonavailability of adequate vaccine stocks and the lack of expertise in prompt diagnosis, treatment, and further control of spread. The current vaccine stock available with the WHO is only about 0.5 million doses [11] , which is insufficient by any stretch of imagination to control an outbreak. It is also a fact that most of the third world nations do not have sufficient access to vaccines in case of an unexpected outbreak. What is worse is that there are no backup laboratories to produce vaccines immediately on a mass scale if any need arises. Moreover, we need trained personnel to administer the latest vaccine to the population, which will be difficult in case of a mass outbreak. Besides, there have been reports of several adverse effects in laboratory studies of the virus [12] . The last reported fatal case of smallpox was that of the medical photographer Janet Parker at the Birmingham Medical School [13] . Judging from this laboratory outbreak of smallpox, the risk of accidental outbreaks in the future from stock piles is evident. Technological advancements which had led to the increase in speed and frequency of air travel by people throughout the world may result in faster spread of smallpox or any other virus in the event of an outbreak such as SARS-CoV [14] . However a heartening fact is that, since smallpox is relatively a slow spreading disease compared to SARS, air travel though could increase the number and distribution of sites with active cases, but the actual human-human spread kinetics may still be slow. Huggins in 1995 found that a drug called cidofovir can block smallpox replication, and he along with Jahrling had believed in around 1999 that in 5-year time, better antismallpox drugs were likely to be discovered. They argued that they had to test the new drugs on the live variola major virus to obtain Food and Drug Administration (FDA) approval and hence virus stocks have to be preserved [15] . The counter arguments to this point of view are that there currently exists at least two compounds which are considered as remedial measures for smallpox such as ST-246 [16] and CMX001 [17] . Moreover, the fully sequenced genome of smallpox virus is also available [18] , which can be used to recreate the virus for laboratory drug testing. ""Also any likely use of drugs or vaccines against smallpox in case of an outbreak would fall under emergency regulatory provisions that allow the use of treatments that have not been completely tested, "" argue a few scientists [19] . The factors in favour of continued preservation of stocks include requirement of an authentic source of naturally occurring virus for research and the fact that smallpox virus is not the only tool for bioterrorism. The Scientific World Journal (1) For further studies, if mutants evolve, we need the original strains for comparison (2) For development of vaccines or antiviral drugs in case of accidental outbreak of the disease (3) Historical reasons for preservation of the viruses to serve as a study material for the future generations (4) There are many dangerous viruses which can also be misused. Therefore there is no harm in preserving the last two stocks of smallpox Main reasons against preservation ( = 24; 36.4%) (1) Fear of misuse and bioterrorism (2) Genetically modified or lab-bred strains will be difficult to curtail if there is an outbreak due to accidental or deliberate exposure to the stock virus (3) Storage is unnecessary without any immediate application of the viruses (4) Since the full genome of the smallpox is available, live stocks are not needed (5) Other similar viruses which are available can be used for development of drugs against smallpox; hence live smallpox viruses need not be preserved Research in biology for developing newer vaccines, newer antivirus drugs, and diagnostic tests against smallpox requires appropriate live strain of viruses, because the viruses artificially reconstructed in the laboratory may or may not exactly recapitulate the biological characteristic of the virus in its native form. For instance, the mutations occurring in a specific strain of virus accumulated over prolonged periods of time might not be possible to be recapitulated in the laboratory in a relatively shorter period of time as a synthetic variola virus (i.e., a clone) would or would not exhibit the same virulence as the ""wild"" uncloned stocks. Even if recapitulated, the exact biologic extrapolation may not be accurate. Also there are not many studies on the validation of the results of antiviral studies done in viruses in their native form versus their sequenced genome forms in biological models. It has not yet been possible to successfully create animal models of smallpox because variola does not affect non-human primates. Though animal models of other strains of pox viruses, for example, monkey pox, are available and even late stages of smallpox can be modeled in macaques [20] , they are still imperfect models compared to an animal model infected with the native strain of the virus exhibiting the disease in its full form. Also, the reason behind why smallpox is so lethal to humans whereas it does not affect animals and the immunological mechanisms behind immunity conferred by the current vaccines all need to be answered by research for which maintenance of the live form of the virus is essential. Intentions to cause bioterrorism not only depend on the presently stored stocks of smallpox virus, but it is possible to recreate deadly virus strains even from related species of smallpox or even from the scratch using today's technology. Even worse is the fact that there is an increased prevalence of infections due to orthopoxviruses like monkey pox and cow pox ever since smallpox was eradicated and vaccination was stopped [21, 22] . Recent controversies on laboratorybred Avian Influenza viruses, in which mutant strains were developed by scientists [23] , throw light on the fact that there are more deadly viruses which can cause accidental outbreaks and can be used for bioterrorism than variola virus. In addition, whether the lab research on such highly dangerous mutant strains should be published or not remains a controversy. Once published, those protocols also have a potential to be misused to develop bioterrorism weapons [24, 25] . This further strengthens the argument that destruction of smallpox virus stocks is not the ultimate solution to the threat of bioterrorism which is the major argument of proponents for destruction of the virus stocks. It should be pointed out that the meeting is a unique one which has brought together the clinicians and basic scientists, in life sciences together, under one roof, and it is known that a cultural divide between clinicians and basic scientists exists all over the world [26] . This unique gathering is of significant importance to this subject because while the clinicians would have seen or understood the implications of the spread of such viruses from the patient-management point of view, the basic scientists on the other hand would have seen it from the laboratory perspectives. The basic scientists would have understood the difficulties to recreate the virus artificially after destroying the natural virus albeit the recent synthetic biology technologies can use chemically synthesized DNAs to produce a pathogen in the laboratory [27] . This could be one of the explanations to the results wherein most of the basic scientists have opted for preserving the organism while among the clinicians it is a mixed reaction. The above mentioned assumptions are compatible with the results in this study, as the clinicians are divided equally in their opinion as 50% of them having supported that the variola stocks should be destroyed and the remaining 50% having supported their continued preservation (Figure 2 ), whereas two-thirds of the basic scientists have supported the continuous preservation of the stocks for future research (Figure 2 ). There are mainly two concerns-one is the fear of bioterrorism and the other is the fear of losing a valuable natural resource for research. The authors are of a view that strict rules and systems for enforcement should be in place against the creation or handling of potentially life threatening organisms to ensure safety of all living beings in the universal environment, while, in permitted and appropriately guarded laboratories, the naturally created organisms are kept alive for future research. The proportion of the students among the participants was 74.31% and the same reflected in the proportion of the respondents as well (75.75%), that is, The Scientific World Journal 5 50 out of 66 respondents. In terms of relevance of the results, if majority of the respondents were senior scientists and physicians instead of being students as in this case, it might be considered carrying a relatively higher significance. However, we would like to state that the implications of this question were thoroughly explained by an expert for a better understanding by the respondents before they answered the questions. The results of the study thus may be considered as the opinion of budding scientists representing the younger generation.@story_separate@Analyzing the previous facts both in favor as well as against preservation of smallpox virus stocks, the authors stand in line with the verdict of the majority that the variola (smallpox) virus need to be preserved for future biological research as well as vaccine preparation to combat future epidemic.","The World Health Organization (WHO) declared eradication of the dreadful disease “smallpox” in 1980. Though the disease has died down, the causative virus “variola” has not, as it has been well preserved in two high security laboratories—one in USA and another in Russia. The debate on whether the remaining stocks of the smallpox virus should be destroyed or not is ongoing, and the World Health Assembly (WHA) in 2011 has decided to postpone the review on this debate to the 67th WHA in 2014. A short questionnaire-based inquiry was organized during a one-day stem cell meeting to explore the views of various health care and life science specialists especially students on this aspect. Among the 200 participants of the meeting, only 66 had answered the questionnaire. 60.6% of participants who responded to the questionnaire were for preserving the virus for future reference, while 36.4% of the participants were for destroying the virus considering the magnitude with which it killed millions. However, 3% of the respondents were not able to decide on any verdict. Therefore, this inquiry expresses the view that “what we cannot create, we do not have the right to destroy.”"
"Optimising techniques to wean patients from invasive mechanical ventilation (IMV) remains a key goal of intensive care practice [1, 2] . To date, no definitive guidelines exist on the best approach to use in the intensive care unit (ICU) [2] . Evidence from structured and systematic and literature reviews of studies, including randomised controlled trials (RCTs) suggests that the use of non-invasive ventilation (NIV) as a weaning strategy (transitioning patients who are difficult to wean to early NIV) may reduce mortality, ventilator-associated pneumonia and ICU length of stay, although this beneficial effect may be limited to patients with chronic obstructive pulmonary disease (COPD) [3, 4, 8] . However, the probability of NIV being cost effective relative to weaning without NIV was modest: between 57% and 59%. For patients with chronic obstructive pulmonary disorder, the probability of cost effectiveness of NIV was much higher (82-87%). Future trials with extended follow-up are needed to reduce uncertainty surrounding the long-term cost effectiveness of NIV. breathing trial (SBT). Patients were randomised to receive either protocolised weaning with extubation to NIV (noninvasive weaning) or protocolised weaning via IMV with daily SBTs (invasive weaning). NIV refers to the delivery of mechanical ventilation without the need for an endotracheal airway. Positive pressure ventilation is delivered to the patient through the mouth or nose via an interface such as a mask or helmet. Clinicians were permitted to use one of three types of SBT in accordance with local unit practices: a T-piece trial, use of continuous positive airway pressure (CPAP) or low-level pressure support (5-7 cm H 2 O). A T-piece trial involves the patient breathing spontaneously through their endotracheal tube, with the appropriate inspired oxygen concentration being maintained by a cross-flow device (T-piece). CPAP involved leaving a standing pressure of 5-10 cm H 2 O delivered via the ventilator at the top of the endotracheal tube but with no assistance on inspiration. A low-level pressure support trial provided 5-7 cm H 2 O inspiratory assistance. Each SBT was scheduled to last for at least 30 min and could be increased up to 120 min in patients considered to be at higher risk of re-intubation (e.g. prolonged ventilation, past history of COPD, heart failure). During SBTs, patients were closely monitored for the following signs of distress or fatigue: heart rate > 20% of baseline or > 140 beats min −1 ; systolic blood pressure > 20% of baseline or > 180 mmHg or < 90 mmHg; cardiac arrhythmias; respiratory rate > 50% of baseline value or > 35 min −1 , respiratory rate (min)/tidal volume (L) > 105 min −1 L −1 ; arterial blood gases; clinical assessments such as agitation, anxiety or depression. A patient was considered to pass the SBT if no signs of distress or fatigue developed. A patient who displayed any sign of distress or fatigue was judged to have failed the SBT. Clinicians were provided with information about the Walsh criteria, which were suggested as guidance to indicate when the patient was ready to commence weaning [9] . The Walsh criteria recommend that meeting all the following conditions indicates readiness for weaning: cooperative and pain free; good cough; PaO 2 :FiO 2 (ratio of arterial oxygen partial pressure to fractional inspired oxygen) > 24 kPa; positive end-expiratory pressure < 10 cm H 2 O; haemoglobin > 7 g dL −1 ; axillary temperature 36-38.5 °C; vasoactive drugs reduced or unchanged over previous 24 h; and spontaneous ventilatory frequency > 6 breaths per minute. Heated humidified oxygen (on both invasive ventilation and NIV) was not mandated or recommended in the study protocol but could be used at clinician discretion in accordance with local unit policy. A sample size of 364 (90% power, two-sided 5% type I error), allowing for a 23% dropout rate, was required to detect a clinically meaningful median difference of 24 h In light of current clinical practices [2] , the findings of existing trials are of limited generalizability to clinical settings across a number of industrialised countries as treatment pathways for COPD exacerbations vary across settings and over time. Whereas patients with respiratory failure would have previously received IMV, in the UK for example, this is now largely reserved for patients who fail a trial of NIV. Furthermore, few published studies have reported the impact of NIV on health-related quality of life (HRQoL) outcomes [3] or economic costs associated with NIV in critical care settings [5, 6] . Where these have been reported, sample sizes are small [6] or focus has solely been on patients with an exacerbation of COPD. Importantly, only one study, conducted in Canada, has estimated the cost effectiveness of NIV as a weaning strategy in patients with COPD [7] , whereas, to our knowledge, no study has estimated its cost effectiveness in patients within the ICU presenting with or without COPD. It is crucial to evaluate the cost effectiveness of NIV before its use can be considered more widely. We therefore present a health economic evaluation from a multicentre RCT comparing protocolised weaning that includes early extubation onto NIV versus weaning without NIV (Breathe study; ISRCTN 15, 635, 197 ).@story_separate@Details of the design and clinical outcome measures for the Breathe study are reported elsewhere [8] . Briefly, patients aged ≥ 16 years recruited from 41 UK critical care units were eligible for randomisation if they had received IMV for over 48 h, were ready to wean and had failed a spontaneous between the non-invasive and invasive group for the primary outcome of time to liberation from ventilation. The primary economic analysis was undertaken from the perspective of the UK National Health Service (NHS) and personal social services (PSS) [10] . The time horizon for the within-trial economic evaluation was limited to a follow-up period of 6 months. In addition, a 5-year time horizon was considered for a long-term cost-effectiveness analysis. A discount rate of 3.5% per annum was applied to both costs and effects during years 2-5 for the longer-term cost-effectiveness analysis [10] . Resource use data were collected from randomisation to 6 months post-randomisation using case report forms (for initial hospitalisations) and participant questionnaires. The resource and cost components associated with the intervention were aggregated into five groups: 1. intensive care support, including organ support, level of care and use of sedatives, 2. tracheostomies, 3. use of high-cost antivirals and antifungals or other highcost drugs in the ICU, 4. hospital care between ICU discharge and hospital discharge, 5. use of any emergency transport to transfer patients between hospital sites. For critical care stays, healthcare resource groups (HRGs) were assigned according to the maximum number of organ systems supported daily during each stay. Critical care HRGs value standard resource expenditures (e.g. staffing, consumables, diagnostics), such that high-cost drugs and interventions were separately collected and valued to derive a total overall critical care cost. Following critical care discharge, the costs of step-down care were based on number of days spent in each step-downward/facility (until death or discharge) multiplied by the respective per diem cost for that level of care [11] . Further details of healthcare resource use can be found in Table 1 in the electronic supplementary material (ESM) and include the number of organs supported, the number of days in ICU, highest level of care and details of tracheostomies, antifungal/antiviral use, inpatient and outpatient care, residential care services, community health and social care, frequency of prescription medications, equipment and aids and additional health resource used by patient or carer/supporter. Differences in resource use between the two intervention groups for the period between randomisation and hospital discharge were determined by (1) comparing the number of days that patients had two or more organs supported while in the ICU (or alternatively, days in level 3 care); (2) comparing differences in overall length of stay in the ICU; and (3) comparing proportions of patients who required tracheostomies, used high-cost antifungals, used high-cost antivirals or needed emergency transport for transfers between hospitals. Patients requiring support for two or more organs and those receiving advanced respiratory support alone were considered to receive level 3 care [12] . Broader health and PSS resource use data (e.g. hospital readmissions, contacts with community health and social care professionals, medication use) were collected at 3 and 6 months post-randomisation using postal questionnaires completed by participants or their primary carers. We also collected data on direct non-medical costs (including travel expenses) incurred by patients and their caregivers, days off work and loss of earnings. Resource use values were converted into costs by applying unit costs obtained from key UK national databases [13] [14] [15] [16] [17] [18] [19] [20] (  The primary outcome for the economic evaluation was the quality-adjusted life-year (QALY) [10] . HRQoL was assessed using the three-level EuroQol 5-Dimensions (EQ-5D-3L) [22] at 3 and 6 months post-randomisation. The EQ-5D-3L descriptive system consists of five dimensions (mobility, self-care, usual activities, pain/discomfort and anxiety/depression), each divided into three ordinal levels: (1) no problems, (2) some or moderate problems, and (3) severe or extreme problems. The UK time trade-off tariff was applied to each set of responses to generate an EQ-5D-3L utility score for each participant [23] . Given the challenges of collecting baseline data from patients in critical care settings, we assumed in the baseline analysis that the baseline utility value was − 0.402, the value assigned by the EQ-5D-3L tariff to an unconscious health state [24] . QALY values for each patient were calculated as the area under the baseline-adjusted utility curve [25] assuming a fixed baseline of − 0.402 (equivalent to an unconscious health state) and using linear interpolation between baseline and followup utility scores. QALYs were also derived from 6-Dimension Short-Form survey (SF-6D) utilities (UK tariff), generated from responses to the SF-12 as a sensitivity analysis [26] , assuming a baseline utility value of zero. Patients who survived the initial hospital admission were also asked to recollect their pre-admission health state using both the EQ-5D-3L and the SF-12 questionnaires. Multiple imputation under chained equations [27] was used for missing resource use or HRQoL data, based on the tested assumption that data were missing at random. Regression models were used to estimate missing costs and QALYs at each time point, by treatment allocation, conditional on fully observed baseline variables: age, sex, randomisation centre, presence/absence of COPD, non-operative/operative status and post-SBT PaCO 2 . In total, 20 datasets were generated using predictive mean matching. Estimates obtained were pooled to generate mean and variance estimates for costs and QALYs in each allocation group over the trial time horizon using Rubin's rule [28] . All mean incremental costs take into account heterogeneity in baseline costs, leading to estimates of incremental costs that would have adjusted for costs arising from differences between groups in terms of severity at baseline [8] . Economic values were summarised by treatment group, resource category and assessment time; differences between groups were analysed using two-sample t tests. Non-parametric bootstrapping, based on 10,000 replications (10,000 was expected to stabilise the confidence intervals [CIs] for point estimates), was used to assess whether differences in mean total costs between allocation groups were statistically significant. EQ-5D-3L utility scores were compared using two-sample t tests. A cost-effectiveness analysis using individual patientlevel data was conducted. Cost-effectiveness results were expressed in terms of an incremental cost-effectiveness ratio (ICER) and calculated by dividing the difference between trial arms in mean total costs by the difference in mean total QALYs. Value-for-money assessments involved comparing the ICER value with a range of cost-effectiveness thresholds. Cost-effectiveness thresholds held by UK decision makers typically range between £20,000 and 30,000 per QALY [10] . Several types of uncertainty analyses were undertaken. Stochastic uncertainty was presented in terms of CIs, decision uncertainty was undertaken using various cost-effectiveness thresholds that determined the INMB, and uncertainty from heterogeneity was implicitly assessed through modelling the pre-specified covariates (presence/absence of COPD, operative status, baseline costs, baseline utility) considered to be related to cost-effectiveness outcomes. In addition, 10,000 estimates of incremental costs and benefits were generated through non-parametric bootstrapping to determine the level of sampling uncertainty around the ICER. The bootstrap replicates were used to populate costeffectiveness scatterplots. We also calculated the incremental net monetary benefit (INMB) of using NIV versus IMV across three cost-effectiveness thresholds: £15,000 [29] , £20,000 and £30,000 per QALY gained. A positive INMB indicates that the intervention is cost effective compared with the alternative at the given cost-effectiveness threshold. Cost-effectiveness acceptability curves (CEACs) summarised the likelihood that NIV was cost effective as the cost-effectiveness threshold varies. Sensitivity analyses included (1) adopting a wider societal perspective encompassing direct non-medical costs incurred by trial participants and their families, and economic values placed on attributable work absences being collected in the case report form by asking patients/carers whether items such as travel cost and income lost occurred and how much cost was incurred; (2) restricting analysis to complete cases; (3) using SF-6D utility scores estimated from the SF-12 for the purposes of QALY estimation; and (4) additionally using the pre-randomisation EQ-5D-3L utility value (recalled at hospital discharge) as a covariate for the purpose of QALY adjustments. Pre-specified subgroup analyses were presence/absence of COPD and operative status. Long-term cost effectiveness was determined over a 5-year time horizon by extrapolating survival beyond 6 months. The 5-year point was arbitrary; but we felt that extrapolating beyond 5 years would have generated too much uncertainty. For a cost-effectiveness analysis conducted over a lifetime horizon, using data from many patients who were censored would potentially lead to highly uncertain survival rates. Observed survival curves ( Fig. 1 in the ESM) showed that most deaths occurred between randomisation and hospital discharge; a 5-year time horizon therefore limited the uncertainty of the long-term cost-effectiveness results compared with modelling over a lifetime horizon. A flexible parametric model was used to predict survival rates at each time point [30] . Flexible parametric models are commonly used in the assessment of the cost effectiveness of cancer drugs for prediction of survival beyond trial followup [31] . Three parametric models were considered: exponential, Weibull and a more flexible model using cubic splines with up to four knots (points at which splines are joined). This involved fitting survival curves using the observed survival data by modelling the background (baseline hazard) risk of death over time and the risk of death due to intervention. The choice of model selection was based on Akaike's information criterion (AIC). The parameter estimates and corresponding standard errors were reported. The observed survival data (time to death) were used along with a censoring variable, and stratified covariates were included in the model. We adopted a conservative approach to estimating (extrapolated) longer-term costs and health utilities beyond 6 months separately for each treatment group. Costs between 6 months and 5 years post-randomisation were estimated by using the observed 3-to 6-month post-randomisation total costs but also adjusting for covariates and multiplying by the predicted survival probabilities over the 5-year post-randomisation period. The trial-based estimates of health utilities (adjusted for covariates) were used as a basis for estimating utility values beyond 6 months. The primary assumption for longer-term cost effectiveness was that future costs and utility patterns beyond 6 months were equal, so only predicted survival rates were likely to drive future costs and QALYs. Thereafter, several sensitivity analyses were undertaken for how future cost and utility patterns could behave, including (1) the 6-month utility values were carried forward (linear constancy); (2) utilities declined in a linear fashion; (3) utilities declined exponentially; (4) survival rates were lower in the NIV arm by 10% (justified by an examination of the plots of log-survival and hazard function); (5) future costs and utilities differed between arms based on 6-month values (carried forward); and (6) future costs were assumed equal but future utilities could differ. The estimated utility values in each of these alternative scenarios (adjusted for proportion of patients alive at the corresponding time point) were used to compute the 5-year QALY estimates. A further sensitivity analysis of longer-term cost-effectiveness outcomes adopted a societal perspective. All statistical and cost-effectiveness analyses were undertaken using SAS ® version 9.4 on a Windows platform. A published SAS macro [31] was used for flexible parametric modelling. Reporting was made in line with the CHEERS statement [32] . The trial protocol was designed by the trial investigators and was approved by South Central C Research Ethics Committee (reference 12/SC/0515). Details of the clinical study have been reported elsewhere [8] . Briefly, 364 patients were randomised: 182 to noninvasive weaning and 182 to invasive weaning. The primary clinical endpoint showed no clinical or statistical difference in median time to liberation from mechanical ventilation: median 4.3 vs. 4.5 days; adjusted hazard ratio 1.1; 95% CI 0.89-1.40. Early extubation to NIV did not shorten time to liberation from any ventilation. Approximately 52 and 50% of all health resource use data were complete at 3 months for the non-invasive and invasive groups, respectively; this was 51 and 46%, respectively, at 6 months ( Table 3  A complete QALY profile was available for about 50% of patients (179/364) ( Table 3 in the ESM). Data were missing because patients either died before 3 months (n = 82), were not available to provide a response (n = 60) or withdrew from follow-up (n = 37). Resource use for the period between randomisation and hospital discharge was generally higher for patients allocated to the invasive group ( Table 1 in the ESM). The proportion of patients who used antifungals was significantly higher in the invasive group (12% for IMV vs. 5% for NIV; p = 0.0168). Broader resource use (post-initial hospital discharge) was similar between the non-invasive and invasive groups. The mean intervention costs from randomisation until hospital discharge were £29,697 and 32,052 for NIV and IMV participants with complete data, respectively: mean cost difference − £2355; 95% CI -7292 to 2750; p = 0.4472 ( Table 1) . The mean total NHS and PSS costs throughout the first 6 months post-randomisation were, on average, lower for the non-invasive group than for the invasive group: £31,711 versus 32,468; mean difference − 756.20; 95% CI -6642 to 5246; p = 0.8321 (Table 1) . Mean societal costs were £31,934 and 32,999, respectively; mean difference − 1065; 95% CI -6804 to 5056; p = 0.7981. The wide CIs for differences in mean costs reflected the uncertainty in the estimates. Significant differences between groups in terms of baseline clinical characteristics were not observed. There were no significant differences in EQ-5D-3L outcomes between the trial groups prior to hospital admission or at 3 months post-randomisation (Tables 4 and 5 in the  ESM) . However, mean EQ-5D-3L utility scores among complete cases were significantly lower at 6 months post-randomisation for the NIV group (0.53 vs. 0.66; p = 0.0147). The mean QALY value was, on average, higher for the NIV group (0.0928 vs. 0.0747; p = 0.4522). The mean improvement in QALYs was because mean utility at a specific time point was based on observed cases and did not consider deaths (utility score of 0) and differential survival rates. Since more patients died in the invasive group, more utility scores were set to zero. Moreover, our QALY estimates are derivations over time, based on modelled estimates of mean utility at each time point, taking into account heterogeneity and missing data. The base-case economic evaluation, using imputed attributable costs and QALYs and covariate adjustment, indicated that-over the first 6 months-NIV was associated with a lower net cost (− £302; 95% CI -5490 to 4760) and a higher net effect (0.02 QALYs; 95% CI − 0.01 to 0.05) and was therefore dominant ( Table 2) . The simulated ICERs showed uncertainty largely across the north-east and south-east quadrants of the cost-effectiveness plane (Fig. 1) . The NIV protocol showed net economic gains based on INMBs of £541, £620 and £779, on average, at costeffectiveness thresholds of £15,000, £20,000 and £30,000 per QALY, respectively ( Table 2 ). The CEAC shows that the probability that NIV is cost effective is approximately 57-59% across cost-effectiveness thresholds (Fig. 1) . INMBs were similar across scenarios considered by the sensitivity analyses, indicating that the results are robust to alternative assumptions ( Fig. 2; Figs. 2 and 3 in the ESM). Both presence of COPD and operative status had a notable impact on cost-effectiveness results ( Fig. 2; Figs. 2 and 3 in the ESM). For patients with COPD, the probability that NIV is cost effective increased to 82-87% (Table 2 ). In contrast, the probability of cost effectiveness was < 30% among postoperative surgical patients. A tornado diagram displaying the impact on the INMB of variations in several inputs is provided in Fig. 5 in the ESM. The base-case extrapolation analysis, based on mortality predictions using the Royston-Parmar model [3 knots; i.e. a RP(4) model] yielded mean survival times (over the 5-year time horizon) of 41.9 versus 33.3 months for the non-invasive versus the invasive group, respectively. Among the three models [exponential, Weibull and RP(4)] used to predict survival rates to 5 years (Table 7 and Fig. 1 in the ESM), the RP(4) model showed the smallest AIC value (a wellestablished metric of model fit). Extrapolated survival rate estimates were in broad agreement with those in published studies [33] [34] [35] , which report 1-, 2-and 3-year survival rates of 69%, 50% and 47%, respectively. For the invasive group, these were 65%, 60% and 50% at years 1, 2 and 3, respectively. There was no statistically significant difference between the two survival curves for survival data observed during the trial (log rank p value = 0.366). The above models also confirmed this ( Table 6 in the ESM). The extrapolated survival to 5 years post-randomisation estimated that 67% of patients were expected to remain alive in the non-invasive group versus 45% for the invasive group. However, given the high uncertainty around these estimates as a result of extrapolation, we also conducted sensitivity analyses ( Table 8 in the ESM) that included an assumption of equal future (beyond 6 months and 5 years) survival rates between arms. Under the assumption that future costs and utility patterns beyond 6 months are equal (based on extrapolated survival data), the mean discounted expected QALYs were 2.25 (noninvasive group) and 1.82 (invasive group) ( Table 7 in the ESM), resulting in an incremental QALY gain of 0.427. The mean NHS and PSS costs over the entire 5-year period were higher in the NIV group than in the IMV group (£43,759 vs. 41,787, respectively). The 5-year ICER associated with NIV was £4618 per QALY gained (an INMB of £10,838 at Fig. 1 a Cost-effectiveness plane, b cost-effectiveness acceptability curve for base case: fixed baseline utility, imputed costs, adjustment for covariates a cost-effectiveness threshold of £30,000). The probability that NIV was cost effective was > 90% at a cost-effectiveness threshold of £20,000 per QALY (Fig. 7 in the ESM) . However, the sensitivity analyses indicated that the probability of cost effectiveness for NIV strongly depended on assumptions surrounding future costs, utilities and survival rates beyond 6 months (Table 8 and Figs. 4 and 6 [tornado plot] in the ESM). In particular, under the assumption of equal survival rates between 6 months and 5 years, and equal future utilities, the mean incremental costs and QALYs were (non-invasive vs. invasive group) £6453 and − 0.092 ( Table 8 in the ESM), respectively, yielding negative INMBs ranging between £7833 and £9213 favouring invasive weaning. This showed that the incremental QALY gains in favour of NIV were largely driven by the higher extrapolated survival rates for the NIV arm. Hence, despite predicted survival rate estimates being in broad agreement with those in published studies [33] [34] [35] , the uncertainty around the long-term cost-effectiveness results should be carefully considered. When the projected survival rates were assumed to be lower by 10% in the non-invasive weaning arm (vs. the invasive arm), the INMB fell by > 90% from £10,838 to 603, and the expected probability of long-term cost effectiveness fell to around 79% (Table 8 and Fig. 8 in the ESM). If future survival estimates reflect patterns shown here (Fig. 1 in the ESM), and future patient costs accumulate based on patterns observed during the trial, without improvements in QALYs ( Table 8 in  This trial-based economic evaluation showed that NIV has potential to be cost effective compared with invasive weaning; the probability of cost effectiveness ranged from 57 to 59%, depending on the cost-effectiveness threshold. This is likely to be largely driven by the difference in the mean ± standard error number of days in the ICU (14 ± 1.08 vs. 15 ± 1.12) and ICU-related costs ( Table 1; Table 1 and 9 in the ESM). This finding remained robust to most sensitivity and subgroup analyses considered. The main exception related to subgroups of patients without COPD and those who required surgery, in whom IMV was the dominant strategy. The primary clinical endpoint of time to liberation from mechanical ventilation did not show a statistical difference: median 4.3 vs. 4.5 days; adjusted hazard ratio 1.1; 95% CI 0.89-1.40. However, a lack of difference in the time to liberation from ventilation does not imply that costs of ICU care beyond this point are no longer relevant for the purposes of cost effectiveness. In this cost-effectiveness analysis, health resource use and HRQoL beyond liberation from ventilation were considered, which could logically result in differing conclusions. We observed that, on average, patients in the invasive group stayed about 1 day long in the ICU than those in the non-invasive group ( Table 1 in the ESM) . Moreover, other costs, including broader NHS and broader societal costs, also influenced the observed cost differential in favour of NIV. In addition, improved QALYs associated with NIV were observed once deaths are accounted for (driven by more deaths in the IMV arm). The cost-effectiveness analysis considered a broader assessment of consequences than the clinical study, resulting in a conclusion that NIV weaning may be cost effective, particularly for patients with COPD. The differences in costs and QALYs between the trial comparators were not statistically significant. However, the trial was not powered to detect such differences over a short-term time horizon. A power calculation for a future trial covering a 5-year follow-up period, using the observed results from this trial, taking into account long-term survival predictions (i.e. using mean costs and projected QALYs and their standard deviations along with a £30,000 cost-effectiveness threshold), would require a sample size of at least 215 per group (430 in total) to show with at least 80% power (5% significance) that the INMB would be > 0. This would be the case even if NIV weaning was more expensive by as much as £2737 (one possible scenario of the sensitivity analyses in Table 8 in the ESM). Hence, this trial provides useful data to prospectively design a future (larger) confirmatory trial with longer follow-up that could demonstrate long-term cost effectiveness. In the more optimistic scenario, where NIV was projected over 5 years to be cheaper by £302 (Table 2 ; Table 8 in the ESM), the sample size required would be around 120 per group (240 in total). The observed probabilities of cost effectiveness observed in this trial, although < 80%, may partly be because the sample size (n = 364 vs. n = 430) was smaller than required to demonstrate cost effectiveness [36] . Previous clinical trials have reported significant clinical benefits with the NIV protocol among patients with COPD, in contrast to studies that enrolled mixed populations [3] . However, none of those studies reported impacts on HRQoL among survivors. Therefore, our findings add new insights into available evidence, emphasising that other factors may need to be considered when deciding on optimal weaning approaches for patients in critical care settings who present without COPD. When the benefits of NIV weaning on mortality were extrapolated to 5 years, the NIV protocol remained cost effective (5-year ICER £4618/QALY (INMB £6568 at a cost-effectiveness threshold of £20,000) with probability of cost effectiveness of NIV > 90%). NIV as a bridge to liberation from mechanical ventilation could be recommended on economic grounds provided that the assumptions of constant costs and health utilities beyond the trial period continue to hold in practice. The longer-term cost effectiveness of NIV is highly dependent on assumptions surrounding costs and benefits beyond 6 months. However, we have included sensitivity analyses for various parameters (including survival rates) while noting that estimated survival rates were broadly similar to those reported in the literature [33] [34] [35] . We also suggest that additional information on longer-term outcomes is needed in future studies to reduce the uncertainty around our estimates. To our knowledge, this is the first trial-based economic evaluation that compares the cost effectiveness of two protocolised weaning strategies for patients in the ICU. Previous studies have compared the cost effectiveness of NIV weaning with other strategies and in different countries and clinical settings. Chandra et al. [7] conducted a cost-effectiveness analysis of multiple interventions for COPD, including a comparative assessment of weaning protocols (NIV vs. weaning with invasive ventilation), within the Canadian setting, using a Markov probabilistic model. The results indicated that weaning with NIV dominated weaning with invasive ventilation; the probability of cost effectiveness for NIV weaning exceeded 99% at cost-effectiveness thresholds as low as $25,000 per QALY gained. Nonetheless, the analysis was restricted to patients with COPD. We also considered using the data from Chandra et al. [7] as an external input but felt that long-term extrapolations of survival could be misleading or unreliable because the populations differed. Other studies reported costs associated with NIV weaning in critical care settings [5, 6] but did not report HRQoL outcomes and were either based on small sample sizes [6] or focussed solely on patients with a COPD exacerbation [37] . A strength of the Breathe trial was that it was prospectively designed for an economic evaluation using individuallevel data. Costs and outcomes were carefully considered in the trial design with a view to reaching a robust costeffectiveness conclusion based on a large sample. However, potential limitations to this analysis do exist. First, we assumed that the baseline utility value for each patient was − 0.402, the value assigned by the UK EQ-5D-3L tariff to an unconscious health state. This assumption is in keeping with broader methodological practice for trial-based economic evaluations conducted in critical care settings [38] . Moreover, we recently demonstrated that applying alternative fixed baseline utility scores generally had no effect on incremental QALY calculations [38] . Second, approximately 35% of QALY data and 6-40% of costs (at the component level) were missing by 6 months. Had our base-case costeffectiveness analysis only considered individuals with complete QALY and cost data, we would have removed approximately 50% of patients from the analysis, which would have likely biased the results. After demonstrating that the data were missing at random, we used multiple imputation to 'replace' missing values to allow a comprehensive analysis using the whole dataset. Third, the modelling undertaken was constrained by assumptions concerning post-6-month costs and health utilities. However, a systematic search of external studies that compared these two weaning approaches revealed that none reported long-term economic outcomes. As a result, the evidence base for extrapolating cost effectiveness is currently weak. Longer follow-up would have reduced uncertainty surrounding our long-term costeffectiveness estimates.@story_separate@The results from the Breathe study indicated that early extubation to NIV did not shorten time to liberation from any ventilation. The probability of NIV being cost effective relative to weaning without NIV ranged between 57 and 59% and was higher for patients with COPD (82-87%). Future trials with extended follow-up are needed to reduce uncertainty surrounding the long-term cost effectiveness of NIV.","BACKGROUND: Optimising techniques to wean patients from invasive mechanical ventilation (IMV) remains a key goal of intensive care practice. The use of non-invasive ventilation (NIV) as a weaning strategy (transitioning patients who are difficult to wean to early NIV) may reduce mortality, ventilator-associated pneumonia and intensive care unit (ICU) length of stay. OBJECTIVES: Our objectives were to determine the cost effectiveness of protocolised weaning, including early extubation onto NIV, compared with weaning without NIV in a UK National Health Service setting. METHODS: We conducted an economic evaluation alongside a multicentre randomised controlled trial. Patients were randomised to either protocol-directed weaning from mechanical ventilation or ongoing IMV with daily spontaneous breathing trials. The primary efficacy outcome was time to liberation from ventilation. Bivariate regression of costs and quality-adjusted life-years (QALYs) provided estimates of the incremental cost per QALY and incremental net monetary benefit (INMB) overall and for subgroups [presence/absence of chronic obstructive pulmonary disease (COPD) and operative status]. Long-term cost effectiveness was determined through extrapolation of survival curves using flexible parametric modelling. RESULTS: NIV was associated with a mean INMB of £620 ($US885) (cost-effectiveness threshold of £20,000 per QALY) with a corresponding probability of 58% that NIV is cost effective. The probability that NIV is cost effective was higher for those with COPD (84%). NIV was cost effective over 5 years, with an estimated incremental cost-effectiveness ratio of £4618 ($US6594 per QALY gained). CONCLUSIONS: The probability of NIV being cost effective relative to weaning without NIV ranged between 57 and 59% overall and between 82 and 87% for the COPD subgroup. ELECTRONIC SUPPLEMENTARY MATERIAL: The online version of this article (10.1007/s41669-020-00210-1) contains supplementary material, which is available to authorized users."
"Studying epidemics is one of the most critical and fundamental investigations, especially after discovering many new diseases caused by distinct types of viruses, such as Marburg [1] , Ebola [2] , Rabies [3] , Smallpox [4] , Hanta [5] , Influenza [6] , Dengue [7] , Rota [8] , SARS [9] , SARS-2 [10] , MERS [11] , Corona [12] , ... etc. Nowadays, the world faces a disaster because of a new virus called COVID-19 [13] . SARS-CoV-2 is the main reason for this new virus (COVID-19) [14] . People with chronic diseases, such as diabetes and pneumonia, especially the elderly, are most vulnerable to the complications of this virus. Work in the area of infectious diseases has been of much interest after COVID-19 from different sides, including vaccines production [15] , statistics [16] , modeling and control * Corresponding author. [13] , and, social behaviors that affect the spread and infection of the viruses [17] . Long time ago, differential equations were used for simulation and modeling of several phenomena from life and science. The differential equations were a very powerful tool in simulation of the applications in electrical engineering, plasma physics, quantum optics, and nonlinear systems [18] [19] [20] [21] [22] [23] . Moreover, mathematical biology is one of the hottest topics that involve several phenomena treated by differential equations [24] . Population dynamics is one of the oldest subjects simulated by the differential equations in the early 15th century [25] . Differential equations have good contributions in the simulation and modeling of the epidemic diseases, with finding new controlling parameters of the transmission and spread of the viruses [16, 26, 27] . HIV is a virus that attacks the immune system of the human. Untreated HIV breaks and kills CD4 cells, which are a type of immune cell called T cells. Over time, as HIV kills more CD4 cells, the body is more likely to get various types of infections and cancers. In this study, we investigate another virus called HIV-1 [28, 29] , which is the most serious one. Based on Dr. Amesh Adalja, [30] statistical study, thirty-four million people have died of the HIV-1 https://doi.org/10.1016/j.chaos.2020.110092 0960-0779/© 2020 Elsevier Ltd. All rights reserved. since the disease was first discovered in the 1980s. However, the powerful antiviral drugs have made it possible to live with HIV for some years. Despite discovering the powerful antiviral drugs, several countries with low-and middle-income are still suffering from HIV, where 95% of new HIV infections occur. Almost more than two-thirds of the people suffering from HIV worldwide live in Africa, where nearly one in every twenty-five adults in Africa has a positive HIV. All these reasons have forced many researchers in bio-mathematics to form a mathematical model of this virus to study the exact and numerical solutions which present a clear image about dynamical behavior [31, 32] . Modeling and simulation of HIV with CD4 Cells by differential equations are reported in many papers [28, 29] . Guedj et al. studied the practical parameters that affect the transmission and spread of the HIV, with T-cells, in a model of HIV dynamics based on a system of non-linear Ordinary Differential Equations (ODE) [33] . Global behavior of delay differential equations model of HIV infection with apoptosis was investigated [34, 35] . Other papers addressed the control and stability analysis of the spread and transmission of HIV virus [29, 36] . In this paper, we investigate the analytical solutions of a biomathematical model and use these solutions to evaluate the numerical solutions. This system is given by [37, 38] 6) are arbitrary constants which represent the rate of production of CD4 + T-cells, the rate of natural death rate, infected CD4 + cells from uninfected CD4 + cells, virus producing cell's death, creation of virions viruses by infected cells, and virus particle death. Moreover, system (1) includes some primary biological models such as The HIV-1 two / three / four / general-component models including virions [39, 40] . The human immunodeficiency virus causes acquired immunodeficiency syndrome (AIDS) as well as infects, damages, and reduces CD4 + T-cells. The body gradually gets more sensitive to infections and loses its safety. AIDS is one of the most serious diseases at the present. This fractional ODE model of HIV-1 studies the non-local property that depends on both historical and current states of the problem in the contract of the classical calculus. For converting the fractional form of the models to integer form, various fractional operators, such as conformable fractional derivative, fractional Riemann-Liouville derivatives, Caputo, Caputo-Fabrizio definition, and Atangana-Baleanu (AB) fractional operator have been derived [41, 42] . Applying the following transformation with AB fractional operator [43]  to the above system 1 , where F is an arbitrary constants, yields Substituting third and the second equation of the system 3 into the first equation of the same equation yields, By fixing the value of death rate of the virus to equal zero, Eq. 4 transforms to be in the following formula This study investigates the accuracy of the obtained analytical solutions of the fractional model of the human immunodeficiency virus (HIV-1) infection for CD4 + T-cells. This model has been frequently investigated to discover more novel properties of it [44] [45] [46] [47] . However, it is the first time to check the accuracy of these studies via taking their obtained analytical solutions, then evaluating the numerical solutions of the model, and calculating the absolute value of error between the analytical and numerical solutions. The future works will be based on the same idea of all previous obtained solutions of the investigated model to show the most accurate solution of this model and to use it in the models of medical applications [47] [48] [49] . The other sections of this paper are organized, as follows: Section 2 applies the septic B-spline scheme [32, 50] based on the obtained analytical solutions via the modified Khater, extended simplest equation, and sech-tanh expansion methods to the fractional ODE model of the HIV-1 infection of CD4 + T-cells to study the numerical solutions via various explicit solutions. Moreover, some sketches of the exact and numerical solutions of the system are presented to illustrate the accuracy of our obtained solutions. Section 4 is devoted to conclusion.@story_separate@This section applies the septic B-spline scheme to Eq. 5 to get the numerical solutions of the fractional model of the human immunodeficiency virus (HIV)-1 infection of CD4 + T-cells. This scheme gives the solution of Eq. 5 in the following formula where c L , E L follow the next conditions, respectively: and  The computational obtained solution of Eq. 5 in [51] via the modified Khater method is given by where 4 δ − χ 2 < 0 . Substituting Eq. 9 with the exact solutions 10 into Eq. 5 gives (L + 7) of equations. Resolving this system leads to the following values shown in Table 1 . The computational obtained solution of Eq. 5 in [51] via the extended simplest equation method is given by where αμ < 0. Substituting Eq. 9 with the exact solutions 11 into Eq. 5 gives (L + 7) of equations. Resolving this system leads to the following values shown in Table 2 . The computational obtained solution of Eq. 5 in [51] via the sech-tanh expansion method is given by Substituting Eq. 9 with the exact solutions 12 into Eq. 5 gives (L + 7) of equations. Resolving this system leads to the following values shown in Table 3 .  This section explains and discusses more about our obtained numerical solutions of the fractional form of the HIV-1 infection of CD4 + T-cells via the septic-B-spline scheme. drug therapy) gives more explanations of the dynamical behavior of viruses. • Solving the bio-mathematical model by applying three analytical schemes (the modified Khater method, the extended simplest equation method, the sech-tanh expansion method) gives more distinct types of solutions. • Using the obtained analytical solutions to calculate the boundary and initial conditions then applying the septic-B-spline to the fractional equation with the evaluated conditions. • The accuracy of obtained solutions is illustrated by calculating the absolute value of error between exact and numerical solutions. • Comparison of the effectiveness of the adopted analytical schemes is shown in Fig. 4 . It shows that the modified Khatermethod is more accurate than the extended simplest equation method and the sech-tanh expansion method. • The septic B-spline is the only member of the B-spline family that can be applied to this kind of equation with the higher order derivative terms. Authors 1 & 2 discussed the research problem with Authors 4 & 5 first, and then approached Author 3 for his opinion. All the authors contributed in developing the main results of the paper. In fact, all authors contributed in each section of the paper, and read and approved the paper for submission. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.@story_separate@This paper has successfully applied the septic-B-spline scheme to the HIV-1 infection of CD4 + T-cells fractional mathematical model with the effect of antiviral drug therapy. This numerical investigation has been based on three analytical applied schemes (the modified Khater method, the extended simplest equation method, and the sech-tanh expansion method) to evaluate the boundary and initial conditions. Also, the accuracy of the obtained numerical solutions has been illustrated and showed the power of the modified Khater method over the other two analytical used schemes. The obtained solutions show that the used technique can be applied to various forms of nonlinear partial differential equations. Performance of the suggested technique reveals that these methods are appropriate for applying to different formulas of nonlinear partial differential equations.","This paper investigates the exact traveling wave solutions of the fractional model of the human immunodeficiency virus (HIV-1) infection for CD4 [Formula: see text] T-cells. This model also treats with the effect of antiviral drug therapy. These solutions calculate both the boundary and initial conditions that allow employing the septic-B-spline scheme which is one of the most recent schemes in the numerical field. We use the obtained computational solutions via the modified Khater, the extended simplest equation, and sech–tanh methods through Atangana-Baleanu derivative operator. The comparison between the exact and numerical evaluated solutions is illustrated by some distinct sketches. The functioning of our numerical method is tested under three computational obtained solutions."
"Diabetes Mellitus (DM) is a major public health concern worldwide and is rapidly increasing in sub-Saharan Africa, where it is projected to affect approximately 47 million people by 2045, almost three times the number in 2015. 1, 2 In Botswana, diabetes is also an emerging problem, and even when the real prevalence is currently unknown, current estimates indicate that at least 4.8% of adults aged 20-79 years were affected by DM in 2017, rising to 5.8% in 2019; 3 an appreciable increase from 0.47% reported in 2004 by the International Diabetes Federation (IDF). 4, 5 This reflects growing concerns with the increased prevalence of DM across sub-Saharan Africa (SSA) increasing mortality rates, with an appreciable number of patients undiagnosed as well as sub-optimally managed. 6, 7 The care of patients with diabetes is multifaceted involving more than just glycemic control. Randomized controlled trials have demonstrated benefits from intensive control of blood pressure and lipids in patients with diabetes, leading to the formation of American Diabetic Association (ADA) goals for blood pressure and lipids. 8, 9 An interaction between infectious and noncommunicable diseases (NCDs) has become an emergent problem across SSA given the high prevalence rates of both and is creating additional challenges for health systems in SSA. [10] [11] [12] The combination of DM and HIV infection represents a collision of two chronic conditions adversely affecting patients and healthcare systems in SSA and wider. 13 Studies have shown an increase in poor glycemic control among DM patients who are HIVinfected with only 50-54% of them meeting the ADA HbA1c goal. [13] [14] [15] [16] Similarly, Davies found that only 46% of patients with both DM and HIV achieved the ADA HbA1c goal of 7%. 17 According to Satlin's crosssectional study where 33% of diabetic HIV-infected patients had inadequate glycemic control, factors associated with inadequate glycemic control included a more recent diagnosis of HIV, use of insulin or any diabetes medication, and higher triglyceride levels. 9 Protease inhibitors (PI) use was associated with lower rates of meeting the ADA goal for HDL cholesterol among patients attending HIV clinics in New York, USA, whereas being male or not of African American ethnicity and use of an older Nucleoside Reverse Transcriptase Inhibitor (NRTI) were associated with lower rates of meeting the ADA goal for triglycerides. 9 Previous global and regional studies have also shown that metabolic control among diabetic HIVinfected patients to be determined by both sociodemographic and economic factors; with age, body mass index, waist-hip ratio, central obesity and smoking being independently associated with metabolic disorders. 5, [18] [19] [20] The past few decades have resulted in appreciable improvements in the clinical outcomes of HIV-infected patients, mostly due to highly active antiretroviral therapy (HAART). Benefits of HAART include suppression of the viral load, improvement in CD4 count, decrease in opportunistic infections, reduction of length of hospital stay, and a reduction in mortality. 21 However, the use of HAART has also resulted in an increase in metabolic dysfunction, including insulin resistance, diabetes, dyslipidemia and lipodystrophy. 22, 23 Even before the availability of HAART, Grunfeld et al and others found that patients with AIDS had elevated plasma triglyceride and free fatty acid levels. 24, 25 Several studies have evaluated the lipid profile in diabetic HIV-infected patients and most of these have shown that less than 50% of patients achieve the goal of ADA related to HDL-cholesterol and triglycerides. 9, [15] [16] [17] Diabetic HIV-infected patients have unique potential cofactors for inadequate metabolic control including the use of specific antiretroviral medications such as nucleoside reverse transcriptase inhibitors (NRTIs) and Protease inhibitors (PIs). 9, 26, 27 Although the prevalence of inadequate metabolic control in the general diabetic population has been estimated in a large number of studies, [28] [29] [30] [31] [32] [33] only a few studies have estimated this prevalence in HIVinfected patients and most of them have assessed the achievement of ADA goal for glycemic control rather than comparing the metabolic control variables among groups of diabetic, HIV-infected and HIV-negative patients. 9, [14] [15] [16] [17] 28 According to the UNAIDS report of 2016, Botswana has the third highest HIV prevalence in the world with the latest reports estimating prevalence rates of 22.2% for the general population and 24% for 15 years and above age group. 34 Botswana was the first African country to establish a National HIV Treatment Program and it has developed substantially over the last decade. 27 The key characteristic is that care is universal and free, making antiretroviral treatment available to all eligible citizens. It was estimated that in 2015 approximately 264,000 adults living with HIV were receiving antiretroviral treatment in Botswana, a coverage of 78%, up from 69% in 2013. 34 With access to treatment, HIV-related mortality in Botswana has declined from 6% in 2003 to 1% in 2011, which is encouraging. 34, 35 However, in Botswana, where DM is a significant and growing health problem along with a known high burden of HIV infection, 34 we are unaware of any documented research reporting prevalence and predictors of metabolic control among diabetic HIV-infected patients. We believe a better understanding will improve awareness of dual comorbidities and hence tailor scarce resources to the better management of patients with both diabetes and HIV infection in Botswana and among similar SSA countries with a double burden of DM and HIV-infection. This is submit your manuscript | www.dovepress.com@story_separate@Diabetes, Metabolic Syndrome and Obesity: Targets and Therapy 2021:14 particularly important at this time as the COVID-19 pandemic with various lockdown and other measures has negatively impacted on the care of patients with both infectious and non-infectious diseases across Africa and wider. [36] [37] [38] Consequently, in the first instance, we principally sought to determine the prevalence of metabolic syndrome and abnormal individual metabolic control variables among HIV-infected patients in Botswana compared to HIV-negative patients. Our secondary objective was to determine the socio-demographic and clinical predictors of metabolic syndrome and individual metabolic control variables among study participants to guide future management. A descriptive case-matched cross-sectional study was conducted from 15th June 2019 to 15th October 2019 at Block 6 Diabetes Reference clinic in Gaborone, Botswana. This is a leading clinic in Botswana offering various services to an estimated 3000 diabetic patients. Services include physician consultations, dietician, diabetes self-management, health education, eye and foot screening, laboratory services and the issuing of medicines. Within this population, cases were identified among patients with Type 2 DM who were confirmed to be HIV-infected and on treatment and matched to similar cases in terms of gender but HIVnegative. Cases and matches were enrolled at a ratio of 1:2. The sample size was calculated by OpenEpi, Version 3, an open source calculator, by using Fleiss Statistical Methods for rates and proportion. 39 The assumption used in the sample size calculation was a prevalence of 76.3% of glucose metabolic disorders among cases 12 and a prevalence of 61.8% among the controls. 40 With the intention of two-sided confidence-level (1-alpha) of 95% and power of 80% with a ratio of cases to control of 1:2, we estimated a minimum sample size for cases of HIV of 119 patients and 238 for control matches; with an estimated total sample size of 357 patients. The majority of enrolled participants were those who visited the clinic for physician consultations. Some patients were also recruited from the other services such as dietitians, eye clinic and nurse educators. Enrollment was undertaken on a daily basis from Monday to Friday. Routinely at block 6 clinic, physicians complete attendance sheets which include recent HIV status. Consequently, physicians consecutively identified patients who were HIV-infected and subsequently introduced the study and asked for their willingness to participate. Patients who agreed were subsequently directed to the research assistants for consenting and enrollment into the study. For each identified case, each doctor identified two matches by gender and also referred them to the research assistants for consenting and enrollment. Inclusion criteria for this study involved having a documented diagnosis of type 2 DM and documented results of HIV status in the past year. On the other hand, patients with type 1 DM, confirmed pregnancy, and those who did not consent were excluded from the study. After signing an informed consent form, patients were interviewed by trained research assistants. A semistructured questionnaire was used with components adopted from the WHO STEPs questionnaire. 41 Additional variables were subsequently included in the questionnaire. These included patients' demographic characteristics (age, gender, marital status and educational level) as well as clinical variables (durations of both diseases, ie, diabetes mellitus and HIV, modality of treatment for DM and antiretroviral (ARV) medication regimen (s) and duration). All recruited patients had their blood pressure taken on the day of the interview using a digital blood pressure device while in a sitting position after a 10-minute rest period. This was performed before consultations. Anthropometric measurements including weight and height were also performed in order to calculate the body mass index (BMI) of patients using the following formula: weight (in Kilograms) divided by height squared (m 2 ). Waist and hip circumference were also measured to enable calculations of waist/hip ratio for categorizing their central obesity status. Research assistants involved in measuring weight, height, waist/hip circumference were oriented on standard measurement procedures before data collection was initiated. Laboratory tests are undertaken routinely at block 6 clinic as a standard of care; consequently, patients did not incur any additional co-payment costs for these tests. These tests are usually performed in a non-fasting state at 3-6-month intervals. If patients' results of lipid profile {total cholesterol, high-density lipoprotein (HDL), low-density lipoprotein (LDL) and triglycerides} and glycated hemoglobin Diabetes, Metabolic Syndrome and Obesity: Targets and Therapy 2021:14 submit your manuscript | www.dovepress.com DovePress (HbA1c) were available and within 3 months prior to data collection, they were recorded as current results; otherwise, patients were referred to the laboratory for venesection on the same day of the interview. Blood was collected at the block 6 clinic laboratory under aseptic techniques to obtain 5-10 ml in plain tubes for the tests. The result of recent CD4 counts and viral load (for cases) were obtained from the electronic medical records and all the results were within 6 months of the day of the interview. Patients attending doctors' consultation were usually tested for either fasting blood glucose or post-prandial blood glucose (finger prick capillary blood test) routinely before they were attended; hence, these results were routinely available. The body mass index (BMI) was classified as either nonobese (<30 kg/m 2 ) or obese (≥30 kg/m 2 ) according to the WHO classification. 42 The WHO STEPS protocol was adhered to with the waist circumference measurement made at the approximate midpoint between the lower margin of the last palpable rib and the top of the iliac crest whereas the hip circumference measurement was taken around the widest portion of the buttocks. 43, 44 According to the WHO classification, central obesity was defined as waist/hip ratio of ≥0.85 for women and ≥0.90 for men. 45 Metabolic syndrome was categorized according to the new IDF criteria 46 which included waist circumference of ≥80cm for women/≥94cm for men plus at least two of the following: Triglycerides ≥ 1.7 or HDL < 1.3 for women/1.0 for men OR Systolic blood pressure (SBP) ≥130mmHg or Diastolic blood pressure (DBP) ≥85mmHg or being on treatment for hypertension OR Fasting blood glucose >5.6 mmol/l or previous diagnosis of type 2 diabetes mellitus. Individual metabolic control variables were defined according to American Diabetes Standard of Care 2019/ SEMDSA 2017 guidelines. 47, 48 The following indicated poor metabolic control: HbA1c >7.0%, LDL-C >1.8 mmol/l, HDL-C <1.0 for male and <1.3 mmol/l for female, Triglycerides>1.7 mmol/l, total cholesterol >4.5 mmol/l, fasting blood glucose >7.2 mmol/l, postprandial glucose >10.0 mmol/l, Systolic blood pressure (SBP) >140mmHg, diastolic blood pressure (DBP) >90mmHg. Statistical Package for the Social Sciences (SPSS) software version 21 was used for data entry, cleaning and analysis. Frequency and percentages were used to describe the data. We compared the proportions of metabolic syndrome and individual metabolic control variables based on gender and HIV status by means of Chi-squared test or Fisher's exact test using bivariate analysis to determine factors associated with metabolic control. A p-value of less than 0.05 was considered statistically significant. Ethical clearance was obtained from the University of Botswana, Ministry of Health and Princess Marina Hospital Institutional Review Boards. The study adhered to the principles of the Declaration of Helsinki. 49 The purpose of the study was fully explained to participants and they individually signed a written informed consent form administered in either Setswana or English, depending on which language the participant was comfortable with, prior to enrolment into the study. Patients received the standard of care and the study did not incur any costs to study participants. All participants were free to withdraw from the study at any time they wished to do so and this would not result in them receiving substandard care. To ensure patient confidentiality, the study participants were identified with unique identification numbers. Study information was stored in a secure cabinet at the University of Botswana and this was only accessible to the Principal Investigator. Studied socio-demographic and clinical characteristics including marital status, diabetes duration and history of hypertension and the use of lipid-lowering drugs were similar between HIV-infected and HIV-negative participants. There was though a significant difference in age categories between HIV-infected and HIV-negative participants (P-value = 0.011). There was also a significant difference in the regimen of antidiabetic treatment with HIV-infected participants more likely to be on combined oral hypoglycemic medications and insulin therapy regimen as compared to HIV-negative participants (37.0% versus 23.9%, P-value= 0.020) [Appendix Table 1 ]. Over half of the HIV-infected participants were diagnosed more than 10 years prior to the study ( (99.2%), were on HAART with 88.6% and 98.2% having CD4 counts of ≥350umol/l and undetectable viral loads, respectively. The rest of the characteristics of studied HIVinfected participants are depicted in [Table 1 ]. Overall, 86% of the study participants were found to have metabolic syndrome by IDF criteria. The difference in proportions of metabolic syndrome between HIVinfected (79.8%) and HIV negative (89.1%) participants was statistically significant (P-value = 0.018) [ Table 2 ]. Age was significantly associated with metabolic syndrome (P-value = 0.008) with proportions with metabolic syndrome for age group ≥40 years ranging between 87.4% −89.7% compared to 67.6% for the age group <40 years. Female gender was significantly associated with metabolic syndrome as compared to males (95.2% versus 69.0%) (P-value <0.001). Marital status, level of education, and diabetes duration were similar between participants with/ without the metabolic syndrome. HIV-infected participants with a shorter duration since HIV diagnosis (0-9 years) were more likely to have the metabolic syndrome (88.9%-92.3%) compared to those with a longer duration since HIV diagnosis of ≥10 years (67.9%-76.7%). However, the duration since HIV diagnosis was not statistically associated with metabolic syndrome (P-value = 0.056). Other HIV-infection characteristics including the duration of HIV treatment, CD4 count, and viral load count, as well as the use of protease inhibitors, integrase inhibitors and NNRTIs, were similar between participants with/without the metabolic syndrome [ Table 3 ]. Female gender showed a significantly higher proportion of low HDL-C compared to males (76.6% versus 50.5%) (P-value <0.001). Female participants were significantly more likely to be obese as compared to males (60.8% versus 19.4%, P-value <0.001). On the other hand, male gender was associated with a higher proportion of participants with elevated diastolic blood pressure compared to females (25.6% versus 11.7%, P-value = 0.001). As for glycemic control using HbA1c, 53.2% and 60.6% of male and female participants, respectively, had poor control with the difference not statistically significant. Central obesity was one of the commonest findings in both males and females accounting for 76.8% and 82.8%, respectively. The rest of the analyzed individual metabolic control variables including LDL-C, triglycerides, total cholesterol, and systolic blood pressure were similar between male and female participants [ Table 4 ]. High triglyceride levels were more common in HIVinfected compared to HIV-negative participants (68.9% versus 51.4%, P-value = 0.004). On the other hand, HIVnegative participants (52.5%) were more likely to be obese (high body mass index) as compared to HIV-infected participants (34.9%, P-value = 0.003). HIV-infected participants had higher proportions of central obesity (87.4%) as compared to HIV-negative participants (77.3%). The difference was however not statistically significant (p-value = 0.057). The HIV-infected group had higher proportions of participants with both high fasting blood glucose and postprandial blood glucose compared to the HIV-negative group; however, the difference was not statistically significant. The rest of the individual metabolic control variables were similar between HIV-infected and HIV-negative participants [ Table 5 ]. We believe this is one of the first studies that has evaluated the prevalence and factors associated with metabolic syndrome in diabetic patients with HIV-infection in a country with a high prevalence of both HIV and diabetes. The published literature is rich with studies of either metabolic syndrome in HIV-infected participants or metabolic syndrome in diabetic participants; however, not the combination. We found a very high overall prevalence of metabolic syndrome of 86% in our participants regardless of their HIV status. In addition, the prevalence of metabolic syndrome was significantly higher in HIV-negative participants (89.1%) as compared to HIV-infected participants (79.8%). Previous studies in Africa have estimated a prevalence of metabolic syndrome among HIV-infected participants ranging from 13% to 58%. 50, 51 We did not find studies on the prevalence of metabolic syndrome among diabetic patients in Africa; however, studies in India revealed a prevalence of metabolic syndrome ranging from 71.9% to 73.85%. 52, 53 Due to the fact that both HIV-infection and DM are pro-inflammatory conditions associated with metabolic disturbances including but not limited to insulin resistance and lipoprotein disturbances, a high prevalence of metabolic syndrome was expected. The fact that metabolic syndrome was highest among HIVnegative diabetic participants emphasizes the need to look closely at individual factors that impact on metabolic syndrome in this group. Older age was significantly associated with metabolic syndrome similar to several previous studies. 54 54 We found the prevalence of the metabolic syndrome to be highest during the early years of HIV diagnosis compared to later years. The difference did not reach statistical significance. Given the limitation of our study design, it is difficult to explain whether the metabolic syndrome was present before the initiation of HAART. It is possible that HAART played a part in the development of the metabolic syndrome as most patients are initiated on HAART at diagnosis, and we will be looking at this more closely in future prospective projects. Our results are similar to findings of other studies in SSA that revealed a higher prevalence of metabolic disorders after HAART initiation. 59, 60 Different classes of HAART including nucleoside reverse transcriptase inhibitors (NRTIs) and protease inhibitors (PIs) have been shown to cause the metabolic syndrome through mechanisms such as increasing insulin resistance, reducing insulin secretion, interfering with glucose transporter type 4-mediated glucose transport, lipodystrophy, alteration in leptin/adiponectin dynamics and mitochondrial dysfunction. 27, 61 The PIs have also been shown to interfere with cellular retinoic acid-binding protein type 1 (CRAB-1) which in turn inhibits peroxisomal proliferator-activated receptor γ (PPARγ). The inhibition of PPARγ leads to insulin resistance and the release of free fatty acids. 27 The prescribing of PIs was not though associated with the metabolic syndrome in our study, similar to a previous study. 12 We believe this can be explained by the small sample size as only 2/116 (1.7%) of our participants were on PIs. On the other hand, the majority of our HIVinfected participants had been on NRTI regimens containing tenofovir as it is currently part of first-line treatment for HIV patients in Botswana. 62 This may partly explain the lack of association between metabolic syndrome and NRTIs for patients who had been exposed to HAART for DovePress a longer duration in our study. Previous studies have also associated older regimens of NRTIs such as stavudine, zidovudine and didanosine with the prevalence of the metabolic syndrome; 61 with the effect noticeably after a longer exposure. However, this is not the case in Botswana as the guideline stipulates patients should be initiated on tenofovir-containing regimens unless there are contraindications. We did not find any association between the use of NNRTIs and the occurrence of the metabolic syndrome; similar to a previous study in the US. 54 Dolutegravir (DTG) which is an integrase inhibitor was introduced to HIV treatment guidelines in Botswana in 2016. 62 Previous studies comparing DTG to NNRTIs and PIs revealed that the former has more of a neutral effect on causing lipid abnormalities and it was only associated with more weight gain. 15, 63 About a quarter of our HIV-infected participants were on DTG-containing regimen and we did not find an association between the use of DTG and the metabolic syndrome. However, this finding needs to be interpreted with caution given the shorter duration of its use in Botswana. Future studies are needed to fully assess the effect of DTG on metabolic control after a longer exposure period to establish whether the previous established weight gain effect has a role to play in the development of the metabolic syndrome in patients with HIV, and we will be following this up in future studies. We did not find any difference in the level of glycemic control between HIV-infected and HIV-negative participants, which was also observed in previous studies in SSA; 18,64 however, 57.1% of the HIV-infected diabetic participants had poor glycemic control in our study as compared to 33% and 46% from studies in Tanzania and Nigeria, respectively. 18, 64 We found that 58.1% of HIVnegative diabetic participants had poor glycemic control, in contrast to findings of US studies that had 44% and 24% of patients with poor glycemic control. 65 This variation may be explained by differences in patients' characteristics and differences in cut-off points for glycemic control with the Tanzania study using a 7.5% cut-off point while we used the ADA cut-off point of 7%. On the other hand, there was a significant difference among HIV-infected and HIV-negative participants on types of antidiabetic treatment prescribed, with the former group having a large number of patients on combination therapy of oral hypoglycemic agents together with insulin therapy. It is possible that HIV-infected participants had presented with poorer glycemic control levels necessitating insulin initiation, which ultimately resulted in seemingly similar levels of glycemic control with HIV-negative participants. This though remains to be shown. Our results showing higher proportions of elevated fasting blood glucose (FBG) and post-prandial blood glucose (PPBG) are similar to the results of a prospective study by Zannou et al which showed similar trends after 24 months of follow up. 59 Similar trends have been observed in cross-sectional studies undertaken in the United States of America. 66, 67 The results of this study are similar to those of Maganga et al in Tanzania 18 but differ from similar studies in other SSA countries which showed no differences in glucose measurements between HIV-infected and HIVnegative participants. 16, 68, 69 The possible reasons for the discrepancies between the various studies include differences in patients' characteristics and methods used for assessing glucose levels. The higher proportions of higher FBG and PPBG in our HIV-infected group may be related to immune reconstitution from the use of HAART as there was a trend of increasing metabolic syndrome (FBG being a component) with higher CD4 (HDL) cholesterol. 8, 24 The same observation was made in our study whereby HIV-infected participants were found to have high degrees of dyslipidemia of 39.3%, 67.7% and 68.9% for hypercholesterolemia, low serum high-density lipoprotein (HDL) cholesterol, and hypertriglyceridemia, respectively, according to ADA criteria. These findings are similar to those of Maganga et al in Tanzania. 18 A very small proportion of our participants achieved target levels of low-density lipoprotein cholesterol (LDL-C) with 14.4% and 18.4% in HIV-infected and HIV-negative participants, respectively. This is in contrast to previous studies which had targeted LDL-C levels in 50%-66% of their participants. 18, 64, [70] [71] [72] Hypertriglyceridemia was the only lipoprotein significantly different between HIVinfected and HIV-negative participants highlighting the possibility of diabetes mellitus playing a significant role in defective lipoprotein metabolism in both groups. The significantly high level of triglycerides among HIVinfected participants on HAART as compared to HIV-negative participants has also been observed in previous studies. 54 There is a possibility of HAART contributing selectively to hypertriglyceridemia; however, this remains to be substantiated with a prospective study given the cross-sectional nature of our study. Despite having a high proportion of patients not reaching target LDL-C levels, only a quarter of our participants were on statins which is a concern. This compares to findings of Mwita et al in the same clinic in Botswana who found 45.5% of eligible diabetes patients were on statins. 73 Our findings are also worse than those of diabetic HIV-infected patients in Tanzania where approximately one-third of patients were on statins. Our results emphasize the need to remind physicians even in a tertiary care setting on the importance of prescribing statins to reduce LDL-C levels and subsequent cardiovascular events in eligible patients, and we will be following this up. In our study, female gender was also significantly associated with not reaching target HDL-C levels by ADA criteria. This is similar to findings in some studies 19, 53 but contradicts the findings of others which consistently showed a high likelihood of women being within target HDL-C compared to men. 64, 72, 74 We are not sure of the reasons behind these differences, but will be exploring this further. Finally, our study also revealed that 34.2% of HIVinfected diabetic participants were obese. This is similar to the findings of a study in the USA which found 38.2% of HIV patients were obese. 75 HIV-negative diabetic participants had a significantly higher prevalence of obesity (52.5%). This is not surprising given that a previous study in Botswana using the same cut-off point for BMI in the same city found a prevalence of 65.5% among patients attending a private hospital medical clinic year 2005 to 2015. 76 However, our results indicate a higher prevalence of obesity compared to the general population in Botswana. According to the Botswana Demographic Survey of 2017, obesity accounted for 10.9% of the surveyed population. 77 Our study has several strengths and limitations. It is the first study in Botswana to evaluate the prevalence of metabolic syndrome and individual metabolic control variables among a population of diabetic patients with HIVinfection. The study adds to the body of literature not well studied in SSA and provides an opportunity for future prospective and intervention studies in the area of metabolic control and associated complications. However, the cross-sectional nature of this study cannot provide causal relationships rather only associations; similarly, the bivariate analysis in this study cannot control for confounding variables emphasizing the need for future prospective studies. The study was also conducted in a tertiary setting whose patients' characteristics may differ from those in the primary/secondary care settings. Despite these limitations, we believe this study has found valuable findings which we are taking forward in our clinic. The data set generated and/or analyzed during this study are included in this submitted manuscript and is available from the corresponding author on reasonable request. Ethical approval to conduct the study was obtained from the University of Botswana Institutional Review Board, Ministry of Health and Princess Marina Hospital Institutional Review Boards. The study adhered to the principles of Declaration of Helsinki.@story_separate@Overall, the metabolic syndrome is an appreciable problem in this leading tertiary diabetic clinic in Botswana among both HIV-infected and HIV-negative participants, with the presence of the metabolic syndrome significantly associated with older age and female gender. Future prospective studies are warranted in our setting and similar SSA settings to enhance the understanding of the role played by HAART in causing the metabolic syndrome. In the meantime interventions such as reminding physicians to regularly assess for individual metabolic control variables in their patients and act accordingly in a multidisciplinary approach involving pharmacotherapy is warranted including increasing use of statins as well as encouraging regular exercise and improved diet. There is also a need to devise programmes specific for diabetic HIV-infected and HIV-negative participants in Botswana to curb the worrying trend of obesity, and we will be following this up.","PURPOSE: We primarily aimed at determining the prevalence of metabolic syndrome and abnormal individual metabolic control variables in HIV-infected participants as compared to HIV-uninfected participants given current concerns. Our secondary objective was to determine the predictors of metabolic syndrome and individual metabolic control variables among the study participants to guide future management. PATIENTS AND METHODS: A descriptive, case-matched cross-sectional study for four months from 15th June 2019 to 15th October 2019 at Block 6 Diabetes Reference Clinic in Gaborone, Botswana. We compared the proportions of metabolic syndrome and individual metabolic control variables based on gender and HIV status by means of bivariate analysis (Chi-squared test or Fisher’s exact test) to determine factors associated with metabolic control. A p-value of less than 0.05 was considered statistically significant. RESULTS: Overall, 86% of the study participants were found to have metabolic syndrome by International Diabetes Federation (IDF) criteria with 79.8% among HIV-infected and 89.1% among HIV-negative participants (p-value = 0.018). Older age was significantly associated with metabolic syndrome (p-value = 0.008). Female gender was significantly associated with metabolic syndrome as compared to male gender (P-value < 0.001), and with a statistically significant higher proportion of low HDL-C compared to males (P-value < 0.001). Female participants were significantly more likely to be obese as compared to males (P-value < 0.001). High triglycerides were more common in HIV-infected compared to HIV-negative participants (P-value = 0.004). HIV-negative participants were more likely to be obese as compared to HIV-infected participants (P-value = 0.003). CONCLUSION: Metabolic syndrome is an appreciable problem in this tertiary clinic in Botswana for both HIV-infected and HIV-negative participants. Future prospective studies are warranted in our setting and similar sub-Saharan settings to enhance understanding of the role played by HAART in causing the metabolic syndrome, and the implications for future patient management."
"The global spread of the novel coronavirus SARS-CoV-2 causing severe acute respiratory syndrome has resulted in social, medical and economic shocks that will be felt for years to come. The impact of the outbreak of the COVID-19 has been felt in many spheres, from international disputes among different countries, which further increased tense trading relationships, to economic stagnation or decline resulting in the collapse of many companies and unemployment [1] . Although, according to modelling research, the strict lockdown policies implemented between 2 and 29 March effectively decreased the spread of the virus averting an estimated 3.1 million deaths in 11 European countries [2] , and prevented 530 million infections in in China, South Korea, Italy, Iran, France, and the US [3] ; it also had devastating effect on economies. Strict social distancing, temporary travel bans and, in some cases, more extreme quarantine measures such as complete lockdowns have caused economic losses estimated to reach hundreds of billions of dollars. The World Bank estimated under the normal scenario that globally GDP is expected to decline by 2.1% with an estimated decline of 2.5% in developing countries and 1.9% in high-income countries [4] . They also estimated the decline in global GDP would reach 3.9% under amplified global pandemic scenarios. More pessimistic estimates state that the world economy will shrink by 6% in 2020 [5] . The prognosis for the tourism sector alone is that there will be a decrease in output of 50-70% [2] and up to 80% by some estimates [6] . Because of the economic importance of tourism, the impact of COVID-19 on travel-related industries has been studied in many countries and destinations around the world; e.g., [7] [8] [9] [10] [11] [12] [13] . It is believed that the recovery from the crisis will not only have the aforementioned direct economic implications, but it will lead to a competition between countries for tourists caused by oversupply and not matching demand [14] . The countries that are recovering from the lockdown begin to offer incentives to attract tourists; e.g., [15] . However, a long-term recovery is expected to be slow and in the process of recovery many companies will not be able to survive [14] . The economic slow-down will also impede fast recovery of the demand for tourism as leisure spending of many families will be affected. Although tourism is, in most countries, not the main economic driver, it is heavily relied on in some regions [16] . Tropical countries and those that depend on tourism throughout the year will be the most affected by the industry slowdown [17] . Empirical studies corroborate this notion. A research carried out on all American countries found that the highest risk category is principally comprised of small countries with expenditures from inbound tourism being much larger than their expenditures from outbound tourism (all of the countries of the Caribbean, with the exception of Haiti and Trinidad and Tobago) [9] . Other significant implication of COVID-19 can be found in the socio-cultural domain. The pandemic led to tensions as anti-Chinese rhetoric appeared based on China as the place of origin and spread of the virus [18] . For many tourism destinations around the world, the willingness for tourism to return is countered by the fear of potential virus transmission from tourists felt among local people. In this context, a study of three tourist cities in china (Hong Kong, Guangzhou, and Wuhan) shows that the residents are willing to pay for mitigation of the risk of COVID-19 contagion [19] . Another study looked at the opposite perspective to find out that 42.3% of tourist to Azores were willing to pay more for safer vacations [15] . The tourism industry has experienced various disruptive events in the past, none of which had a long-term negative impact on tourism [17] . Early evidence on the impact of COVID-19 suggests that its scale is much more extensive than with the earlier events. While many of the chain hotels located near beaches can afford a complete halt of activities while providing support (albeit limited) to their employees, the tourism sectors of many tropical developing countries present a greater challenge. A significant proportion of their workforce consists of informal workers [20] who are highly vulnerable to a sudden loss of income [21] . A reopening of the tourism industry is, therefore, unavoidable to protect the livelihoods and businesses. For the many countries that offer ""sun, sea and sand"" (3S) tourism, beach tourism accounts for a significant amount of their total national revenue [22] . Given this, governments feel the pressure to reopen beaches at the first signs of a drop in COVID-19 cases and perhaps even sooner, although under strict monitoring and control measures. International travel is already resuming despite many countries still being amid the pandemic [23] . As restrictions on travel are lifted, the United Nations World Tourism Organization (UNWTO) emphasizes the need for responsibility and for prioritizing safety and security [24] . This is a crucial moment to strengthen governance through promoting cooperation among local governments, civil society and academia. There is a clear need for the involvement of experts in the development of strategies to respond to crisis events such as COVID-19 [23] [24] [25] . The objective of this paper is to provide a scientific basis for understanding the key issues relevant to beach tourism management in this context. These include tourists' risk perception, environmental considerations directly related to COVID-19, and management strategies designed to limit the risk of contagion on the beach. The contribution of this paper lies in its interdisciplinary approach to deliver the latest findings (many accessed in their preprint and in-press versions), from psychology and health and environmental sciences that are highly relevant to beach tourism and the challenges presented by COVID-19. Particular attention was given to identifying knowledge gaps evident in the areas of COVID-19 risk perception, with the drivers explaining the risk-taking behavior and the protective strategies employed by beachgoers. Gaps were also found in areas such as presence of SARS-CoV-2 in bathing waters and the sand, the potential of contaminated sand being a viable route of transmission and the impact of the use of chemical disinfectants on the marine environment and on bathers. The paper identifies research prospects in these areas, additionally pointing out other questions, such as new carrying capacity methods, with the opportunity given by COVID-19 in estimation of the impacts of visitation and beach-litter.@story_separate@An integrative literature review was chosen as an appropriate method that synthesizes secondary data about a wide range of topics directly relevant to tourist beaches. Two principle criteria were employed in the selection of papers to be included in the review. First, the papers had to present the latest empirical findings to provide the most up to date information. For this reason, many of the selected papers were in press format and were yet to be published. Second, only the papers in areas directly related to beach-user safety and the environmental hazard presented by SARS-CoV-2 were considered. Although a number of studies addressed some behavioral and environmental aspects of SARS and MERS in the past, the scale and characteristics of the SARS-CoV-2 transmission are different, which led the authors to believe that the social and environmental responses are different. Hence, this study intended to include principally the studies on COVID-19, integrating the findings from SARS and MERS studies as proxies only when COVID-19 studies were not available. Because researchers are often the ones advising authorities to open/close beaches in many parts of the world, this paper sought to identify relevant research to advance their own studies and knowledge on the most relevant topics. With the continuing uncertainty about the impact of COVID-19, several possible scenarios are facing the tourism industry. These predictions range between a total collapse of the industry to an optimistic global switch to more sustainable consumption (see, for example, [26] [27] [28] ). The most pessimistic scenario assumes that the extreme social perception of epidemiological risks seen in the first months of the pandemic will remain until the development of a reliable vaccine. In this scenario, the fear of infection creates a deep fear of social contact, which indisputably would have a devastating impact on beach tourism. Public fear would lead to a further loss of travel's appeal and this would, in turn, force the authorities to increase restrictions on the use of beaches. Because of the high level of informality in the sector, this move would increase unemployment and cause consequent socio-economic conflicts because beach tourism would then be concentrated in private facilities considered safe [29] . At the other end of the spectrum, the optimistic scenario assumes a change in approach to tourism by the government and private sector [16] . The recovery from COVID-19 is most likely to overlap with the currently taking place, albeit slowly, global transformation of the current economic systems to those that have a net zero carbon footprint (carbon neutrality) [23] . As noted by Hall et al. [30] , many industry experts thus optimistically assume that the pandemic presents a transformative moment or opportunity for a new opening leading to quicker adaptation of more sustainable environmental solutions [30] . The assumption that new beach tourism will emerge also assumes that a greater social appreciation of the natural and cultural value of beaches will emerge-an awareness reinforced by a collective reflection on the pandemic. Many scientists and industry professionals see in the crisis a global opportunity to rethink old ways and reset the industry [27, 28] . However, the early evidence shows that this is unlikely to be the case and that a return to business as usual [26] or possibly even increased growth to overcompensate for losses, is the most likely scenario [17, 28] . Under this scenario, experts and scholars alike predict a domestic tourism recovery in the short term and a recovery of international tourism in the long term [28] . The shift to a carbon-neutral economy and the implementation of more sustainable practices is also unlikely because of the priority that will be given by governments and companies to the recovery from economic loss [13] . Several scholars and industry specialists have also predicted that demand for international beach tourism will be significantly reduced, but that this drop will be partially offset by domestic tourism [29] which is usually the first to recover after crisis [30] . Experts suggest that less accessible and isolated destinations will be perceived as having a lower risk of contagion and consequently will be preferred [25, 31] . People are also more likely to reduce personal mobility, choosing their local environments that can meet their various aspirations [16, 32] . They are likely to travel independently or in small groups to recover from self-isolation [25] and to avoid crowded places [33] . Beach users are likely to arrive earlier in the morning or later in the evening to avoid congestion and human contact [29] . It is still too early to confirm or reject the above predictions. Early evidence suggests that the pessimistic scenario is, at least partially, unlikely. The result of an online survey of 4000 respondents from 140 countries carried out by Bloom Consulting [31] states that 35 and 45%, respectively, declared that they would not travel unless the virus is either controlled or fully eradicated. The remaining 65% were willing to travel. The optimistic scenario seems equally unlikely. A transformation in tourism is possible only through changes in both the demand and supply side of the market [26] . Some beach destinations might, in the long term, make a switch to more sustainable consumption and some consumers might reassess their priorities. At present, however, news articles indicate a complete lack of change in the approach of tourists and the industry in some of the countries most affected by the COVID-19 crisis, such as the United States of America, Spain, Portugal, Canada, Brazil, and the United Kingdom (UK) [34] [35] [36] [37] [38] . This is not only the case with respect to domestic tourism. In Spain, local governments in various famous beach destinations had to close their beaches because of the arrival of tourists from the UK [39] . Because of the long period of isolation and warm summer weather in the northern hemisphere, thousands of people have ignored social distancing and government recommendations and have flocked to local beaches in great numbers [38] . The decisions made by tourists about their chosen activities and travel destinations are influenced by their perceptions of safety and security [40] . Risk perception is a subjective psychological construct that is influenced by cognitive factors (knowledge and understanding of risks), emotional and experiential factors (personal experience), and social, cultural, and individual factors such as gender, education, and ideology [41] [42] [43] [44] . In general terms, studies show that the more risk people perceive the less likely they are to visit public places such as beaches and the more likely they are to cooperate with government recommendations and to adopt health-protective behaviors [42, [45] [46] [47] [48] . As of the time of writing, various studies had been published on the perception of the risk of contagion by COVID-19. Dryhurst et al. [42] provided an international analysis of COVID-19 risk perception amongst 6991 individuals surveyed across ten different countries. The determinants of an individual's risk perception were their experience with the virus, social amplification (hearing about the virus from friends and family), pro-sociality, individualistic worldviews, self and collective efficacy, trust, and personal knowledge. Other researchers identified the voluntariness of taking the risk to be another factor that negatively affects the perception of risk [41] . Wise et al. [48] surveyed 1591 individuals in the United States of America (USA). The results show that in the early stage of the pandemic subjects perceived their risk of being infected as relatively high and as even higher five days later when the number of infections in the country increased. Interestingly, people rated the average person in the USA as having the highest risk of infection, but themselves as having the lowest risk (optimism bias). Similar results were obtained by Gerhold [49] in the UK and Kuper-Smith [50] and the UK, USA and Germany. In both cases, those who reported lower-perceived risk also reported low engagement in protective behaviors and information seeking. De Bruin and Bennett [45] explored the perception of the likelihood of infection and subsequently fatality and found that the median perception of the risk was higher in the case of infection than in case of infection fatality. This was consistent with the results of a similar study by Gerhold [49] . The study also confirmed the finding observed by Wise et al. [48] that the perception of risk increased with time as a result of increases in the number of infections and/or information campaigns. The above-mentioned studies suggest that besides the cultural factors, beachgoers are likely to underestimate their risk. This may be due to the voluntary nature of taking the risk, optimism bias, poor social amplification, lack of reliable information or having no direct experience with the virus. Although the perception of risk of infection is relatively high among the general population, the perceived risk of fatality is much lower, especially among younger people who are most likely to visit a beach. This is consistent with studies on adolescent risk-taking (e.g., [51, 52] ). Because closed indoor spaces are perceived to present the highest risk for virus transmission, while outdoor places are perceived to be safer [49] , beaches are likely to be considered safe. Studies also show that when the number of positive cases and fatalities decreases, the perception of risk may also decrease. The perception of risk is also likely to decrease in the absence of significant changes in the number of infected cases. As explained by habituated action theory, this takes place where there has been prolonged exposure to the risk without a negative outcome [53, 54] . On the contrary, when the situation gets worse, the perception of risk may increase [48, 49] . This would explain the mass beach visitation in the UK and other locations. When the rate of coronavirus deaths and new infections fell considerably and the lockdown restrictions were eased, the risk perception among beachgoers dropped dramatically ""as if the last three months of fear and caution over the coronavirus vanished overnight"" [38] . The most concerning for beach management in times of pandemic is the consistency of studies confirming a positive relationship between risk perception and the likelihood of engaging in preventive measures. If people who go to beaches do not perceive the risk, they are also likely to ignore preventive measures. This then further entrenches a general community perception that the activity is low-risk-a result explained by social action theory [52] . The need for management strategies that are effective and controllable is thus indisputable. This section of the paper is dedicated to environmental considerations brought to light by COVID-19. There have been several unexpected environmental impacts that might have dire consequences on beachgoer safety and entire beach tourism destinations. The sudden desertion of tourist beaches has had a short-term positive environmental impact on coastal environments that have had time to recover from human pressures [55] [56] [57] . The resulting visual and acoustic qualities of undisturbed coasts have been noted in the media (see, for example, [58, 59] ) and by early empirical research papers that confirmed these observations [57] . It is currently unknown whether these changes are substantial and long-term. Besides the obvious temporal consequences, the pandemic also brought about various threats. Because of the high level of risk perceived by some individuals, it is safe to assume that the number of visitors to rural and natural beaches-which are usually less congested-will rise. Even a relatively small increase in visitors to beaches that are highly susceptible to anthropic pressures will have a greater environmental impact than a similar increase on urban beaches [60] . The increased visitation rate may cause not only an overload of scarce public facilities on these less visited beaches but may also encourage the construction of public infrastructure and facilities to cope with the demand and to ensure visitors' safety (this might include parking, mechanical cleaning, and lifeguarding). These changes would have a significant environmental impact [61, 62] . The potential impact wastewaters being infected by SARS-CoV-2 is highly relevant in the developing country context [63] . Despite the hostile conditions of the marine environment, viruses can maintain their infectious capacity in the water for prolonged periods, reaching the aquatic environment via sewage discharges [64, 65] . They can also remain on solid particles suspended in the water column or on sand particles on the beach [66] . According to a recent review study by Kitajima et al. [67] , an increasing number of studies have demonstrated the presence of SARS-CoV-2 in stool from patients confirmed as having COVID-19. This discovery indicates yet another route of contagion-fecal transmission. Preliminary results confirmed the presence of the virus in raw sewage [68] [69] [70] [71] . The lack of treatment of domestic sewage and the discharge of rainwater directly or indirectly through rivers to the sea is a serious threat to the people utilizing that water in many coastal locations [72, 73] . These people may get infected through either ingestion, inhalation, or dermal contact. Most bathing water monitoring programs include an evaluation of the concentration levels of microorganisms from fecal pollution (enterococcus and E. coli) [66] . On many tourist beaches, these levels surpass the maximum permitted thresholds after rainy days when rainwaters mixed with untreated or partially treated sewage enter the sea [74, 75] . Despite the bacteriological monitoring carried out by local authorities on many tourist beaches, there are currently no protocols for the detection of SARS-CoV-2 in the water or sand on a beach. The second environmental consideration that should be closely analyzed by local authorities and beach managers is the impact of the chemicals that are used in huge quantities to disinfect public spaces [76] . The conventionally used quaternary ammonium surfactants and their derivatives are capable of inactivating the SARS-CoV-2 virus [77] . However, these substances also affect the marine organisms when transported to the ocean via sewage and rainwater drainage systems [78] . They can cause an ecotoxicological impact that could reduce the populations of some animal groups (e.g., algae, fish, mollusks, barnacles, rotifers, starfish, shrimp and others) [79, 80] . Some of these organisms are responsible for removing significant amounts of organic matter, as well as chemical and biological pollutants [77] . This would harm the environmental quality of beaches, which is a critical point in attracting visitors [60] . Finally, there is an issue of increased usage of protective gear such as gloves and masks that are often inappropriately disposed of [56, 81] . These items when discarded can be transported to the sea by wind or rainwater either directly or indirectly through rivers, which are considered one of the main sources of beach-litter [82, 83] . In coastal areas without good rainwater treatment, these potentially contaminated items are likely to end up in the sea and/or be washed out on beaches presenting a hazard to marine organisms such as crustaceans, mollusks, birds, turtles and fish that ingest microplastics from degraded synthetic material [84] . The presence of these items has already been reported in different locations on the planet [85] [86] [87] . However, it should be also noted that the absence of beachgoers, one of the main sources of litter on beaches [88, 89] , has a contrary effect. An evaluation carried out on Brazilian beaches showed lower than usual concentrations of plastics and other waste typically found in the dry strip of sand where most bathers are usually set up [90] . Similar findings were presented by Ormaza-González and Castro-Rodas [57] in the case of Ecuadorian beaches. Integrated beach management must address not only the immediate challenges of ensuring the safety of beachgoers and the appropriate environmental conditions to reopen beaches, but should also include long-term strategies that respond to the uncertainties posed by a prolonged pandemic (Table 1) . It should be recognized that the most appropriate strategies will differ depending on the beach type (natural, urbanized or resort) and local conditions [91] . Because the paper focuses on issues related to COVID-19, many general beach management strategies are not discussed. Currently, several interesting, albeit often unfeasible, ideas to ensure beach-user safety are circulating online. Beach booths made of plexiglass isolating beach-user groups from each other were reported to have made an appearance in Santorini, Greece [92] and to have been considered on various Italian beaches [93] . Although acrylic sheets withstand exposure to high levels of UV light, saltwater spray, and significant temperature changes without shattering, fogging or losing transparency, the idea is unrealistic. The booths are costly, reach unbearably high temperatures inside even with an open roof, and their daily cleaning requires considerable effort and chemicals. On the other hand, some more realistic voluntary measures have been proposed to deal with the requirement of social distancing on a beach. The most common is social distancing tapes or circle templates that are used to mark off an area around beach users [94] which are being sold on some beaches in Spain [95] .  Implementation of a rotation system for small enterprises on the beach Individuals who have the COVID-19 virus may be presymptomatic or asymptomatic. To avoid transmission in these cases, the bulk of management strategies center on ensuring social distancing. At most beaches in the world that still allow beach visitation, toilets have been closed to minimize the risk of skin contact with potentially infected surfaces. Because beach users tend to concentrate in areas with a wide range of facilities such as drink or snack stalls, kiosks, showers, etc., their closure or separation is recommended. In urban and resort beaches, social distancing can be achieved through a greater separation of deckchairs, parasols, beach huts or tents for rent. The beaches that use wooden catwalks for visitors' movement on the beach should provide two lanes separated by at least a 2-m gap to limit close contact with those passing in opposite directions. Finally, when full protective measures cannot be implemented on the entire beach, a zoning system may be considered, separate areas for different user groups with stricter measures only in areas designated for high-risk groups such as the elderly. Commonly employed in the hospitality sector are strategies related to service delivery such as the separation of tables, provision of mostly outdoor dining spaces, and requiring online reservations. Chinese tourists who experienced the 2003 SARS epidemic also employed voluntary measures such as ordering delivery or takeout food [25] , although this can also increase waste generation. Finally, local governments can opt for strategies centered on spreading tourists across different locations by promoting beach alternatives or developed beaches. In many places around the world, the beach product is inseparably related with beach resort operations. According to the preliminary findings of a longitudinal study by Gursoy and Chi [96] , hotels are likely to see a slower post-pandemic recovery due to a reluctant attitude of potential customers (over 50% of respondents) to dine and stay in hotels. However, as the industry is recovering, it is important to highlight a number of strategies that are applicable to these indoor spaces. Because hotels and tourism accommodations are equally susceptible to contagion any indoor space visited by large numbers of people and a high degree of interaction, besides the general prevention measures that are usually compulsory for all hotels such as frequent hand washing, physical distancing, wearing a mask and frequent disinfection procedures [97, 98] , the response from hotel managers should include a set of more specific strategies. Based on the experiences from the SARS pandemic, scholars point out the value of having a disaster management plan with standard operating procedures (SOPs) that can greatly enhance the effectiveness of the response [99] [100] [101] . Existing SOPs that do not include epidemiological risk should be adjusted and implemented in accordance with the recommendations of local and national public health authorities with the aim to prevent cases, effectively manage cases, and mitigate impact among clients and staff, including cleaning and disinfection of rooms occupied by ill persons [98, 102] . It is advisable to keep a logbook of actions and measures carried out that can be evaluated and used to further improve the SOP [100] . Because the reception desk staff belong to the highest risk of infection group, if possible, staff members should not be older or with underlying health conditions. Reception desk employees should be equipped with disinfectant/wipes for surface cleaning, disposable gloves and apron, full-length long-sleeved gown and biohazard disposable waste bag [102] . The general recommendations of the WHO [102] for hotels state that they pay a special attention to disinfecting surfaces that are frequently touched such as handrails, elevator buttons, handles, doorknobs, computer keyboards, key cards, writing utensils, toilets, handwashing basins, baths, etc. [98] . Guests should avoid handling food at the buffets and special care should be given to the disinfection of the commonly used surfaces such as drink dispensers after each use. The concentration of water disinfectants in swimming pools should be kept at the upper recommended limits. The WHO also recommends to have a maximum of four persons for 10 square meters and separation of sitting guests of at least 1 m. Consideration should be given to closing the recreational areas for children, as well as other public areas and facilities such as gyms [100] . Resorts should provide guidelines to the staff on how they should communicate the action plan to guests. Informative posters and leaflets can amplify the key messages among guests and staff. Communication strategies not only increase the awareness about the risk, but also decrease their fear and increase confidence among visitors and staff members [103, 104] . Moreover, the resort staff should undergo the training required for understanding and adopting the measures that could protect their health and that of others [98, 100, 102] . It is especially important because many standard procedures that has been followed for years are likely to change (cleaning, washing, operating cafeteria, communication with guests, etc.). Technology is seen as an alternative solution to deal with the actual and perceived risk of the virus contagion in the hotel industry [96, 97, 100, 103] . The internet-based communication technologies are currently used in hotels around the world at the management level for work-related communication and customer interaction with hotel staff and facilities (self-check-in, voice control of room service, etc.) [100] . However, the implementation of more advanced technologies such as infrared scanners, app-based keyless doors, touchless elevators, digital menus, automatic water/soap/drink dispensers, kiosk check-in machines, robot cleaning systems, electrostatic sprayers, ultraviolet-light technology, and advanced heating, ventilation, and air conditioning (HVAC) systems [96, 97, 100, 103] , dependent on the ""good will"" of hotel owners and the availability of appropriate resources [97] . According to a recent study, both restaurant (64.71%) and hotel (70.42%) customers indeed believe that these technologies will be necessary to minimize human-to-human contact, and many of them (30-40%) are even willing to pay more for increased safety precautions [105] . Other empirical studies confirm this notion stating that the technology-mediated solutions lead to lower levels of perceived health risk among hotel visitors [103] . The absence of data about the possibility of SARS-CoV-2 contagion through seawater, and a lack of protocols for the detection of viruses in the water and sand on a beach means that a precautionary approach is recommended. The established relationship between fecal bacteria and viruses, including SARS-CoV-2, requires that bathing should be banned on beaches exposed to significant discharges of untreated sewage and rainwater runoff. On urban beaches that record high levels of fecal contamination after rainfalls, a no-bathing period of at least 72 h is recommended after rain [64] . In the absence of water-quality monitoring, access to the sea should be closed until more information about the virus and fecal contamination is available. In the early stage of the pandemic, when a timely response was vital, public authorities scrambled to secure any type of disinfectant substances. There is evidence, albeit anecdotal, of the use of highly toxic substances such as sodium hypochlorite (bleach) to disinfect public beaches [106] . The only beach cleaning methods that are recommended are the aeration of sand to allow the UV light to act as a natural disinfectant [107] or the adding of ozone to mechanical beach cleaners to protect the sand from bacteria and viruses [108] . In terms of the disinfection of other public spaces, public authorities should carefully review the usage of substances based on quaternary ammonium surfactants that can enter the sea through rainwater runoff. The concept of beach carrying capacity (BCC) has become particularly relevant. Commonly criticized as imprecise to calculate and hard to control, it is still one the most important tools for visitor management in beach environments [109] . BCC allows authorities to determine the maximum beach load based on user densities or availability of parking spaces. The BCC is especially important as it allows a calculation of the space, measured in square meters, required by visitors for their psychological comfort (psychological carrying capacity). The BCC concept has been criticized as difficult to implement due to limits on the ability of authorities to control the number of visitors on multiple-access public beaches. The suppression of parking spaces has been the most common strategy to limit capacity [109] . However, due to the restrictions posed by the COVID-19 pandemic, stricter control measures are not only advisable but also widely acceptable. For control over the number of visitors a ""one entrance/exit"" strategy is recommended, along with measures to create separate lanes of people and limit close contact between people passing in opposite directions. When establishing the maximum user density using BCC factors to consider are the usable beach area, visitors' spatial patterns (visitors tend to use the first 30m from the sea) [110] , and social groupings (social ""bubbles"" requires less space overall). Once established, the BCC can be managed through a reservation system, as has been proposed in countries such as France and Spain, to control the number of visitors to their mostly urban and resort beaches [111, 112] . The alternative to reservations is a tracking system such as that implemented in Belgium that indicates the density of visitors on specific beaches and reroutes them to less populated areas [111] . The social distancing of visitors on the beach can be controlled by implementing roped-off grids varying in the area and based on party size [112] or through the voluntary measures such as distancing tapes described earlier. Enforced separation of user groups may not be required as long as the total number of visitors on a beach is controlled and there is enough space; even before COVID-19, beachgoers usually voluntarily practiced social distancing through crowd-avoidance self-regulation [110] . Social carrying capacity studies on beaches established that, depending on the beach type, beachgoers require an area of between 8 m 2 and 22 m 2 per user for a pleasant experience [110, 113, 114] . The lower threshold, applied mostly to urban beaches, is likely to increase given that psychological comfort depends on a beach user's level of perceived risk in different stages of the pandemic. When calculating the BCC, it is thus advisable to consider both the physical distancing and psychological comfort. Protection motivation theory states that engagement in protective measures is significantly influenced by high levels of perceived risk [45, 46] . Because perceived risk is influenced by the knowledge and understanding of risks by the people who perceive it and on social amplification [42] , obtaining information from a variety of sources, such as the government and media, is of crucial importance [47] . Public authorities have to work closely with media outlets to share knowledge and build confidence to reduce fear [41] . Transparency about scientific uncertainty does not undermine public trust [115] , hence ""clear communication of risk could aid the development of accurate risk perception, in turn facilitating engagement in protective behaviours"" [48] (p. 10). The provision on the beach of information about the virus itself and of the measures taken by beach management to ensure safety are two strategies designed to regaining visitor confidence. Signage with information on the risk of contagion and recommendations for appropriate behavior, including the beach's carrying capacity and allowed beach-user density (if controlled), should be displayed. Risk perception studies show that there is a subset of individuals who do not seek out information and can only be targeted by direct outreach methods such as emergency alerts on cellphones [48] . The objective of a beach management organization (BMO) is to regulate the interrelations between different elements in a coastal system intended to achieve a balance between economic activities and the environmental health of the beach [116] . A BMO assumes the central decision-making role, coordinating the often conflicting interests of stakeholders and mediating conflicts when necessary. In a way, BMOs are similar to destination management organizations (DMOs), but because they operate on a much smaller scale they are more focused on beach management and conflict resolution and much less on marketing and destination-level tourism planning. The integration of institutional and private stakeholders into a BMO allows for greater coordination of decisions about crisis response strategies that are both suitable and as fair as possible to a variety of beach stakeholders. These strategies include the implementation of a rotation system for the operation of beach microenterprises (e.g., food stalls, peddlers, and equipment rentals) ensuring an equal opportunity to earn at least a partial income. The benefit of the active involvement by a variety of stakeholders and local communities is that they are all then more likely to assume personal and collective responsibility for applying response measures [117] . BMOs, especially on certified beaches, can also exercise their institutional power by pressuring government agencies to provide funds and to take specific actions [118] to ensure the best possible response to the pandemic-including such measures as providing protective gear, hiring more staff, and involving volunteers. Other relevant advantages of BMOs is their ability to generate cooperative relationships between small and medium beach enterprises and beach peddlers to take advantages of economies of scale in purchasing goods and protective gear and thus maximize their potential income [29] . In times of crisis, resources are allocated to new priorities and research teams around the world scramble to develop science-based solutions to ensure that the most effective measures are taken. The coronavirus pandemic introduced many new and complex problems for which the scientific community has no answers. Scholars have already called for interdisciplinary approaches to deliver scientifically grounded feasible solutions [25] that would increase the safety of citizens and at the same time provide a viable way to engage in familiar tourism and leisure activities. In the context of beach tourism, there are many gaps in knowledge that need to be addressed to ensure a safe beach experience. This section focuses on these gaps in the hopes of sparking interest within the scientific community to research in a safe and precautious manner using such methods as online surveys and scientific webinars. COVID-19 risk-perception studies have confirmed that individuals who perceive less risk are less likely to cooperate with government recommendations and to adopt protective measures [25, 42, [45] [46] [47] . Given the evidence that beaches still attract thousands of visitors even in countries that are experiencing increasing COVID-19 cases, there is a need to investigate risk perceptions, with the drivers explaining the risk-taking behavior and the protective strategies employed by beachgoers. Researchers may want to understand why a subset of individuals do not engage in seeking information and protective measures to develop direct outreach methods on beaches. In terms of environmental considerations, beach safety must include investigating the presence of SARS-CoV-2 in bathing waters and the sand, especially on those beaches that receive untreated or partially treated domestic sewage. On the other hand, because viruses are known to have the ability to survive for extended periods on sandy beaches under conditions of low light and high humidity, there is a need to estimate the lifespan of SARS-CoV-2 in beach-type conditions and to consider the efficiency of sand aeration and UV solar radiation in deactivating the virus. Currently, there is still no information on the ability of the virus to remain viable in seawater, although early evidence from Spain indicates this presents a low risk [119] . Likewise, no studies have been published about contaminated sand being a potential route of transmission. Research gaps are also evident in the field of beach ecotoxicology [76] . Further studies are required on the impact of the use of chemical disinfectants and possibly safer biological alternatives on the marine environment and on bathers who are exposed. Given the recent media coverage, it seems that many coastal ecosystems have recovered quickly, within a few months of the halt of tourism activities. Whether this recovery has been substantial remains unknown. Before the pandemic, the possibility of investigating the resilience of many intensively used beaches was very limited. There is now also an opportunity to estimate the impact of visitation by comparing the environmental conditions of partially recovered beaches (the baseline) with their post-pandemic condition when visitation resumes. A very similar opportunity is presented in the field of beach-litter assessment. The proportion of marine litter that can be attributed specifically to beach users has been very difficult to estimate as there are many possible litter sources [120] . There is a considerable amount of data regarding litter on extensively used tourist beaches collected during the summer high season that cannot easily be compared with data collected in the low season when environmental conditions are very different [121] . The existing summer data can, however, be compared with new data collected on beaches that, because of COVID-19, are relatively empty during the same period. Additionally, the pandemic's impact on the quantity of marine litter washing up on beaches can be estimated. The beach carrying capacity tool also requires further research attention as it remains the primary means of calculating the maximum allowed number of beach users at any given time. The conditions presented by COVID-19 require a re-evaluation of the tool. Health and safety recommendations and environmental factors need to be taken into consideration. Now, more than ever, the estimation of a beach's social carrying capacity (SCC) is of the utmost importance as it takes into account the beach users' perceptions of risk as relevant to their perceived safety and hence the overall quality of the experience. There is also an evident need for research into the addition of a new component to the carrying capacity calculation that is centered on the epidemiological risk posed by COVID-19. The epidemiological carrying capacity (ECC) should consider the space required by each beach-user group to limit contagion but should also consider the issue of beach access. Provision should be made for well-spaced one-way entrance, exit and walking paths to limit the chance of people coming into close contact with one another as they move about. Additionally, given the propagation characteristics of SARS-CoV-2, the calculation of the ECC at a beach should consider social groupings or bubbles rather than only individual users. The visitor part size could be calculated by analyzing data from the reservation systems that are becoming a popular means for controlling visitor numbers. The grid system being implemented on many beaches should take into account both the ECC and the SCC, as these components are likely to determine the final carrying capacity-except perhaps in the case of highly natural or protected beaches. Further research is required on visitor-flow patterns and SARS-CoV-2 transmission characteristics to guide new methods for calculating beach carrying capacity. The research has two major limitations that stem from the fact that the SARS-CoV-2 pandemic is still a recent phenomenon and the body of relevant research, although growing fast, is still limited. Therefore, the findings from a limited number of studies may not be conclusive and contradictory results from future studies can be expected. Although the majority of the topics addressed in this paper and not new and were addressed previously in relation to the SARS and MERS pandemics, the scale and characteristics of SARS-CoV-2 transmission are different, which is likely to produce different social and environmental responses. For this reason, the studies carried out on SARS and MERS in the past that were used in this research as proxies in the absence of SARS-CoV-2 research may not be completely accurate.@story_separate@The SARS-CoV-2 epidemic has already had unprecedented social consequences causing economic losses estimated to reach hundreds of billions of dollars. Of the possible future scenarios, the evidence suggests that a sustainable reopening of the tourism industry is unlikely. Rather, and more pessimistically, it seems inevitable that the priority given to economic recovery will lead to the more pessimistic business as usual scenario. Many governments have already decided to reopen tourist beaches, even without a drop in COVID-19 levels. Considering the economic importance of 3S tourism for many destinations around the globe, governments, civil society and academia alike must act to develop and implement viable solutions to face the crisis. In the interests of more quickly answering the key pandemic-related questions facing beach tourism destinations, this paper has focused on the latest findings from various disciplines highly relevant to beach tourism, pointing out the knowledge gaps and research opportunities. The studies of COVID-19 risk perception demonstrated that there is a clear relationship between an individual's likelihood of cooperating with government recommendations and their adopting health-protective behaviors. Outside of cultural factors, beachgoers are likely to underestimate their real personal risk because of the voluntary nature of taking the risk, optimism bias, poor social amplification, a lack of reliable information, and no direct experience with the virus. These findings are especially relevant in the context of public spaces where the risk of contagion is high. The pandemic has also highlighted various, in some cases unexpected, environmental threats, impacts and concerns. These include the impact of the increased number of visitors to rural and natural beaches, the contamination of bathing waters by the virus, the lifespan of the virus on the sand, the impact of chemicals used for disinfection, and the pollution caused by discarded protective gear. Various strategies can be used to deal with these threats including imposing a bathing ban on beaches exposed to significant discharges of untreated sewage and rainwater runoff, reviewing the use of certain disinfectants, and using aeration and sand ozonation techniques. There is a range of strategies for beach-user safety centered on ensuring the physical distancing of visitors and management of their risk perception. These include closure or separation of certain facilities, providing new service delivery options, using online reservation systems, promoting alternatives to the beach, implementing a strict carrying capacity, providing information to manage risk perception, and establishing a beach management organization. Though some of these strategies have proven to be effective, many have never been implemented and tested in COVID-19 conditions. The future lines of research should be focused on investigating risk perceptions specifically in the context of COVID-19 as it has proven to have different transmission rates and hence perceive risk from the previous viruses (SARS, MERS). The drivers explaining the risk-taking behavior, protective strategies employed by beachgoers, and measures to develop direct outreach methods on beaches also require investigation. In relation to the environmental considerations, the presence of SARS-CoV-2 in bathing waters and the sand and the potential of contaminated sand being a viable route of transmission, as well the impact of the use of chemical SARS-CoV-2 disinfectants on the marine environment and on exposed bathers, require further investigation. Finally, studies that adapt the concept of carrying capacity to meet the safety requirements are warranted. With COVID-19 becoming a new normal, researchers have to act quickly to confront its challenges.","The strict quarantine measures employed as a response to the COVID-19 pandemic have led the global tourism industry to a complete halt, disrupting the livelihoods of millions. The economic importance of beach tourism for many destinations has led many governments to reopen tourist beaches, as soon as the number of infection cases decreased. The objective of this paper is to provide a scientific basis for understanding the key issues for beach tourism management in these circumstances. These issues include risk perception, environmental considerations directly related to beaches and COVID-19, and management strategies designed to limit the risk of contagion on the beach. The contribution of this paper lies in its interdisciplinary approach to delivering the findings from the latest studies, highly relevant for beach tourism, in psychology, health science, and environmental science (often in preprint and in press format). Particular attention was given to identifying the knowledge gaps evident in the areas of COVID-19 risk perception, with the drivers explaining the risk-taking behavior and the protective strategies employed by beachgoers. Gaps were also found in areas such as the presence of SARS-CoV-2 in bathing waters and the sand, the potential of contaminated sand being a viable route of transmission, and the impact of the use of chemical disinfectants on the marine environment and on bathers. The paper identifies research prospects in these areas, additionally pointing out other questions such as new carrying capacity methods, the opportunity given by COVID-19 in estimation of the impacts of visitation and beach-litter."
"The cerium-based heavy fermion materials, which exhibit a variety of fascinating and exotic properties (including topology, unconventional superconductivity, quantum criticality, mixed-valence behavior, Kondo physics, and so on), have renewed a lot of interests in recent years 1, 2 . It is generally believed that the physical and chemical properties of ceriumbased heavy fermion materials are governed by their 4 f electronic structures, which are very sensitive to the surrounding environment, such as external pressure, temperature, element substitution, and electromagnetic field, etc. For instance, Ce 3 Bi 4 Pt 3 (a noncentrosymmetric Kondo insulator), is such an archetypal heavy fermion compound 3 . Experimentally, it has been found that a phase transition from topological Kondo insulator (TKI) to Weyl-Kondo semimetal (WKSM) could be realized via simple Pt-Pd substitution, i.e., from Ce 3 Bi 4 Pt 3 to Ce 3 Bi 4 Pd 3 4 . Further theoretical calculations suggest that the underlying mechanism of the TKI-WKSM transition is the large mass difference between Pt and Pd atoms, which leads to a big discrepancy in the strength of spin-orbit coupling, and thus has an unprecedentedly drastic influence on the hybridization between 4 f and p electrons 5 . Very recently, high pressure X-ray diffraction and electrical transport measurements for Ce 3 Bi 4 Pt 3 reveal that uniform compression can enhance the f − p hybridization and 4 f electron delocalization, and finally lead to closure of the Kondo gap 6 . Besides chemical doping and external pressure, it is discovered that strong magnetic field is capable of suppressing the Kondo gap as well, and inducing a Landau Fermi-liquid (metallic) state in Ce 3 Bi 4 Pd 3 7 . These experimental facts manifest that Ce 3 Bi 4 Pt 3 and its substitution series Ce 3 Bi 4 (Pt 1−x Pd x ) 3 (0 ≤ x ≤ 1) are versatile platforms for studying the interplay between topology and electronic correlation under the influence of external conditions. In addition, CeT In 5 (T = Co, Rh, and Ir) 8, 9 and CeM 2 Si 2 (M = Ru, Rh, Pd, and Ag) 10 are also classic examples for examining the intriguing properties of cerium-based heavy fermion materials. In the present work, let's turn to another interesting series of cerium-based heavy fermion compounds, namely cerium monopnictides CeX, where X=N, P, As, Sb, and Bi. These compounds crystallize in the rock-salt structure (see Fig. 1 in which the Ce atoms form a face-centered-cubic Bravais lattice, while the X atoms occupy the octahedral voids in the lattice 11 . Owing to the peculiar electronic and magnetic properties (see Table I respectively. The comparatively small crystal field splitting enables the random distribution of spins for CeSb and CeBi, which could be easily influenced by temperature or magnetic field. Such anisotropic exchange interaction is regarded as an important factor for driving these magnetic transitions 11 . Considerable experimental progresses have been achieved on CeX compounds to disclose their electronic structures and magnetic properties. Their band structures, density of states, and Fermi surfaces have been extensively studied by using the photoemission spectroscopy (PES), angle-resolved photoemission spectroscopy (ARPES), optical conductivity, and de Haas-van Alphen (dHvA) quantum oscillation [18] [19] [20] [21] . As for CeSb, high resolution ARPES data demonstrate the change of Fermi surface topology during the magnetic phase transition [22] [23] [24] [25] [26] [27] [28] and imply the dual nature of 4 f electrons (being itinerant or localized). On the theoretical side, the experimental photoemission spectra of CeX have been roughly reproduced by first-principles calculations [29] [30] [31] [32] [33] [34] . Particularly, the mixed-valence nature, lattice dynamics, and elastic properties of CeN 35-37 , the band structure and Fermi surface topology of CeSb 38 are well studied. In most cases, the 4 f electrons are assumed to be localized for heavier CeX. The f − p mixing model [39] [40] [41] based on the anisotropic hybridization between the Ce-4 f level and the ligand X-p band is widely utilized to explain the complex antiferromagnetic ordering phases 11 . However, since the 4 f electrons are usually correlated, the traditional first-principles approaches often underestimate the electron correlation and can not formulate a reliable physical picture of the 4 f electronic structures of CeX. Furthermore, large spin-orbital coupling and intricate magnetic ordering states make the theoretical calculations quite difficult. Consequently, it seems tough to acquire an accurate and comprehensive description for the electronic structures of CeX. Though much effort has been devoted to understanding the unusual properties of CeX in past decades, there are still lots of issues and questions that need to be solved and answered. First of all, how do the 4 f electronic states evolve when X goes from N to Bi? In general, the lattice constants and strength of spin-orbital coupling should vary with respect to X's atomic mass. The hybridization between Ce-4 f and X-p bands should be modified as well. We suspect that these changes could probably drive a 4 f itinerant-localized crossover or transition in this series. Second, how to explain the complicated magnetic ordering states in CeSb and CeBi? Are they related to the increment of 4 f electronic localization or anything else? Third, it is suggested that valence state fluctuation and orbital-dependent electronic correlation are universal features in cerium-based heavy fermion materials. We wonder whether CeX could evince similar behaviors or not. Notice that CeN was recognized as an intermediate mixed-valence compound [42] [43] [44] [45] [46] [47] . In addition, CeP undergoes an isostructural transition (∼ 8% volume collapse) together with considerable change of 4 f valence state under moderate pressure 48 . However, we know a little about the other cerium monopnictides. In order to tackle these problems, we try to study the electronic structures of CeX thoroughly via the density functional theory merged with the single-site dynamical mean-field theory 49 . According to the calculated results, we find that CeX is a good testing bed not only for exploring evolution of 4 f electronic states tuned by spin-orbital coupling, but also for studying subtle entanglement between electronic correlation and magnetism. The rest of this paper is organized as follows. In Sec. II, the computational details are introduced. In Sec. III, the electronic band structures, total and partial 4 f density of states, hybridization functions, 4 f self-energy functions, and histograms of atomic eigenstates are presented. The excellent consistency between calculated and experimental data is illustrated. In Sec. IV, we attempt to clarify some important topics about the 4 f itinerant-localized crossover and the possible relationship between electronic correlation and magnetic ordering states. Finally, Sec. V serves as a brief conclusion.@story_separate@As mentioned above, since the 4 f electrons in CeX are correlated, we have to consider the correlation effect carefully in the calculations. In the present work, we employed the density functional theory plus single-site dynamical mean-field theory (DFT + DMFT) approach 49 . It incorporates the band picture inheriting from the DFT part, and a non-perturbative treatment to the 4 f electronic correlation from the DMFT perspective. It has been widely used to study the electronic structures of many cerium-based heavy fermion materials [8] [9] [10] [50] [51] [52] [53] . Note that the DFT + DMFT method has been applied to study CeX's electronic structures a few years ago [32] [33] [34] . Those works using the non-crossing approximation as quantum impurity solver could reproduce the Kondo peak around the Fermi level. However, the other works employing the spin-polarized T -matrix fluctuation-exchange approximation and Hubbard-I approximation as quantum impurity solvers failed to capture the experimental 4 f states at the Fermi level of CeX 30 . Here we used the WIEN2K code to perform the DFT calculations, which implements a full-potential augmented plane wave formalism. The experimental crystal structures of CeX are used. The muffin-tin radii for Ce and X atoms are 2.5 au and 1.9 au, respectively. The k-points mesh for Brillouin zone integration is 21 × 21 × 21. The generalized gradient approximation, namely the Perdew-Burke-Ernzerhof functional 54 is adopted to express the exchange-correlation potential. The spin-orbital coupling is explicitly included as well. The basic idea of the DMFT is to map the quantum lattice model to a quantum impurity model self-consistently and solve the obtained quantum impurity model by using various quantum impurity solvers 49 . We employ the EDMFTF software package 55 to accomplish this job. The constructed multiorbital quantum impurity models are solved using the hybridization expansion continuous-time quantum Monte Carlo impurity solver (dubbed as CT-HYB) [56] [57] [58] . As mentioned before, the Ce-4 f orbitals are treated as correlated. The Coulomb repulsion interaction U and the Hund's exchange interaction J H are 6.0 eV and 0.7 eV, respectively 10,53 . The fully localized limit (FLL) scheme 59 is used to describe the double counting term in 4 f self-energy functions. In order to simplify the calculations, we not only utilize the good quantum numbers N and J to reduce the sizes of matrix blocks of the local Hamiltonian, but also make a truncation for the local Hilbert space 58 . Only those atomic eigenstates with N ∈ [0,3] are retained in the calculations. The lazy trace evaluation trick is used to accelerate the Monte Carlo sampling further. Since the inverse temperature β = 100 (T ∼ 116.0 K), it is reasonable to retain only the paramagnetic solutions. We perform charge fully self-consistent DFT + DMFT calculations. Of the order of 80 DFT + DMFT iterations are required to obtain good convergence for the chemical potential µ, charge density ρ, and total energy E DFT + DMFT . The convergence criteria for charge and energy are 10 −5 e and 10 −5 Ry, respectively. The Matsubara self-energy functions Σ(iω n ) generated in the last 10 DFT + DMFT iterations are collected and then averaged for further post-processing. At first, we performed analytical continuation on the Matsubara self-energy functions Σ(iω n ) by using the maximum entropy method. Then the obtained self-energy functions on real axis Σ(ω) are used to calculate the momentum-resolved spectral functions A(k, ω) and density of states A(ω) 55 . The momentum-resolved spectral functions A(k, ω) of CeX along the high-symmetry lines X−Γ−W in the first irreducible Brillouin zone are shown in Fig. 2 . Clearly, this series can be roughly classified into two kinds. As for CeN, the most prominent feature is the intense flat band structure near the Fermi level, which is likely from the contributions of 4 f orbitals. It indicates that the 4 f electrons in CeN are itinerant and take part in chemical bonding actively. As for CeP, CeAs, CeSb, and CeBi, the situations are somewhat different. Their spectral functions share some common characteristics: (i) For CeP and CeAs, the flat band features near the Fermi level are still discernible, but they become much dimmer and weaker than that is observed in CeN. For CeSb and CeBi, the flat bands around the Fermi level are almost invisible, implying the completely localized 4 f orbitals. (ii) The ligand p bands are slightly renormalized and shifted toward the Fermi level as compared to those of CeN. We also notice hole pockets at the Γ-point corresponding to X − 5p bands, and electron pockets at the X-point belonging to Ce-5d state 25 . (iii) The 4 f − p hybridization is apparent when ω > 2.0 eV. In Fig. 3(a) and (b) , the total and 4 f partial density of states of CeX are shown, respectively. For CeN, there exist sharp and strong quasiparticle resonance peaks in the vicinity of Fermi level, and a large ""hump"" between 3 eV and 8 eV. According to Fig. 3(b) , the quasiparticle resonance peaks consist of the low-lying 4 f 5/2 [see Fig. 3(b2) ] and high-lying 4 f 7/2 [see Fig. 3(b1) ] states. The splitting energy between these two states is approximately 300 meV, which is in accordance with those measured in the other cerium-based heavy fermion compounds 8, 9 . The predominant contribution to the ""hump"" comes from the upper Hubbard bands. The central energy is about 4.5 eV. Since most of the 4 f states are unoccupied, the majority of 4 f spectral weights is above the Fermi level. The lower Hubbard bands are extremely weak. Concerning the rest of cerium monopnictides, the quasiparticle resonance peaks are greatly reduced. For CeSb and CeBi, these peaks nearly disappear. The upper Hubbard bands are shifted obviously to the Fermi level, which suggest again that the Ce-4 f orbitals become more localized and correlated when X = P, As, Sb, and Bi than X = N. Figure 3 (c2) and (c1) depicts hybridization functions for the 4 f 5/2 and 4 f 7/2 states, respectively. It is observed that whether the 4 f 5/2 state or the 4 f 7/2 state, − ∆(ω = 0)/π (i.e, the 4 f hybridization function at the Fermi level) in CeN is always larger than those in CeX (where X = P, As, Sb, and Bi). It means that when X changes from N to Bi, the hybridization between Ce-4 f and X's ligand orbitals is gradually suppressed. The density of states of CeX has been extensively studied by using PES several decades ago. In order to verify the correctness of our calculations, we try to compare the calculated results with the available experimental data in Fig. 4 . Let's concentrate on CeN at first. The representative two-peak structure with a small shoulder peak around -2 eV is correctly reproduced by our DFT + DMFT calculations 31, 43, 47 . For the unoccupied state, the broad ""hump"" between 3 eV and 8 eV is successfully captured. This feature is ascribed to the 4 f 2 atomic multiplets. It is worth pointing out that in the previous DFT + DMFT calculations with Hubbard-I approximation as quantum impurity solver 30 , the authors failed to reproduce the quasiparticle resonance peaks near the Fermi level 43, 44 . As for CeP, CeAs, and CeSb, the calculated results are in good agreement with the experimental spectra, including the shoulder peaks near -3 eV and the small quasiparticle resonance peak in CeP. The small discrepancies between the theoretical and experimental spectra are likely attributed to the uncertainty in the Coulomb interaction parameters and the use of oversimplified double counting scheme 59 . Thus, we come to a conclusion that our DFT + DMFT calculated results are reliable and reasonable. In general, the electronic correlation effect is encapsulated by the self-energy function. Traditionally, the self-energy functions can be calculated via the Dyson' equation 49 . The resulting data are usually fluctuating and full of noise. In the present work, in order to obtain high-precision data for the self-energy functions, we try to measure them directly in the CT-HYB quantum impurity solver 58 behaviors of the 4 f 5/2 and 4 f 7/2 states are completely different, which means that the 4 f electronic correlation in CeX is probably orbital dependent. It is not at all surprised because this phenomenon has been identified in many cerium-based heavy fermion materials 10, 52 and strongly correlated 5 f electron systems 60 a few years ago. Second, the 4 f self-energy functions of CeN are quite distinctive from those of the other cerium monopnictides. For example, the low-frequency part of 4 f 5/2 state of CeN exhibits remarkable quasi-linear behav-ior. It signifies a (heavy) Fermi-liquid state. However, the corresponding parts of CeX are convex (X = P and As) or concave (X =Sb and Bi). Third, the intercept of self-energy function in y-axis is approximately zero for CeN. While for the other cerium monopnictides, the intercepts are finite. It means that the low-energy scattering of 4 f electrons in CeN is much smaller than those in the rest of cerium monopnictides. Fourth, the low-energy scattering of the 4 f 7/2 states is usually smaller than that of the 4 f 5/2 states. In panel (a), the UPS data (filled red circles) and BIS data (filled green circles) are taken from Ref. [43] and Ref. [47] , respectively. In panels (b)-(d), the experimental UPS data are taken from Ref. [43] . The Fermi levels E F are represented by vertical dashed lines. Notice that the spectral data have been rescaled and normalized for a better visualization. Based on the self-energy data, the quasi-particle weight Z and effective electron mass m can be evaluated via the following equation 49 : where ω 0 = π/β and m e denotes the mass of non-interacting band electron. The calculated Z and m are summarized in Table II . We find that the 4 f 7/2 states are less renormalized. Its large (R ∼ 1.25 for CeN, and R > 20 for CeP, CeAs, CeSb, and CeBi), it is concluded that these materials are in the socalled orbital-selective heavy fermion state, or equivalently, orbital-selective localized state 10 . Valence state fluctuation or mixed-valence behavior is a common feature in many cerium-based heavy fermion materials 10 . In the present work, by utilizing the atomic eigenvalue probability p Γ , which stands for the probability to find out a 4 f valence electron in a given atomic eigenstate |ψ Γ , we can make a reliable estimation about the magnitude of valence state fluctuation in CeX. The CT-HYB quantum impurity solver is capable of recording the atomic eigenvalue probability p Γ 58 . In Fig. 6 , the calculated results for CeX are illustrated as histograms. Here, the atomic eigenstates |ψ Γ are labelled by using some good quantum numbers such as total occupation N and total angular momentum J. We discover that the valence state histograms of CeN [see Here we used three good quantum numbers to label the atomic eigenstates. They are N (total occupancy), J (total angular momentum), and γ (γ stands for the rest of the atomic quantum numbers, such as J z ). Note that the contribution from N = 3 atomic eigenstates is too trivial to be visualized in these panels. are |N = 1, J = 3.5, γ = 0 and |N = 0, J = 0.0, γ = 0 . Their probabilities account for 22% and 20%, respectively. The probabilities for the rest atomic eigenstates are negligible. Therefore, it is suggested that the 4 f electrons in CeN favor to fluctuate among the above three principle competing atomic eigenstates and become itinerant through hybridization with ligand electrons. When X changes from N to P, As, Sb, and Bi, the corresponding atomic eigenstate probability for |N = 1, J = 2.5, γ = 0 soars from 50% to 90%. At the same time, the atomic eigenstates probabilities for |N = 0, J = 0.0, γ = 0 and |N = 1, J = 3.5, γ = 0 decrease rapidly. For CeP and CeAs, they account for less than 4%. For CeSb and CeBi, they are less than 1% and are nearly invisible in Fig. 6(d) and (e). It seems that the 4 f electrons in CeP, CeAs, CeSb, and CeBi are very localized, and virtually confined to the primary atomic eigenstate |N = 1, J = 2.5, γ = 0 . Meanwhile, the corresponding valence state fluctuations are very weak. In short, the redistribution of atomic eigenstates probabilities strongly relies on the atomic number of X. By summing up the atomic eigenstates probabilities p Γ with respect to N, we can derive the distribution of 4 f electronic configurations. It will provide further information about the 4 f valence state fluctuations and mixed-valence behaviors. In CeN, on one hand, the 4 f 1 configuration is predominant and its probability is about 70%. On the other hand, the probabilities of the 4 f 0 and 4 f 2 configurations are about 20% and 9.2%, respectively. It indicates the mixed-valence nature of CeN, which accords with the findings of previous experiments [42] [43] [44] [45] [46] [47] . For X = P, As, Sb, and Bi, the 4 f 1 configuration actually becomes more overwhelming. Its probability is larger than 90%, while those of the 4 f 2 and 4 f 0 configurations decline to less than 6%. It means that the 4 f valence state fluctuation in CeN is the most remarkable. When X grows from N to Bi, the 4 f valence state fluctuations will be greatly suppressed. In consequence, the mixed-valence behaviors will become very trivial. In this section, we would like to discuss some important issues and questions. 4f itinerant-localized crossover or transition. According to the momentum-resolved spectral functions and density of states, we believe that the 4 f electrons in CeN is itinerant, while they tend to be localized in CeP, CeAs, CeSb, and CeBi. In other words, the 4 f itinerant-localized crossover may occur between CeN and CeP. Provided that the N atoms in CeN are substituted gradually by X atoms (X = P, As, Sb, or Bi), a 4 f itinerant-localized crossover is naturally expected. Then a new question rises: what's the driving force of this crossover? First, when X grows from N to Bi, the lattice constants of CeX increase monotonously 11 . The unit cell volume of CeBi is almost twice of the one of CeN (see Table I ). The larger Ce-Ce bond length is, the more localized the 4 f electrons become. Second, we think that the spin-orbital coupling effect of X's p orbitals should play a nontrivial role in this crossover. Generally, the spin-orbital coupling is stronger for the heavier elements, where the electrons acquire large velocities near the nucleus. So, λ Bi,6p > λ Sb,5p > λ As,4p > λ P,3p > λ N,2p , where λ denotes the strength of spin-orbital coupling. Thus, the hybridization between Ce's 4 f and X's np (n = 2 ∼ 6) orbitals should be tuned inevitably by the spin-orbital coupling. Notice that this mechanism is quite similar to the TKI-WKSM transition observed in Ce 3 Bi 4 (Pt 1−x Pd x ) 3 series, which is actually driven by the difference in spin-orbital coupling between Pt and Pd atoms [3] [4] [5] . Evolution of 4 f electronic structures in CeX. The 4 f electronic structures of CeX share some common features. The 4 f electrons are all correlated. The electronic correlations are orbital dependent, i.e, the 4 f 5/2 states are much more correlated than the 4 f 7/2 states. The evidence is that the quasiparticle weight Z (or effective electron mass m ) of the 4 f 5/2 states is much smaller (or larger) than the one of the 4 f 7/2 states. On the other hand, the 4 f electronic structures of CeN differ from all the other CeX compounds obviously. The 4 f electrons in CeN are itinerant, with strong valence state fluctuation. We can observe the quasiparticle resonance peak in the Fermi level and Fermi-liquid-like behavior in the lowfrequency parts of 4 f self-energy functions. Conversely, the 4 f electrons in the other CeX compounds are totally localized. The 4 f − p hybridization near the Fermi level and valence state fluctuation are rather weak. Apparently, their self-energy functions deviate from the description of Landau Fermi-liquid theory. They are also not mixed-valence compounds under ambient condition, though there are some experimental evidences that these compounds might come to be mixed-valence under moderate pressure 48 . Electronic correlation and magnetism in CeX. Usually the ground states of CeX (X = P, As, Sb, and Bi) are antiferromagnetic, except for the paramagnetic CeN 11 . It is easy to understand because the 4 f electrons in CeX are localized and tend to form local moments. Among these compounds, CeSb and CeBi are well known due to their complicated magnetic phase diagram under pressure or under magnetic field [12] [13] [14] [15] . Previous studies suggested that these unusual magnetic properties originate from the small crystal field splitting. Due to the 4 f localization and 4 f − p mixing effect, the crystal field excited state with Γ 8 character is pulled down below the crystal field ground state Γ 7 . Then stacking magnetic structures are formed with strongly polarized Γ 8 Ce layer and paramagnetic Γ 7 Ce layer. This scenario looks good, but it requires the cubic symmetry. However, when CeSb and CeBi transform from paramagnetic phase to ordered phases, their crystal structures also distort from the cubic ones to the tetragonal ones 11 . Furthermore, their lattice constants in the tetragonal structures (along a-axis and c-axis) diminish with decreasing temperature when T < T N . Thus, the above model may be not enough to explain the atypical magnetic properties of CeX. A credible model for this problem should at least take the temperature-dependent crystal structures and the corresponding crystal field splitting into considerations. Anyway, we anticipate that the 4 f electronic correlations should play a vital role in the electronic structures and magnetic properties of CeSb and CeBi with tetragonal symmetry. More DFT + DMFT calculations are being undertaken.@story_separate@In summary, the 4 f electronic structures of cerium-based monopnictides CeX (X=N, P, As, Sb, and Bi) have been systematically investigated by using the DFT + DMFT approach. The momentum-resolved spectral functions A(k, ω), total and 4 f partial density of states A(ω) and A 4 f (ω), hybridization functions, Matsubara self-energy functions, and 4 f valence state fluctuations are studied. The calculated results are consistent with the available experimental data. However, since the experimental data are very limited, most of the calculated results act as useful predictions. It is confirmed that 4 f states of CeN are the most itinerant among the five compounds and display mixed-valence behavior. When X goes from N to Bi, the 4 f electrons in CeX turn out to be more and more localized. It is proposed that the 4 f itinerant-localized crossover probably takes place between CeN and CeP, which is accompanied by vanishing of quasiparticle resonance peak in the Fermi level and regression of 4 f valence state fluctuation. In particular, the orbitaldependent 4 f correlations are identified in CeX. Their 4 f 5/2 orbitals are more correlated and more renormalized than the","In order to unveil the 4f electronic structures in cerium monopnictides (CeX, where X = N, P, As, Sb, and Bi), we employed a state-of-the-art first-principles many-body approach, namely the density functional theory in combination with the single-site dynamical mean-field theory, to make detailed calculations. We find that the 4f electrons in CeN are highly itinerant and mixed-valence, showing a prominent quasi-particle peak near the Fermi level. On the contrary, they become well localized and display weak valence fluctuation in CeBi. It means that a 4f itinerant-localized crossover could emerge upon changing the X atom from N to Bi. Moreover, according to the low-energy behaviors of 4f self-energy functions, we could conclude that the 4f electrons in CeX also demonstrate interesting orbital-selective electronic correlations, which are similar to the other cerium-based heavy fermion compounds."
"There is currently evidence that the likelihood of pandemics will increase in the future because of increased global travel and integration, urbanization, changes in land use, and greater exploitation of the natural environment [1, 2] . Respiratory diseases such as influenza, coronavirus, enterovirus, and other contagious respiratory pathogens are the main risks for potential pandemics [3] . In December 2019, pneumonia of an unknown etiology was confirmed in Wuhan, in the Chinese province of Hubei. The causal agent was soon identified as coronavirus, and the International Committee on Taxonomy of Viruses named the virus severe acute respiratory syndrome coronavirus (SARS-CoV-2). The World Health Organization (WHO) named the disease '2019-new coronavirus disease' (COVID- 19) and declared that COVID-19 was a fast-developing pandemic [1] . Globally, as of 9:36 am Central European Time (CET), 21 March 2021, there have been 122,271,944 confirmed cases of COVID-19, including 2,700,669 deaths, reported to WHO [4] . Currently, there are 5101 trials ongoing [5] for diagnostics, treatment, or prevention of COVID-19. One of the most rapid ways to fight new pandemics is to repurpose existing drugs in the market. At the same time, it is also equally essential to increase the speed of gas permeability of OSTE; in comparison to PDMS, it is <10% of the water vapor value for the flexible OSTE material [36] . This can be especially useful in replicating hypoxic conditions in various organ-on-a-chip (OOC) devices, as PDMS chips suffer from oxygen ingress [15, 30] . Finally, it previously has been shown that OSTE does not suffer from small molecule diffusion in the bulk of the material as PDMS does [26, 36, 37] . However, no LOAC devices from OSTE have been shown in the literature to the best of our knowledge. Therefore, this study aimed to test OSTE as an alternative material for LOAC prototype development, which could be applied in drug testing and repurposing for contagious respiratory diseases in the future. We demonstrated that OSTE has significantly less small molecule and fluorescent dye absorption than PDMS, as expected, while it has similar properties of membrane particle absorption and inhibitory effect on enzymatic reactions but lower light transmission than PDMS. We developed a simple LOAC microfluidic device from both OSTE and PDMS and compared functionalization with cell cultures based on immunofluorescence. Image analysis by confocal microscopy was significantly impaired due to the lower light transmission of OSTE, which affected the further development of LOAC from OSTE. In conclusion, LOAC from OSTE prototype optical properties should be addressed to make it applicable in researching respiratory diseases and drug development and repurposing in the future.@story_separate@To directly evaluate and compare PDMS and OSTE material properties in enzymatic reactions, such as complementary DNA (cDNA) synthesis and Polymerase chain reaction (PCR), we fabricated~1 mm thick round test pieces with 14 mm diameter. These test pieces were die-cut from large sheets of material using a manual die-cutter. PDMS sheet was fabricated by casting PDMS (Sylgard 184, Dow Corning, 1:10 crosslinker/base ratio, w/w) onto a glass plate with laser-cut cast acrylic sidewalls, degassing for 30 min in −800 mbar pressure, and curing at 85 • C in a convection oven (Shanghai Yuanhuai Industrial CO DZF-6020, Shanghai, China). After curing, the outer 2 mm of the material (i.e., closest to the mold's acrylic sidewalls) was discarded to eliminate any PDMS material that could have been contaminated with acrylic leachates. OSTE sheets were fabricated by pouring OSTE 322 (Mercene Labs, mixed as per instructions on the bottle, 1.09:1 Part A/Part B, w/w) onto polystyrene Petri dishes with bottom surface roughness (R a ) of around 20 nm, degassing for 30 min in -800 mbar pressure, and UV curing for 5 min with a UV intensity (i-line) of 6.8 mW/cm 2 . Finally, OSTE samples were cured on a hot plate at 60 • C overnight between two Polytetrafluoroethylene (PTFE) sheets to ensure OSTE did not stick to the hot plate surface. PDMS and OSTE sample optical properties were measured using a spectrophotometer (Cary7000, Agilent Technologies, Santa Clara, CA, USA) on previously described pieces. The spectra were measured in transmission mode for three samples from each material and averaged between 300 and 800 nm wavelengths with a 600 nm/min scan rate, an interval of 2 nm, and no light polarization. Measurements were performed twice for each sample. The sample transmission plots were obtained by integrating the total area under the curve and normalizing it to a soda-lime glass (Kyocera, Osaka, Japan) sample. Statistical significance was calculated by Mann-Whitney test. For evaluating the material absorption of small molecules, three pieces from each material were immersed in 100 µM rhodamine B solution, made by dissolving rhodamine B powder (Sigma-Aldrich, St. Louis, MO, USA) in de-ionized (DI) water for 24 h at a temperature of 20 • C. After immersion, samples were thoroughly rinsed with DI water before measuring transmission spectra. Measurements were performed twice. Sample absorption was evaluated as the integrated area of the trough between 450 and 600 nm. Area integration was done via Origin 9.0 software with a baseline fit to the curve to determine the trough. A soda-lime glass sample was used as a reference, which had no change in transmission spectra post rhodamine B immersion. The area under the curve results were normalized to the PDMS sample. Statistical significance was calculated by Mann-Whitney test. 2.4. PDMS and OSTE Polymer Effect on RNA Isolation, cDNA Synthesis, and Quantitative Reverse Transcription (qRT)-PCR Reaction PDMS and OSTE round pieces were washed with 70% ethanol overnight, air dried, and washed with sterile 1× PBS overnight. Washed pieces were cultivated with 1 mL DMEM (41966-052, Thermo Fisher, Waltham, MA, USA) supplemented with 10% FBS (F7524-500 ML, Sigma-Aldrich) and 50 µL/mL Primocin (ant-pm-2, InvivoGen, San Diego, CA, USA) for 48 h in a humidified incubator at +37 • C, 5% CO 2 to mimic cell culturing environment. After 48 h, 700 µL Qiazol from miRNeasy Micro Kit (217084, Qiagen, Hilden, Germany) was directly added in wells with polymers, representing cell lysing. Wells without polymers were used as controls. Experiments were performed in duplicates. One microliter (1 × 10 9 copies/µL) of synthetic spike-in UniSp6 from miRCURY LNA RT Kit (339340, Qiagen) was added to the lysate to monitor polymer effect on RNA isolation, cDNA synthesis, and PCR reaction since it should be similar to the control sample. cDNA was synthesized by miRCURY LNA RT Kit, and qRT-PCR was performed by miRCURY LNA SYBR Green PCR Kits (339346, Qiagen) and UniSp6 primer assay (339306, Qiagen) by applying ViiA 7 Real-Time PCR System (Thermo Fisher). Reactions were performed in technical duplicates on each biological duplicate. Ct values were compared between samples, and the p-value was calculated by the Mann-Whitney test. To test biological entities and in immunofluorescence applied dye absorption, PDMS and OSTE single-channel devices were microfabricated. PDMS devices were fabricated by casting PDMS (Sylgard 184, Dow Corning, Midland, MI, USA, 1:10 crosslinker/base ratio, w/w) onto custom machined aluminum molds and degassing around 30 min in −800 mbar pressure. The channel height was 180 µm, and channel width was 1 mm with surface roughness (R a ) of around 30 nm parallel and orthogonal to the channels. After curing, devices were peeled out from the molds, and inlets were punched using a biopsy puncher. To bond the devices, PDMS and a clean soda-lime glass slide were placed in UV-Ozone cleaner (Novascan PSDP-UV8T, Boone, IA, USA) for 5 min; after ozone exposure, PDMS and glass were immediately brought into contact and cured overnight on a 65 • C hot plate with pressure on the stack of around 2 kPa. OSTE devices were fabricated using OSTE 322 (Mercene Labs, Stockholm, Sweden), which was mixed according to the instructions (Part A to Part B 1.09:1 (w/w)) and degassed for 30 min at 800 mbar pressure shortly before casting onto the aluminum molds also used for PDMS device fabrication. OSTE was cured for 4 min with a UV intensity (i-line) of 6.8 mW/cm 2 . After curing, devices were peeled out from the molds, and inlets were punched using a revolving hole punch. Finally, the OSTE slabs were brought into contact with a clean soda-lime glass slide, and the whole assembly was cured on a 100 • C hot plate overnight with a pressure of around 8 kPa on the whole stack. Extracellular vesicles (EVs) isolated from cell cultures were used to evaluate material properties on membrane particle absorption and mimic virus particles. EVs were isolated and characterized from ASC52-telo (SCRC-4000, ATCC, Manassas, VA, USA) cell culture similarly to previously described protocols [38, 39] . Single-channel devices were washed with 70% ethanol overnight, dried, and washed with sterile 1× PBS overnight. Then, 20 µL of 1.02 × 10 8 EVs/mL in 0.02 µm filtered 1× PBS were injected into channels and incubated for 1 h at +37 • C. After incubation, EV solutions from channels were collected and measured by nanoparticle tracking analysis (NTA) with an NS 300 instrument (Malvern, Philadelphia, PA, USA) equipped with green (532 nm) laser and scientific Complementary metal-oxide-semiconductor (sCMOS) camera and compared with input sample. Here, 0.02 µm filtered 1× PBS was used as a negative control. Measurements were performed on five 30 s videos that were recorded using camera level 12. The data were analyzed using NTA software v3.0 with the detection threshold 8 and screen gain at 5. Experiments were performed in biological duplicates and measured in technical duplicates. p-value was calculated by the Mann-Whitney test. CellVue (MINCLARET-1KT, Sigma-Aldrich) was selected for dye absorption tests since it is a small fluorescent molecule often used in cell culture experiments to label cell membranes. Channels were washed with 20 µL Diluent C and incubated with 2 × 10 −6 M CellVue dye in Diluent C according to the manufacturer's protocol applied for cell membrane labeling and incubated for 1 h at +37 • C. Sterile 1× PBS was used as a control. Next, channels were washed 3 times with sterile 1× PBS and analyzed using a confocal laser scanning microscope (TCS SP8, Leica, Germany) with 633 nm excitation laser (helium-neon, Thorlabs Inc, Newton, NJ, USA). Image processing was done by ImageJ bundled with 64-bit Java 1.8.0_172 and used to quantify CellVue absorption. Normalized total fluorescence of CellVue was calculated by integrated density -(selected area × mean fluorescence of background), where integrated density summarizes all pixels in a selected area. Experiments were performed in biological duplicates, and all measurements were performed in technical duplicates. Statistical significance was calculated by Mann-Whitney test. PDMS devices were fabricated by casting PDMS (Sylgard 184, Dow Corning, 1:10 crosslinker/base ratio, w/w) onto custom machined aluminum molds and degassing of around 30 min in −800 mbar pressure. The top and bottom channels had a channel height of 200 µm; the top channel width was 1.2 mm, whereas the bottom channel width was 1.0 mm to account for alignment tolerances. Aluminum molds had a surface roughness (R a ) of around 50 nm parallel and orthogonal to the channels. PDMS was cured for 2 h at 85 • C in a convection oven. Fluid outlets were then cut using a 1.25 mm biopsy puncher. PDMS channels were bonded to a track-etched polycarbonate (PC) membrane (it4ip S.A., Louvain-La-Neuve, Belgium) with a nominal pore size of 3 µm and density of 1.6 × 10 6 /cm 2 . Track-etched membranes were chosen due to their uniform pore distribution and highly controlled porosity. PC was the material of choice due to better bonding performance with OSTE materials, where generally polyethylene terephthalate (PET) is used as a release liner material. The bonding protocol consisted of a modified amine-epoxy protocol based on the previously described method [40] . PDMS surfaces were modified by immersion in 1% aqueous GLYMO (2530-83-8, Sigma-Aldrich) solution for 20 min after a 5 min oxygen plasma treatment step in a plasma asher (PVA TePla AG GIGAbatch 360M, Wettenberg, Germany); similarly, PC membranes were modified with 1% aqueous APTES (919-30-2, Sigma-Aldrich) solution for 20 min after a 1 min oxygen plasma treatment step. After surface modification, all three parts were blow-dried with nitrogen and immediately brought into contact using a custom jig. Further, the PDMS-PC-PDMS stack was cured overnight on a 65 • C hot plate with pressure on the stack of around 2 kPa as visualized in Figure 1a . OSTE devices were fabricated using OSTE 322 (Mercene Labs), which was mixed according to the instructions (Part A to Part B 1.09:1 (w/w)) and degassed for 30 min at −800 mbar pressure shortly before casting onto the aluminum molds with a similar surface finish to PDMS devices. Top and bottom channels were made from the same aluminum molds with a channel height of around 180 µm and a width of 1.0 mm. The differences from PDMS molds stem from the tolerances in the computer numerical control (CNC) milling process. OSTE was filled into the mold cavity using a custom setup and cured for Micromachines 2021, 12, 546 6 of 17 4 min with a UV intensity (i-line) of 6.8 mW/cm 2 . After the UV curing step, the bottom pieces were pressed together with the same PC membrane and cured at 60 • C for 1 h on a hot plate with around 8 kPa pressure on the stack. The top piece was fabricated with the same parameters. After the UV curing step, inlets were punched using a revolving hole punch. The top piece was then brought into contact with the PC membrane, and the whole assembly was cured on a 60 • C hot plate overnight with a pressure of around 8 kPa on the whole stack, as visualized in Figure 1b . After bonding, devices were placed in a plasma asher (PVA TePla AG GIGAbatch 360M, Wettenberg, Germany) for 5 min to activate the PC membrane and OSTE surfaces. mbar pressure shortly before casting onto the aluminum molds with a similar surface finish to PDMS devices. Top and bottom channels were made from the same aluminum molds with a channel height of around 180 µm and a width of 1.0 mm. The differences from PDMS molds stem from the tolerances in the computer numerical control (CNC) milling process. OSTE was filled into the mold cavity using a custom setup and cured for 4 min with a UV intensity (i-line) of 6.8 mW/cm 2 . After the UV curing step, the bottom pieces were pressed together with the same PC membrane and cured at 60 °C for 1 h on a hot plate with around 8 kPa pressure on the stack. The top piece was fabricated with the same parameters. After the UV curing step, inlets were punched using a revolving hole punch. The top piece was then brought into contact with the PC membrane, and the whole assembly was cured on a 60 °C hot plate overnight with a pressure of around 8 kPa on the whole stack, as visualized in Figure 1b . After bonding, devices were placed in a plasma asher (PVA TePla AG GIGAbatch 360M, Wettenberg, Germany) for 5 min to activate the PC membrane and OSTE surfaces. The process is based on the epoxy-amine bonding process described previously [40] . Amine functionality was assigned to polycarbonate (PC) membrane to enhance extracellular matrix (ECM) adhesion to the membrane. (b) Offstoichiometry thiol-ene (OSTE) device fabrication workflow. To retain membrane flatness during the assembly process, the bottom piece was pre-bonded and under-cured to the membrane first before bonding the top piece and fully curing the assembly. Before cell seeding, both PDMS and OSTE LOAC devices were washed in 70% ethanol overnight, dried, and washed in sterile 1× PBS overnight. Upper and lower channels were coated by 0.5 mg/mL collagen IV from human placenta (C5533-5MG, Sigma-Aldrich), diluted in sterile 1× PBS, and incubated overnight in cell incubator at +37 • C and 5% CO 2 . On the following day, 2E7 HUVEC-2 cells/mL (354151, Corning, NY, USA) were seeded in the bottom channel, and the device was inverted and left overnight in DMEM/F12 (31330095, Thermo Fisher) supplied with 1× LVES (A1460801, Thermo Fisher) and 50 µg/mL Primocin (ant-pm-2, InvivoGen) at +37 • C and 5% CO 2 in static conditions for cell attachment. The next day, the LOAC device was inverted back, and A549 (CCL-185, ATCC, Manassas, VA, USA) cells were plated at a concentration of 3.5 × 10 6 cells/mL in DMEM/F12 (31330095, Thermo Fisher) with 10% FBS (F7524-500ML, Sigma-Aldrich) and 50 µg/mL Primocin (ant-pm-2, InvivoGen) and left overnight for cells to attach at +37 • C and 5% CO 2 in static conditions. Next, LOAC devices were connected to a syringe pump (ISPLab02, Baoding Shenchen Precision Pump Co. Ltd., Baoding, China), and top and bottom channels were perfused with their respective media at 2 µL/min per channel in the withdraw (vacuum) setting. Channel-specific media were preconditioned in the incubator for 1 h at +37 • C and 5% CO 2 and stored inside the incubator during LOAC incubation. Media was changed every 48 h. After seven days, LOAC was analyzed by immunofluorescence and confocal microscopy to compare cell co-culture growing on both LOAC devices. Before immunofluorescence, primary antibodies were conjugated with either FITC Conjugation Kit (Fast) Lightning-Link (ab188285, Abcam, Cambridge, UK) or with Lightning-Link APC Antibody Labeling Kit (705-0030, Novus Biologicals, Littleton, CO, USA). Cells cultured in LOAC devices were washed by perfusing 1 mL of sterile 1× PBS; the membrane was labeled with CellVue Claret Far Red Fluorescent Cell Linker Mini Kit (MINCLARET, Sigma-Aldrich) by washing once with Diluent C, incubating with 2 × 10 −6 M CellVue dye in Diluent C for 5 min, blocking the staining with 1% BSA (A7906, Sigma-Aldrich), and washing with 1 mL sterile 1× PBS. Cells were fixed with filtered 4% paraformaldehyde (P6148-500G, Sigma-Aldrich) solution in sterile 1× PBS for 30 min, washed with 1 mL sterile 1× PBS, and permeabilized with 0.5% Triton X-100 (93443, Sigma-Aldrich) for 5 min. Then, cells were blocked with 5% BSA (A7906, Sigma-Aldrich) for 2 h and washed with 1 mL sterile 1× PBS. Cells were then stained overnight at +4 C with FITC conjugated Anti-ZO1 tight junction protein antibody (ab216880, Abcam) diluted 1:100, FITC conjugated Anti-Mucin 5AC antibody (ab3649, Abcam) diluted 1:100, and APC conjugated Anti-CD31 antibody (ab9498, Abcam) diluted 1:500. After Ab incubation, channels were washed with 1 mL sterile 1× PBS, counterstained for 5 min with DAPI (D9542, Sigma-Aldrich), and washed with 1 mL PBS. DAPI-stained channels of PDMS and OSTE chips were captured with EVOS M5000 imaging system (AMF5000, Invitrogen, Carlsbad, CA, USA). Channels were captured with transmitted light and EVOS Light Cube, DAPI 2.0 (AMEP4950, Invitrogen) using EVOS 10× Objective (AMEP4981, Invitrogen). Transmitted light and DAPI images were combined using EVOS M5000 built-in software to reflect cell distribution on the chip membrane. Chips were imaged using a confocal laser scanning microscope (TCS SP8, Leica, Germany). Confocal Z-stacks were scanned using 10×/NA 0.30 objective (Leica, Germany). DAPI was imaged using a 405 nm excitation laser (diode), and FITC was imaged using a 488 nm excitation laser (argon). Cell Vue and APC were imaged using a 633 nm excitation laser (helium-neon). PDMS LOAC Z-stacks were scanned over a 150 µm range at 3 µm intervals to cover both HUVEC and A549 cell layers. OSTE LOAC images were captured at a single position on Z-axis. Images and Z-stack maximum projections were then pro-cessed using Leica Application Suite X software (Version 3.7.4.23463, Leica Microsystems, Germany). Three-dimensional Z-stack reconstruction was performed using LAS X 3-D Viewer (Leica Microsystems, Germany). Crucial aspects of the LOAC and OOC overall in drug repurposing and development are cell monitoring by fluorescent or confocal microscopy, florescent dye, absorption of small molecules and larger entities, and effect on enzymatic reactions, which are usually applied in downstream analysis. PDMS and OSTE pieces were compared in terms of their relative light transmission across the 300-800 nm range, covering excitation and emission peaks for commonly used fluorescent dyes [41] . From Figure 2a , it can be seen that PDMS has almost the same light transmission as glass in the selected range. Normalized light transmission of glass is 100% standard error of the mean (SEM) ± 1.21% (the glass was used as reference), while PDMS has 96.62% SEM ± 1.01%, which has been long seen as one of the advantages of PDMS in optically sensitive applications [17, 18] . In comparison to PDMS, OSTE has lower normalized light transmission-78.67% SEM ±1.65%. It has already been reported that OSTE 322 material seems to suffer from strong light scattering [32] . This likely explains the reduced transmission of the material across the whole investigated wavelength (300-800 nm) range seen in Figure 2b . Small drug absorption by materials, which is crucial for drug studies, has [42] been characterized by rhodamine B absorption of PDMS and OSTE polymer pieces. Figure 2c shows normalized rhodamine B absorption that demonstrates that OSTE has significantly reduced small molecule absorption compared to PDMS, while glass has 0 absorptions compared to PDMS, which was used as reference (normalized rhodamine B absorption of glass 0%, SEM ±0%; PDMS 100%, SEM ±1%; OSTE 13.31%, SEM ±0.13%). These results confirm previously published results and suggest that OSTE is much better suited for LOAC microfabrication for drug testing purposes [43] . Next, given that there is a significant emphasis on cell analysis with PCR and sequencing in the OOC field, it is of high importance to evaluate if the materials utilized in LOAC devices would not affect any downstream enzymatic analysis methods. To evaluate this, PDMS and OSTE pieces were cultivated in cell culture media in the incubator to mimic cell culturing conditions and directly lysed with Qiazol to evaluate any leakage of material in the buffer during lysing that could hamper typical downstream cell analysis processes such as PCR. An equal amount of spike-in in PDMS, OSTE, and control lysates was added to quantify inhibition on qRT-PCR. Figure 2d shows qRT-PCR cycle threshold values for sample materials. Significant differences in cycle threshold values would show any leachates inhibiting the RNA isolation, cDNA synthesis, or qRT-PCR. However, there was no significant difference in spike-in amplification between control and PDMS samples, which is in line with previously published results [44] . At the same time, there were no significant differences between OSTE, PDMS, and control sample also, which suggests that OSTE is at least as good as PDMS for cell downstream analysis purposes. To test PDMS and OSTE absorption of larger biological entities, we used singlechannel PDMS and OSTE devices. Absorption of larger entities such as virus particles is crucial for research of respiratory disease by LOAC. We used extracellular vesicles (EVs) to mimic virus particles since they share some similarities with retroviruses, such as size and composition, and EVs often play significant roles in virus infection [45] . Figure 2e represents a number of particles in the input sample representing control and output sample from PDMS and OSTE channels measured by NTA. There was a slight increase in particle amount in both PDMS and OSTE samples compared to the control sample, 1.2 × 10 8 , standard deviation (SD) ± 8 × 10 6 ; 1.15 × 10 8 , SD ± 1 × 10 7 ; and 1 × 10 8 , SD ± 2 × 10 6 particles per mL, respectively. However, these differences were not statistically significant, suggesting that both materials are suitable for research on larger biological entities such as EVs and viruses. As a final test, we performed a fluorescent dye CellVue absorption test in singlechannel devices by confocal microscopy (see Figure 2f ,g). CellVue is often used in cell membrane staining for cell, EV, and virus interaction research critical in LOC for respiratory disease research. Figure 2h demonstrates that OSTE absorbs 6.63 times less CellVue dye than PDMS (normalized total CellVue fluorescence of PMDS is 2358.63 SEM ± 7.94, while OSTE had 355.33 SEM ± 4.45), suggesting that OSTE likely has less background noise for membrane labeling than PDMS or that this is an artifact from differences in light scattering since laser settings were the same. Crucial aspects of the LOAC and OOC overall in drug repurposing and development are cell monitoring by fluorescent or confocal microscopy, florescent dye, absorption of small molecules and larger entities, and effect on enzymatic reactions, which are usually applied in downstream analysis. PDMS and OSTE pieces were compared in terms of their relative light transmission across the 300-800 nm range, covering excitation and emission peaks for commonly used fluorescent dyes [41] . From Figure 2a , it can be seen that PDMS has almost the same light transmission as glass in the selected range. Normalized light transmission of glass is 100% standard error of the mean (SEM) ± 1.21% (the glass was used as reference), while PDMS has 96.62% SEM ± 1.01%, which has been long seen as one of the advantages of PDMS in optically sensitive applications [17, 18] . In comparison to PDMS, OSTE has lower normalized light transmission-78.67% SEM ±1.65%. It has already been reported that OSTE 322 material seems to suffer from strong light scattering [32] . This likely explains the reduced transmission of the material across the whole investigated wavelength (300-800 nm) range seen in Figure 2b .  The LOAC devices presented herein consist of two microfluidic chambers separated by a porous membrane. Before biology experiments, device fabrication and bonding parameters were thoroughly optimized. It was found that devices cured with a highintensity UV light (such as mask aligner, with the intensity of 20-50 mW/cm 2 ) tend to yield poorer bonding performance in the form of unbonded areas despite slight under-curing of OSTE. This likely can be attributed to triggering OSTE thermal curing step; therefore, all devices were fabricated utilizing a non-collimated UV-LED light source [35] . After the bonding process, devices were placed in a plasma asher to activate the surface before ECM coating steps and reduce the contact angle of OSTE and PC [46] . To evaluate the devices' bonding performance, top and bottom channels were connected to a pressure system, and DI water was passed through the device at 1 mL/min in each of the channels. During the pressure testing, the device was carefully examined for leaking. The flow rate was chosen to be around two orders of magnitude higher than the operating flow rate serving as a safety factor. Next, we compared LOACs microfabricated from OSTE and PDMS (see Figure 3a ,b) for HUVEC (human umbilical vascular endothelial cells) and A549 (human lung carcinoma) cell coculturing on each side of the membrane by using the simple LOAC culturing protocol described in methods. PDMS, OSTE, and PC materials are already proven in numerous articles to be compatible with different cells growing on them [8, 9, 11, 32, [47] [48] [49] [50] [51] [52] ; therefore, we aimed to compare standard LOAC functionalization and application in immunofluorescence since there is a difference in light transmission and fluorescent dye absorption between polymers. HUVEC cells were used to represent endothelial cells on the bottom channel, while A549 cells were used as representative of lung epithelium on the top channel. Cells were cultured seven days on both devices without any leakage or bubble problem thanks to an in-house developed LOAC cultivation jig with shut-off valves for each channel at both entry and exit of the fluid paths (see Figure 3c ). While the cell monolayer was monitored in the PDMS device, it was impossible to evaluate cell confluence of cells in the OSTE device with the EVOS M5000 Imaging System, probably due to decreased light transmission (see Figure 3d ,e). sity UV light (such as mask aligner, with the intensity of 20-50 mW/cm 2 ) tend to yield poorer bonding performance in the form of unbonded areas despite slight under-curing of OSTE. This likely can be attributed to triggering OSTE thermal curing step; therefore, all devices were fabricated utilizing a non-collimated UV-LED light source [35] . After the bonding process, devices were placed in a plasma asher to activate the surface before ECM coating steps and reduce the contact angle of OSTE and PC [46] . To evaluate the devices' bonding performance, top and bottom channels were connected to a pressure system, and DI water was passed through the device at 1 mL/min in each of the channels. During the pressure testing, the device was carefully examined for leaking. The flow rate was chosen to be around two orders of magnitude higher than the operating flow rate serving as a safety factor. Next, we compared LOACs microfabricated from OSTE and PDMS (see Figure 3a ,b) for HUVEC (human umbilical vascular endothelial cells) and A549 (human lung carcinoma) cell coculturing on each side of the membrane by using the simple LOAC culturing protocol described in methods. PDMS, OSTE, and PC materials are already proven in numerous articles to be compatible with different cells growing on them [8, 9, 11, 32, [47] [48] [49] [50] [51] [52] ; therefore, we aimed to compare standard LOAC functionalization and application in immunofluorescence since there is a difference in light transmission and fluorescent dye absorption between polymers. HUVEC cells were used to represent endothelial cells on the bottom channel, while A549 cells were used as representative of lung epithelium on the top channel. Cells were cultured seven days on both devices without any leakage or bubble problem thanks to an in-house developed LOAC cultivation jig with shut-off valves for each channel at both entry and exit of the fluid paths (see Figure 3c ). While the cell monolayer was monitored in the PDMS device, it was impossible to evaluate cell confluence of cells in the OSTE device with the EVOS M5000 Imaging System, probably due to decreased light transmission (see Figure 3d ,e). Further, we compared HUVEC and A549 cells between PDMS and OSTE LOACs by confocal microscopy (Figure 4) . Cell nuclei were stained by DAPI, while CD31 (platelet endothelial cell adhesion molecule) antibody stained with APC was used as an endothelial cell-specific marker and antibody against tight junction marker ZO1 (Zonula occludens-1) stained with FITC was used as functional HUVEC monolayer marker. Results confirmed that HUVEC cells were growing in a monolayer on PDMS LOAC as expected based on CD31 and ZO1 markers. FITC-stained MUC5A antibody was used as a marker for lung epithelial cells since A549 cells produce this protein [53] that is human airway specific [54] . Based on Figure 4 , results confirm that A549 also produces a monolayer on PC membrane in PDMS LOAC. CellVue was used as membrane dye for A549 cells and compared the CellVue background between PDMS and OSTE LOACs since there was a significant difference in absorption tests. While CellVue had some background noise that could be subtracted by image processing in PDMS LOAC, we could not obtain any good-quality pictures from OSTE LOAC with confocal microscopy. When imaging PDMS LOAC, DAPI signal was initiated with 1% of diode laser power, yet in OSTE LOAC, some barely specific fluorescence was detected only with 90-100% laser power. These results suggest that CellVue absorption differences are due to the poor light transmission and clearly demonstrate that OSTE polymer light transmission characteristics or LOAC production from OSTE need to be improved since LOAC microscopy is a crucial aspect of cell monitoring and evaluation of response to respiratory diseases and treatments. Further, we compared HUVEC and A549 cells between PDMS and OSTE LOACs by confocal microscopy (Figure 4) . Cell nuclei were stained by DAPI, while CD31 (platelet endothelial cell adhesion molecule) antibody stained with APC was used as an endothelial cell-specific marker and antibody against tight junction marker ZO1 (Zonula occludens-1) stained with FITC was used as functional HUVEC monolayer marker. Results confirmed that HUVEC cells were growing in a monolayer on PDMS LOAC as expected based on CD31 and ZO1 markers. FITC-stained MUC5A antibody was used as a marker for lung epithelial cells since A549 cells produce this protein [53] that is human airway specific [54] . Based on Figure 4 , results confirm that A549 also produces a monolayer on PC membrane in PDMS LOAC. CellVue was used as membrane dye for A549 cells and compared the CellVue background between PDMS and OSTE LOACs since there was a significant difference in absorption tests. While CellVue had some background noise that could be subtracted by image processing in PDMS LOAC, we could not obtain any goodquality pictures from OSTE LOAC with confocal microscopy. When imaging PDMS LOAC, DAPI signal was initiated with 1% of diode laser power, yet in OSTE LOAC, some barely specific fluorescence was detected only with 90-100% laser power. These results suggest that CellVue absorption differences are due to the poor light transmission and clearly demonstrate that OSTE polymer light transmission characteristics or LOAC production from OSTE need to be improved since LOAC microscopy is a crucial aspect of cell monitoring and evaluation of response to respiratory diseases and treatments. In Figure 5 , we demonstrate functional monolayer formation on both sides of PC membrane in PDMS LOAC, where ZO1 (green) and CD31 (red) positive HUVEC cells are on the bottom channel and A549 cells are on the top channel. Due to the poor light transmission, it was impossible to acquire the same image set with confocal microscopy on OSTE LOAC. Therefore, we have not confirmed functional monolayer formation on both sides of the PC membrane within OSTE LOAC. However, based on previous literature of In Figure 5 , we demonstrate functional monolayer formation on both sides of PC membrane in PDMS LOAC, where ZO1 (green) and CD31 (red) positive HUVEC cells are on the bottom channel and A549 cells are on the top channel. Due to the poor light transmission, it was impossible to acquire the same image set with confocal microscopy on OSTE LOAC. Therefore, we have not confirmed functional monolayer formation on both sides of the PC membrane within OSTE LOAC. However, based on previous literature of PC membrane [51, 52, 55] and OSTE polymer biocompatibility [32, 47, 48] and that some signal was detectable from DAPI and CellVue, this suggests that cells grow inside OSTE LOAC, but the quality of cell monolayer cannot be evaluated. These results confirm that there were no problems with the current cell growing protocol on PDMS and probably on OSTE LOAC and that the final conclusion about OSTE LOAC cell cocultivation on the membrane cannot be drawn due to the light transmission problems. PC membrane [51, 52, 55] and OSTE polymer biocompatibility [32, 47, 48] and that some signal was detectable from DAPI and CellVue, this suggests that cells grow inside OSTE LOAC, but the quality of cell monolayer cannot be evaluated. These results confirm that there were no problems with the current cell growing protocol on PDMS and probably on OSTE LOAC and that the final conclusion about OSTE LOAC cell cocultivation on the membrane cannot be drawn due to the light transmission problems.  One of the key reasons to look into alternative materials for LOAC and OOC fabrication is the manufacturability of the devices mentioned by the ORCHID roadmap [56] . PDMS microfluidic device fabrication has been refined over the years since the original publications of Whitesides, yet it remains slow due to the PDMS curing time [13, 19, 24] . The unscalable manufacturing process is an inherently limiting factor of truly widespread OOC device uptake [57] . Furthermore, in addition to the channel fabrication process being complex, the bonding protocol is also nontrivial. The use of commercially available tracketched membranes allows using a scalable membrane fabrication process. However, it also requires an elaborate surface modification to PDMS and membrane material to ensure covalent bonding, such as that seen in Figure 1 of this article, based on the protocol developed previously [40] . If PDMS membranes are used for this process, then the bonding process is significantly more straightforward (i.e., just plasma treatment [58] ) and slower due to PDMS curing. Microfluidic devices have been realized using various polymer materials [59] . Thermoplastics are often regarded as the ultimate material class for mass manufacturability due to the fact that a large number of materials are compatible with injection molding, capable of producing millions of units per annum [57] . As examples, it is worth mentioning polypropylene [60] , polycarbonate [61] , and polymethyl methacrylate (PMMA) [62] . Yet, this article has been focused on materials that can be prototyped in a typical academic setting, with access to a standard cleanroom environment, but can also be used to fabricate devices at larger volumes. Such material examples are OSTE, Flexdym [63] , Styrene Ethylene Butylene Styrene (SEBS) [26] , and polyurethane [64] . SEBS-based and polyurethane materials can be especially useful due to their flexible nature, which can be used in active element fabrication such as valves [64, 65] . We have focused on OSTE material particularly due to its bonding properties. Contrary to PDMS, OSTE fabrication relies on UV curing and the emerging reaction injection molding process [66, 67] . Furthermore, there have been examples of simple microfluidic structure reaction injection molding [35] . Although  One of the key reasons to look into alternative materials for LOAC and OOC fabrication is the manufacturability of the devices mentioned by the ORCHID roadmap [56] . PDMS microfluidic device fabrication has been refined over the years since the original publications of Whitesides, yet it remains slow due to the PDMS curing time [13, 19, 24] . The unscalable manufacturing process is an inherently limiting factor of truly widespread OOC device uptake [57] . Furthermore, in addition to the channel fabrication process being complex, the bonding protocol is also nontrivial. The use of commercially available track-etched membranes allows using a scalable membrane fabrication process. However, it also requires an elaborate surface modification to PDMS and membrane material to ensure covalent bonding, such as that seen in Figure 1 of this article, based on the protocol developed previously [40] . If PDMS membranes are used for this process, then the bonding process is significantly more straightforward (i.e., just plasma treatment [58] ) and slower due to PDMS curing. Microfluidic devices have been realized using various polymer materials [59] . Thermoplastics are often regarded as the ultimate material class for mass manufacturability due to the fact that a large number of materials are compatible with injection molding, capable of producing millions of units per annum [57] . As examples, it is worth mentioning polypropylene [60] , polycarbonate [61] , and polymethyl methacrylate (PMMA) [62] . Yet, this article has been focused on materials that can be prototyped in a typical academic setting, with access to a standard cleanroom environment, but can also be used to fabricate devices at larger volumes. Such material examples are OSTE, Flexdym [63] , Styrene Ethylene Butylene Styrene (SEBS) [26] , and polyurethane [64] . SEBS-based and polyurethane materials can be especially useful due to their flexible nature, which can be used in active element fabrication such as valves [64, 65] . We have focused on OSTE material particularly due to its bonding properties. Contrary to PDMS, OSTE fabrication relies on UV curing and the emerging reaction injection molding process [66, 67] . Furthermore, there have been examples of simple microfluidic structure reaction injection molding [35] . Although the shown parts do not exhibit the same complexity as a LOAC, it is a step towards mass manufacturability. For example, multichip curing in a single step exposure can be achieved using shadow masking of liquid runners between cavities, which removes the die-cutting step required in the PDMS fabrication process. Similarly, the OSTE bonding process is significantly simplified, owing to the available epoxy groups on the surface. The bonding process is then finished with a long-curing step, like in the PDMS process. In both processes, the temperature limiting factor is membrane material used in OOC devices [68] . Therefore, it is feasible to assume that OSTE could be utilized for the volume manufacturing process of LOAC devices through the use of reaction injection molding [67] . One of the main advantages of OOCs, including LOACs, is the several tissue level co-functionalities in fluid-air flow that is not possible currently for other in vitro models. Therefore, these models are attractive for preclinical tests for new drug testing or repurposing [10, 23, 69] . This is particularly attractive in multi-OOC systems where drug offtargets or synergy can be modulated. However, PDMS strongly absorbs small hydrophobic molecules, which is a significant drawback and requires extensive computational modeling in silico for drug pharmacokinetics and pharmacodynamics to recapitulate in vivo conditions. Moreover, new drug lead absorption in PDMS needs to be quantified before testing in OOC from PDMS, bringing additional variability in computational models [29] . In this study, we confirmed that OSTE polymer is a better-suited material than PDMS for drug testing and repurposing goals since it has several times less absorption of rhodamine B, which represents a small molecule drug in Figure 2b , which has also been published in several papers [31, 32] . In this paper, to the best of our knowledge, we demonstrated for the first time that OSTE does not inhibit RNA isolation, cDNA synthesis, or PCR, which is critical for downstream analysis of cells in LOAC. According to previously published data, PDMS has one of the most negligible inhibitory effects on PCR among many polymers similar to polypropylene (PP) or polytetrafluoroethylene (PTFE) that are used in conventional tubes, plates, and microchips used for PCR [44] . According to our data represented in Figure 2c , OSTE has the same properties in PCR as PDMS, suggesting that it is suitable for downstream analysis in LOAC and OOC overall or another PCR-based lab on a chip device for on-point diagnostics, for example. For the first time, we also demonstrated that OSTE does not significantly absorb EVs, which is similar to PDMS based on NTA data in Figure 2d . Commercial adipocytederived mesenchymal stem cell EVs were selected due to mesenchymal stem cell-derived EVs' therapeutic effect, which could be applied in the future for drug transport and treatment of different diseases [70] . Since EVs are involved in almost all physiological processes, including pathophysiology and recovery of lungs from respiratory diseases, including virus infection, they are promising candidates for future treatments, including vaccination [71] . Therefore, LOACs and OCCs overall or lab on a chip devices from OSTE could be potentially applicable for drug-loaded EV testing or isolation/production. However, microscopy remains a critical LOAC and OOC evaluation aspect, and many of the experimental results are dependent on reflective/transmissive light microscopy and confocal microscopy [72, 73] . A notable drawback of OSTE material is the optical characteristics. The most glaring difference between PDMS and OSTE is the total transmittance, as seen in Figure 2a , which has been reported previously, especially for OSTE 322 that has substantial absorption below 380 and 420 nm [35] . This is significant because DAPI is a widely used fluorescent molecule with an excitation peak of 358 nm [74] . Furthermore, the nature of the loss of transmission is essential-previously, it has been noted that the loss occurs from the light scattering properties of OSTE rather than just pure absorption [32] . The scattering is also evident in the confocal microscopy images presented here, which lack the crisp image quality and detail seen in PDMS devices. Moreover, some reports have mentioned that heat treatment could affect OSTE optical properties [75] ; however, here the thermal treatment of OSTE LOAC was critical for bonding of OSTE and PC membrane. As a summary, see Table 1 for a comparison between PDMS and OSTE for various properties important for LOAC development. Table 1 . A comparison between polydimethylsiloxane (PDMS) and off-stoichiometry thiol-ene (OSTE) properties. Light transmission in the 300-800 nm range Good, comparable to glass Intermediate, multilayer chips suffer from significant light scattering Cell viability Good cell viability [23] Acceptable cell viability [48] Bonding to PC membrane Intermediate steps necessary to functionalize PDMS and membrane materials [40] Readily bondable via epoxy groups available prior to thermal treatment [47] Surface modification Intermediate steps necessary [76] Readily available -OH or -SH groups [31] Stability in chloroform and ethanol, solvents used for cell fixing Medium-poor [77] Good [75] Gas permeability High [78] Low [36] To capitalize on the improved material characteristics, further study would be necessary for understanding the light scattering cause in the material. An immediate research direction would be a thorough study of UV polymerization wavelength and intensity effect on OSTE transmission. Furthermore, given that there are two components in the OSTE mixture, the effect of material separation in time (for example, during the degassing step) must be evaluated. Further, the effect of thermal treatment must be carefully examined with the intent to evaluate if the temperature profile and absolute curing temperature have any effect on the scattering properties.@story_separate@In conclusion, this paper reports the use of OSTE materials for LOAC fabrication and contrasts it with conventional PDMS LOAC devices. As discussed in the respective sections, OSTE materials have clear advantages over PDMS in terms of device fabrication, large volume scalability, and small molecule absorption. However, current material optical characteristics impede widespread adoption of the OSTE materials for OOC, especially given the crucial contribution of confocal microscopy in OOC research. Therefore, for unlocking the true OSTE material potential, material transmission properties must be improved, emphasizing reducing the light scattering of the material.","Current in vitro models have significant limitations for new respiratory disease research and rapid drug repurposing. Lung on a chip (LOAC) technology offers a potential solution to these problems. However, these devices typically are fabricated from polydimethylsiloxane (PDMS), which has small hydrophobic molecule absorption, which hinders the application of this technology in drug repurposing for respiratory diseases. Off-stoichiometry thiol–ene (OSTE) is a promising alternative material class to PDMS. Therefore, this study aimed to test OSTE as an alternative material for LOAC prototype development and compare it to PDMS. We tested OSTE material for light transmission, small molecule absorption, inhibition of enzymatic reactions, membrane particle, and fluorescent dye absorption. Next, we microfabricated LOAC devices from PDMS and OSTE, functionalized with human umbilical vein endothelial cell (HUVEC) and A549 cell lines, and analyzed them with immunofluorescence. We demonstrated that compared to PDMS, OSTE has similar absorption of membrane particles and effect on enzymatic reactions, significantly lower small molecule absorption, and lower light transmission. Consequently, the immunofluorescence of OSTE LOAC was significantly impaired by OSTE optical properties. In conclusion, OSTE is a promising material for LOAC, but optical issues should be addressed in future LOAC prototypes to benefit from the material properties."
"e rapid development of computer information technology makes nursing management develop into intelligence gradually. Foreign researchers use Internet technology to connect nursing management to the hospital information network so that medical staff can obtain patients' physical signs data remotely. e nursing management system uses information technology to set various parameters in advance. e system can set various parameters in advance, the system can remind the patient to take medicine on time according to the parameters set, and the collection of the patient's signs' data through the Internet can be sent to the terminal, for doctors and family members to view. e social medical service system in China is not perfect. Most patients go to tertiary hospitals regardless of the severity of their illness, which makes the emergency departments of tertiary hospitals overcrowded. When emergency medical resources are inevitably dispersed to noncritical patients, the ability of the entire emergency medical system to treat critically ill patients will decrease. With the increasing workload of emergency departments, manual statistical data and information will be lost to varying degrees, and the preservation, collection, and analysis of emergency medical data lack norms and efficiency. e lack and lag of emergency information management lead to difficulties in the evaluation, monitoring, and analysis of emergency service process, medical quality, workload, and content. Medical information or medical services of digital, network, and information inaction is referred to by computer science and the modern network communication technology and database technology, for each hospital between patients and hospitals between their respective departments to provide information and management information collection, storage, processing, extraction, and data exchange and satisfy the functional requirements of all authorized users. With the development of wireless information technology, mobile communication technology, and Internet of ings technology, it is possible to effectively preserve and standardize the clinical data of emergency patients through the establishment of an integrated information system before and within the hospital. It has become an urgent problem for the development of the emergency department to strengthen the information construction and the application of nursing management data. Ang et al. aimed to better apply the nursing information system to nursing management, so as to promote the change of hospital nursing management mode [1] . Jiang proposed the application effect of emergency triage safety management in emergency nursing, effectively improving nursing satisfaction, reducing the incidence of safety accidents, and enhancing the quality of nursing [2] . Al-Fattah et al. proposed to implement the integration of information system platform, prehospital emergency, hospital emergency department, and other emergency procedures seamless; the sharing of medical information of all patients makes the communication between medical care more accurate and timely, optimizes the emergency work process, significantly shortens the rescue time of patients, and saves lives for patients [3] . Yu et al. proposed an intelligent clinical nursing management information system, which can systematically manage the basic work of nurses, monitor the quality of nursing work in real time, record the work of nursing, evaluate the performance of nursing, and so on, meeting the requirements of nursing management normalization [4] . Li et al. introduced an information zed nursing information system in the outpatient department to reduce the treatment congestion in the outpatient and emergency infusion room, improve patients' satisfaction in the clinical nursing process, and reduce the drug depletion rate and the incidence of nursing complaints and errors [5] . e application and development of nursing information system in the process of clinical medicine practice, the comprehensive development path and development needs of modern clinical nursing work in China show the characteristics of close correlation that cannot be ignored, and the prosperity and development of modern clinical nursing practice create and provide support and guarantee conditions that cannot be ignored [6] . Sharing the communication data information module with the primary hospital nursing information system can provide active support for the construction and development of the remote nursing expert system [7] . Nursing theoretical knowledge and practical experience can be shared in different regions and different hospitals, and remote nursing practice assistance guidance can be carried out. is is the sharing of advanced nursing practice experience across the country, enabling the comprehensive skills of daily nursing practice work organization at all levels of primary hospitals to continuously improve, thereby providing solid and sufficient experience and supporting conditions for construction and promotion.@story_separate@e system software consists of two aspects of database and application software,   system database through HIS database related tables, to  establish patients primary index, application area table, and  orders table, to establish relevant views, and to make nursing  system tables associated with HIS data table and data table  field be extended, so as to improve the efficiency of the system maintenance and access [8] . e system is developed by using 3-layer B/S architecture. e database is connected by Oracle DBC driver to realize the mutual connection of data. e hospital Intranet enables access to fixed workstations and mobile nursing workstations via the wireless network in the hospital. For other clients such as handheld nursing record terminals, nurse workstations, and mobile ward rounds, the system can be accessed through all compatible browsers except for PDA terminals. e PDA terminal is developed with the template of the Smart Device Cab Project, which can work online or offline. e specific software process is shown in Figure 1 . Care. e intelligent emergency centre applies intelligent information technology to the emergency environment with strict realtime requirements. As the intersection of modern information technology and medicine, the intelligent emergency centre has broad development space and practical value. According to the current situation of the low emergency degree in emergency centres and combined with the urgent and hectic characteristics of emergency medical work, an information project was designed and implemented [9] . Based on the hospital network platform, the layout structure, business process, service management, and quality control of outpatient and emergency departments are optimized in an all-round way, so as to alleviate the pain of patients. In this whole process link to develop the corresponding information system, intelligently building this platform, with platform ""all-in-one-card"" as the core, it runs through the whole process of prehospital appointment, in-hospital reception and treatment, posttreatment inquiry, and evaluation: making an appointment, self-help leading examining, selfhelp do card, prepaid phone, registering, clinic queuing howl, outpatient doctor workstation, taking medicine denominated in a queue, auxiliary diagnosis department charge confirmation, blood line up your turn, check the line up your turn, inspection result message booking, inspection report self-service printing, mobile infusion, EICU intensive care, patients self-help query, hospital self-help evaluation system, restructuring the medical treatment process with the implementation point deduction mode, changing the traditional medical treatment mode of outpatient department, realizing the complete sharing of business data, the maximum optimization of the medical treatment process, and the quality supervision of the whole process. According to the work unit of the emergency centre, the digital system designed eight subsystems, including registration workstation, triage workstation, consulting workstation, emergency room workstation, ICU workstation, doctor workstation in observation ward, nurse workstation in observation ward, infusion room workstation. Registration workstation is to complete the patient's basic information entry and registration. e triage workstation will guide the patient to the corresponding consultation room according to the condition. e clinical workstation completes the treatment of very urgent and critically ill patients. According to the particularity of the work of the emergency room, the functions of the emergency room workstation include the management of access to the department, the treatment of medical orders, the documentation of the course of the disease, and the management of expenses, realizing the automatic valuation of medical orders and the automatic generation of various disposal orders. e ICU workstation system is equipped with a nurse station and doctor station, automatically generating special nursing record sheets, physical sign observation sheets, and other nursing medical documents and automatically completing fluid balance calculation, critical score, and other services. According to the patient's postoperative recovery, the best treatment plan was developed. e system also supports research statistics, providing first-hand information for the development of evidence-based medicine and critical care medicine. Under observation ward medical and nursing workstation system based on computer of HIS ward management pattern, design features, setting up beds, patient access management, orders issued, copying and filling in the inspection sheet, checklists, writing electronic medical records, medicine and nursing information of the input, pricing, cost management, nursing information management, information query, and so forth, realize the automatic valuation of medical orders, automatic generation of all kinds of disposal lists, reduce the number of manual duplicate copies of medical orders, reduce the workload of medical staff, and realize bedside record of patients' condition information. Set up patient emergency treatment information, vital signs' information, and other files. Complete relevant information input and inquiry processing of patients in emergency operating room workstation. According to the work requirements of the infusion room, the system is equipped with infusion sheet management and patient call response management to complete the call, input, query, and processing of the patient infusion sheet; generate infusion bottle label and respond to the patient's call and other functions. e basic mechanism diagram is shown in Figure 2 . Client application: an intelligent terminal used by medical staff to edit and consult business data through this application, which is the user's actual operation interface. It is mainly composed of two parts: Android terminal and iPad. Application server: sandwiched between the database server and the client application, it plays the role of data exchange, is responsible for the implementation of the business logic of the whole system, can carry on the security authentication and the system upgrade to the client, has the role of connecting the preceding and the following, and is the core key of the whole system. Part of the application server is made up of mature and functional Microsoft IIS. Database server: the lowest level of the system, mainly composed of the existing information system of the hospital, which provides data and updates. His, LIS, and EMR are its main components. e monitoring framework is shown in Figure 3 . e intelligent nursing system is that nurses use mobile intelligent terminals to scan the barcode of patients' wristbands, accurately identify the identity of patients, input vital signs beside the patient's bed, and write the condition observation, so as to reduce paper transcribing, reduce workload, and improve the accuracy and timeliness of nursing work [10] . Nurses scan the barcode of medicines and check patients' wristbands through handheld smart terminals to avoid medical errors. At the same time, the closed-loop implementation of the doctor's order is achieved, and the traceability problem of the infusion and drug is realized. e mobile nursing subsystem not only ensures the safety of patient diagnosis and treatment but also evaluates the performance of nursing work, guarantees the safety of patients' clinical nursing, improves Journal of Healthcare Engineering the nursing level, and normalizes nursing behaviour [11] . e functional requirements of the intelligent nursing system are shown in Figure 4 . Bulletin board: this interface displays the basic information of the ward of the hospital, such as the number of patients in the ward, the grade of nursing, and the information of critical patients. e key point records at work should be viewed and edited by all nurses in the ward, and the head nurse needs to manage; the ward nurses work in some requirements of the record. Execution of medical orders: the mobile nursing subsystem is mainly to execute the clinical medical orders and complete the closed-loop operation of medical orders. Entering the doctor's order execution window and scanning the barcode on the patient's wristband, you can view the doctor's order items that the patient has executed and the doctor's order items to be executed. e patient's infusion bottle sticker, drug package barcode, and test tube barcode can be scanned again to complete the closed-loop execution of infusion and medication and test doctor's orders. e barcode of patients' wristbands, drugs, infusion, and test specimens can be matched one-to-one, which not only realizes the traceability of medical orders but also effectively fills the last ten meters of medical workstations and patients' beds. Nursing evaluation: within 24 hours of admission, nurses need to conduct admission nursing evaluation, fall bed evaluation, and pressure sores nursing evaluation on patients through mobile handheld terminals, so as to understand the basic situation of patients, determine the nursing level of patients, reduce a large number of nurses to copy labour, and improve nursing efficiency. Nursing records: all nursing services received by patients should be recorded through intelligent terminals, nursing data should be saved, patient nursing data should be summarized, effective management of nurse nursing services can be realized, service quality can be improved, and patient satisfaction can be promoted. Vital signs: the nurse can connect the internal network of the hospital through the intelligent handheld terminal and record the patient's temperature, pulse, respiratory frequency, frequency of urine, and faces beside the patient's bed, which will be synchronized to the hospital database server and sent to the doctor in time to provide a medical diagnosis. At the same time, the system can carry out the batch entry of vital signs and accurate and efficient entry of patients' nursing indicators. Voice call: when the infusion is about to end in the ward, the patient's family members or patients themselves need to go to the nurse station to remind the nurse that the infusion is about to run out, and the needle can be pulled out or the bottle can be changed. In practice, the information that ""infusion is about to end"" cannot be conveyed to the treating nurse due to various conditions of the patient. It not only affects the treatment of patients but also has life-threatening risks, and the timeliness of artificial oral replacement cannot be guaranteed. rough the intelligent nursing system, a direct voice call can be made to the current treatment nurse in the ward, informing the responsible nurse in the first time, so that patients can run fewer errands and much information. ermometer distribution: at this stage, the patient's temperature is mainly measured through a mercury thermometer, and then the nurse takes back the thermometer and records it. Due to the large number of rooms and patients in the ward, it is often not possible to clearly record the distribution of thermometers. e intelligent nursing system has the function of thermometer management, which records the distribution of the thermometer and reminds the nurse to recover the thermometer after the end of the measurement time, so as to achieve accurate management. Ward patrol: according to the time required by the standard of graded nursing, the nurse needs to patrol the ward according to the condition and needs of special patients. e mobile nursing system adopts a ward inspection interface, which is convenient for nurses to scan patients' wristbands, conduct ward inspections, record inspection data, and collect them at Journal of Healthcare Engineering the end of the month, so as to realize the assessment of nursing quality. Integration is to integrate several small systems with different functions into a large system with multiple functions by using systems engineering theory, so as to give full play to the comprehensive efficiency of the large system [12] . Intelligent emergency centre is an intelligent system with a complex structure that integrates HIS, LIS, PACS, critical care information system, emergency information system, and other functions related to emergency treatment. e system will be doctors and nurses operation process; inspection, application for inspection, and return of results; handling of charge accounting; and department management and other links into the computer management system, to replace the flow of people with information flow, to realize the emergency centre of the flow of people, logistics, capital flow, and other information implementation of decentralized collection, unified management, centralized use, and all the automated mode of sharing, to provide a fast navigation device or interface for medical staff. It is the emphases and difficulties of the research to enable doctors to switch quickly among various systems and apply various information synthetically. System integration is shown in Figure 5 . In the construction of the mobile medical system, the wireless network is the top priority. Only a scientific overall structure of the wireless network can achieve stable and high-speed data transmission. e ward in the hospital is different from the general conference venue. ere are many rooms in the hospital. e room area is large, the walls are thick, the corridor of the department is long and narrow, and there are various cables such as oxygen, circuit, and communication above the ceiling, and the environment is complex. In view of the above special environment and the actual situation of the hospital, wireless controller, wireless switch, and wireless access point are adopted to build the hospital wireless network [13] . e topology of the wireless network is shown in Figure 6 . Triage System. e automatic classification of quantified triage indicators and region-oriented intelligent triage system are based on the quantifiable triage standard system of emergency reexamination and triage, and the intelligent software of emergency reexamination and triage is developed by using computer technology to realize computer-aided reexamination and triage [14] . e intelligent triage system for patients' disease classification mainly takes the symptoms of body parts as the evaluation guide; inputs the patient's name, age, vital signs parameters, pain score, adjustment parameters, and so on the computer; and automatically generates the patient's condition grade. At the same time, a printer is connected to print out the patient's information and grade on red, yellow, and green printed paper and affixed to the patient's medical record. e principle of disease classification and treatment time in our hospital is as follows: red refers to Grade 1 resuscitation or endangered patients, and patients are directly sent to the emergency room in the red zone. e first-grade critically ill patients were rescued immediately and treated within 15 minutes in the red zone or the yellow zone. Level 2 emergency patients: according to the condition, emergency arrangement of priority diagnosis and treatment is generally done within 30 minutes of emergency treatment in the yellow area. Level 3 nonemergency patients are treated sequentially in the green area and treated within 2 hours. e triage desk is equipped with an auxiliary triage system connected to command platform, which releases the general information of the next emergency patient on the LED display terminal in advance, including gender, age, main illness, departure place, and approximate arrival time; makes full preparations for supplies; and informs the doctor in advance of the layout and equipment of 1 division. At the time of developing the intelligent triage system, the environment of the emergency room was modified to match its function. ree colours of red, yellow, and green represent the three areas in the transformed emergency department, and the background decoration and medical signs in each area are corresponding colours. e red area is the emergency room, the yellow area is the emergency treatment area, adjacent to the emergency room, and the green area is the general treatment area. In addition, red, yellow, and green landmarks are adopted in the emergency area, and the arrow guiding function is clear at a distance. e emergency hall provides TV and wireless network and other services to improve details and humanized services, which to some extent reduces the anxious state of patients waiting for treatment. Division nursing human resources allocation and management personnel management is mainly for the yellow area, the green area, and the prediagnosis table. Since the general diagnosis and treatment is divided into yellow area and green area, one more itinerant nurse is added to carry out treatment and health education and other services for emergency patients in the yellow area. e other nurse is mainly responsible for the order of the rooms in the yellow area and the green area and the coordination of patient diversion. Due to the use of the new intelligent system, the workload of prediagnosis increased. Two nurses were assigned to the post and the admission system of prediagnosis nurses was established. Only those with certain working experience, clear thinking, and qualified training of the new system could take the post. e prediagnosis nurse makes the correct disease classification, communicates with the itinerant nurse in time, and plays a guiding role in the patient's regional consultation. Patient safety control is primarily concerned with problematic areas of health care, particularly the identification of patients during the administration of drugs, transfusions, or blood products; the drawing of blood or the testing of other clinical specimens; or the delivery of treatment or procedures. e hospital's compliance with patient safety objectives is regarded as an independent decision item in the review and decision-making process, and the one-vote veto system is implemented. It can be seen that patient safety plays a very important role in the quality of hospital medical service. At present, barcode technology has been widely used in the system, outpatient and emergency mobile infusion management system, prescription dispensing subsystem, etc., providing a simple and convenient management approach for the safety control of outpatient and emergency patients. A barcode is a mark that expresses a group of information by arranging multiple black bars and blanks with different widths according to certain coding rules [15] . e bar is the part that is less reflective of light, and the empty is the part that is more reflective of light. Barcode technology is born along with the development and application of computer and information technology. It can be converted into binary or decimal information which can be recognized by the computer. It is a new technology integrating coding, printing, identification, and data acquisition and processing. In the process of medical service, the barcode can be used as the identification information and Journal of Healthcare Engineering scanned by the scanning gun to realize the rapid extraction of the corresponding patient information from the database of the computer system. e hospital tests equipment to achieve all online. e whole inspection process includes barcode label printing, barcode specimen identification processing, report feedback transmission, and self-service query and printing of clinical departments and outpatient and emergency patients. Barcode technology plays an important role in the diagnosis and treatment of outpatient and emergency infusion. In the past, the outpatient and emergency infusion is manually checked by the nurse, the liquid, and the filling of drugs. e nurse checks again and again, which is inefficient and risky. e outpatient mobile infusion management system automatically generates the patient's barcode information by computer and automatically prints two sets of infusion cards with identical information, including the patient's name, gender, age, drug dose, concentration, drip count, time of treatment, and other information. One is given to the patient for scanning at the time of injection, and the other is affixed to the pouch. Before the injection, the nurse uses a handheld computer to scan the barcode of the infusion card on the soft infusion bag. An electronic voice system automatically calls the number, and the patient hears the call and arrives at the puncture table. If the patient's line does not match the barcode of the fluid, the computer will send an alarm to remind the patient not to inject. After the infusion, the palm computer scanning, the patient's total bag of infusion, each bag of drug type, specific time, and infusion process can be clear at a distance, before pulling out the needle to understand the patient's treatment effect, after the confirmation of the end of treatment and then pulling out the needle. e use of barcode can effectively identify the identity of patients and drugs in both directions, which adds a safe defence line for outpatient and emergency infusion management. While optimizing the traditional workflow, it reduces the labour intensity of nurses and improves their work efficiency. e infusion card and bottle label are automatically printed to avoid the hidden danger caused by unclear handwriting by hand, check the infusion drugs with patients, and implement the system of three checks and eight pairs into the monitoring state to ensure the safety of patients' medication and treatment. Application. After the nurse scans the barcode of the patient's wristband through the intelligent terminal, the patient is identified and the detailed information of the patient is displayed, as well as the content of the doctor's order. According to the type of the doctor's order, the label is divided into infusion book, treatment book, medication book, doctor's order book, new start, and new stop. e nurse can directly click the execute button on the doctor's order information to execute or confirm the execution by scanning the outer package of the drug, the infusion bottle sticker, and the barcode on the test tube. Meanwhile, the system automatically records the executor and the execution time [16] . e diagram of the medical order processing module is shown in Figure 7 . By observing the patient's mental state, consciousness, and mobility, the nurse writes the nursing assessment sheet, the risk assessment sheet of falling on the bed, and the admission assessment sheet [17] . At the same time, nurses can record nursing and nursing measures in the intelligent terminal and can also save the written content as a template and view the historical nursing data. e module diagram of disease observation is shown in Figure 8 . After the nurse logs in the system, the patient is displayed in the form of a label, which contains the patient's name, gender, age, bed number and other basic information. After clicking the label, the detailed information of the patient can be viewed, including diagnostic information, nursing level, allergy drugs, attending physician, and other information. e nurse can also click on the patient's label to see the patient's medical record and examination report, as well as the patient's expense information: advance payment, expense details, and account balance. e comprehensive query module is shown in Figure 9 . Relief algorithm was first proposed by Kara, aiming at two classification problems, and its theoretical basis is as follows: a good feature should make the similar sample of the nearest neighbour characteristic values between the same or similar, and the nearest neighbour value differences between the different classes of samples or difference are very big, therefore giving each feature corresponding weights for sorting; the greater the weight of characteristic, the stronger the feature classification ability; on the contrary, the feature classification ability is weak. By setting the feature weight threshold or the number of feature subsets, the corresponding feature selection can be carried out, and the feature can be evaluated according to the feature's ability to distinguish the close samples. e main idea behind Relief's algorithm is that good features should keep samples of the same class close and samples of different classes away. Assume that the interval is defined as the maximum distance that the decision surface can move without changing the sample classification, which can be expressed as where Y x means similar to x and h x with x means nonhomogeneous nearest neighbour. e weight of the update attribute P of sample X can be expressed as 8 Journal of Healthcare Engineering Koromiko extended the Relief algorithm to get the Relief algorithm, which can be applied to multiple sample cases. When dealing with multiclass problems, the Relief algorithm does not uniformly select the nearest neighbour samples from all sample sets of different classes but selects the nearest neighbour samples from each sample set of different classes and selects k nearest neighbour samples. e weight update formula is Artificial Fish Swarms Algorithm is a swarm intelligence optimization algorithm based on fish swarm behaviour proposed by domestic scholars Ian Jixi and Li Xiao lei et al. According to the bionic characteristics of fish swarm, it simulates foraging, clustering, and tailgating behaviours of fish swarm by constructing artificial fish [18] , so as to realize Journal of Healthcare Engineering optimization. Artificial fish include the following behaviours. Foraging behaviour: assume that the current state of the artificial fish is Xi, and select a state x within its field of vision at random. If Yi < I, it will move further in this direction; otherwise, it will move one step at random. If the constraint is not satisfied, cut it out: Patients with acute and critical illness are mainly aged, and by referring to the previous data of researchers, three levels of proportions are summarized, among which patients with grade over 60 years old account for 58.45% of the total number of patients, and patients with grade A only account for 41.55%. It can be seen that the elderly population is typical potential acute patients, and the elderly patients with grade A are the focus group in the dynamic triage process. e correlation of blood pressure indexes in patients with II grade critical illness was gradually enhanced with the increase of blood pressure, suggesting that blood pressure was of great significance in the recognition of the disease in patients with the normal grade, while there was no significant correlation in patients with the mild or moderate grade. e possible reasons for the correlation were as follows: III grade acute patients with more acute symptoms, but the vital signs are stable, although there is a risk of deterioration, but there is no risk of life-threatening or disability, so the abnormal blood pressure exposure is not obvious; IV grade nonemergency patients mostly have no acute symptoms, and almost no exposure to abnormal blood pressure, or after brief exposure, it can be relieved and disappear after rest. Breathing and blood oxygen saturation index reaction is a more sensitive index for the human body, breathing >20 times/min, the total number of I∼III levels accounted for 89%, and 60.98% were III level; therefore the postoperative respiratory management of critically ill patients directly affects the survival and quality of patients, which is acute and potentially dangerous. Paying close attention to changes in breathing is important for early recognition and correct treatment. On the basis of preliminary construction of emergency patients nursing difficulty evaluation index system, the results summed up four categories of influencing factors: patient factor, nurse factor, organizational factor, and nursing equipment condition factor. Among the patient factors, 7 factors were extracted, including age, severity of disease, self-care ability, mental and psychological status, social support system, compliance, and expectation [19] . Patient expectation refers to the degree to which patients place their expectations and wishes on medical institutions before treatment, which is mainly reflected in treatment effect, treatment cost, medical service quality, length of hospital stay, and pain in the process of diagnosis and treatment. e influencing factors include individual needs of patients, commitment of medical institutions, medical experience of patients, and severity of diseases [20] . e firstlevel indicators are broken down as follows: the general condition of patients covers the age and complications of patients. Severity of disease in emergency patients, self-care ability: according to the industry recommended standard published by the National Health and Family Planning Commission Barthes Index Evaluation Table, the evaluation can be divided into four grades, which are, respectively, heavy dependence, moderate dependence, mild dependence, and complete self-care. Mental health status: the Hospital Anxiety and Depression Scale is a self-rating scale [21] , including 2 subscales of anxiety and depression, respectively, for 7 questions of anxiety (A) and depression (D). 0-7 are asymptomatic; 8-10 are suspicious. 11∼21 categories definitely have anxiety. Social and economic status: social and economic status is mainly evaluated by three indicators: social support, economic income, and education level. Compliance: observe the patient's compliance to treatment, nursing, and examination and comprehensively evaluate the patient's overall compliance by asking the patient's doctor in charge and the responsible nurse, which can be divided into five grades: very cooperative, cooperative, basic cooperative, noncooperative, and very noncooperative. In the completion of the design and coding of intelligent nursing information system, in order to ensure that the software shows fatal functional errors, improve the reliability and efficiency of the implementation of the software. Software validation is required according to software functional requirements. When testing the software, make sure you can deploy the appropriate operating system to the business server and install the basic operation profile and execution file. In this paper, 20 emergency patients were randomly selected and 5 outstanding nursing staff were selected to complete the basic operation, statistical function, and operational function tests of the software functional modules, and the test results of the functional modules were collected and recorded. e basic operation tests are shown in Table 1 . Ten emergency patients were selected for the test, and the basic operation tests of inquiry, physical sign entry, drug dispensing, nuclear drug, patrol, and nursing records were recorded, respectively. e test results are shown in Figure 10 . In order to verify the reliability of the statistical function of the intelligent emergency information system, the beds, workload, execution status, and reminder of the emergency department of a simulated hospital for one week were tested and entered into the system. e test results are shown in Figure 11 . e sensitivity of the intelligent nursing system to the screening of patients in the emergency was tested, and 20 emergency patients were randomly selected to enter the sign information. e sensitivity is shown in Figure 12 . e basic operation test in this paper mainly includes inquiry, physical sign entry, drug dispensing, nuclear drug, patrol, and nursing record test. e passing rate of basic operation test and system function test of the intelligent nursing system is more than 94%. Simulating the intelligent nursing system and summarizing the test data, the sensitivity of emergency patient screening was 82.22%, and the predictive index was 85.82%, but 90% are not ideal; it may be related to the small sample size in this study; I and II patients only accounted for 25% of the total; the emergency cancer accounts for about 75% therefore, the specific degree as an important evaluation index system, namely, the identification capability of the emergency patients. Calculating system specificity through artificial fish swarm algorithm, the specificity was 94.67%, and the prediction index for nonemergency patients was 91.03%, indicating that the system had a good effect on objective and quantitative identification of nonemergency patients. In the case of limited time and emergency resources, potential risk patients could be found, so as to ensure the safety of emergency patients and assist triage nurses in making decisions on a reasonable allocation of emergency resources. Relevant data are available upon request to all authors. e authors declare that there are no conflicts of interest. @story_separate@In this paper, an intelligent information zed nursing system is constructed to realize the information sharing in the whole domain, and intelligent auxiliary diagnosis and treatment information is provided to facilitate nursing inquiries of diagnosis and treatment information, which is of great help to patients' follow-up medical activities such as condition analysis, medical history grasp and supplement, and disease diagnosis. e data information stored completely through barcode intelligent information makes it more convenient for medical staff to collect cases and carry out nursing management. At the same time, nursing management online and bar code information collection, without manual transcription and triage, make nursing management faster, more accurate, and more reliable. is shortens the time for emergency nurses to complete the special care record form and effectively reduces the workload of medical staff. It maximizes the integration and utilization of quality-related information through emergency care terminal services.","This paper is combined with the intelligent nursing information system to build the emergency nursing platform architecture, from the system emergency procedures, system functionality, network environment deployment, and database design aspects of the discussion. Based on hospital information security, the nursing monitoring system of the intelligent nursing information system is constructed to realize network communication, which is clear and intuitive. The intelligent information system is applied to safety control, medical order information, condition information, and information inquiry, which can save working time and complete the rapid transmission and accurate execution of medical order, making the network communication of medical care more quick and convenient and maximizing the overall efficiency. Based on the disordered phenomenon of registration triage, the Relief algorithm is used to classify the aetiology and triage, and the combination of medical advice, information query, and IT technology is optimized, so as to eliminate the phenomenon of round diagnosis, insert number, and improve the medical environment of waiting for diagnosis, taking medicine, examination, and testing. Finally, through the testing of system information security, information traceability, and rapid information query, the problems in nursing management have been basically solved."
"In the present era, we witness that contemporary firms are highly competitive considering their industrial sector. And their supply chains have impacted a lot on the industrial sector mainly to improve the services of firms, along with creating the awareness for environmental, social, and governance dimensions in sustainability to promote their businesses. However, there are lots of issues considering environmental performances such as; deforestation, carbon emissions and footprint, pollution of the air, water, and soil resources, excessive use of non-renewable resources, social issues like workers health, safety and security, violation of labor acts and governmental regulations, social risk management, partner relationships considering societal impacts, education, and welfare measures. (Jamali & Karam, 2018) . These issues can raise concerns regarding environmental, social, and governance performances and innovative solutions are needed to reduce these impacts. Green manufacturing is one solution to aid the supply chain to improve green performances, but it takes care only of the environmental factors, as we see from the literature (Gandhi et al., 2018) . Also, we can observe from the literature that there are increasing considerations for sustainability and related issues in the supply chain context allowing for the present COVID-19 crisis (Chowdhury et al., 2021) . The organization can avoid or mitigate these issues to keep track of their supply chains toward creating resilient and sustainable supply chain (SSCM) for future (Lins et al., 2017; Rajesh et al., 2021) . Many firms have ongoing SSCM practices in India; however, researches in the direction to investigate the financial performances of firms and its' association with sustainability in supply chains is lacking in the literature, particularly in the developing country context. This paper examines the sustainable supply chain interventions that can benefit the environmental, social and governance performance and can simultaneously improve the operational and financial performance of firms. The purpose of this study is to investigate the financial performance of firms, where SSCM practices have been well implemented. We hypothesize that companies already adopted SSCM practices have significant positive impacts on their financial performances. We consider in this study, a sample of 25 leading firms that have adopted SSCM practices, successfully. To investigate this, we extract the secondary data of Environmental, Social and Governance (ESG) scores from a database of Thomson Reuters using Bloomberg terminal. We further investigate the direct relations that can influence the SSCM practices of Indian firms and to achieving financial performances. Through the measures of ROA (Return on Asset), ROE (Return on Equity), we relate the sustainability performances of firms with financial performances.@story_separate@Sustainable Supply chain management (SSCM) involves all the activities leading to reduce the carbon footprints of products and to improve the ecological, environmental, and social well-being. Sustainability as a concept has been adopted in supply chains, later evolved into the theory and practice of sustainable supply chains (Dubey et al., 2017) . Ecological modernization theory is one of them stating the monetary benefits for considering the ecological considerations. Although sustainable supply chain management was initiated in a direction to addressing the environmental and social issues, it can give significant benefits for the firm on a long run (Mathivathanan et al., 2017) . Several studies (Lee & Schniederjans, 2017; Narimissa et al., 2020a Narimissa et al., , 2020b Rajesh, 2021) confirm that sustainable supply chains have significant benefits considering competitive advantages and for having a greener image for future. The impacts of sustainable supply chain practices on the financial performances of firms were also explored in the context of several developed economies (Hong et al., 2018; Yusuf et al., 2019) . Several studies have discussed on the financial success of firms in a number of ways. The metrics used to assess SSCM's financial performance can fall into three categories as, lower costs, higher revenues, and increased stock prices. Several researches have looked at operating efficiencies, higher overall cost efficiencies, and risk control at reduced costs (Gopal & Thakkar, 2016; Miroshnychenko et al., 2017) . Also we observe from the literature that some studies have looked at higher sales, higher profitability, price premiums, new business entry, and social license to function under increased revenues, etc. Studies in the direction that sustainability initiatives leading to financial benefits are also seen in the literature (Esfahbodi et al., 2016; Feng et al., 2018) . Most of these works in sustainability are in the context of developed economies. Although some literature can be found linking sustainability and financial performances in the context of developing economies, an in-depth study linking the economic, social, and governance performances and financial performances are not seen in the literature. ESG scores and sustainability reporting practices are gaining increased attention in recent times. As we see, many organizations are seen manipulating the system just to avoid reporting several non-sustainable practices. This manipulating of practices will lead to cause a great damage to the environment, as well as the social life of people. The resources on this planet are limited and it is already alarming time to save them and the present generation will have to use resources in a sustainable manner for the existence and arrival of future generations. As we see in the Indian context, the resources are scarce and people are not getting sufficient supply to even fulfill their basic needs. This is high time is to move toward sustainable supply chain management for the benefit of people and the environment. If there are financial benefits associated with moving toward sustainability, then more firms will adopt sustainable supply chain practices. This paper is motivated from the scarce literature on observing the relation binding environmental, social, and governance performances and the financial performances of firms (Tseng et al., 2019; Wolf, 2014) . There are several researches conducted that analyze the sustainability of supply chains, but in case of India there are limited researches available that consider the sustainability performances and financial data of firms. Hence, we analyze the impact of implementing sustainable supply chain management practices considering the environmental, social, and governance indicators on the financial performance of firms in the Indian perspective. The governance side of performance indicators in sustainability has not been considered in most of the researchers conducted on studying sustainability impacts over financial performances. Also, the impacts of any controversies in ESG over the financial performances of firms are not studied in detail. Many researchers have considered the internal factors that influences the financial performance (Wang, & Sarkis, 2017) , while many other researchers have considered only the economic dimension of sustainability and firm performances. Hence, we consider the internal as well as external factors to find the relationship between SSCM practices, based on ESG and financial performance considering the profitability of firms. This research has three major objectives shown as follows. • To investigate the sustainable supply chain practices and ESG scores of firms in the Indian context. • To examine, if the firm performance in the environmental, social and governance side can positively affect its financial performances in the Indian context. • To investigate whether any controversies on ESG can negatively affect the financial performance of firms in the Indian context. We analyze the connection between ESG ratings and firm performances, where firm performance includes both accounting and market results, and where profitability is defined through ratios of ROI and ROE. The remainder of the paper is organized as follows. Section 2 includes the detailed literature review on sustainability performances, which is followed by the proposed empirical model and hypothesis development in Sect. 3. Section 4 confers the methodology for analysis, which is followed by the results of the PLS analysis and the related discussions in Sect. 5. Section 6 elaborates the conclusions of the study, limitations and scope for further studies. Section 7 includes the implications of the study. Particularly in the last two decades, the debatable topic is the social and environmental practices in companies and how it can play a significant role in increasing the brand image of company and the value base in management and business ethics. The concept of environmental and social responsibility has been perceived by firms in various manners (Narimissa et al., 2020a (Narimissa et al., , 2020b . Social responsibility implies that organizations, not withstanding, expanding investor esteem, must act in a way that benefits society. Social duty has been progressively critical to speculators and purchasers who look for ventures that are gainful, as well as value adding considering the benefits to society and the environment. As we know in the fundamental idea of any business, society is not being considered as a partner. Hence, the need of social responsibility must be inculcated into business philosophies. Social responsibility, as it applies to business, is referred to as corporate social responsibility (CSR). Quite a few researchers have dissected the effect of corporate dependable practices and non-monetary data on business execution and have observed non-unique results. Several studies prove that there are significant positive impacts of environmental sustainability practices on financial performance variables (Ortas et al., 2014) . Wang and Sarkis (2017) investigated 500 large United States companies to examine the linkages among environmental, social, and financial performance and found a significant positive relationship of environmental considerations over financial performances. Tchaikovsky (2017) also found a significant positive relationship between environmental sustainability practices and financial performances of US-based firms. Jayaram et al. (2008) examined several firms that integrate sustainable supply chain practices and observed the relations to the performance of the company. Sybertz (2017) concluded that SSCM approaches involving environmental, social, and economic output can have a positive relation to firm performance. As per him, it is oversimplifying to comment that; embedding sustainability in a supply chain and seeking to optimize value for all those in the chain would bring financial benefits for all. Büyüközkan and Berkol (2011) analyzed the parameters of economic, social, environment performances of firms though Quality Function Deployment (QFD) model and House of Quality (HOQ) models. Balaji (2010) and analyzed the data of Small and Medium Enterprises (SMEs) in India and studied on the relations binding bankruptcy of firms and their sustainability performances. Garg (2015) examined several practices of chosen organizations in India and analyzed 20 firms in the manufacturing sector performing CSR activities and studied of the impacts of the same on performance of firms. Balatbat (2012) explained the effect of ESG scores on financial performance of firms that are listed in Australian Securities Exchange and showed that ESG scores are weak positively related to financial performance of firms in the Australian context. Hence, considering the extant literature in this direction, it is observed that the sustainable supply chain practices and the ESG performances of firms may have a positive impact on the financial performances of firms. We observe the same as a pilot study in the Indian context considering the data of 25 firms. A detailed literature on recent works related to sustainability performances of firms and linking the same with their financial performances are shown in Table 1 . We have utilized the ecological modernization theory as the base for proposing relationship between sustainability performances measured on ESG performances and linking the same with financial performances of the firm. Ecological modernization theory points on the economic benefits of environmentalism (Mol & Spaargaren, 2000) . The theory focuses on the environmental re-adaptation of economic growth and industrial development. Hence, the theory suggests that the economy and ecology can be favorably combined for the productive use of natural resources and the environmental media to enhance the labor productivity and capital productivity. Based on the theory, we assume that there is a positive relationship between the sustainability performances of firms and their financial performances. The ESG indicators that are taken for consideration in this study are detailed as follows. These indicators are provided by Thomson Reuters in their sustainability performance evaluation, based on the performances of firms in environmental, social, and governance indicators. Environmental, Social, and Governance (ESG) scores provided by Thomson Reuters can depict the sustainability performance assessment of firms, and provide directions for improvement in this area (Thomson Reuters ESG Scores, 2019) . Environmental gauges to measure environmental impacts of firms can act as a nature steward. Social gauges can measure how firms can handle relations with staff, merchants, purchasers, and understand the social hierarchy, where it works. Governance gauges can indicate the level of authority of a corporation, executive compensation, reviews, inner controls, and investor rights. Observed the effect of gender diversity on the board of directors in consideration of the financial performances and also observed the mediating role of environmental, social, and governance orientation in this relationship Pointed that gender diversity in turnover has effects on the financial performance of firms The study does not show information regarding a threshold below and above, which the effects may not manifest or reversed, respectively Recommended the use of critical mass theory and group heterogeneity theory to explore further on the topic Several studies confirm that the ESG scores can act as indicators of the sustainability performances of firms, although the economic dimensions of sustainability are not considered, as such in the measurement scheme. The ESG combined scores provide an idea of the adjusted and definite positioning of the ESG yield of a firm, based on the indicators in the ESG columns with an overlay of ESG controversies assembled from worldwide news sources (Thomson Reuters ESG Scores, 2019) . This score is designed to limit the ESG scores, considering the negative reports from media. The evaluations are done by incorporating this into the general ESG score, the impact of major and minor ESG controversies on emissions, labor controversies or any product or brand controversies. When the firm has not been put under the limelight of any such debates or controversies, then this discounted ESG score is equal to ESG combined score (Tamayo-Torres et al., 2019). During a particular financial year considered for the study, if the firm has been involved in any type of scandals, or the firms have to pay any kind of penalties that affects organization's reputation, then ESG controversies scores are generated (Thomson Reuters ESG Scores, 2019). The ESG controversies score will be deducted from the total ESG to obtain the discounted measure of ESG, called as the ESG combined scores. If the firms have seen the new developments related to negative events like disputes, fines, lawsuits etc., ESG controversies scores are generated and the same is calculated based on 23 of the latest ESG controversies topics, as calculated by Thomson Reuters. The empirical model developed for the study is shown in Fig. 1 . This model measures the impacts in financial performance due to attaining the sustainable supply chain practices. Based on the empirical model, we develop the research hypothesis for the study. Environmental sustainability of firms and their supply chains are evaluated based on different sub-parameters in ESG, such as the ownership of polluted property, disposal of hazardous waste, control of radioactive emissions, or compliance with environmental regulations of the Government etc. The firm's businesses relationship is considered as a long-term objective. So, it creates social responsibility to all the stakeholders. The political commitment is also significant in supply chains, as firms keep away from irreconcilable circumstances, when enrolling board individuals, do not utilize political commitments to get unduly great treatment and, perceptibly, do not take part in untrustworthy exercises. These practices can perk up the ESG score and legitimately sway on corporate money related outcomes in India. Environmental supply chain practices and green initiatives can directly contribute to the firms' ESG score and there are assumed positively effects on the performances of firms in the Indian context. Several researches on environmental sustainable practices examine the impact of ESG on public and organizations' economic performance (Albitar et al., 2020) . And many researchers concluded the positive effects of environmental performances on firm performances and in some cases direct relationships were not found. Social sustainability indicators also have a direct relation over overall ESG performances of firms. Researches in sustainable supply chains have particularly considered social sustainability and examined its focus on human values and concerns. In a broader sense, social concepts include public health, health and safety, local community issues, stockholder issues etc. (Uyar et al., 2020) . Moreover, some researches in this direction have shown positive impacts of social factors on overall ESG performances and firm performances, although direct researches in this direction are limited. Positive governance measures can directly impact the performance of firms (Albitar et al., 2020) . And from several past studies, ESG scores have been found positively related to organizational efficiency. A positive relation can exist among the governance performances and the firm performances in the Indian context. We observe this relation through establishing the empirical model. No formal governance is needed, when the potential conflicts of interests are minimal and the potential conflicts of interests increases, when the number Hypothesis 1c: Governance factors can positively and significantly impact ESG scores. As we discussed earlier, ESG controversies scores can negatively impact on the sustainability performances of firms (Dorfleitner et al., 2020) . This measure of sustainability is dependent on the discounted ESG data of ESG controversies with the ESG discussions overlay caught from worldwide media sources. These discussions and media coverage can badly impact the firm's image and it tarnishes the CSR activities entertained by the firm. We observe similar relations, as discussed in the previous section that environmental, social, and governance scores, separately can have positive and significant impact on the ESG combined scores, which are discounted for ESG controversies. Similarly, we also observe that environmental, social, and governance scores, separately can have negative and significant impact on the ESG controversies scores. The following hypotheses are formed based on the above discussions.  There can be a positive connection between the actual ESG score and the financial performance indicators of the firm, such as Return on Assets (ROA) and Return on Equity (ROE). ROA is an indicator representing how profitable a company is relative to its total assets. ROA provides a manager, investor, or analyst an idea to how efficiently the firm can be managed using its assets to generate earnings. ROA is displayed as a percentage. ROE is calculated as a measure of the financial performance and is calculated by dividing net income by shareholders' equity. As shareholders' equity is equal to a firm's assets minus its debt, ROE is considered as the return on net assets. ROE is considered as a measure of how effectively management can use company's assets to create profits. Similarly, the ESG combined scores can have a positive influence the financial performance measures, such as the ROA and ROE (Garcia & Orsato, 2020) . And the ESG controversies scores can have a negative influence over the financial performance indicators of the firm, such as the ROA and ROE. Based on the foregoing discussions the following hypotheses are formulated.  Structural equation modeling (SEM) techniques are widely employed in many empirical studies involving primary and secondary data (Rajesh, 2020) . This section covers and explains the partial least squares (PLS) approach used to obtain necessary results and to achieve research objectives. PLS is selected in this research, as it has some advantages over regression such as; it can robustly handle more descriptor variables than compounds. Also, it can handle non-orthogonal descriptors and multiple natal results (Cramer, 1993) . The predictive accuracy is high and there is a much lower risk of chance correlation. Even then, there are some limitations such as; higher risk of overlooking actual correlations and sensitivity toward relative scaling of the descriptor variables. To evaluate and investigate the relation binding SSCM practices with financial performances of Indian firms, we have chosen 25 Indian firms that are consecutively rated for their sustainability performances by the ESG ratings of Thomson Reuters and the data being collected from the Bloomberg terminal. About 54 firms appear to be continuously rated for their sustainability performances in the past 5 years, considering the Indian context (Thomson Reuters ESG Scores, 2019). We have done a sorting of these firms and selected well-established firms considering the market potential and narrowed down the number of firms under consideration to 25. We observe the information about corporate natural management, corporate administration, and divulgence for selecting the 25 established firms in the Indian context. The data of these companies were used to investigate the sustainable supply chain practices through ESG ratings and to examine the financial performances in terms of ROA or ROE. These 25 firms are indicative to study the relationship between sustainable supply chain and financial performance and the study need to be extended for more firms to make vast generalizations in the Indian context or in the context of other developing economies. Thomson Reuters' Environmental, Social and Governance scores (ESG) are the main source for this empirical analysis. ESG reports have also considered numerous internal social environment and governance management rankings, as well as external programs such as the socially responsible supply chain management, environmental supply chain management programs. The ESG report of firms can offer full reports on the performance on corporate ESG and sustainability management. Environmental responses to supply chain management can demonstrate a company's attempt to minimize any reckless environmental actions within the supply chain. For instance, it can be observed to study how an organization can limit the material waste in its fabricating process, or permit providers to actualize progressed natural administration frameworks. On the other hand, social responses in supply chain management allow us to understand how much a firm or its supply chain underpins human rights and prosperity of laborers and welfare of society, considering the supply network. Adding to it, the green supply chain management initiatives can cover several areas of illegal practices harming the environment and conflicts of interest that the firm should avoid. The rating scores on governance performances can thus provide insights to distinguishing firms that are solely focused on internal sustainability management or externally applicable businesses and those firms, who are focused on both internal and external sustainable supply chain management initiatives. We observe the information on corporate financial reports from the annual financial statements (e.g., total assets, profit after tax, net debt, and sales before extraordinary items). Specifically, we measure ROA using the formula of profit before extraordinary objects divided by total assets to determining company's efficiency for using its investments to generate earnings. We also measure ROE and it can be observed that ROE ratios can vary considerably from high to low considering industry group and type of industries. The multiple linear regression (MLR) models have been applied for testing the empirical relations binding ESG data and financial performances. Here, we observe the data of 25 Indian firms who practice sustainability in their supply chain activities. Structural equation modeling (SEM) techniques can be employed for the empirical study and we use the partial least square (PLS) regression analysis of 25 Indian firms. All of these firms have practiced the SSCM initiatives during a considered period of 5 years from 2014-2018 for the study. PLS is done in a way to examine the constructs and the latent variables contributing to ESG performances and we observe those subsequent influences over financial performances. All SEM analyses were conducted using smart PLS 3 software and we use the partial least square analysis to find the result. The detailed results and discussion of findings are elaborated as follows. In this study, we observe the data of 25 Indian firms from the Thomson Reuters ESG and the information is extracted from the Bloomberg terminal. All of these firms were performing sustainability practices in their supply chains. The major variables considered and the basic descriptive statistics for the study are elaborated in Table 2 . As from the data, we see that 25 sample firms considered for the study has a mean value for ROA as 1.18, a mean value for ROE as 2.85. Our sample has different mean values for different ESG measures as; for ESG score, the mean value is 58.26; for ESG combined score, the mean value is 48.07; and for ESG controversies score, the mean value is 42.84. The descriptive statistics summarizes that the ESG scores are found to be increasing in subsequent years and ESG controversies show gradual decreases. The matrix of correlation among items is shown in Table 3 . We observe a positive correlation among the environmental, social, and governance scores, and the ESG scores, separately and the same is true for ESG combined scores. Also there is a negative correlation among these factors and the ESG controversies scores of firms. In this study, we investigate and the relationship among sustainability indicators of firms and the financial performances using path analysis in PLS. We have used the smart PLS 3 software and analyzed the impacts of ESG score, ESG combined score, and ESG controversies score on financial performances of firms. The results of multivariate tests using  We have checked the reliability of data to check for its appropriateness. To analyze the relation between the latent variable and the indicators of model, we have observed the values for outer loadings through PLS method. The square of the outer loading values can provide measures of indicator reliability. The indicator reliability should be greater than 0.5 (threshold level) (Yang & Green, 2010) . In the case of our samples, the indicator reliability values of latent variables are lying in the ranges of 0.4 to 0.7. In the reliability test, problematic indicators are removed, if their exclusion from the PLS model leads to an improvement in the average variance extracted (AVE). The composite reliability of items should be above 0.5 (threshold) (Yang & Green, 2010) . In our case for the latent variables considered, community score, management score and shareholders scores shows lesser values than 0.4. The outer loadings and the AVE measures, as observed for the model are shown in Table 4 . To check the model's ability to describe the indicator's variance, convergent validity is checked through the AVE values. Fornell and Larcker (1981) suggested an AVE threshold value of 0.5 as an acceptable measure of convergent validity. In our sample, the variables of environment, social and governance have AVE values 0.668, 0.567 and 0.335, respectively. We can see that the AVE values are satisfactory for the environmental and social factors, and in the case of governance, it is indicating a low convergent validity. This can be due to the small data set considered for the study. The model has been evaluated to analyze the coefficient of determination (R square). In our research, ROA, ROE, ESG scores, ESG controversies scores, and ESG combined scores are the latent variables considered. From PLS path model, the overall (R square) found to be acceptable. The coefficient of determination can vary from the minimal value 0.25 to 0.5 and then to 0.7, to describing a weak, moderate and strong predictable variance level (Hair et al., 2013) . The R square and the adjusted R square measures are shown in Table 5 . We use a small sample, and hence, the reliability measures appear low. This is a pilot study and this needs to be extended further to understand the exact effects of the ESG scores on financial performances of firms. The residuals matrices are constructed showing the residuals of each relationship. We have also conducted a goodness of fit analysis for a linear model. In smart PLS, the connections between builds can be observed using their path coefficients and the related measurements by means of the bootstrapping technique. The constructed path model is shown in Fig. 2 and the path coefficients and their significance on bootstrapping is indicated in Table 6 . The t statistics and p values are used to observe the significance of the relations representing the path. We conduct t-test with a significance level of 0.05 and if the t statistics in the path coefficient matrix is greater than 1.96, we conclude that the path is relevant and significant. Path coefficients considering many relations are not appearing significant (except marked in red color), as we consider only a pilot study in the Indian context with few firms of about 25 only. The study can be extended further to evaluate the hypothesis using more Table 6 , we can infer that the ESG scores, ESG combined scores and ESG controversies scores may not have significant effects on the financial performances of the firms, measured by ROA and ROE (see first six rows in Table 6 ). The results are not exhaustive as the results are based on a pilot study and the reliability indicators are low. In this study, we have observed the result of model fit of our sample size of 25 Indian companies. The datasets were observed from the Thomson Reuters and is extracted from the Bloomberg terminal database. The hypotheses were tested using t-tests and level of significance was taken as 0.05. To test the hypotheses of this research, we employ a bootstrapping using the smart PLS 3 software and have calculated the p value and compared those with the alpha value. The hypotheses were tested for various components representing the relationships among latent variables. The detailed results of hypotheses testing are shown in Table 7 . We have examined all hypothesis test results and out of them many hypothesis were not supported (except marked in red color) based on the observed sample size. As shown in Table 7 , Hypothesis 1a, Hypothesis 2a, Hypothesis 2b, Hypothesis 2c, and Hypothesis 3a are having acceptable t values. So all of these hypothesis are supported and the rest all hypotheses are not supported. The study can be extended to more companies to generalize the conclusions of the same. As this study is as a pilot study in this direction with limited number of firms under consideration, we possibly do not get enough evidence to support the financial gains of a firm, considering their sustainability performances. The hypotheses need to be tested for a large data of firms for the generalizability of the findings and to improve the reliability of the results. This is a stated limitation of the study. The study points to several theoretical and managerial implications. Considering the present situation of COVID-19, the problem is more relevant, as for managers it is important to understand whether the environmental, social, and governance performances of organizations can contribute to financial gains of firms. The theoretical implications of the study are in the direction focusing on ecological modernization theory. The very theory discusses on the economic benefits of environmentalism. Based on the theory, we assume that there may be a positive association among the environmental, social, and governance (ESG) scores and financial performances of firms. The positive association was not evidenced from the data of 25 Indian firms taken for analysis. Although the results were not conclusive, we observe that the environmental, social, and governance performances need not always result in financial benefits, particularly with developing economies. The results have to be validated with large datasets from developing firms. Thus, practitioners may be more interested in increasing the long-term profits or sustainable competitive advantages of firms. Hence, the assumption that the sustainability performances of firms can always lead to their financial advantages may not be always true. This is an important finding for managers to consider environmental, social, and governance performances on a non-profit making motive. These findings agree with the critics of ecological modernization theory that it fails to protect environment. Adding to it, critics argue that the theory cannot deal anything with the capitalist economic mode of production. Having profit motive in mind, while considering the environmental protection can certainly lead to environmental degradation, as per the critics of the theory. The critics of the theory points that it is form of green-washing and suggests that technological advances alone cannot achieve resource conservations or environmental protection; especially, if left to self-regulation practices in business. Hence, according to the findings of the study, it is not mandatory that environmental or social considerations in business can always results in financial gain. The results also suggest that good governance practices may focus on sustainable development, rather than short-term financial benefits of firms.@story_separate@We investigate the association between environmental, social, and governance practices of firms and their financial performances in the context of Indian firms. This investigation is a pilot study to explore the binding relationships. Utilizing information from the Thomson Reuters ESG and balance sheet of firm's reported data in the Indian context; we examine the sustainable supply chain practices. Here, we observe no significant connection between the joint execution of environmental, social, and governance practices and the financial performances of Indian firms, which are measured on basis of ROA and ROE. As we can see from the year-wise analysis that ESG benefit cannot occur in short-term, but it may take some time to create its benefits for firms and to show positive reflections in their financial performances. This is also another limitation of the study considering the data. Supply chains become increasingly complex, as it connects the corporate exercises with the natural and social effects. It is assumed that the companies attempt SSCM activities can achieve benefits in their competitive performances and the same was evaluated in this study in the context of a developing economy. Specialists can use these binding relations to build up measurement models and to help firms to comprehend these effects. This can be a possible direction of future work. We have conducted the study in the Indian context and observed the data of about 25 firms. Hence, this can be considered as a pilot study in this direction and the same can be extended to several firms in the Indian context or to firms operating in developing economies, as well. This will help us to generalize the conclusions of the same. This is another possible direction for future work. Also, the data of Thomson Reuters include those firms that are major players in the Indian business and can afford to have well-established practices in the environmental, social, and governance directions. SMEs in the Indian context were not incorporated in the study due to lack of ESG data. Primary data can be collected for this and the effects of ESG indicators over financial performances in SMEs can be studied. This appears as a limitation and a potential scope of future research in this area. The study can be further extended in the direction as a comparative study among different firms operating in various developing economies, so that the similarities and differences, while considering the environmental and social factors and the financial performances of these firms can be studied. As already mentioned; this study considers a small data of established firms in the Indian context, who consider environmental, social, and governance performances well and are consistently rated by Thomson Reuters for ESG performances. The results of the study cannot be generalized for all firms, as the considered data size is small. But the results can throw insights into the direction that the implementation of environmental, social and governance need not always necessitate financial benefits for the firm. Also, the results points toward the critics approach to ecological modernization theory. This can be an interesting direction for future work, as researchers can explore more on ecological modernization theory and the economic benefits of firms, while considering environmental and social factors into practice. Also, the critics approach to ecological modernization theory can be studied based on a large data and the findings of the study can be validated or generalized. More details regarding the theoretical and practical implication of the study are discussed in the next section.","In this research, we examine empirically the impact of sustainable supply chain practices on financial performances, considering the case of Indian firms. Here, we use a sample of 25 Indian firms listed for their sustainability performances in the Thomson Reuters Environmental, Social and Governance (ESG) scores. The sustainability performance data have been accessed from the Bloomberg terminal, where the overall sustainability performance on ESG is measured as a discounted score on ESG considering various controversies on ESG reported for the firm. And for the study, we associate financial data using the profit indicators of firms. We perceive that the sustainable supply chain practices considering environmental, social and governance performances may not positively impact the financial performance measured by the Return on Asset (ROA) and Return on Equity (ROE), during the considered period of five years for the study. We construct the empirical model and use Partial Least Square (PLS) regression modeling to analyze the results. The study can be further extended for many Indian firms and for firms across different developing economies, as well. The major implications of this research are to observe for firms and their supply chains whether the implementation of Environmental, Social and Governance (ESG) practices can help them in achieving financial benefits, along with other competitive advantages. The research is built on the concept and theory of ecological modernization, which suggests for the economic benefits of environmentalism."
"Hendra virus (HeV) and Nipah virus (NiV) are paramyxoviruses of the genus Henipavirus with pteropid bats (i.e. flying-foxes; Pteropus sp., Family Pteropodidae) being the primary wildlife reservoir [1] . Evidence of henipavirus infection has been found across the range of pteropid bats from eastern Australia, southeast Asia, Bangladesh, India and Madagascar [2] . Henipavirus infection has also been found to be present in Eidolon helvum, a species of fruit bat (Family Pteropodidae) that occurs throughout sub-Saharan Africa [3, 4] . The potential to cross species boundaries from bats to domestic animals and humans causing fatal infection appears to be a consistent feature of henipaviruses wherever they have caused disease (Australia and Asia). Given that over two billion people live in the area where Pteropus or Eidolon bats are present, even sporadic spillover to humans may result in a significant number of human infections. Henipaviruses have the potential to infect a wide range of mammalian species, and Hendra virus has spread from flying-foxes to horses in Australia on at least 20 reported separate occasions (five involving horse-human transmission), most recently in 2011 [5, 6, 7] . Seven humans have become infected with HeV via contact with infected horses, resulting in four fatalities [5, 8, 9] . In peninsular Malaysia and Singapore during 1998 and 1999, Nipah virus infected pigs and humans resulting in the death of over 100 humans and the culling of over one million pigs [10] . Since that time, there have also been at least 10 outbreaks of NiV disease in humans in Bangladesh and India, with the resultant death of over 140 people. There is also clear evidence of humanto-human transmission of NiV [11, 12] . In spite of the major health concerns, the knowledge of the epidemiology and ecology of these viruses is limited [1, 13, 14] . How the viruses are maintained in bat populations is not fully understood, nor is how the viruses avoid extinction as their host species become immune. In addition, whether these viruses are predominantly horizontally or vertically transmitted is also uncertain [12, 15, 16] , with the apparent viral latency and recrudescence in some human HeV and NiV infections suggesting that henipavirus infection dynamics may differ significantly from the closely related morbilliviruses [17] . A previous study by Plowright et al. [14] on the infection dynamics of HeV in the little red flying-fox, Pteropus scapulatus, in the Northern Territory of Australia suggested that viral transmission may be predominantly horizontal, with pregnancy and lactation suggested as risk factors for infection. However, Plowright et al. [14] sampled multiple colonies over time, leaving the possibility that sampling was not confined to a single population. Here, we focus on the transmission of HeV in a single colony of flying-foxes over a 25-month period that was approximately 10 km from the location of a spillover of infection to a horse and human in October 2004 [5, 8] . We sampled the spectacled flying-fox, Pteropus conspicillatus, a species which is restricted in distribution to the Wet Tropics bioregion of north Queensland [18] . We present data on the infection dynamics of HeV within this flying-fox colony over the 25-month period. We investigated the association of antibody response to the lifecycle stage of the host and the hypothesis that HeV is maintained by episodic infection with periodic virus outbreaks taking place.@story_separate@A total of 521 Pteropus conspicillatus were sampled over the six sampling sessions with an overall seroprevalence to Hendra virus of 56% (95% C.I. 51-60). The logistic regression model that included age, sampling session, sex, reproductive status and weight best fitted variance in seroprevalence, thus these variables were analysed in more depth (DAIC c = 0.00; v i = 0.57; all other models DAIC c .2 and v i ,0.2; Appendix 1 in supplementary information). Models that included two-and/or three-way interactions had DAICc.5, thus were not investigated further. Weight and forearm length predictor variables were highly correlated (r 2 = 0.72), but the model that included forearm length and not weight had limited support (Table S1) . Temporal variation. Seroprevalence steadily increased over the six sampling sessions from 44.7% in January 2005 to 69.4% in February 2007 ( Figure 1 ; Table S2 ). Variation with sex and reproductive status. Seroprevalence of female bats did not differ significantly from male bats (female: 58.7%; male: 53.7%; log binomial regression p = 0.25; Table 1 ). However, pregnant females had a significantly higher seroprevalence than both males and all other female bats (pregnant females: 70.3%; all other bats: 54.6%), and were 1.3 times more likely to have antibodies to HeV than the rest of the bats sampled (95% CI: 1.03-1.61; log binomial regression p,0.05). Bats sampled in early lactation had a significantly higher seroprevalence than male and all other female bats (early lactation: 75.0%; all other bats: 54.9%). Hence, those in early lactation were 1.4 times more likely to have HeV antibodies than the others sampled (95% CI: 1.05-1.78; log binomial regression p,0.05). When seroprevalence was compared among the reproductive categories of adult females, pregnant bats and those in early lactation had a significantly higher seroprevalence than nonreproductive adult females (pregnant: 26 Variation with age. Seroprevalence was highest in bats of the adult category (60.3%), followed by juveniles (58.3%), while sub-adults had significantly lower seroprevalence (39.8%) than both adults (logistic regression p,0.001) and juveniles (logistic regression p,0.05; Figure 2 ; Table S3 ). Variation with bodyweight and forearm length. Seroprevalence did not show statistically significant variation with bodyweight and forearm length although a non-significant linear trend was observed between seroprevalence and bodyweight (Table S4 ). Hendra virus antibody titre levels were determined by serial dilution of the sera in the virus neutralisation test. Significant differences in titre levels were found according to sampling session (Kruskal-Wallis test p,0.0001), bodyweight (Kruskal-Wallis test p,0.0001), pregnancy status (Wilcoxon test p,0.001) and sex (Wilcoxon test p,0.01; Table S5 ). Age, forearm length and lactation status were not significant risk factors for antibody titre level (Kruskal-Wallis and Wilcoxon tests p.0.05). Antibody levels varied greatly across the sampling sessions with animals sampled in January 2005 having a median titre of 15 (mean rank 25.7) followed by consistent increases to a peak median titre of 80 (mean rank 205.6) in September 2006, followed by a drop to 30 (mean rank 58.8) in February 2007, the final sampling session. Individuals of greatest bodyweight (.850 g) had the lowest antibody titres with a median of 20 (mean rank of 113.3). Females had significantly higher antibody titres than males (median titres 40 and 20; mean ranks 161.2 and 134.2, respectively) with the highest titres observed for pregnant females (median titre 80; mean rank 197.2). Those in any stage of lactation did not show a significantly higher antibody titre than the rest of the bats sampled. Previous studies have suggested that henipaviruses are maintained in flying-fox populations through episodic infection in a metapopulation structure, and do not persist endemically within a single population [14, 19] (See Figure 1 , panel C). Our findings do not support this hypothesis, but support an alternative pattern of endemic infection in the population. This endemic infection dynamic is consistent with a study on viral excretion of NiV in Pteropus lylei in Thailand, where seasonal excretion of virus was observed to occur from the same small colonies each year [12] . Our findings on HeV antibody titre levels show a peak in September 2006 (median titre = 80; mean rank = 205.6) when all adult female bats sampled were at a late stage of pregnancy. This is plausibly consistent with a ''boosted'' immune response subsequent to the previous sampling session (March 2006: median titre = 40; mean rank = 73.96). The following sampling session showed a decrease in titre level (February 2007: median titre = 30; mean rank = 58.8), but an increase in seroprevalence from 62.1% in September 2006 to 69.4%. This finding is consistent with a period of increased viral transmission during late pregnancy that had resolved by the time the majority of females were lactating, as evidenced by the consequent increased seroprevalence. Indeed Pourrut et al., [20] have suggested that altered immune function in late pregnancy may cause a transient surge in viral replication of filoviruses in African fruit bats. Our finding that late pregnancy and lactation were risk factors for HeV seropositivity are concordant with results presented by Plowright et al. [14] on P. scapulatus. Furthermore, the reproductive cycle in other bat species has been linked to seropositivity and viral activity of filoviruses, coronaviruses, lyssaviruses and astroviruses [20, 21, 22] . Our study identified the highest seroprevalence in the first few weeks of lactation, indicated by a seroprevalence of 75.0% in females carrying pups ( Figure 2 ). Our study also supports the conclusions of Field (unpublished data) and Plowright et al. [14] that maternal transfer of HeV antibodies to juveniles likely occurs, evidenced by the higher seroprevalence of juveniles (58.3%) than sub-adults (39.9%), and a correspondingly higher seroprevalence of adults (60.3%). These findings are consistent with horizontal transmission of the virus, however the observed seroprevalence pattern does not preclude the occurrence of vertical transmission. Vertical transmission may contribute to viral persistence in bat populations, and there is evidence that vertical transmission of HeV occurs from experimental infection studies of flying-foxes and guinea pigs [15] and from natural infection in wild flying-foxes (Field unpublished data) [16] . Numerous viruses can be transmitted both horizontally and vertically (e.g. transplacentally), including human polyoma virus, bovine viral diarrhoea virus, feline leukaemia virus and parvoviruses (porcine, canine and feline) [17] . For females to be classified as adult in our study they must have shown signs of prior lactation (i.e. enlarged nipples; figure 2), and hence a previous pregnancy and lactation. Our finding that HeV seroprevalence in early lactation was significantly higher than adult females that were not pregnant or lactating (early lactation; 75.5%; not pregnant or lactating: 48.7%; p-value = 0.047) is evidence for a decline in HeV seroprevalence in females in the life stage following lactation. Such a decline suggests that detectable immunity to HeV is not long lived in P. conspicillatus, and the pattern seen may reflect seasonal variation in response to repeated exposure. This variation is contrary to the assumption that HeV induces long-lived detectable immunity in P. conspicillatus and P. poliocephalus (e.g. [14] ), and suggests that the transmission dynamics of henipaviruses may be different to those of the closely related morbilliviruses. Indeed, the mechanism of survival of henipaviruses at the population level appears more likely to be one of endemic infection, perhaps similar to that found in bovine viral diarrhoea virus, classical swine fever or some herpes viruses utilising persistent infection, or vertical transmission, as found in arenaviruses or retroviruses [17] . These patterns of infection require much smaller critical host population sizes, in contrast to viruses that demonstrate an acute self-limiting episodic infection pattern determined by: a build-up of susceptibles, introduction of virus, and environmental conditions that promote spread (e.g. measles, Newcastle disease virus or canine distemper virus; [17] ; See Figure 1, panel B) . A previous serial cross-sectional study by Plowright et al. [14] over a 16-month period on little red flying-foxes, P. scapulatus, sought to determine the factors that drive HeV spillover. Their study suggested that age, sex, body size, pregnancy, lactation, season and mating behaviour were possible risk factors for HeV infection, and that horizontal transmission was the major mode of transmission between individuals. They also reported a rapid seroprevalence decline between two successive sampling sessions. However, given their sampling at multiple locations, the expansive geographic distribution and highly nomadic nature of P. scapulatus [23] , it is plausible that they were not sampling the same population over time. In contrast, the species in our study, P. conspicillatus, is not a nomadic species and has a restricted distribution to the Wet Tropics of northeast Queensland [18] . Furthermore, our study was conducted within 10 km of a location where an HeV outbreak occurred in October 2004 [5] , and we collected samples from a single colony of P. conspicillatus on six separate occasions over a 25-month period. Consequently, we are confident that we followed the infection dynamics of a single population of P. conspicillatus over the study period. Nonetheless, interpretation of results from studies in wild animal populations should be made with care. Capture of bats by mist-netting provides a statistically non-random sample of the population, and the practicalities of sampling from a roost site of many thousands of individuals also precludes following individuals over time. To counter these issues, we sought to investigate potential bias and confounding effects where possible. Future studies on henipavirus infection dynamics in wild bats may benefit from: permanent marking of individuals to identify possible repeated capture and sampling of some animals; improved diagnostic capabilities to increase the probability of detection of viral shedding; and improved telemetry methods to enhance the understanding of movement of individuals between roost locations. Our findings do not support the episodic infection hypothesis for HeV persistence in our study population. Rather we suggest that endemic infection of P. conspicillatus occurs, perhaps with periodic pulses of viral transmission associated with late pregnancy and early lactation. The consistent increase in seroprevalence over the duration of our study, together with increasing titres over the first five sessions followed by a drop in titre in the last session, also suggest the presence of inter-annual factors may be affecting viral transmission. An increase in viral transmission associated with pregnancy in flying-foxes is plausibly concordant with the temporal pattern of some HeV incidents in horses in Australia [5] , and of NiV outbreaks in humans in Bangladesh and India [12] . This pattern suggests that the risk of henipavirus transmission from flying-foxes to domestic animals and/or humans is higher during the gestation period of flying-foxes. Thus, it is plausible that spillover risk may be uniformly spatially distributed wherever pregnant flying-foxes are present. The observed spatial clustering of henipavirus incidents may be confounded by: surveillance intensity (passive surveillance is the only method used, with heightened awareness of disease likely in areas where previous incidents have occurred); variation in flying-fox population density (there is evidence of increasing movement of flying-foxes in eastern Australia into urban and rural areas [24] ); variation in horse density and husbandry practices; and as yet unidentified predisposing ecological or environmental factors (e.g. climate). A scenario of persistent henipavirus infection with viral latency and recrudescence in flying-foxes has been proposed by Field (unpublished data) and Sohayati et al., [25] . Viral latency and recrudescence has also been shown to occur in a human HeV case [26] , and at least 12 human NiV cases [27] . This infection dynamic could lead to the endemic infection pattern seen in our study. However, it is plausible that different social structures of host populations (e.g. panmixia, metapopulation, seasonal aggregation) may favour different mechanisms of maintaining infection at the population level (e.g. predominantly horizontal transmission, predominantly vertical transmission, seasonal explosive outbreaks, repeated viral excretion by persistently infected individuals). Consequently, population structures and mechanisms of maintenance of infection may reflect the biology of the host species and level of ecological disruption, rather than only the biology of the virus. As such, it is likely that different host populations may have varying levels of risk of infection spillover to domestic animals and/or humans. All study animals were captured and sampled at the Gordonvale roost site (145u46.749E, 17u4.869S ). This site is in a 4.5 hectare mixed woodland and forest remnant, surrounded by sugarcane plantations and suburban housing. It has been permanently occupied by Pteropus conspicillatus for at least ten years, usually containing 40,000 to 60,000 individuals, constituting approximately 20% of the Australian population of this threatened species [18, 28] . This is the closest known flying-fox colony to the property where the HeV spillover event occurred in October 2004 [5] . The Australian population of P. conspicillatus is geographically isolated from other populations of this species in northern Papua New Guinea and the Moluccan islands of Indonesia [29] . Flying-foxes were caught in mist nets, anaesthetised with isoflurane (Isoflurane, Laser Animal Health Pty Limited) and oxygen via an anaesthetic machine using the protocol of Johnson et al. [30] for sampling. Data collected were sex, bodyweight, forearm length, approximate age and reproductive status according to descriptions detailed in Table S6 . Animals were marked with coloured acrylic lacquer on their hind claws to prevent resampling within the same session, and were then released at the site of capture after recovery from anaesthesia. Age classification of sampled bats was performed as described by Hall and Richards ([23] ; Figure 3 ; Table S6 ). Bats being carried by their mother were classified as juvenile (estimated age 0 to 3 months old; Figure 3 , A). Free flying bats that lacked signs of sexual maturity (e.g. small or non-descended testes in males; lack of enlarged nipples in females) were classified as sub-adults (estimated age 3 months to 2 years; Figure 3 , B, C). Bats that showed signs of sexual maturity (e.g. large and descended testes in males; visibly enlarged nipples indicating a previous pregnancy and suckling of young in females) but did not show signs of severe wear on all molar teeth were classified as adults (estimated age 2 to 8 years; Figure 3 , D, E, F). Bats that showed signs of severe molar wear on all molar teeth, including at least two molars worn to the level of the gingiva, were classified as aged (estimated age 8 years and older; Figure 3 , G). Female bats were further classified according to their reproductive status (Table S6 ). If a foetus could be palpated while anaesthetised bats were classified as pregnant (this is likely to represent females in the last trimester of pregnancy; [31] ). Bats from which milk could be expressed from their teats and were captured carrying a young were classified as early lactating. Bats from which milk could be expressed from their teats, but were not carrying young, were classified as late lactating (P. conspicillatus carry their young continuously for the first month after birth, after which time the young are left in a crèche while the females forage for the remainder of lactation [20] ). Bats that were not classified into any of the previous three categories, which would include females in earlymid pregnancy, were classified as non-reproductive. Blood samples were collected by venepuncture of the propatagial vein with a 23 or 25 gauge needle and 1 mL or 3 mL syringe depending on the animal size. Blood was allowed to clot in 2 mL tubes for 24 hours before centrifugation and separation of serum and storage at 4uC. Antibody titres to Hendra virus were determined by virus neutralisation test (VNT) according to [32] at the Australian Animal Health Laboratory (Geelong, Victoria); the World Organisation for Animal Health (OIE) reference laboratory for Hendra and Nipah virus diseases. The tests were carried out under biosafety level 4 conditions as Hendra virus is a dangerous human pathogen with a high case fatality rate and for which there is no vaccine or effective treatment [33] . A serum sample was considered positive if it neutralised 100 TCID 50 of HeV at a dilution of 1:10 or greater in the VNT, since bat sera at lower dilutions have produced a high rate of toxic or non-specific reactions on neutralisation tests. To investigate the association of potential risk factors with serostatus, data were analysed by multivariate logistic regression. We used Akaike's Information Criterion corrected for small sample sizes (AIC c ) for model selection [34] . Models were ranked according to the difference between the best-fitting model and the AICc value of model i (DAICc). The strength of evidence for alternative models was measured by AICc weights (v i ). For the potential risk factors identified by multivariate logistic regression to be important in explaining variation in serostatus, we performed log binomial regression analyses to further analyse their associations with serostatus. The relative risk of being seropositive was determined for these predictor variables. Due to the smaller sample sizes, Fisher's exact test was used to investigate the hypothesised effect of reproductive status on serostatus in adult females, where serostatus of non-reproductive adult females was compared with those categorised as either pregnant, early lactation or late lactation. As titre levels were not assumed to conform to an a priori distribution, two measures appropriate for comparing titre data among groups where serial dilution of sera produce logarithmic dilutions were used. These measures were the median titres with an interquartile range, to indicate statistical dispersion, and the mean rank titres, which indicates the mean rank value for the titres of animals within a particular category when all animals are ranked according to titre level. Subsequently, non-parametric models were fitted to the data. For risk factors with two levels, a simple Wilcoxon test was performed. For risk factors with three or more levels, a Kruskal-Wallis test was performed. All animal work followed the guidelines of the American Society of Mammalogists guidelines [31]  Table S1 Model selection results for the effects of risk factors and the seroprevalence. The model selection statistics are: number of parameters in model (K), Akaike's information criterion corrected for small sample sizes (AIC c ), difference between model i and the model with the smallest AIC c (DAIC c ), AIC c weights (v i ) and evidence ratios (v i /v j ). Only models with DAIC c ,5 are shown. (DOC) Figure 3 . Features for classification of age categories of Pteropus conspicillatus used in this study. Age classification features (highlighted by a red circle in B, C, D, E) for both sexes. Key features include: juvenile bats (A) carried by their mother (estimated age 0 to 3 months old); sub-adult bats (B, C) were free flying that lacked signs of sexual maturity, including the lack of enlarged nipples for females (B) or small or non-descended testes for males (C; estimated age 3 months to 2 years); adults bats (D, E, F) showed signs of sexual maturity, including visibly enlarged nipples indicating a previous pregnancy and suckling of young in females (D) or large and descended testes in males (E), and but did not show signs of severe wear on all molar teeth (F; estimated age 2 to 8 years); aged bats (G) showed signs of severe molar wear on all molar teeth, including at least two molars worn to the level of the gingiva (estimated age 8 years and older). doi:10.1371/journal.pone.0028816.g003@story_separate@An improved understanding of the infection dynamics of henipaviruses in bat populations should facilitate the development of effective risk management strategies for disease spillover from bats to domestic animals and/or humans. Here we show that HeV infection in a population of Pteropus conspicillatus is likely to be endemic rather than episodic, as previously proposed for HeV in flying-foxes. We also present evidence for seasonal viral activity suggesting that immunity to the virus may wax and wane on a seasonal basis. These findings should inform disease risk management and approaches for modelling henipavirus infection in bat populations. If the ongoing threat that these viruses pose to public health is to be mitigated, further work is required to clarify the principal mode(s) of transmission of henipaviruses in bats, and to comprehensively determine how these viruses persist in their reservoir hosts.","This study investigated the seroepidemiology of Hendra virus in a spectacled flying-fox (Pteropus conspicillatus) population in northern Australia, near the location of an equine and associated human Hendra virus infection in late 2004. The pattern of infection in the population was investigated using a serial cross-sectional serological study over a 25-month period, with blood sampled from 521 individuals over six sampling sessions. Antibody titres to the virus were determined by virus neutralisation test. In contrast to the expected episodic infection pattern, we observed that seroprevalence gradually increased over the two years suggesting infection was endemic in the population over the study period. Our results suggested age, pregnancy and lactation were significant risk factors for a detectable neutralizing antibody response. Antibody titres were significantly higher in females than males, with the highest titres occurring in pregnant animals. Temporal variation in antibody titres suggests that herd immunity to the virus may wax and wane on a seasonal basis. These findings support an endemic infection pattern of henipaviruses in bat populations suggesting their infection dynamics may differ significantly from the acute, self limiting episodic pattern observed with related viruses (e.g. measles virus, phocine distemper virus, rinderpest virus) hence requiring a much smaller critical host population size to sustain the virus. These findings help inform predictive modelling of henipavirus infection in bat populations, and indicate that the life cycle of the reservoir species should be taken into account when developing risk management strategies for henipaviruses."
"The year of 2020 was marked by the outbreak of COVID-19 around the world. In Canada, while the closure of public schools lasted from March to September, university campuses remained closed in the Fall term, with courses having to be delivered online. As online education is not a new phenomenon per se, many researchers have already explored how teachers adapt their practices (Badia et al. 2017; Moreira 2016) and how teachers perceive online courses (Comas-Quinn 2011; Cook 2018; Gonzalez 2009; Murphy, Rodríguez-Manzanares, & Barbour, 2011) . However, the relationship between teachers and students in the online context, although relevant (Woods & Baker 2004) , is an area which has not been well explored from the teacher perspective, especially in higher education (Hagenauer & Volet 2014) . I confirm that this work is original and has not been published elsewhere, nor is it currently under consideration for publication elsewhere. Indeed, the abrupt shift to online teaching raised many questions as to how to adapt practices to this new environment. But while certain topics are indeed challenging (if not impossible) to be taught from a distance, a major concern that arouse from this context is around relationality. As a participant from this study observed, ""You can be socially distant, and you can wear a mask, you can protect yourself and students can protect themselves but there's no messaging around how we can foster interactions in spite of these challenges that we are facing"" (Fabien). When ethics is understood as relation rather than a component of it, as Levinas articulates, how can education (ethically) take place in an online environment? The purpose of this paper is to explore the ways in which the abrupt shift to online education unveiled the nature and challenges of an ethic of hospitality. Although stemming from the online context, the implications of online instruction to professors' relationality are also instrumental in illuminating the complexities and ambiguities of a teacher's responsibility even in what could be considered the ""normal circumstances"" of face-to-face instruction.@story_separate@Many of the benefits and challenges of online education have been widely registered in the literature (e.g., Afrouz & Crisp 2021; Davis et al. 2019; Jones 2015; Reamer 2013) . For instance, convenience, flexibility and inclusion of diverse learners are often noted as characteristics that have greatly contributed to the increase demand for online education in the recent years. On the other hand, course quality, disempowerment, privacy and isolation are some of the challenges of online education commonly noted by research worldwide (e.g., Afrouz & Crisp 2021; Burke & Larmar 2020; Davis et al. 2019; Reamer, 2013) . For that reason, many studies emphasize that teacher social presence is essential to students' engagement, satisfaction and learning in the online context (e.g., Raza et al. 2020; Song et al. 2019; Sung & Mayer 2012) . Social presence is a complex term, but it can be defined as ""perceptual experience of being psychologically involved in the interaction with others in a mediated environment"" (Song et al. 2019, p. 449) . The term can thus be understood as an attenuated form of presence, a feeling of community that allows learners to feel connected on a human level. Social presence is then often interpreted instrumentally as a way to keep students engaged enough in the class to learn the material. As Biocca et al. (2003) argue, social presence does not require physical presence and thus is not binary as physical presence is. The sense of being together can exist through the mediation of interfaces, hence the importance given to immediacy in the literature. Amongst the innumerous possible definitions, immediacy can be understood as ""the psychological distance between a communicator and the recipient of the communication"" which is conveyed ""through speech and associated verbal and nonverbal cues"" (Tu & McIsaac 2002, p. 134) . But while immediacy and social presence are common concepts in the related literature, they are mostly used either to address students' perceptions of their teachers or as strategies employed by teachers to foster student learning and engagement. There is a paucity of research, however, that investigates how such terms relate to and evidence the challenges and complexities of a teacher's relational responsibility in the online environment as an ethic of hospitality would require-a discussion that this paper seeks to bring forward. For Levinas (1972 Levinas ( , 1982 Levinas ( , 1995 , the singularity of the face of the Other (Autrui, absolute other) indicates how despite its proximity, alterity is irreducible, beyond comprehension, and something I am infinitely responsible for. For Levinas, the attempt to know, understand, categorize the Other, means trying to reduce the infinity of the Other to what I can comprehend, possess, thus an act of violence to their uniqueness. Therefore, rather than trying to understand how the Other differs from me, it is my responsibility to uphold and respond to the Other's uniqueness as it appears and without expecting anything in return. Responsibility, for Levinas, is thus immediate, asymmetrical and unlimited. Following Levinas' conceptualization of responsibility, Derrida dedicated great extent of his work to analyzing the question of hospitality. While Kant had touched on it in Perpetual Peace, in 1795, Derrida explores a conceptualization of hospitality that is not bound by territory but is rather unconditional, offered to the one we do not even know the name. Even though Derrida did not apply the metaphor specifically to the realm of education, its postulates have been greatly embraced by academics when conceptualizing hospitable education (e.g., Biesta & Egéa-Kuehne 2001; Bryzzheva 2018; Hung 2013; Peters & Biesta 2009; Ruitenberg 2016; Zembylas 2020) . Although the characteristics of hospitality can be analyzed in different ways (cf. Ruitenberg 2009), the metaphor may be well summarized by pointing out that genuine hospitality is unpredictable, unconditional, and uncomfortable. Firstly, Derrida emphasizes that hospitality is truly evidenced not when it is offered after careful planning. Rather, hospitality is expressed when the host is caught by the surprise of the unexpected knock on the door. As Derrida (2007) explains, ""there can be an event only when it's not expected, when one can no longer wait for it, when the coming of what happens interrupts the waiting"" (p. 443). While one may wonder about and conceptualize the arrival of an Other, genuine hospitality-as an event-is unforeseeable, vertical. The arrival of the Other, with its absolute alterity, will then always be a complete surprise. Applying the metaphor to the realm of education, Ruitenberg (2011) illustrates the idea with another metaphor: the empty chair carefully and thoughtfully placed in the classroom. The Other may or may not arrive, but if that moment comes, there will be room for them. In other words, the teacher-host must invest time in preparation for the student-guest who might arrive, but in a way that intentionally engages in the deconstruction of their own planning. As Ruitenberg (2016) puts it, the teacher should be constantly asking oneself: ""Does what I am about to do leave a possibility for my assumptions about knowledge and teaching and learning to be upset by a new arrival? Does it close down a space for future questioning or questioners?"" (p. 30). In a similar vein, when conceptualizing ethics as relation Derrida concurred that the self is unconditionally responsible for the uniqueness of the Other, whoever that may be, for only then will one be able to respond to the complete unknown Other: ""absolute hospitality requires that I open up my home and that I give not only to the foreigner…but to the absolute, unknown, anonymous other… without asking of them either reciprocity (entering a pact) or even their names"" (Derrida 2000, p. 25) . Receiving a beloved one at home may not be a big challenge, but ""hospitality is not merely receiving that which we are able to receive"" (Derrida 2007, p. 451) . Following this idea, Ruitenberg observes that the ethical challenge for educators does not lie on having a perfectly prepared class in response to who I think I already know or understand. As Derrida (2007) puts it, ""I must not even be prepared to receive the person, for there to be genuine hospitality"" (p. 451). If genuine hospitality requires not trying to mold or contain the Other into my own understanding, the challenge for educators is rather to respond to that ""fundamentally ungraspable … student … in a way that lets her or him be in otherness, that does not seek to recognize or otherwise close the gap with this singular other"" (Ruitenberg 2011, p. 32 ). Given its unpredictable and unconditional nature, genuine hospitality is thus inevitably uncomfortable for the host. Moreover, in addition to not knowing when or who, the host is a ""hostage"" to the Other (to use Levinas' terminology) for not expecting anything in return or ever even knowing if hospitality has in fact taken place. In fact, the aporia of hospitality is evidenced in its own definition, for to say ""welcome"" to the Other is already to demarcate the threshold of one's property. Saying ""make yourself at home"" is to already tell the Other that that is not their home. Derrida (2007) therefore emphasizes that ""the event, if there is one, consist in doing the impossible"" (p. 449) because the impossibility of hospitality ""continues to haunt the possibility… it may have taken place but it's still impossible"" (p. 452). The im-possibility of responsibility (Fagan 2016 ) also stems from the fact that there is always another Other present (or the Third, as Levinas put it). Therefore, as Fagan (2016) summarizes, within the demand for unconditional justice, absolute duty and so on, there is always already, immediately, the necessity and demand to submit to the demands of generalization, calculation and conditionality. It is this double imperative, from the limits of theory, which undoes the certainty of all of theoretical claims. (p. 97) The im-possible nature of genuine hospitality thus shakes teachers' sense of security and comfort that would stem, for example, from feeling in control of the class and having students responding to the lesson as expected. However, Ruitenberg (2016) clearly explains that, In an ethic of hospitality, the question is not whether the host feels comfortable in the presence of the guest. In fact, the openness to the other required in an ethic of hospitality means that the arrival of the other may destabilize the host's prior sense of self. (p. 30) In the midst of the challenges of an unprecedented time, the purpose of this paper is to explore the ways in which the abrupt shift to online education unveiled the nature and challenges of an ethic of hospitality, especially in the online context. A purposive sampling was used to recruit 12 professors who teach at faculties of education in 4 different Canadian provinces (Ontario, Alberta, Saskatchewan, and New Brunswick) and who were teaching online as a direct consequence of COVID-19. Semi-structured interviews were conducted with participants through Zoom and a pseudonym was attributed to each professor at the time of transcription. Each interview lasted for about 30 to 50 min. At the time of the interviews, professors had between 2 and 22 years of experience in their institutions. Participants had the opportunity to revise and edit their interview transcript if they wanted to. In order to protect professors' identities, I chose not to disclose their institutions or associate their quotes with provinces. This research was approved by the Ethics Review Board of the University of Manitoba. Although the small sampling may not allow this research's findings to be generalized to the whole country or other educational settings, this paper may serve to illuminate how the complex postulates of the theoretical construct of hospitality may inform teachers' conceptualization of relationality and responsibility regardless of the context. The collected data was thematically analyzed and reorganized with special attention to the complexities and ambiguities in enactment of a teacher-host's responsibility in the online environment, which I present and discuss in the next sections. Certainly, the shift to online education as a consequence of COVID-19 pandemic was unforeseeable in itself. But as most participants had never taught online, this new format was an added unprecedented experience. The majority of participants expressed how their first strategy was to try to reproduce the in-person class. As Fabien said: ""it's my desire to make it as similar to a face-to-face environment as possible"". However, some professors realized that in trying to do that they were often being unsuccessful, the lack of responsivity being one of its evidences, ""like pulling teeth"" (Helen) from ""a wall of silence"" (Anthony). This unforeseen context created a perceived ""strange muffled atmosphere"" (Brianna), which required participants to be attentive to different types of communication during class so as to overcome the uncomfortable silence they were encountering. Examples of those were the chat box and non-verbal icon reactions (e.g., thumbs-up, claps, smile). As Brianna observed, ""you don't realize how much you are tethered to the body signals of a group of people when you are teaching"". Communication out of class was another noticeable component that was affected by this new modality. Most professors mentioned how before the pandemic they would always arrive in class earlier or leave later and have their office doors opened for students to come in. During the pandemic, however, professors felt the urge to be tuned to their emails and available seven days a week and in non-habitual times ""because I never know when a student is going to write"" (Deborah). While having the doors of one's office open might be perceived as a gesture of unlimited welcoming, looking through the lens of an ethic of hospitality I argue that underneath such open-door policy is still the control of when and where the teacher-student encounter can take place. Then, when teaching online, professors felt that they were ""trying to enact that same kind of responsiveness and openness in the digital realm"" (Gabriella). But given that during the pandemic the only way that a student could reach out to professors was through email (or another online platform), unless professors turned off their internet/notifications they were bound to be contacted by students at any time. In other words, what might have been perceived as unconditional hospitality prior to the pandemic was still limited to the office hours or the physical presence of the teacher in the classroom before or after class. During the pandemic, what might have been perceived as unconditional hospitality could have been a reflex of the inevitability of being connected to the internet which, in turn, may have been conducive to their being in contact with students more often. In such context, professors could often assure students of their presence so they ""feel that someone is really caring for them"" (Ian). Furthermore, if prior to the pandemic professors could have taken longer to reply to a student's email, it seems that it was the present context that heightened their concern for students' well-being-an aspect that I return in the next section. So, although the literature shows that social presence is a much greater concern in online classes, what is clearly common among participants is how their perceived urgency was linked to the stress and challenges caused by the pandemic, not necessarily the immanent responsibility to the invisible face that would speak at an unpredictable time. This may be noticed by the fact that some professors observed that the familiarity with the medium became a potential threat to their own practices as well. That is, how the physical absence of students, even if just for a brief moment, led them to ""hide"" themselves behind the screen. As Fabien said, ""I need a constantly reminder that I'm going through, as I'm preparing for my courses, because it is quite easy to distance yourself from this because of the technology"". Jones (2015) observes that the quality of interaction is affected by the different education formats, spontaneity being part of the equation. Yen and Tu (2011) also note that the absence of non-verbal communication and social cues can pose challenges and lead to frustration and miscommunication between teachers and learners. As Davis et al.'s (2019) research also alluded to, some professors in the present study feel that the lack of connection makes the class ""a little bit less emotional"" (Gabriella) and impinges on students' sense of accountability, often leading them to be silent, disruptive and even disrespectful-situations they believe could have been avoided in a face-to-face class with different ""management strategies"" (Gabriella). As Helen illustrates: … it's interesting because a couple of times they all had their screens on and I jumped into the meeting and a couple of them shut their screens off... all I was doing was checking-in and they were shutting their screens off and I was like ""what just happened?"" I mean, I didn't say that to them but when I was jumping between the 4 groups I thought that was really weird. Maybe that's normal. I don't know, I don't know what's normal teaching online. But I thought it was a little weird, I was offended, put that way. I was offended. Having been removed from the comfort of the classroom and forced to enter an unknown environment was felt by most professors as a source of worry and tension. However, it also serves as a reminder that ""in an ethic of hospitality, [teachers] can offer hospitality to newcomers only in the spaces in which they themselves are guests"" (Ruitenberg 2016, p. 25) . Hospitality is not an exchange, but an unconditional, ongoing and unforeseen gift that stems from a unique encounter. Approaching students with pre-conceptualized gifts will not only fail to respond to students' uniqueness but also potentially become a burden to teachers who may feel unequipped or even offended by a perceived lack of acceptance of the gift. Recognizing her own tendency to expect reciprocity from students, Ruitenberg (2016) observes: If students leave classes early, or do not attend all classes, I may interpret this as a personal affront... But when I pause and ask myself who or what is being affronted, I discover a rather inhospitable and controlling teacher. (p. 81) Conversely, hospitable education is about responding to the uniqueness of the unknown Other in the unique context created by our unique encounter in time and space, without trying to reproduce what is known to me-i.e., responding based on what I have experienced (and mastered) in the past. At the same time that some professors saw the opportunity to be connected with people from all over the world as something positive, all participants commented how challenging it had been for them to meaningfully connect with students. An evidence of the challenge was mentioned by John, who noted that a teacher may be able to recognize students' faces, but not names, after years. Thus, not able to ""put a name to some of the faces"", a common remark among professors was ""I don't feel I know my students at all"", as Helen said. Mary's question was also at the heart of this study: ""I had half of them with a blank screen. And ethically how do you connect with a blank screen?"". Surely the psychological effect of students' physical presence in the face-to-face context may have outweighed any lack of social presence on their part (cf. Biocca et al. 2003) , which maintained professors' engagement and responsivity. Not surprisingly, unable to count on students' physical presence anymore, most professors emphasized how challenging it had been for them to be responsive to students because ""they feel much farther away from me relationally"", as if there were ""a big gap there"" (Anthony). As the literature also alludes to (Sung & Mayer 2012) , professors in this study were feeling ""disengaged"" with their students and that ""it's harder to care about people you don't meet"" (Edmund). Notwithstanding, a common observation among participants was that they were now even more sensitive to students' comfort, valuing their well-being more than the content to be taught or assignment deadlines, and more about ""learning how they are doing as people, on a more personal level"" (Ian). But the sensitivity to students' well-being also led professors to face the tension between their willingness to be supportive and the increase of their workload. In the interviews, professors stated how teaching had blended in their day, where the boundaries between work and home were not so clear anymore and where ""time has lost its rhythm"" (Deborah). Some participants were feeling how the amount of feedback and preparation was even causing them ""physical strain"" (Gabriella). As Jones (2015) observes, social presence in online education requires creativity and extensive investment throughout the week. While email communications were already present before the pandemic, this being the only way in which students could reach out to professors outside of the classroom time raises the question of when and how often teachers should respond to students. Burke and Larmar (2020) and Jones (2015) note that it is necessary that teachers of online courses establish clear boundaries with students from the outset so that interactions can take place in a timely and supportive manner while respecting the realities of instructors' own limitations. So, despite being potentially more sensitive to students' needs, some professors in the present study also commented that they had also been seeking to respect their own well-being and ""exercise boundaries to figure out worklife balance"" (Brianna). Gabriella, for example, mentioned that she opted for asynchronous classes for her own well-being and being able to care for her young child. Edmund, in turn, felt that his responsibility to himself meant recognizing his limitations and that ""there is no way I can offer the same course I did last year or the year before"". While there is an increased expectation of teacher presence in online education as research shows its positive influence on student engagement and learning, Ruitenberg (2016) explains that hospitality is not necessarily a student-centered pedagogy, because ""a strong student-centeredness can become a way of abandoning one's responsibility as an educator"" (p. 91). Expanding on Ruitenberg's original argument, I argue that hospitality is not necessarily student-centered because the host's wellness does matter. That is, because hospitality requires a guest and a host to take place, hospitality must be sustainable lest it eliminate the host. The threshold of one's property is both what makes the host a host and what allows the host to (un)welcome the guest. Some professors felt that they were more sensitive to students' well-being as well as their own, but they were still trying to understand how to do so without letting it impinge on their responsibility. It might be argued, though, that instructors were not necessarily more successful in attending to everyone's well-being, but that the pandemic context pandemic context made those efforts more pressing. Given that hospitality is not bound by one's professional duties nor contingent upon a pandemic, the question that arises to be further explored (Author, forthcoming) thus is: how can hospitality be infinitely asymmetrical and unconditional without neglecting the host's well-being? This study also complexified the notion of social presence and raised the question of what quality of presence demands responsibility. As it seems, teachers' perceived responsibility before the pandemic may have over-relied on the physical presence of the student. However, just like it is possible for the teacher to be socially present while physically absent, student presence escapes the presence of the face-the face is in fact just the trace of that which is infinite, as Derrida (1978) argues. When Levinas speaks of the face, he does not mean necessarily a literal face. The face is an expression of the proximity and infinitude of the Other, what cannot be reduced to perception (Fagan 2016) . The self's responsibility ""begins"" not by seeing the face but by hearing its voice, being exposed to the disquieting impact of alterity which ""places the ideas of my autonomy and identify in question"" (Fagan 2016, p. 53) . As Derrida (2007) observed, ""because I'm responsible for the other and it's for the other that I decide; it is the other who decides in me, without in any way exonerating me from 'my' responsibility"" (p. 455). Different from Burke and Larmar (2020) who, based on Nodding's ethics of care, claim that ""an online pedagogy of care begins with relationships"" (p. 6, italics in the original), hospitable education is not focused on the relationship between self and Other but on the Other as Other. While it does begin with a social encounter, hospitality (and hospitable education) entails an immediate and responsible response to alterity-although that is not a synonym of being Other-centered. While the pandemic context exacerbated the unpredictability of people's lives and heightened professors' sensibility to such unforeseen events (as every event is [Derrida 2007 ]), hospitality is not limited to compassion or empathy. Hospitable education goes beyond an ethics of care because it is not about morals, virtues or politics, but about immediate and unconditional responsibility, something that is not focused on the relation itself but the uniqueness of the Other. In other words, a teacher may not care about the student but that does not eliminate one's responsibility towards them. Responsibility, Levinas shows, is immediate. The face of the Other is always there, naked and vulnerable, thus making the self infinitely responsible for them. The way in which one chooses to respond, however, may or may not be responsible-whether it is out of care or not. Care or empathy in themselves are not enough because those can be ultimately self-centered relations (Todd 2003) . Therefore, a question that arises from these findings is whether professors' pressing and arguably tactful responses to students would still be haunting them were they not in the context of a pandemic. Finally, if the increased demand for online education in the twenty-first century appeared to threaten face-to-face classes, the interviews and outcomes of COVID-19 demonstrated how the teacher-host is indeed irreplaceable. Physical presence may not be necessary, but personhood remains essential not only to student engagement and well-being (Burke & Larmar 2020; Song et al. 2019 ), but to the possibility of hospitality. Research shows that teacher self-disclosure is a major component of social presence and positively associated with student satisfaction and consequent learning outcomes (Raza et al. 2020; Song et al. 2019) . Self-disclosure can be understood as a conscious and intentional process of revealing personal information to others which, Song et al. (2019) observe, tend to be carefully selected. However, the abrupt shift to online education meant that professors who were not used to teaching online when the pandemic hit had to suddenly face the ""vulnerable"" and ""intimidating"" challenge of being exposed to students on the camera in their own homes. Indeed, privacy is a major ethical issue that has been associated with online education (Reamer 2013 ). In the present study, only one professor said that they make the use of camera mandatory-albeit not penalizing students who claim to be having issues with the technology. In most cases, it seems that it was professors' own recognition of how uncomfortable being on the camera in one's home can be, ""like you are crossing into your private life"" (Deborah), that have led them not to demand that from students-although this is something they encourage and thank. The unconditional nature of hospitality suggests that ""the guest, the invited hostage, becomes the one who invites the one who invites, the master of the host"" (Derrida 2000, p. 125) . Because teachers had no option in synchronous classes but to have their cameras on (while not being able to demand the same from students), it seems as though they were experiencing not only a feeling of discomfort but also powerlessness to some extent. This context, however, pointed to relevant aspects regarding teacher control and identity, which are at the center of hospitality. As this participant notes: We're opening ourselves up in healthy and not healthy ways that we've never done before. And I think it gets at relationship, but I think it also raises questions about identity both for our students but also for us as professors. (John) Indeed, hospitable education is not about exercising mastery of a medium or being comfortable with the classroom (whether virtual of not) as if we owned it. In fact, as Derrida argued, genuine hospitality will necessarily be uncomfortable for the host. Because responding to the Other in their otherness means necessarily escaping the confinements of the self, educators in the pursuit of hospitable education should be in a constant process of self-reflexivity and questioning the ways in which their comfort with the classroom and practices may be a result of their ""irresponsibility"". Moreover, the abrupt shift to the online format meant that professors had to learn how to navigate platforms and use programs they were not used to, demanding extra effort and enhancing their sense of insecurity because ""I make mistakes, I hit the wrong buttons"" (John). Having to learn about new apps and technologies was thus perceived as ""time-consuming"", ""demoralizing"", ""tough"", something which ""introduces some layer that bumps up against our relationship and responsibility"" (John). While it is undeniable that learning new technologies can be time-consuming, their perceived insecurity may also be directly associated with a common idea that teachers ought to be in control of everything that happens in the class. Conversely, some professors had no choice but to accept students' help with technology during class-and some even received feedback after class to congratulate them on their success with it. Implications of these findings question the extent to which educators may over-rely on certain mediums as being the best approach to teaching a certain topic. An ethic of hospitality demands that the teacher-host be ready to adapt to unforeseen circumstances and respond to the uniqueness of each person in each unique context they are in-a complex and multifaceted challenge that the pandemic evidenced. In this study, professors' pursuit of immediacy to foster students' learning experience was not followed by the rewards they were used to getting. Students' faces and body language used to give professors a major sense of security in the face-to-face context, both as a source of feedback to their practices (e.g., knowing whether students were understanding or enjoying) and to sustain their perceived ""sense of control"" (Louise) (e.g., knowing who is in the classroom and doing what). As Charles said, ""If I see you smiling and looking happy, that's good"". But teaching in this new context ""brings out insecurities"" (Louise); ""I'm talking away about whatever and I have zero idea whether they are listening, if what I'm saying makes any sense, whether what I'm saying is any interesting… it's like I'm talking to a wall, literally"" (Charles). Therefore, while seeking to respect their privacy, being able to see students' faces and seeing that ""they are at least there"" (Ian) served to greatly alleviate professors' anxieties and insecurities. However, professors emphasize that at the end of the day, it is students' choice whether they want to have their camera on or not, and that their comfort should come first. Without the feedback they were used to having, though, professors often commented how challenging and demotivating the experience had been for them, frequently speaking in term of ""loss"", ""grievance"", ""loneliness"", ""void"", ""nothingness"" and ""emptiness"", which makes them feel that the class is ""not as alive, it's not as engaging"" (Anthony). The common small talk that used to follow professors in the hallway was contrasted with the abrupt ""End meeting for all"" button that ends the online class and whatever energy was going on between teachers-students. As Anthony said, not ""being fed the energy from the students"" and ""the rewards of feeling like a lift from being in front of the class, which I usually get"" is ""definitely not as fun"". However, it is possible to observe that seeing students' face also seemed to trouble professors. As Louise said, ""there's an emptiness of always talking to a screen, even if there is a person"". Deborah shared the same feeling: I feel excited when I see people in there, but there is also a loneliness… I was always sad after I left the meetings because we were not together, right? So it is with mixed feelings. I think one of the biggest challenges in trying to convert everything online is that there is this grieving process that, you know, in one of the classes I have 77 students, I'm not going to know them in the way that I am used to knowing students in the face-to-face environment. (Deborah) As professors alluded, this feeling of grievance may be due to the pandemic context we were living in, when physical distancing was not an option. However, Derrida (1978) observed that the face is not just a surface, the face ""is also that which sees…which exchanges its glace"" (p. 98). The face, as an expression of infinity, speaks; ""le visage me demande et m'ordonne"" [the face asks me and orders me] (Levinas 1982, p. 94) , which is why Derrida (1978) argues that ""violence, then, would be the solitude of a mute glace, of a face without speech"" (p. 99). Being able to see students but not necessarily have their social presence to a desired degree (as they thought they had before) was perceived as a tombstone, the remembrance of was not there, what could not be present. In his semiotic works, Derrida states that every sign (as the words) points to what is absent, it grieves. Every representation is a misrepresentation because what is unique cannot be re-presented, it cannot be contained in a sign-the face being but a trace of infinity. Thus, both seeing and not seeing students' faces behind the screen functioned as an illustration of that which cannot be contained. The subjective quality of social presence, however, is certainly something complex to measured (Biocca et al. 2003) and corroborates the idea that hospitality is not something that can be premeditated or limited to one's professional duties because it is contingent upon a unique interaction in a unique context. While most studies related to online education have focused on the ways in which teacher immediacy positively influences student satisfaction and learning, an ethic of hospitality focuses on the host's responsibility when receiving the Other, not on student achievement or learning outcomes. In other words, hospitable education is not a means towards an end-it entails a disposition that goes beyond what one will get as a result. Hospitable education is an immediate and infinite responsibility that is not bound by professional duties and not just a means to foster student learning and engagement. While the extent to which physical presence is (not) a synonym of social presence seemed to be unquestioned by professors, the interviews revealed how students' presence in the online classes is not a synonym to social presence. Notwithstanding, in the online context professors had to be contented with the unexpected ways in which students made their presence known and had to respond to their personhood ""as it turned up"" (to use Derrida's [2000] expression). The metaphysics of presence that seems to have guided professors' practices before the pandemic was put in question with the physical absence of students and greatly distressed participants who, even when seeing them, were not empowered to comprehend them as they used to. The undesirability of totalization and impossibility of grasping one's uniqueness was evidenced by the unreachable Other behind the screen. This study thus points to the asymmetrical and unconditional nature of hospitable education. The absence of students' faces, which professors relied so much on to guide their classes and their own sense of accomplishment, was heavily felt by participants who expressed feelings of loss, grievance and emptiness. Curiously, Derrida (2007) notes that ""there is a certain affinity between hospitality and mourning"" (p. 453). Not being able to rely on the power and familiarity with the medium they used to have served to demonstrate that in genuine hospitality we never know if we are being successful, if the Other has accepted the gift, if hospitality has taken place. Although before the pandemic professors may have assumed that students felt welcome based on their body language, the inevitable grievance of hospitality is its impossibility (Fagan 2016) and the fact that it is only the guest who may finally claim ""I am welcome"". One trait of hospitable education is the discomfort experienced by the host who is a hostage to the alterity of the Other. This study illustrated how responding to unique encounters may often lead to perceived feelings of powerlessness. Participants expressed uneasiness given the uncertainties of teaching online, especially as it relates to achieving their goals without the strengths they believe they had prior to the pandemic, such as supplying materials or being able to engage students physically. Mary, for example, commented how she had carefully and thoroughly planned an activity where students would do movements in front of the camera, but that she had to change it in response to students' reaction: I was so excited to get that started but they just stared at me and I was the moving one and they were sitting at their computer because I realized they hadn't associated movement with computer yet. They thought of computer as a sit-down modality. And I was introducing movement, that didn't register. So in that moment of responsibility I sat down and I calmed down and I said, ok, how are you? (Mary) As it seems, in the first moment professors' judgement of their responsibility was tied to and based on their face-to-face encounters. Such conceptualization is not only unproductive (given the impossibility of reproducing what cannot be present) but also posed a burden on professors who felt unequipped to fulfil their responsibilities. Some professors, however, observed how the unprecedented context they were teaching in was nonetheless ""forcing some healthy changes too"" (Edmund). Although participants emphasized that their pedagogical philosophies had not changed, most professors commented how much time they were spending rethinking their practices to be able to sustain their values while trying to achieve their goals and to ""connect with the learners in a more relational level"" (John). Some professors commented being aware, even before the pandemic, that each class is a unique response to the group of learners in that term. As Edmund puts it, ""I adapt very much depending on how the class is… I've never taught the same course, because every time is entirely different, because it depends upon the people in the room, the context, the space we are in"". Notwithstanding, it seems that the shift to online education led professors to be ""suddenly focused on their teaching by accident"" (Edmund), in an unprecedented way rethinking their practices and how to enact their responsibility in the unique context they found themselves and their students in. At the same time, however, and perhaps due to the pandemic context and the perceived lack of responsivity from students, professors often felt the need to provide students with more structure than they would in a face-to-face class: I think with the online environment there is a little bit less flexibility because when students start, you want to give them a sense of security, that is well-planed, it's there, [so] they can find what they need without having to resort to me all the time. So everything needs to be there and needs to be laid out perfectly. So the degree of uncertainty about how certain things will unfold, I think needs to be less. Because if there's too much uncertainty, students get lost more easily in the online format. (Ian) As it seems, the ethico-political aporia, or the im-possibility of responsibility (Fagan 2016) , which is inevitably present in the face-to-face encounter, appears to have been emphasized in the online (and pandemic) context whereby teachers may find it difficult to think about individual students when everyone is lumped together in a faceless mass-an aspect that Sung and Mayer (2012) alluded to. Derrida (2007) noted that an event is ""an event insofar as what's happening was not predicted"" (p. 456). He explained that an event also requires the possibility of repeatability for it to be an event, which seems to be highly pertinent to education. The frustrations professors faced did not only ""accidently"" lead them to rethink their practices-it illuminated the unpredictable, uncertain, and tenuous nature of teaching and relationality. Being forced to reassess one's teaching and the way they relate to students seems to have unveiled the complexities and ambiguities of an educator's responsibility in a more acute way than they might have perceived in normal circumstances. Although a class has to be repeatable in nature (which is what allows teachers to prepare their classes), it is also ""immediately engaged in substitution""; that is, ""substitution is not simply the replacement of a replaceable uniqueness: substitution replaces the irreplaceable"" (Derrida 2007, p. 452) . Hospitable education goes beyond what one can plan; it requires preparedness for that which cannot be repeated, for ""when I welcome a visitor, when I receive the visitation of an unexpected visitor, it must be a unique experience each and every time for it to be a unique, unpredictable, singular, and irreplaceable event"" (Derrida 2007, p. 453 ). Hospitality requires ""ignorance"" (Todd 2003) , undecidability, a constant suspension of decision, embracing the cipher without deciphering, however uncomfortable that might be. The tension between providing a structure to the course and being open to students' voices might have been heightened by professors' insecurities with the new format as well as trying to provide students with some certainties in the midst of an unprecedent context. It should be noted, however, that such undecidability is exactly what makes an ethic of hospitality possible and should not be perceived as a weakness-or perhaps be embraced exactly as the desirable weakness of education ). Conflicts of interest I have no conflicts of interest to disclose.@story_separate@The present paper sought to analyze the challenges and complexities of hospitable education and the implications of such ethic to teachers' practices which were unveiled by the abrupt shift to online education caused by the COVID-19 pandemic. While professors did not necessarily approach teaching with hospitality intentions nor is it possible to claim that hospitality has taken place in their courses, analyzing teachers' experiences in light of the postulates of an ethic of hospitality brought forward some of the complex and nuanced facets of hospitable education, especially when students' physical absence is not an option. Although the promise of repetition is a necessary characteristic of education and of hospitality, the unique context created by the COVID-19 pandemic served to illustrate the unpredictable, unconditional and uncomfortable nature of hospitable education-challenges to be faced in every educational setting and context. Although physical presence or at least being able to see students' faces may facilitate bonding, hospitable education means welcoming the Other without knowing anything about the student, making no assumptions, and completely surrendering to the infinite alterity of the Other without expecting anything in return. When everything the teacher-host relies on is taken away, what is left is the responsibility they have for the one whom they do not even know the name, for the ""voice without a look"" (Derrida 2000, p. 31 ).","With the outbreak of COVID-19 pandemic in 2020, teaching online became a norm for universities in Canada. Besides the challenges of teaching topics that may be impossible to be taught online, a major issue that the mandatory physical distancing brought is the relationality between teachers and students. In order to investigate how educators were making sense of such changes, semi-structured interviews were conducted with 12 education professors across Canada. In light of Derrida’s and Ruitenberg’s ethic of hospitality, this paper explores how the abrupt shift to online education unveiled the nature and challenges of hospitable education, especially in the online context. The implications of online instruction to professors’ relationality, however, are also instrumental in illuminating the complexities and ambiguities of a teacher’s responsibility even in what could be considered the “normal circumstances” of face-to-face instruction."
"Autoimmune disorders in humans are characterized by the breaking of immunological tolerance to self-antigens [1, 2] . Many autoimmune diseases involve specific target cells or organs, such as pancreatic ~-cells being destroyed in insulin-dependent diabetes type 1 (IDDM) [3] or the destruction of axonal myelin sheaths in multiple sclerosis (MS) [4, 5] . Although the pathological picture has been well documented, knowledge of the precise etiology for triggering the autoimmune process, the mechanisms of pathogenesis and the initial self-antigen(s) involved is still vague. In many instances, autoantibodies or autoreactive T cells are found to precede the onset of clinical disease by several years. For example, antibodies to glutamate decarboxylase or other islet cell proteins are detectable in prediabetic individuals [6] . This opens the possibility of detecting early 'self'-antigens that may be involved in the autoimmune process and of designing specific therapeutic approaches to prevent development of disease even after the destructive process has started [1] . Two new strategies, oral tolerance induction and the use of MHC class I restricted peptides to modulate the autoreactive immune response, have shown promise in this regard [7] [8] [9] . Viruses and other infectious agents have been implicated in autoimmunity on the basis of several findings. First, although most autoimmune diseases are associated with one or multiple host genes [2, 10] , environmental factors also play a role [11, 12] . Support for this concept comes from studies performed in monozygotic twins. For example, although studies link several haplotypes of the MHC with an increased risk for either IDDM or MS [3, 4] , the incidence of MS and IDDM has been shown to vary in monozygotic twins [4, 13, 14] and is also dependent on geographic location and lifestyle [1, 15] . Hence, autoimmune diseases have a muhifactorial etiology. Second, autoimmune responses and autoimmune diseases are newly generated or enhanced by infection with a variety of human DNA and RNA viruses in humans and experimental animals [16] [17] [18] [19] . Antibody to DNA, to RNA, and to red blood cells can be elicited (New Zealand white [NZW] mice) or levels enormously enhanced (New Zealand black [NZB] × [NZB × NZW] F1 mice) by persistent infection with a DNA virus such as polyomavirus or with an RNA virus such as lymphocytic choriomeningitis virus (LCMV) [19] . Superantigens have also been implicated in triggering autoimmune responses [20] . One potential mechanism whereby viruses could cause autoimmunity is 'molecular mimicry' [19,21°,22] . Viruses, as well as microbial agents, may possess protein structures or shapes that mimic normal host self-proteins. An immune response elicited against the pathogen will eliminate it, but can also cross-react with one or more self-antigens that share determinants with the agent. The immunopathological process could continue chronically or be reinitiated by multiple viral infections. Hence, disease could continue after the triggering agent has been eliminated (i.e. so its presence is no longer detectable); a 'hit and run' phenomenon. Evidence for such cross-reactive immune responses between viruses/microbes and self-antigens has been noted by many investigators [19,21°,22-24,25°°,26] . The influence of host genetic factors in the susceptibility to autoimmune diseases is taken into account as well in molecular mimicry, since host genes control immune responses to various infectious agents, play a role in the expression of viral receptors, and can influence viral replication. A major difficulty in establishing a correlation between triggering events such as a viral infection and the actual autoimmune disease is the usually long lag period that often precedes the clinical onset of disease. This makes recovery of virus or microbes difficult later on when autoimmune disease has developed. To circumvent part of this difficulty and allow a detailed analysis of the kinetics of a developing disease, transgenic technology can be employed to place a viral gene into specific cells of interest (~ cells, oligodendrocytes, etc.) using a tissue-specific promoter. The foreign (viral or microbial) gene is integrated into the host gene, passed on to progeny, and in essence becomes a self-antigen. This allows critical assessment of the immune response and cellular factors, that is, cytokines, that play a role in breaking tolerance to the 'self-marker protein' and thereby cause autoimmune disease. In addition, such models serve as useful tools for developing and testing novel therapeutic approaches and for critically evaluating the role of other self-antigens in the autoimmune process. This review provides recent data gathered in vitro and in vivo illustrating how viruses can trigger autoimmune diseases. Concepts for understanding the pathogenesis of virus-induced autoimmune diseases as well as novel immunotherapeutic approaches will be discussed.@story_separate@The induction and development of autoimmune processes involves the breaking of tolerance (unresponsiveness) to self-antigens that are present in the periphery. Although the vast majority of T cells bearing TCRs with high affinities for self-antigens are eliminated during their development in the thymus (through a process termed negative selection [27] ), naive/resting precursors of potentially autoreactive cells with lower-affinity TCRs can escape this selection process and travel to the periphery [28--33] . These cells are not activated under normal conditions. Viruses, during the course of infection, induce the release of multiple cytokines such as IFN-y, which induces elevated levels of MHC class I and class II expression. Tough, Borrow and Sprent [34"" ] have shown that memory T cell proliferation can be initiated through the release of type I interferons (IFN-ot and -13). Viruses can also polyclonally activate lymphocytes and, if these include potentially autoreactive lymphocytes, such effector cells can then (cross)-react with potential self-antigens [35] . Along this line, infection with one virus activates memory T lymphocytes that have been generated in response to other, earlier (different) viral infections [36] . Certain populations of these cross-activated cells may be potentially autoreactive and able to induce autoimmune diseases. There are multiple examples of either antibodies or T cells, or both, cross-reactive with self-antigens in individuals with autoimmune diseases (reviewed in [19,21°,22] ). For example, Dan Kaufman and his colleagues [37] provided evidence for an association of Coxsackie B4 virus (P2-C protein) and glutamate decarboxylase-65 reactivity in the onset of autoimmune diabetes mellitus in humans. Recently, examples of cross-reactivities or abnormal immune responses between viruses and self-proteins have been described for rheumatoid arthritis [38] , celiac disease [39] , primary biliary cirrhosis [21°], myocarditis [40] , systemic lupus erythematosus [41] , and MS [42] . Among viruses implicated in autoimmunity [19, 22] are Epstein-Barr virus [18] , human T lymphotropic virus type I [43, 44] , hepatitis C virus [45] , cytomegalovirus (CMV) [46] , HIV [47] , corona viruses [48, 49] , mumps, Coxsackie [16, 37] and herpesvirus-6 [17] . Furthermore, epidemiological evidence has linked rubella, CMV, and Coxsackie viruses with clusters of IDDM cases. When predicting potentially cross-reactive sequence matches for proteins and/or peptides, in addition to linear sequence homologies, conformational considerations need to be taken into account. For instance, a one amino acid substitution in a nine amino acid peptide can completely abrogate binding to the MHC or recognition by the TCR [50,51°°] . Computer models and information obtained from crystal-structure analysis have facilitated these studies. In elegant studies, Wucherpfennig and Strominger [25 °°] evaluated the cross-reactivity of known myelin basic protein (MBP)-reactive T-cell clones derived from MS patients by using computer-predicted peptides from a variety of viruses (herpes simplex virus, Epstein-Barr virus, adenovirus, influenza A virus) and one bacterium that infects humans, Pseudomonas aeruginosa. Peptides were selected on the basis that their primary and secondary structures would fit into the HLA-DR MHC groove [25""°,26] . The data showed that these peptides were all able to bind to and cause proliferation of the clonal MBP-reactive T cell with affinities either greater than or equivalent to that of the known MBP peptide. Thus a single TCR has the capacity to be activated by peptides from five different exogenous sources and one self-peptide. In other studies, T cell lines established from MS patients were reactive to both MBP and a sequence from the human respiratory coronavirus 229E [49] . Additionally, molecular mimicry between human transa!dolase, which in the brain is expressed selectively in oligodendrocytes, and HTLV-I/HIV-1 Gag proteins has also been described [44] . Human transaldolase stimulated proliferation of peripheral blood lymphocytes from patients with MS; in addition, autoantibodies to human transaldolase were detected in the serum and cerebrospinal fluid of these patients. Thus, molecular mimicry between viruses and oligodendrocyte proteins may be important in the etiology of a human central nervous system autoimmune disease such as MS. Ray Welsh and his colleagues [36] found that memory T cells reactive to one virus can be activated by other viruses. These investigators showed that repeated viral infections, even with different and unrelated viruses, stimulated the activation of memory lymphocytes produced in response to previous viral infections. This work suggests that memory lymphocytes may be activated more easily, express a different TCR repertoire than naive lymphocytes or have a lower-affinity TCR, thereby facilitating cross-activation. An example is a virus-induced autoimmune oligodendrocyte disease recently established by Evans et al. (CF Evans, MS Horwitz, MV Hobbs, MBA Oldstone, unpublished data). In this model, the nucleoprotein (NP) or glycoprotein (GP) of LCMV was expressed in oligodendrocytes using the MBP promoter. When replicating LCMV was given parenterally, LCMV-specific T lymphocytes (predominantly CD8 ÷ but also CD4 ÷) were activated in the periphery, crossed the blood-brain barrier, entered the central nervous system, and caused disease. As in to the findings by Welsh and co-workers [36] , central nervous system disease was enhanced when LCMV and Pichinde or vaccinia viruses were administered sequentially in intervals of six weeks to several months, Slow-onset IDDM Induction of slow-onset and rapid-onset IDDM by LCMV in RIP-LCMV transgenic mice. Slow-onset IDDM occurs in mouse lines with thymic expression of the viral (self) antigen and is dependent on CD4 + and CD8 + lymphocytes. Transgenic mice with slow-onset IDDM develop islet infiltration as early as seven days after LCMV infection. The islet-infiltrating lymphocytes are predominantly (> 95%) found around the islets, not inside the islets. This peri-insulitis persists for up to six months in RIP-NP H-2 b mice, and lymphocytes found around the islets produce more Th2 than Thl cytokines. Approximately six months post LCMV infection, the cytokine profile 'switches' rather suddenly from Th2 to Thl, associated with infiltration into the islets and development of IDDM one to two weeks after this switch has occurred. Rapid-onset IDDM occurs in lines without such thymic expression and depends on CD8 CTLs. whereas the inoculation of Pichinde or vaccinia viruses in the absence of LCMV challenge did not cause disease. A number of transgenic animal models have been created to study the pathogenesis of autoimmune diseases. In some models, the influence of environmental factors or pathogens was discovered serendipitously; the incidence of spontaneous autoimmune disease increased when transgenic mice were housed in quarters not free of specific pathogens, in the presence of naturally occurring bacteria and/or viral flora. For example, Hammer, Taurog and colleagues [52] generated HLA-B27 transgenic rats that spontaneously developed an autoimmune syndrome resembling ankylosing spondylitis. When the rats were transferred into germ-free conditions, most signs of the disease disappeared. Similarly, with the TCR-transgenic model for experimental allergic encephalomyelitis that resembles MS (developed by Goverman and colleagues [53] ), the incidence of experimental autoimmune encephalomyelitis increased in pathogen-containing rooms. At present, for neither model is the precipitating agent(s) or event(s) known. Other laboratories have specifically generated transgenic models to study the role of infection in autoimmune diseases [32,33,54°] . Of these, models that use the rat insulin promoter (RIP) to express LCMV proteins in l~-cells of the islets of Langerhans have proven to be very informative. Such transgenic mice were engineered independently by our laboratory [32] and by that of Hengartner and Zinkernagel [33] . In neither case did the transgenic mice develop IDDM or any other immunopathology of the 15-cell islets unless infected with LCMV, after which more than 90% of the mice developed IDDM. The RIP-LCMV transgenic model comes in two 'flavors'. In the first, the viral transgene is expressed only in the 13-cells of the islets of Langerhans. IDDM following LCMV challenge develops rapidly and nearly all 13-cells are destroyed within two weeks [33, 55] . Anti-LCMV (self) CD8 ÷ lymphocytes are mandatory for generating IDDM. Depleting CD8 ÷ lymphocytes with antibodies or crossing these rapid-onset IDDM mice on backgrounds lacking the CD8 or 1~2-microglobulin genes results in the abrogation of IDDM [55] . This indicates that 13-cells in the islets can be destroyed by cytotoxic T lymphocytes (CTLs), either directly or indirectly. To determine whether direct killing of 13-cells by CTLs is dependent on the perforin pathway, IDDM in RIP-LCMV transgenic mice was studied in mice in which the perforin gene had been knocked out [56] . In the absence of a functional perforin gene, IDDM did not occur. Other factors, however, such as cytokines, are also involved in the process in which perforin and CD8 ÷ cells are necessary for 13-cell destruction. For example, when perforin-competent CD8 ÷ CTLs are present in RIP-LCMV mice, no 13-cell destruction occurs in the absence of IFN-7, indicating that the destruction of l~-cells is a muitifactorial process (our unpublished data). In addition to perforin-mediated killing, lysis has been observed in the perforin-independent Fas pathway. No other direct killing pathways are known, but indirect 'bystander' death of p-cells through the secretion of cytokines and other factors is worthy of future investigations. In the second type of RIP-LCMV model, a slowly progressive injury of 13-cells produces IDDM within six months of viral infection. The delay until IDDM occurs depends on the MHC background [55] . H-2 d mice develop IDDM within two months, but H-2 b mice require three to six months. In these slow-onset IDDM mice, the viral transgene is expressed in both the pancreas and the thymus but not in other tissues [55] . Because of transgene expression in the thymus, high-affinity antiself (viral) CTLs are eliminated by negative selection. The low-affinity antiself CTLs 'escape' thymic negative selection, traffic to the periphery, and are identifiable in islet infiltrates. To confirm and expand these findings, an LCMV antigen was purposely expressed as a transgene in the thymus by using a thymus-specific promoter [57] . Such transgenic mice also removed high-affinity anti-LCMV (self) CTLs and low-affinity cells passed to the periphery, supporting the proposed model of affinity dependence of thymic positive and negative selection [27, 28] . The RIP-LCMV model is illustrated in Figure 1 . Depletion of CD4 + and/or CD8 + lymphocytes using monoclonal antibodies showed that, although CD4÷ T cells are not required for inducing IDDM in transgenic lines with fast-onset IDDM [55] , they are required in the slow-onset lines where high-affinity T cells are deleted. Hence, the low-affinity anti-LCMV (self) CD8+ CTLs need CD4 + T cells to generate IDDM, although the CD4 + cells when tested in vitro did not lyse LCMV-infected target cells directly. These studies show that the expression of viral or self antigens in the thymus is not sufficient to induce complete tolerance and that a slowly progressive autoimmune process can be triggered when such low-affinity CTLs that escaped negative selection become activated. CD4 ÷ memory cells that can react with LCMV antigens might be required locally to drive the autoimmune process if autoreactive CTLs are of lower affinity. It is not known whether infiltrating lymphocytes found in insulitis lesions are mainly of the naive or of the memory phenotype and how many of them are fully activated. Other studies have shown that generation or maintenance of CD8 + memory CTLs can be impaired in the absence of CD4 + lymphocytes [58--60,61°° ]. Low-affinity, self-reactive CTLs demonstrated in the RIP-LCMV transgenic model [55] have also been found in other transgenic models ofautoimmune disease and viral infections [31, 57] . Viral infections can induce both high-affinity CTLs that are important for viral clearance and lower-affinity CTL responses. Potentially, these low-affinity CTLs can recognize different viral peptides, other viruses and several MHC alleles; hence, such lower-affinity CTLs display less specificity than high-affinity CTLs and might be prone to more cross-reactivity with other antigens [30, [35] [36] [37] ]. Numbers of antiviral (self) CTLs correlate with the incidence and onset of autoimmune disease [23,61°°] . Following LCMV infection in RIP-LCMV transgenic mice that express the transgene only in ~-cells, specific anti-LCMV CTLs are generated with a precursor frequency of approximately 1/100 during the primary burst (day 6--8 post intraperitoneal infection). This number drops to a frequency of 1/1000 by 30-40 days post intraperitoneal infection and represents LCMV-specific memory CTLs present in the host [61""',62] . When, instead of being inoculated with replicating LCMV virus, the transgenic mice are given vaccinia virus recombinants expressing the LCMV transgene, the numbers of LCMV-specific CTLs induced are 1-2 logs less [60] , and IDDM does not occur. When the numbers of self-reactive CTLs increase, however, or when these cells proliferate locally in the islets of Langerhans, through the presence of the costimulatory molecule B7.1 or of cytokines such as TNF-ot in R-cell islets [61°', 63, 64] for example, then IDDM occurs after infection with vaccinia virus-LCMV recombinants. Thus, a critical number of activated autoreactive CTLs is needed to induce IDDM [54"" ]. When there are insufficient numbers of CTLs, IDDM does not develop unless other factors are present in the islets, which expand the CTL numbers and/or enhance the inflammatory process [60, 63, 64] . This quantitative aspect in the induction of autoimmune diseases is also observed in TCR transgenic models in which either spontaneous disease is induced or the incidence of disease is enhanced when all or most T cells are engineered to bear the same self-antigen-reactive TCR [33,53,54"" ]. Other evidence for this concept comes from a report by Genain etal. [29] , who showed that myelin-reactive cells isolated from the blood of normal, healthy marmosets caused encephalomyelitis when activated and amplified in vitro and infused back into the same animal, caused encephalomyelitis. Expression of LCMV transgenes or of B7.1 alone in l-cells using the RIP does not induce autoimmune diabetes [32,33,61°°] . When both B7.1 and LCMV transgenes are coexpressed in 13-cells, however, antiself (viral) CTLs are activated spontaneously in the absence of viral infection, and spontaneous IDDM occurs [61"" ]. In contrast, when the LCMV transgene is expressed both in the thymus and in the islets, and B7.1 is expressed in the islets, no activation of anti-LCMV (self) CTLs or IDDM occurs. When these mice are given a challenge of LCMV, however, rapid-onset IDDM develops within 14 days after viral challenge. In contrast, in their singly transgenic RIP-NP littermates, IDDM develops only four to five months after viral challenge. The rapid IDDM is associated with increased numbers of antiviral (self) CTLs and a predominance of IFN-T-producing lymphocytes in the islets. In the singly transgenic RIP-NP littermates with slow-onset IDDM, fewer antiself CTLs are made, and more IL-4 and IL-10-producing T lymphocytes are found in the islets. Immune responses are believed to be regulated by a balance of Thl and Th2 cytokines [65] . IL-2 and IFN-y are grouped as Thl-type cytokines and have proinflammatory effects. IL-2 is involved in the differentiation and activation of CD4 + and CD8 + T cells, natural killer cells and APCs, while IFN-T is involved in antiviral defense mechanisms and upregulates the expression of MHC class I and II molecules. IL-4 and IL-10 are Th2 cytokines and along with TGF-[3 have been shown to inhibit Thl cells in vitro and are thought to have an inhibitory effect on certain immune responses in vivo. When IFN-T is coexpressed in the islets with the LCMV transgene, unresponsiveness to the viral transgene is broken, as is apparent when LCMV (self)-specific CTLs arise spontaneously and IDDM occurs without the usually required LCMV infection [66°]. In contrast, when the IFN-y gene is knocked out, such RIP-LCMV mice do not develop autoimmune diabetes and MHC molecules are not upregulated in their islets following LCMV infection (MG yon Herrath, MBA Oldstone, unpublished data). When IL-2 is coexpressed with the LCMV transgene in 13 cells, neither the spontaneous generation of anti-LCMV (self) CTLs nor IDDM follows, despite spontaneous infiltration by CD4 + cells and upregulation of MHC class I on islet cells. The kinetics of IDDM quickens and the severity of disease increases, however, when RIP-LCMVx RIP-IL-2 mice are challenged with LCMV [67] . Yet, when transforming growth factor-13 or IL-10 is coexpressed with LCMV antigens in l-cells, unexpectedly no variation in the incidence of IDDM is observed [68] . A common theme emerges from these studies. Mice that do not develop IDDM have more IL-4-producing lymphocytes in their islets, whereas those developing IDDM have more IFN-T-and fewer IL-4-producing cells. Preliminary studies in collaboration with Nora Sarvetnick indicate that RIP-LCMV transgenic mice that also express IL-4 in their l-cells do not develop IDDM after viral challenge. Recently, expression of IL-4 in islets of nonobese diabetic mice has been shown to prevent IDDM [69°]. These findings not only suggest a major immune regulatory role for these cytokines in IDDM, but also show an interesting correlation between destructive or nondestructive autoimmune infiltration in relationship to the local cytokine milieu. A role for other cytokines, such as IL-12, remains undetermined at this point. Antigen-specific therapy that tolerizes antiself T cells and thereby eliminates the driving force of T-cell-mediated autoimmune diseases can be achieved by two different mechanisms. First, peripheral tolerance can be induced by clonal deletion, anergy induction or exhaustion of autoreactive T cells through the presence of large amounts of self-antigens in the periphery and/or through the lack of costimulation when antigen is presented. Experimental treatment has been designed based on these observations [9] . Aichele et aL [7] successfully prevented IDDM in RIP-LCMV-GP transgenic mice by immunizing them with a specific LCMV-GP peptide. This treatment resulted in the tolerizing of LCMV GP-specific CD8 ÷ CTLs. A second mechanism employs the approach of 'bystander suppression' or immune deviation [8] . Here, tolerance is initiated by immunization with an antigen that induces cells that can mediate suppression by secreting suppressive cytokines such as transforming growth factor-l and IL-4 [8] . The idea is to turn on a Th2-type profile response in the target area. Th2 immunosuppressive cytokines are made primarily by CD4 ÷ lymphocytes, but may also be produced by CD8 + cells and y/8 T iymphocytes. Low doses of orally administered antigen are believed to favor active suppression through the activation of Th2 lymphocytes, whereas higher doses favor clonal anergy of autoreactive inflammatory Thl lymphocytes, for example, lymphocytes reacting with glutamate decarboxylase in IDDM [70] . By this means, it is possible to treat an organ-specific disease (e.g. IDDM [1-cells in the pancreatic islets]) even when the initiating autoantigen is not known. Perhaps antigen-specific regulatory T cells are activated in Peyer's patches of the gut, home to the specific tissues where the oral antigen 'resides' naturally (e.g. insulin in t-cells) and is presented by APCs, if inflammation is ongoing. As a consequence, their local secretion of immunosuppressive cytokines suppresses the autoimmune response and disease. Despite the controversy over whether this therapy works by active suppression [8] (the generation of transforming growth factor-~ and IL-4), or by clonal anergy or 'exhaustion' [71] of self-reactive T lymphocytes, treatment has proven effective for a variety of T-cell-mediated experimental autoimmune diseases [8] . Interestingly, oral insulin therapy also prevents virus-induced IDDM in RIP-LCMV transgenic mice [72] . This therapy is successful when begun before the initiation of IDDM, but is also effective when administered during the course of diabetes. The therapy only succeeds in the model, in which the viral transgene is expressed on both I]-cells and in the thymus, however, and, hence, works only against lower-affinity antiself (viral) effector T cells [55] . The therapy is not effective when the transgene is expressed only on l-cells and when IDDM is caused by high-affinity effector T lymphocytes [72] . Finally, since viruses can interfere at different levels in the antigen-presentation pathway, expression of such genes in the area targeted for autoimmune activity can be expected to block autoimmune disease. The adenoviral E3 complex genes interfere with transport of MHC class I molecules to the cell surface. Coexpression of E3 complex genes, specifically gpl9, in [3-cells of the islets of Langerhans (RIP-E3 transgenic mice) together witfh LCMV proteins using the RIP prevents virus-induced IDDM (MG von Herrath, S Efrat, MBA Oldstone, MS Horwitz, unpublished data). Herpes simplex virus ICP47 (infected cell protein) proteins and CMV US (unique short) 2 and USll proteins [73] both prevent antigen presentation in vitro, although at present there is no proof that these viral proteins interfere with antigen presentation in vivo.@story_separate@Based on the transgenic models, a likely scenario for the sequence of events occurring in virus-induced autoimmune disease is that, first, peripheral unresponsiveness is broken through the activation of cross-reactive (probably low-affinity) T lymphocytes. These cells then home to the respective target organ (for example, the 1-cells in IDDM or oligodendrocytes in MS), where the self-antigen is recognized. In response, the activated lymphocytes release cytokines and possibly other mediators and then, for example, CD8 ÷ CTLs begin to kill target cells bearing the respective MHC class I peptide complex. Depending on the precursor frequencies of the cells initially triggering the autoimmune process, the local cytokine milieu, and the MHC background genes involved, autoimmune disease can occur either fast, slow, or not at all. Damage is probably mediated either indirectly through the release of cytokines or directly through CTL-mediated lysis of the target cells. Once damage has occurred, a chain reaction is set into place, during which more potential self-antigens may be released, presented by professional APCs, and the autoimmune response is extended to other self-antigens in the target site. This sequence of events would explain the occurrence of autoantibodies that precede the clinical onset of disease, such as islet-cell antibodies found in IDDM. If cross-reactivity between a virus 'A' and a self-antigen 'A' is the initial triggering event, subsequent infections with different viruses 'B' through 'X' (which do not necessarily have to cross-react with self-antigen 'A'), could activate and expand vitus-'A'-specific memory T (i.e. CD8 ÷) lymphocytes, thus enhancing disease severity ([361; CF Evans, MS Horwitz, MV Hobbs, MBA Oldstone, unpublished data). This scenario would explain the epidemiological findings that associate autoimmune diseases with multiple viral infections and may also explain why, in a disease like MS, the cerebrospinal fluid contains elevated gamma-globulin levels (antibodies) specific for multiple viruses. Specific antiviral therapies that inhibit viral replication and spread or tolerize the antiviral immune response in model systems may eventually provide a guide for curing autoimmune diseases. If viruses or microbial agents are a major contributing factor to human autoimmune diseases, as several studies reviewed here suggest, and if these agents can be identified, then one would expect a reduction in the incidence of autoimmune diseases when more effective and broadly applicable antiviral therapies become available.","The braking of tolerance or unresponsiveness to self-antigens, involving the activation of autoreactive lymphocytes, is a critical event leading to autoimmune diseases. The precise mechanisms by which this can occur are mostly unknown. Viruses have been implicated in this process, among other etiological factors, such as genetic predisposition and cytokine activity. Several ways have been proposed by which a viral infection might break tolerance to self and trigger an autoreactive cascade that ultimately leads to the destruction of a specific cell type or an entire organ. The process termed ‘molecular mimicry’ and the use of transgenic models in which viral and host genes can be manipulated to analyze their effects in causing autoimmunity have been particular focuses for research. For example, there is a transgenic murine model of virus-induced autoimmune disease, in which a known viral gene is selectively expressed as a self-antigen in β cells of the pancreas. In these mice, insulin-dependent diabetes develops after either a viral infection, the release of a cytokine such as IFN-γ, or the expression of the costimulatory molecule B7.1 in the islets of Langerhans. Recent studies using this model have contributed to the understanding of the pathogenesis of virus-induced autoimmune disease and have furthered the design and testing of novel immunotherapeutic approaches."
"Modelling of disease transmission via compartmental models is well established and generally highly effective. However, the differential equations of these models depend on good estimates of underlying rate parameters and will then provide a continuous solution under the assumption that the population is well-mixed and homogeneous (i.e. all individuals have equal contact with all others). Under these assumptions disease propagation is driven by the parameter R 0 -the ratio of the rate of new infections to the rate of removal of infectious individuals from the transmission pool. Typically, and particularly for contemporary and evolving transmission, these parameters can be somewhat difficult to estimate [1] , [2] . We propose an alternative approach to modelling the dynamic transmission of diseases. A consequence of this alternative approach is that the main determinant of epidemic The associate editor coordinating the review of this manuscript and approving it for publication was Derek Abbott . dynamic behaviour is the contact network between individuals rather than precisely chosen optimal values of epidemic rate parameters. The physics literature is rife with models of propagation dynamics on networks. We observe that different societal control measures manifest as distinct topological structures and model city-level transmission of an infectious agent. Our approach models changing control strategies by changing the features of the underlying contact network with time. This approach allows us to model the likely time course of a disease and, perhaps surprisingly, we find that this approaches is both quantifiable and robust to uncertainty of the underlying rate parameters. This report is intended as a guide to computational modelling of reported epidemic infection rates when good estimates of underlying epidemiological rate parameters are not available. The model provides a useful prediction of current control strategies. Nonetheless, we emphasise that the methodology and techniques are not (of themselves) novel, they have been discussed extensively in the references cited VOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ herein. The primary novel contribution of this paper is the interpretation of complex network topologies as the principal relevant parameter to characterise control, and a commentary on the live application of this approach in pandemic response and recovery. While global efforts to model the spread and control of coronavirus continue, we are taking a decidedly local approach. We focus our modelling and discussion on transmission in Australian cities, which we characterise as large heterogeneous populations. In particular, we focus on the most isolated of all cities in Australia, Perth, the capital of Western Australia. For the purposes of this manuscript we treat Perth as a single isolated urban centre of approximately 2 million people. Epidemic parameters, which we describe later, either follow established epidemiological values or are estimated to fit the time course of infection data. While our model is specific to one city, we intend that the methods and conclusion are generic and will be useful elsewhere. Our model is a model of contact graphs. Different contact graphs are utilised to model different contact patterns within the population and hence model the effect of different control measures. In Sec. II we introduce and discuss a small amount of the most relevant literature, and in Sec. III we proceed to describe our model.@story_separate@It is no exaggeration to say that pandemic spread of infectious agents has very recently attracted wide interest. Mathematical epidemiology is a venerable and well respected field [3] . Propagation of disease in a community is modelled, under the assumption of a well-mixed and homogeneous population via differential equations characterising movement of individuals between disease classes: susceptible (S), exposed (E), infectious (I) or removed (R). The standard compartmental (i.e. SIR) model dates back to the mathematical tour de force of Kermack and McKendrick [4] . The model assumes individuals can be categorised into one of several compartments: S or I; S, I or R; or, S, E, I, or R being the most common. Transition between the various compartments is governed by rate parameters a and r and it is the job of the mathematical epidemiologist to estimate those rates -and hence, when dI dt < 0 and the transmission is under control. In the standard SIR formulation the condition dI dt < 0 can be expressed as aI (t) r ≡ R 0 < 1. Somewhat confusingly, R 0 is also used in the physics literature to denote the threshold itself -as in [5] where R 0 is derived in terms of moments of the contact network degree distribution. Nonetheless, efforts to estimate the relevant parameters for the coronavirus pandemic are currently underway and are best summarised (from our local perspective) by the technical reports of Shearer et al [6] , Moss et al [7] and co-workers. Conversely, the renaissance of interest in mathematical graphs under the guise of complex networks [8] , [9] , has raised considerable interest in propagation of infectious agent-like dynamics on such structures [10] , [11] . Commonly, the agent is either modelling the spread of information or infection. When one is restricted to the spread of infectious agents on a network (in the context of epidemiology, a contact graph) several interesting features arise. In particular, if that contact graph is a scale free network (i.e. it has a power-law degree distribution), then the criterion on the key epidemic threshold to ensure control of the outbreak (for the SIS model) becomes R 0 = 0 [5] . Disease transmission becomes faster than exponential. In effect, what happens is that the power-law distribution of the scale-free network ensures that there is finite probability of the epidemic reaching an individual with an arbitrary large number of contacts. The number of secondary infections arising from that individual will be unbounded and transmission is guaranteed to persist. Of course, in the real-world nothing is unbounded and Fu and co-workers [12] showed that a piece-wise linear/constant infectivity was enough to ensure a positive epidemic threshold. Surprisingly, however, little of the work in the physics literature on epidemic transmission has examined transmission on real-world networks. The first evidence (to the best of our knowledge) that epidemic transmission did really occur on a scale-free contact graph was provided by Small and others [13] for the transmission of avian influenza in migratory bird populations. Curiously, though, the data presented there gave an exponent for the scale-free distribution of approximately 1.2, significantly lower than the often cited ''usual'' range of (2, 3) -that is, the distribution not only had divergent variance, but also divergent mean. The emergence of an earlier coronavirus, associated with the Severe Acute Respiratory Syndrome (SARS) in 2003, provided an opportunity to apply the structures and concepts of complex systems to the modelling of infectious diseases. Small and Tse [14] introduced a complex network based model of propagation and showed good agreement between simulations of that model and available case data. They found that epidemic parameters widely quoted in the literature were only consistent with observed case data when including significant nosocomial transmission [15] . Finally, and most importantly for the current discussion, the scale-free topology of the model [14] explained super-spreader events through contact rather than requiring pathologically highly infectious individuals [16] . Both the network-based models used to model SARS in 2003 [14] - [16] , and the model we describe here are network models of contact between individuals. Unlike what we will propose in this current communication, the model of SARS in 2003 was topologically stationary [14] - [16] . The model assumed a lattice with long-range (i.e. smallworld) connections following a power-law degree distribution. In those papers [14] - [16] time varying control strategies were reflected only in changes of the rate parameters. The current coronovirus outbreak (that is, COVID-19) poses a different and unique challenge. Since February 2020 (and up to the time of writing) global transport networks and daily movement of individuals have been disrupted on a global scale. Entire cities and countries have engaged in various levels of ''lockdown''. We argue that it is neither appropriate nor sufficient to model this simply by modifying the rate of transmission or rate of removal. In the current work we propose a network switching model through which the topology of the network changes to reflect various changes in government and community mitigation and control strategies. In Sec. III we introduce our model structure, and Sec. IV explores analytic expressions for the epidemic growth rate. In Sec. V presents our results for the case study of most interest to us, and Sec. VI describes a process for optimising model parameters based on observed caseload data. We assume nodes in our network can be in one of four states, corresponding to the four states of the standard SEIR model: susceptible S, exposed E, infected I , and removed R. Rates govern the probability of transition between these states, with transition from S to E occurring only through neighbour-toneighbour contact on the graph with a node in state I . For comparison, the standard SEIR compartmental differential equation based formulation is given by [3] : where p, q, r ∈ (0, 1] determine the rate of infection, latency and removal respectively. Clearly, if we desire dE dt + dI dt < 0 we need pS(t) r < 1. The parameter q determines the average latency period, and hence the ratio of p and r determines the rate of spread. The assumptions underpinning models of the form (1) are that the population is fully mixed -that is, contacts exist between all members of the population, or, rather, every individual is indistinguishable and contacts occur at a constant rate between individuals. One way to extend this model is to introduce multi-group or stratified (perhaps by age, vulnerability, comorbidity, or location) transmission models. Doing so for coronavirus transmission is reasonable and has been extensively covered elsewhere: for the Australian perspective see [17] , [18] However, this would require estimating distinct values of p, q and r for each strata. We choose an approach which has fewer free parameters 1 and model infection at the daily time scale. Let A be an N -by-N binary symmetric matrix, a ij = 1 iff individuals i and j are in contact. The matrix A is the adjacency matrix of the contact network which we model. We suppose that all individuals, excluding a small number who are exposed (E), are initially susceptible (S). Then, at each time step (each day): S → E a susceptible node i becomes exposed if there exists a node j that is infectious (I) and a ij = 1 with probability p; E → I an exposed node becomes infectious with probability q; and, I → R an infectious node becomes removed (R) with probability r. The model structure is depicted in Fig. 1 . Structural patterns of contact within the community are then modelled by varying the structure of the network A. In this paper we propose distinct models corresponding to the different control strategies. in the following four subsections, the control strategies which we consider are: III-A no control, modelled with a scale free network; III-B hard isolation, modelled as a lattice; III-C no mass gatherings via a random graph; and III-D ''social'' distancing via a small-world network. We explore these four distinct network structures in the following subsections. Let B denote an N -by-N unweighted and undirected scale-free network. For simplicity (and rapidity of calculation) we generate this network via the preferential attachment algorithm of Barabasi and Albert [8] -there are good reasons for not doing this (notably that the rich club will be highly connected [19] - [21] ). Nonetheless, simulations presented here did not depend on the choice of the Barabasi-Albert model over alternatives including the configuration model or likelihood approaches [21] . The network B is parameterised by k 2 the number of new edges associated with each new node and so we represent it as B(k) (if each new node contributes k 2 new edges, then the mean degree will be k). Here, to ensure comparable number of edges, we choose k = 4. The network B(k) provides a model of random contacts in a community. There is ample evidence that individual contact patterns follow an approximately scale-free distribution. Specifically, in the context of the current pandemic, there is clear evidence in large scale community spread of COVID-19 at sporting events and other mass gatherings which are well modelled via the tail of a scale-free distribution [22] - [24] . Due to the random wiring of connections between nodes we expect contact network B to yield at least exponential growth of infection. The tail of the degree distribution is unbounded and so the actual growth rate is greater than exponential. Let L denote a regular two dimensional lattice with periodic boundary conditions. Each node has four adjacent neighbours. For consistency with what follows we denote this as L(0). Growth of infection on a lattice will be equivalent to diffusion in two dimensions and hence the infected population VOLUME 8, 2020 FIGURE 1. Model flow chart. A Graphical representation of the model state transition process. Each node can be in one of four states S, E , I, or R with transition between them determined by probabilities p, q and r and the contact process of elements a ij of the network adjacency matrix A. Hence node-i has probability pa ij of being infected through contact with node-j . will grow geometrically -in the case of the configuration discussed here growth is sub-linear. Lattice configuration is used here as an approximation to hard isolation: individuals do not move in geographical space and are therefore only connected to neighbours. Intuitively, one might expect a hard isolation model to consist of small isolated clusters corresponding to individual family units. In addition to being uninteresting -for the very obvious reason that transmission would cease -such a model is overly optimistic. Transmission would still be expected to occur between neighbours (in the ordinary sense of the word). The regular lattice configuration model is able to model such infection between family units, and adjacent dwellings. This is exactly the philosophy behind the model structure of [15] . Let L(1) denote a random graph (ala Erdös-Renyi [9] ) with mean degree equal to four. Connections between nodes are chosen uniformly at random and constrained to avoid multiple edges or self-loops. That is, each edge is assigned to connect two randomly chosen nodes within the network, subject to the constraint of no self-loops and no multiple edges. At the opposite extreme to L(0) we denote by L(1) the lattice graph with no lattice structure -all connections have been rewired and hence correspond to complete random wiring. In other words, while L(1) is not a lattice it is the limiting case of L(q) for q → 1. Unlike B the degree distribution of L(1) is binomial (there is a fixed constant probability that a link exists between any two random nodes, independent of all other structure). Hence, while B will be characterised by super-spreader events (spiky outliers in the daily infection count), spreading with L(1) contacts is exponential but devoid of extreme events. The random graph model represents a mixing populace with limitations placed on mass gatherings. Finally, let L(s) denote a Watts-Strogatz [25] twodimensional lattice with random rewiring with probability s. That is, the network L(s) is constructed as a regular lattice L(0) each edge emanating from node-i has a probability s of being disconnected from the neighbour node-j and then rewired between node i and random node-k (in doing so, one node will decrease in degree by one, and one will increase by one). For s > 0, the graph L(s) is an imperfect approximation to L(0). That is, individuals are bound in a lattice configuration due to being geographically constrained. However, a fraction of individuals still exhibit long range connections. Effectively, the model L(s) assumes that the populace is practising what is referred to in the popular press as ''social distancing'' (everyone is fixed at a home location and connected only to others in the same vicinity). However, there is some finite limit to compliance with the enforced isolation. A probability s of a given link switching and therefore connecting random nodes corresponds to a fraction c = (1 − s) k of nodes compliant with these distancing measures since all there k edges are not switched. In opposition to the standard and rather flawed nomenclature, we will refer to this control strategy as physical distancing. We now provide estimates of the characteristic growth rates for propagation on the network structures described above. A widely used approach [26] is to replace the compartmental equations (1) with distinct equations for nodes of each degree. What we describe here is the approach commonly adopted in the physics literature. For an excellent treatment of the original theoretical biology approach see [27] . Let S k , E k , I k and R k denote the number of nodes of degree k in state S, E, I or R. The system (1) then becomes where P( ) is the degree distribution of the network A. In general P( ) is a little unsatisfactory as we should really compute the sum over P( |k). But even in the SIS case, doing so becomes rather unwieldy. Conversely, for SEIR-type or (SIR) epidemics the asymptotic state is trivial: 0)). This provides no insight. Nonetheless, we are interested in growth rate which is determined via decrease in the susceptible population In the scale free case P( ) ∝ −γ and hence growth is superexponential: high degree nodes have a contact rate proportional to their degree and a non-zero probability of connecting to other high degree nodes. Conversely, suppose that each node has a fixed degree In our lattice model L = 4. The growth rate is then given by pS k LI L , system (2) immediately reduces to (1), and one is left with the usual exponential growth or decay. Hereafter, we are considering only nodes of degree k = L and will drop the subscript k for convenience. However, for s < 1 this reasoning is flawed. Employing (1), assumes perfect mixing and hence random distribution of infectious and susceptible nodes on the lattice. Under diffusion the infectious nodes will spread in a single cluster: nodes in that cluster will be in class E, I or R and the remainder of the population will be susceptible. The cluster will be of size E + I + R and the exposed boundary will be of size scaling with √ E + I + R, nodes on that exterior will be either E or I (we assume that diffusion is fast enough that the removed nodes are interior -this is certainly only an approximation and will depend on relative values of p, q and r), but only the nodes in state I are infectious. Hence, the number of infectious nodes in contact with susceptibles will scale with a quantity bounded by I E+I +R √ E + I + R and √ E + I + R -mostly likely around I E+I √ E + I + R. 2 On average, only half the links from an infectious node will point to a susceptible (the remainder will point to other nodes in the cluster), hence, the number of susceptible nodes connected to an infected node is approximated by and the proportion of susceptible nodes that satisfy this condition will be Hence, the expected number of new infections from a lattice diffusion model is obtained from the product of the rate p and the contact between these exposed infected and susceptible individuals We note in passing that typically S E ∝ I > R -certainly during initial growth, or in the case of limited penetration. Moreover, the arguments above only hold when S E, I . Finally, in a small-world model there is probability s of a link pointing to a random distant location. With S E +I +R we assume that that link is pointing to a susceptible node and so the expected number of new infections is now Since, E and I are linearly proportionate, the first term scales (very roughly) like (E + I + R) the second like SI . That is, a mixture of the sub-linear growth dictated by the lattice (with proportion 1 − s) and the classical compartmental model (1) with probability s. Considering the E and I individuals as a single pool, the rate of new infections is balanced by the rate of removal rI and so infection will grow if In this section we first present results of the application of this model. We choose a city of population of approximately 2.1×10 6 (Perth, Western Australia) and perform a simulation with initial exposed seeds and contact network A = B (for 0 ≤ t < t * ). The transition time t * is the time with I (t) > I th for some threshold infection load I th for the first time (i.e. Latency period of q = 1 7 is comparable to observation, the other parameters are estimated derived from the values used in [6] , [7] for Australian populations. These parameter values ensure growth in infection for t < t * but barely endemic otherwise (for A = B). That is, these parameters are selected to match the observed data for our principle region of interest. Subsequent parameter sensitivity computation will indicate that variation of these parameters does not change the qualitative features, only the scale of the observed simulations. I (t) < I th for all t < t * and I (t * ) ≥ I th . For t > t * we set B = L(s) for various values of s. In what follows we will use p(t > t * ) to denote the value of parameter p assumed for all time t > t * , similar notation is adopted for p(t < t * ) and also for parameter r. The epidemic parameters which we have chosen for this simulation are illustrated in Table 1 . We do not wish to dwell on the epidemiological appropriateness of these parameters -except to say that the were chosen to be consistent with our understanding of epidemiology and also gave results that appropriately coincided with the available time series data. The specific parameter values described in Table 1 were computed to be consistent with those employed by [6] , [7] . However, the models described in [6] , [7] are more epidemiologically detailed than ours and hence the parameter values reported here represent an agglomeration of various rates. Moreover, we confirm empirically that the rate of spread implied by these parameter choices shows very good agreement with the transmission data for Australia -see Sec. VI-B. Some brief notes on the effect of parameter selection are in order. First, varying I th will delay the transition to a ''controlled'' regime and produce a larger peak. The parameter q is largely determined by the epidemiology of the infection, and for coronavirus COVID-19 is fairly well established [7] . It does have an important influence on the time delay of the system, but that is not evident from Fig. 2 . Second, the parameters p and r for t < t * also determine the initial rate of spread -as standard epidemiology would expect. Third, the value of these parameters for t > t * determine the length of the ''tail''. In all simulations these parameters are chosen so that a well mixed population would sustain endemic infection. It is the network structure, not fudging of these parameters that causes extinction of the infection -this will be further illustrated in Fig. 3 . Figure 2 depicts one ensemble of simulations. Of note from Fig. 2 is the complete infection of the population without control. Conversely, the random Erdös-Renyi graph L(1) has a sufficiently narrow degree distribution that the infection does (slowly) die away. Various values of L(s) with s ∈ (0, 1) have the expected effect of gradually decreasing the total extent and duration of the outbreak. However, it is important to note that the 90% confidence windows are very wide and overlap almost entirely -while, on average smaller s is better this is very often not evident from individual simulations. This is due to random variation in the initial spread for t < t * . It is very clear from Fig. 2 that the variance between simulations is similar in magnitude to variation in parameters. However, parameters in Fig. 2 correspond to moderate parameters p and r and a wide variation in social isolation. In an effort to understand the parameter sensitivity of this simulation we perform repeated simulations over a wide range of p(t > t * ) and r(t > t * ). For all selected values we generate 20 simulations of 300 days each and compute several indicators of infection penetration • Mean total infection: The total number of individuals that become exposed, infected or removed during the duration of the simulation. That is, max t S(0) − S(t) = S(0) − S(300). • Mean maximum infected: The maximum daily reported number of infections -that is, the maximum number of new infected individuals: • Half recovered time: The time in days required for half the simulations to entirely eliminate infection. That is, the median (over simulations) of the minimum (over time) t such that E(t) + I (t) = 0 Results for I th = 100 are reported in Fig. 3 , varying I th simply scales the reported numbers (data not shown). Depicted in Figs. 3 and 4 are computed values of the mean total infection. The other parameters described above behave in a consistent manner. Figures 3 and 4 starkly illustrate the importance, for the coronavirus pandemic of 2020, of implementing and stringently enforcing isolation. Without isolation the epidemic impact is limited only for very optimistic values of p(t > t * ) and r(t > t * ). Otherwise, the mean behaviour indicates infection growth by two orders of magnitude within 300 days -almost complete penetration. Our simulations indicate that this first becomes a risk as physical distancing is less than 90% effective. There is a boundary in our simulations which appears below 90% isolation and grows to include even moderate values of the other epidemic control parameters p(t > t * ) and r(t > t * ). In part, our aim with this communication is to dissuade the application of modelling of time series to predict certain specific futures. That is, we are interested in simulation and inferring structure from the ensemble of such simulations. The random variation reported in Fig. 2 should discourage all but the most determined from prediction. Nonetheless, it is valid to ask two questions of observed time series data: (1) what parameter values are most likely given this observed trajectory, and (2) which trajectory (or set of trajectories) are most consistent with the current state. The first question we will address via a greedy optimisation procedure, to be described below. The second question is equivalent to asking for an ensemble estimate of the current state of exposed but undetected individuals within the community. A complete study of this second problem is beyond the scope of the present discussion, but some points are worth considering before we return to the issue of parameter estimation in Sec. VI-B. Finally, in Sec. VI-C we provide some estimates of the effectiveness of various control measures during recovery phase, subsequent to localised eradication. Table 1 . Surface (a) and (b) exhibit linear scaling with changing parameter values p(t > t * ) and r (t > t * ), while for (c) and (d) that growth is exponential. That is, when compliance with isolation measures drops below 90% there is an explosive growth in the level of infection with p(t > t * ) and r (t > t * ). As noted previously, there is very significant variation between trajectories for the same model parameter settings. While this means that the construction of more complex models -solely from time series data -is inadvisable, it is natural to seek to explain this variability. Simulations conducted above for an SEIR model with nontrivial latency period indicates that at any instance in time there is a large number of exposed but undetected individuals within the network. The location of this exposed class within the network (their distribution relative to hubs, for example) explains the variation we observe. This has been demonstrated by simulation from repeated random distributions of exposed individuals. It is easy to estimate the expected number E(t) from the time series I (t) and R(t), however, the distribution of these individuals on the network is not uniform. The question that must be addressed to resolve this issue is what is the expected distribution of E(t) random walkers on a network A? In the interest of clarity and succinctness, we do not address this issue here. For the purposes of Sec. VI-C, below, we simply model a re-introduction of infection as a small number of exposed individuals randomly distributed on the contact graph. A separate problem is to determine the maximum likelihood values of the parameters p, q, and r for a given population N and I th from an observed time series. This can be decomposed to several discrete steps. 1) We suppose that q is fixed and estimable by other means. For COVID-19, for example, q should yield a latency period of 7-14 days [7] , [17] . Hence q ∈ ( 1 14 , 1 7 ). 2) Determine the epidemic peak from the time seriesthis will define the turning point and the time when growth changes from exponential for geometric. This will allow one to determine I th and the corresponding Table 1 ). Note that panel (a) has a linear ordinate, panel (b) and (c) are depicted with a logarithmic scale. As in Fig. 3 we observe explosive growth in impact with lower levels of compliance. t * . In effect we are now seeking a turning point of the total number of infections (S(0)−S(t)) and not just I (t) as done in Fig. 2 . 3) For t < t * determine p(t < t * ) and r(t < t * ). The ratio of these two parameters determines the epidemic growth rate via R 0 4) For t > t * it remains to determine s, p(t > t * ) and r(t > t * ). We note that s controls the extent to which the system is driven by diffusion geometric) versus exponential growth. But, for now, the best we can do is a greedy likelihood maximisation process. Note that, in the event that the peak has not yet been reached (i.e. t < t * ) it is not even sensible to attempt to estimate the parameters s, p(t > t * ) and r(t > t * ). Nonetheless, in this situation one can estimate instantaneous (or windowed) values for R 0 and attempt to pick the end of the exponential growth phase. The latency introduced by q somewhat complicates this process. Figure 5 illustrates the result of such a calculation. Finally, we note (as is indicated in the illustrated exemplars) that we assume a single policy change-point t * -this is clearly inappropriate for more complex time dependent responses. 3 Of course the value of t * is actually determined by societal responses and control measures instituted in response to an outbreak. That is, it should, in principal be observable. Nonetheless, it is not clear that this will necessarily translate to the time when control takes effect -nor will it necessarily be possible to reduce it to a single control point. Hence, the value of t * we introduce here is a single parameter value corresponding to the single moment in time when a broad range of control measures modify the dynamics of the epidemic. Finally, in Fig. 6 we explore the effect of control measures to mitigate against reemergence of the virus. We assume a healthy population and five individuals in state E. We then simulate various different control measures, again modelled via complex networks as contact graphs. The population is 2.1 × 10 6 , as before, and the parameters p = 1 10 , q = 1 8 and r = 1 4 represent a state of heightened vigilance -but not sufficient to suppress infection. Each of the control measures described in the figure is modelled as follows • Contact tracing (CT) is modelled by assuming that a fraction w of the population has adopted contact tracing through their mobile device. Hence, if an infection were to occur between two such individuals, that infection will be extinguished via intervention from authorities. The fraction of links that are effectively removed is w 2 . • N person limit is modelled by truncating the scale-free network so that no node has degree larger than N . This is equivalent to the treatment described in [12] . • No Mass gatherings are modelled with A = L(1), as before. • d% Stay home models physical isolation of a fraction  This model has its origins in the severe societal challenge of COVID-19, when the population of Perth was facing the prospect of loss of 30,000 lives, and hospitals being over-run within two or three weeks if the rate of escalation continued. The model was first used in a pandemic response workshop for a city of 100,000 people, led by the second author. The model results informed the importance in influencing In each case the epidemic diffusion is fitted to data up to the end of the exponential growth phase (that is, the point of inflexion on curves S(0) − S(t )). Simulations up to this time point t * effectively seed the network and provide a distribution of infectious and exposed individuals within the community. Beyond this point we simulate the application of small-world control network structure L(s) for various values of s. Here we illustrate s = 0.013, s = 0.026 and s = 0.054 corresponding to 95%, 90% and 80% control. Actual observed time series data is also shown and illustrates exception effectiveness of control measures for various Australian states. people's behaviour, to greater than 90% compliance, and hence the guidance to give to the city officers in the workshop. It served to demonstrate the dramatic range of outcomes which were possible, depending on the behaviour of constituents of the city, and degree of social distancing achieved. This proved very effective in enabling appropriate action, both in the workshop and afterwards with the city response seen as a model. Subsequently the results were shared on professional social media, and an online conference, influencing thousands more. In combination with effective timely coordinated state and federal government polices, and a high level of societal compliance, a very strong result of virus suppression was achieved. The model was further developed to update progress, within two weeks, and at the time of the workshop debrief this was used to show the importance of continuing measures in suppression, and the rate at which rapid outbreak could occur, even in the context of strong initial suppression. This allowed the appropriate focus to shift towards a positive recovery. Again this was shared locally and internationally to provide hope for others and influence behaviour. Subsequently, the actual case data within the state was plotted against the forecast range, and this was shared with state scientific authorities, enabling a constructive discussion FIGURE 6. Recovery and return. Here we depict the effect of various palliative control measures in the event of a reemergence of infection (modelled here by a population seeded with 5 exposed (infected but asymptomatic) individuals. The four solid lines represent a return to mass gatherings (black), a 50 person limit on gatherings (red), no mass gatherings (blue), and continued physical distancing (green). The dashed lines model the same scenarios with the addition of 50% of the population adopting and using contact tracing software (CT). Note that the red (second solid) line grows exponentially, the black line (top) is faster than exponential and the blue and green (bottom) lines are significantly below exponential. In all cases these lines represent the median of 100 simulations. about the correlation between application of selected state and national control measures and outcomes. The extension to modelling different approaches to recovery continues in a similar mode, with distinctive results, and the model outcomes have since been included in briefings for state health authorities and COVID-19 safety training. To gain most value from the model, its results have been interpreted in a variety of environments, including most recently in collaborative virtual reality mode, in a digital Public Health Emergency Operations Centre (PHEOC) (Fig. 7) . This has the advantage of rich immersion in the data, while allowing deep multi-party interaction and dialogue to discern appropriate observations, and at the same time allow parties to engage together from anywhere in the world. At the time of drafting, the number of new cases of COVID-19 has for the first time reached zero, with only seven fatalities in the State to date, remarkably low compared to world averages. A few weeks later the disease had been eliminated from the hospital system in Western Australia. VOLUME 8, 2020 FIGURE 7. Immersive multiperson visualisation of the model in context using virtual reality. Here illustrated in our implementation of a digital Public Health Emergency Operations Centre, where the model is integrated into wider contextual information such as national trends and geospatial information. The model is used to communicate scenarios allowing stakeholders able to draw conclusions collaboratively in context. Source code for all calculations described in the manuscript is available on https://github.com/m-small/epinets. Data was obtained from https://github.com/CSSEGISandData/ COVID-19.@story_separate@The model we present here has a small -perhaps minimal -number of parameters, and describes the observed dynamics of pandemic disease transmission. When applied to data from the global outbreak of coronavirus in 2019/2020, the model provides good qualitative agreement with observed data across population centres. Nonetheless, identical simulations with new initial conditions yield vastly different outcomes. The variance of our model predictions is large, and in fact much larger than the variance observed between distinct epidemiological parameter values. Hence, choice of optimal transmission rates is a secondary concern behind appropriate description of contact patterns and transmission mitigation strategy. Our results indicate that particular simulations of models that claim to have predictive power within that prediction envelope may be prone to over-interpretation. Finally, despite modelling a complex system with complex networks we have demonstrated the sufficiency of a minimal model. Models with large numbers of parameters which are fitted to time series data are unnecessary and likely to be unreliable and misrepresent the underlying dynamical process. Our model emphasises accurate reproduction of the qualitative behaviour of the system, this does not preclude the construction of more complex models when sound epidemiological reasoning dictates it is necessary and when informed with direct evidence to allow for quantitative estimation of the relevant parameters. While we are reluctant to make predictions from, or over interpret the application of, this model to the current coronovirus pandemic, our results indicate that strict physical isolation in combination with monitoring and the usual transmission mitigation strategies are required to minimise impact. Below 80% compliance with physical isolation measures risks catastrophic spread of infection (Fig. 4) . This data is consistent with the evidence of explosive growth of infection experienced in some localities. Without decisive and potentially severe intervention, similar disasters are likely to occur in regions with weaker health systems. In the simulations described above, we do not make any attempt to ensure ''pseudo-continuity'' between time varying manifestations of A. That is, nodes that are connected for one network are not more likely to be connected after switching the network topology. We could see no simple and generic way in which to achieve this. Moreover, we did not detect any excessive mixing that one might expect should this mismatch be an issue. It is worth noting that the computation cost of this model -despite being a population level simulation -is not great. We simulate the state of an entire population, but at each iteration the updates are determined entirely by a predefined contact structure. For population size N and simulating T time steps the computational time cost is NT . The memory requirement is N + N log N (for a sparse contact network and population state vector). This modest computation demand mean that the algorithm can be successfully deployed in immersive, interactive and real-time environments. In Sec. VI-A we raise the issue of estimating the expected distribution of unobserved infection sites (i.e. state E) on a network. Should the model described here prove relevant, this will be an issue of immense importance to the proper quantification of uncertain future behaviour. Figure 6 illustrates the application of these technique for future scenario planning. Finally, the social context and utility of this modelling is demonstrated by its live use in shaping the planning and implementation of a highly effective response to COVID-19 on a city and state level. Ultimately, one must ask what is the purpose of modelling. Epidemic disease transmission is a fairly simple mathematical problem -exponential growth followed by decay. The difficulty is in reliably estimating parameters. We show that the contact structure provides a direct and effective approach to model control strategies. In addition to the information provided by our simulations, we describe in Sec. VII the application of these methods to effectively inform and influence policy makers.","We show that precise knowledge of epidemic transmission parameters is not required to build an informative model of the spread of disease. We propose a detailed model of the topology of the contact network under various external control regimes and demonstrate that this is sufficient to capture the salient dynamical characteristics and to inform decisions. Contact between individuals in the community is characterised by a contact graph, the structure of that contact graph is selected to mimic community control measures. Our model of city-level transmission of an infectious agent (SEIR model) characterises spread via a (a) scale-free contact network (no control); (b) a random graph (elimination of mass gatherings); and (c) small world lattice (partial to full lockdown—“social” distancing). This model exhibits good qualitative agreement between simulation and data from the 2020 pandemic spread of a novel coronavirus. Estimates of the relevant rate parameters of the SEIR model are obtained and we demonstrate the robustness of our model predictions under uncertainty of those estimates. The social context and utility of this work is identified, contributing to a highly effective pandemic response in Western Australia."
"Information and communication technology (ICT) framework provides a novel perspective to fight human diseases [1] - [3] . In this respect, molecular communication could pave the way for a solution to develop therapeutic and diagnostic platforms. Recent Severe Acute Respiratory Syndrome-CoronaVirus 2 (SARS-CoV2) pandemic have resulted in a [4] , [5] . In [4] , the authors model Dengue virus transmission inside the body from its entrance to the host to the transmission to affected organs. The channel considered, which is from skin to the receiver organs, is characterized in terms of noise sources and path loss. Aerosol transmission, in which droplets carry virus, is the another means of virus transport mechanism. In [5] , the authors determine the aerosol channel impulse response and find the response of their system for the sources such as breathing, coughing and sneezing. On the other hand, a study considering the SARS-CoV2 transmission process through the human respiratory tract from molecular communication perspective is is yet to be studied in the literature. SARS-CoV2 enters the host human through the nose, mouth and eyes. We consider the case that droplets carrying viruses enter the host human from the nose. Viruses travel via mucus flow in the respiratory tract and reach host cells as illustrated in Fig. 1 . SARS-CoV2 virus binds a special receptor on the host cell called angiotensin-converting enzyme or ACE2. Binding is followed by a time delay, τ , which is due to the mechanisms needed for virus replication. In this study, we consider this system and accordingly develop a model for the human respiratory tract by separating the respiratory tract into seven segments. Our aim is to determine the impulse response of the SARS-CoV2-ACE2 binding process to investigate the probability distribution of binding locations. The binding location distribution, which depends on several system parameters including ACE2 density and mucus flow rate offers vital information on the course of disease. Our contributions can be summarized as follows: • Proposing a novel model of human respiratory tract that reduces complexities of the original system: We model human respiratory tract by partitioning the tract into seven segments from nasal cavity to alveoli. • Determining impulse response of SARS-CoV2 infection process for the first time in literature • Calculating ACE2 receptor densities in the different regions of the respiratory tract: Based on the available data on surface parameters, we calculate ACE2 receptor density crudely. • Investigating the effects of mucus layer thickness, mucus flow rate and ACE2 density on the virus population reaching the different regions of the respiratory tract: Our results shows that mucus flow rate and ACE2 densities affect the respiratory regions where the viruses reach drastically. The rest of the paper is organized as follows. In Section II, we provide a brief background about SARS-CoV2. In Section III, the developed system model is outlined. In Section III, the diffusion model for viruses diffusing through the mucus layer is derived. Next, in Section V, the impulse response of the system for different receptor and virus concentration is determined. In Section VI, Markov Chain model of the events following the binding process are stated. In Section VII, the simulation results are presented. Finally, conclusions are stated in Section VIII.@story_separate@Severe Acute Respiratory Syndrome -CoronaVirus 2 (SARS-CoV2), also named novel-coronavirus (2019-n-Cov), has been identified as the causative infectious agent of coronavirus disease- , responsible for the current pandemic. Covid-19 has turned from a local pneumonia outbreak, which originated in Wuhan, China in December 2019, into a global pandemic in a matter of months, which has as of now, October 2020, caused more than a million deaths worldwide and spread to more than 200 countries [6] . Belonging to the family of coronaviruses, SARS-CoV2 is the third and the newest coronavirus in the family to cause an epidemic, just as SARS-CoV in 2003 and MERS-CoV in 2012, and the only one to cause a pandemic. SARS-CoV2 is reported to be a zoonotic viral disease. Bats, snakes, and pangolins have been cited as potential reservoirs based on genome sequencing studies [7] - [9] . Although it predominantly causes pneumonia and associated comorbidities, Covid-19 is considered to be a syndrome, given that it affects multiple different organs and systems within the human body. Typical clinical symptoms of the patients include fever, dry cough, difficulty of breathing (dyspnea), fatigue, joint pain (arthralgia), muscle pain (myalgia), and loss of sense of smell (anosmia) [10] - [12] . The presence of high variety of pathological events are attributed to different pathophysiological mechanisms involved in SARS-CoV2 and proves that it is more than a respiratory syndrome. Current epidemiological data suggests that SARS-CoV2 is an airborne viral disease, meaning that it is transmitted through respiratory droplets and droplet nuclei, which are mostly spread during human-to-human contact [13] - [15] . Respiratory droplets (> 5−10µm in diameter) and droplet nuclei (aerosols) (< 5µm in diameter ), are generated and expelled/disseminated from an infected person during speaking, shouting, coughing, or sneezing [16] . Indirect surface transmission, i.e., fomite transmission, and orofecal transmission have also been reported [17] - [19] . Some studies have detected stable SARS-CoV2 viral RNA on solid surfaces such as plastic, aluminum, and stainless steel, yet the significance of fomite transmission is still debated with contradicting views [17] , [18] . The main pathway of SARS-CoV2 inside the human host is reported to be the respiratory tract. Mucosal openings such as the nose, eyes, or mouth have been identified as the principal sites, where the initial viral entry takes place [20] . Although there are numerous possibilities for viral entry, one pathway a virus particle could take on the macroscopic level is as follows. A virus laden particle enters through the nasal cavity, with the help of the downstream flow of mucosal secretions and gravity, it travels down through the pharynx, larynx, and trachea, enters a bronchi, passes down to bronchioles and finally reaches alveoli. On a microscopic level, once the virus laden droplets reach mucosal membranes, they diffuse through the mucosa (consisting of mucus, periciliary layer, and midlayer) and attach to certain membrane receptors on host cell surfaces, the most prominent one being ACE2, which has been identified as the primary functional receptor for SARS-CoV2, just as for SARS-CoV [21] - [24] . The current knowledge on SARS-CoV2 infection indicates that the elderly are more susceptible and vulnerable to the infection, while children seem to be the least affected group. Numerous studies report lower rates of SARS-COV2 infection with milder symptoms in children compared to adults [25] - [27] . Some studies attribute these results to their findings that ACE2 expression in children is lower than that of adults. [28]- [30] . Other possible reasons held responsible for lower rates of morbidity and mortality from SARS-COV2 in children include: the differences in immune responses between children and adults, differences in ACE2 receptor distribution patterns, and lower rates of testing in children due to abundance of asymptomatic cases [31] . The morphological structure of the virus comes to prominence when discussing viral binding processes. SARS-CoV2 is an enveloped, positive-sense, single-stranded RNA virus and similar to its prior relatives SARS-CoV and MERS-CoV, it belongs to Betacoronavirus genus of the coronavirinae family. SARS-CoV2 viral genome contains four major structural proteins: the S (spike) protein, the M (membrane) protein, the N (nucleocapsid) protein, and the E (envelope) protein [32] . The S protein has a trimeric structure, consisting of an S1 receptor binding subunit and an S2 fusion subunit. During viral infection, S1 and S2 subunits are cleaved by a metalloprotease, TMPRSS-2 (transmembrane protease serine 2), which facilitates viral entry. The S1 subunit functions as the part, which directly binds to the host cell receptor, i.e., ACE2 receptor, creating a Receptor Binding Domain (RBD). The S2 subunit takes role in membrane fusion [33] . Following viral binding, there are two possible pathways of viral entry for enveloped viruses into host cells: either cytoplasmic fusion in which their envelope fuses with plasma membrane and they release their genome into cytosol, or endosomal membrane fusion (endocytosis) in which they are engulfed by an endosome, and their membrane is fused with the endosomal membrane [34] , [35] . There are multiple mechanisms of endocytic entry suggested by various studies, involving clathrin dependent, caveolae dependent endocytosis [36] , [37] , and clathrin independent, caveolae independent endocytosis [38] , [39] . In Section II, we presented physio-morphological structure and behavior of the virus, regarding its entry mechanisms into human body and target cells. Here, we will present our system model. We assumed that the virus carrying aerosol particles enter the human host through the nose, and diffuse through the mucus layer in the nasal cavity, where ACE2 receptors are found most abundantly [40] . The diffusion of the virus takes place in the mucus layer, which renders the shape of the respiratory tract insignificant. Given the fact that the mucus layer is continuous within the respiratory tract [41] , we assume a cylindrical tube with radius r(y) and length l. The change in the radius throughout the tract has limited effect, unless it also modulates the properties of mucus, especially the mucus thickness. For a large portion of the respiratory tract, mucus layer covers the periciliary layer and a thin surfactant layer separates the two [42] . In our work, we assume that the surfactant layer reduces the surface tension between these two layers to a negligible value, and consequently ignored. Furthermore, we assume that the diffusion coefficient, D, of the virus in periciliary and mucus layers to be the same. In a healthy respiratory system, the mucus inflow to the alveoli is countered by the mucus outflow due to the pericilliary layer. We ignored the mucus outflow mechanism as it may turn the mucus flow into a very complex turbulent fluid model. In other words, we treat it as if it is one single layer. The existing works studying ACE2 distribution and mucus flow do not comment on differentiations within a region, i.e., ACE2 are homogeneously distributed. Hence, our model assumes cylindrical symmetry. The virus moves under the influence of the mucus flow from nasal cavity to the alveoli. We partition the respiratory system into seven parts, namely Nasal Cavity, Larynx, Pharynx, Trachea, Bronchi, Bronchiole and Alveoli. Our model is presented in Fig. 2 . Due to the complicated structure of the tracheobronchial tree, we assign transition regions to the closest region. Furthermore, since after each branching, the individual branches become narrower but more numerous, we used the surface area, S i , of each of the seven regions, i ∈ {1, 2, ..., 7}, to calculate its corresponding radii values, r i as where l i is the length of the i th region. The resultant the respiratory tract is shown in Fig. 3 . Note that Fig. 3 is not to scale, as the corresponding radii for alveol region is two orders larger than the next region, i.e., bronchiole region. Due to the cylindrical symmetry assumption, we can make a longitudinal Upon entering the mucus and periciliary layer, viruses use their viral S-spike proteins to bind to ACE2 receptors on host cell surfaces [43] . We will use the binding rate, λ, to describe the binding process. Due to the spherical shape of the coronavirus, we safely ignore the effect of the orientation of the virus at the time when it makes contact with the ACE2 receptor. As viruses bind to ACE2 receptors on the host cell's membrane surface, ACE2 receptors are downregulated. Therefore, the number of surface receptors decreases [44] , [45] , making it less likely for other viruses to bind. We consider two scenarios depending on the ACE2 receptor concentration and the virus population: • Large virus and large receptor concentration • Only large virus concentration As stated in Section III we assume a constant mucus flow rate, v from the nasal cavity to pharynx, larynx, trachea, bronchi, bronchiole and finally to the alveoli. Furthermore, the viruses also diffuse with a diffusion coefficient, D, in the mucus layer. The virus concentration is derived using Brownian Motion with drift. We assign y axis for the distance from the entrance of nasal cavity, x axis for the distance from a longitudinal cutting point and z axis as the depth in the mucus layer. Due to the assumption of cylindrical symmetry, the reference point for x coordinate is arbitrary. If a droplet containing N viruses is incident to the mucus level at the location (x 0 , y 0 , z 0 ), the virus concentration at time t is  The standard deviation for Brownian motion is given as σ = √ 2Dt. Therefore, 95.45% of the population of viruses falls into a sphere of radius 2 √ 2Dt, centred at (x 0 , y 0 +vt, z 0 ), while 99.7% into a sphere with the same centre and radius of 3 √ 2Dt. Hence, for vt 3 √ 2Dt, drift dominates the diffusion and diffusion along the y-axis can be ignored. Similarly, for vt 3 √ 2Dt, drift is dwarfed by diffusion and can be ignored. Fig. 4 shows the dominating trends for Brownian Motion with drift. For v = 50µms −1 [46] and D = 1.5 × 10 −11 m 2 s −1 [47] , the diffusion of the virus in the respiratory tract is shown in Fig. 5 . Clearly, the effects of the diffusion is only visible for large t. The ACE2-Virus binding can be modelled by obtaining the virus population distribution over the respiratory tract. To achieve this, we start with modelling the kinematics of a single virus incident on the mucus layer. Later, we use our findings as stepping stones to reach impulse response for different scenarios as described in Section III. We begin our analysis by considering a single virus is moving under the influence of mucus flow. The mucus layer has a thickness of h(y) and a velocity of v, while the respiratory tract radius is r(y), where y lies in the direction of the respiratory tract from nose to lungs. Then, at any segment dy, the concentration of the virus due to a single virus is given by The time ∆t that the virus spends in a segment of length ∆y is The probability that it binds to a single ACE2 receptor in the segment with length ∆y becomes where λ is the molar association constant, N A is the Avogadro's constant and λ 1 = λ/N A is the association constant for a single virus. Note that in the last step, we used first order Taylor series expansion, i.e., e x = 1 + x for small x. Then, p nb , the probability of not binding during ∆t is If the ACE2 concentration per unit area at y is f (y), then number of ACE2 receptors, n(y), in the patch of length ∆y becomes n(y) = 2πr(y)f (y)∆y, (11) and the probability of the virus evading all ACE2 receptors in the same patch, p e (y), is expressed as p e (y) = p n(y) nb where from (13) to (14) we use the first order truncation of the binomial expansion, i.e., (1 + x) n = 1 + nx for |nx| 1, which holds due to N A being much larger than any other value in (14) . This assumption is especially effective for ∆y → dy. From (15), we reach the rate of binding in the patch of length dy as Then, we find to the number of viruses at y, N (y) using an initial value problem with rate p b (y) where we used the fact that dy = vdt by definition. An important observation is that N (y) obtained in (21) does not necessarily normalise. DefiningV as where l is the total length of the respiratory tract,V gives us the rate of the viruses that reach the end of the respiratory tract, i.e., alveoli. Here, since the viruses cannot travel neither forward nor backward, we assume that they will eventually bind to an ACE2 in alveoli. So far, we only assumed the existence of a single virus to reach (21) . As stated in Section III, there are several scenarios depending on • N (y) = 2C(y)πr(y)h(y)dy, the total number of viruses number on a dy thick strip, • n(y) = 2πr(y)f (y)dy, total number of ACE2 receptors in the same strip, • E b , the expected number of virus bindings in the same strip. E b is loosely calculated by replacing C 1 with C and carrying out (3) to (15) . Hence, (3) becomes and replacing f (y) with n(y)/2πr(y)dy, A(y)N (y)n(y), where A(y) does not depend on ACE2-virus bindings. Since each ACE2-virus binding destroys both a virus and a receptor, both the virus and the receptor concentrations are affected. Hence, our model must incorporate variations in the concentrations. Each binding causes the number of viruses, N (y) and number of receptorsn(y) to change as Using (25), (26) and (27) we reach two assumptions: 1) Large N (y): If N (y) is large and E b N (y), the total virus concentration remains constant within the same segment. 2) Large n(y): If n(y) is large and E b n(y), the total ACE2 concentration remains constant within the same segment. Since A(y) is quite low, i.e., on the order of 10 −10 , for some cases, both of these assumptions hold. Fig. 6 illustrates under which conditions these assumptions hold. As Fig. 6 shows, when both N (y) and n(y) are large, the assumptions may not hold. Since A(y) depends on r(y) and h(y), the boundaries may change. Note that changes in n(y) causes a change in the system. As the system parameters change with the input, the system is no longer linear time-invariant. As a result, obtaining the impulse response when (27) does not hold is of no practical use. 1) Large n(y): For large n(y), the virus-ACE2 bindings do not change the receptor number in the same segment. Hence, regardless of how many bindings happen in a given segment, the binding probability of any virus in the same segment is constant. As a result, (17) , multiplied by N (y) gives the expected number of binding on the segment. Furthermore, (21) multiplied by the incident virus count, N 0 , gives us the virus population reaching to any location y of the respiratory tract, i.e., Hence, the virus concentration is simply the derivative of (28), i.e., . We proceed to obtain impulse response, I(y, t), by adding the unbounded or free virus distribution,V (y). The free virus population is situated at the location y = vt of the respiratory tract, due to the fact that virus movement on the respiratory tract is solely under the influence of mucus flow. The total number of free viruses is equal to the difference between the initial number of viruses and the total number of bound viruses.V where δ(.) is the Dirac Delta function. Thus, the impulse response becomes 2) Large N (y) only: In case E b N (y), viruses outnumber the ACE2 receptors. This causes all ACE2 receptors to bind to a virus. Bound virus distribution in the respiratory tract is the same as the ACE2 receptor concentration. Note that, since bound ACE2 receptors downregulate, large N (y) only case cannot be modelled as a linear time invariant system. n(y) = 2πr(y)f (y)dy, and where u(.) is the step function and is used to assured that virus distribution is limited to the region 0 − vt, i.e., the range  We find the impulse response by adding (33) and (34), i.e., 3) No Limiting Case: In case both assumptions fail, active number of ACE2 receptors constantly changes due to the binding viruses. Therefore, no assumption can be made for this case, and neither (31) nor (34) holds. As a result, there is no closed form expression for this case. In the host cell, the virus replicate and new virions are released out of the cell via exocytosis. We can model this process with a stationary Markov Chain with six states, namely, binding (B), endocytosis (EN D), release of viral RNA (C), replication (R), degradation (D), exocytosis (EXC) as illustrated in Fig. 7 . The bound virus, can enter the cell via endocytosis, which is mediated by ACE2 receptors. In the state-B, the virus is found bounded to the ACE2 receptor. COVID-19 is an RNA virus, i.e., virus can replicate in the cytoplasm. Thus, in the state-C viral RNA is released to the cytoplasm. In the host cell, the virus can be degraded by lysosomes [48] , which is represented by the state-D. The transition matrix of the Markov Chain representing the life cycle of the virus in the host cell, Q, is given by where we set G 1 = −r BE , G 2 = −(r EC + r ED ), G 3 = −(r CR + r CD ), and G 4 = −(r RD + r RE ). The transition rates are provided in Table II .  represents corresponding state occupancy probabilities. The relation between the states can be expressed as [49]  which has a solution of (38) is in the form of Using eigenvalue decomposition, we can express Q as where v i is an eigenvector of the matrix and λ i the corresponding eigenvalue. As a result, we can express (39) as Thus, the probability of transition from the state j to the state k in time t is given by Morphometric measurements of the respiratory tract such as length, diameter, surface area, and mucosal thickness were obtained from estimation studies, journals, databases, and anatomy literature. Given the non-uniform shape and the continuously narrowing nature of the respiratory tract, as in the tracheobronchial tree, we use median values for branching or narrowing structures. Divided into 23 generations of dichotomous sections, the tracheobronchial tree designates a generation for each divided branch starting from trachea, generation 0, and ending at alveoli, (generation 23). The first 16 generations, from generation 0 to generation 16, are defined as the conducting zone, i.e., no gas exchange takes place in this region. From generation 17 to generation 23 is called the transitional and respiratory zone, where gas is exchanged within functional units [50] . The generation 0 directly gives the dimensions for the trachea. Generations 1 to 4 are assumed to be bronchi, 5 to 16, bronchiole and 17 to 23 alveoli respectively. For our parameter data, we mainly use Weibel's ""Dimensions of Human Airway Model A"" and Gehr's ""Annexe A. Anatomy and Morphology of the Respiratory Tract"" [51] . Although there are studies investigating ACE2 receptor gene and protein expressions across different tissues and in specific cell types using single cell RNA sequencing gene expression profiling datasets, mass spectrometry and immune assay techniques, [24] , [52] - [54] , to the best of our knowledge, data on the number of ACE2 receptors on different tissues is not explicitly stated in studies. Most studies provide relative expressions of the receptor in different tissues, shown as proportions, percentages, or plots with no numeric values. Some studies address circulating ACE2 levels, which we cannot directly utilize as we need tissue-specific values. There exist some studies which report ACE2 expression data in animals, which are not compatible with our work either [55] . The primary challenge of this work is to obtain the ACE2 receptor densities in different tissues of respiratory tract. The lack of studies giving these values is mostly due to the difficulty of measuring ACE2 receptor concentration in a diverse population of all ages. To address this challenge, we exhaustively search among various literature to calculate our estimated values. The specific works that we use are referenced in Table III . Therefore, we first gather data on the percentage of ACE2 expressing cells for the seven region model described in Section III. Then, we search for the total number of cells in each region. For tissues in which there is no sufficient quantitative data on the percentage of ACE2 expressing cells, the relative proportions of ACE2 expressions of two or more tissues, one of which we have previously calculated are used. Then, we calculate the number of ACE2 expressing cells in each tissue accordingly. Note that these preliminary calculations are the estimates based on the currently available data in the literature. Due to the lack of data, the effect of age in SARS-CoV2 susceptibility cannot be directly analyzed. However, we investigate the effects of thicker mucus as seen more in elderly and effects of higher ACE2 concentration in nasal cavity as observed in smokers. In this section, we first present the impulse response simulation and then continue with simulating the effect of mucus flow rate, v, ACE2 receptor density, f (y) and mucus thickness, h(y), on the virus-ACE2 binding. 1) Impulse Response of Unobstructed Viral Progression: In Section. V, we find an analytic expression for the impulse response of unobstructed viral progression through the respiratory tract. Here, we confirm our analytic expression with a Monte Carlo simulation in Fig. 8 . The physiological parameters that we use in the simulations are presented in Table III . For the Monte Carlo simulation, we divide the respiratory tract to ∆y = 5µm patches. The initial number of viruses are N 0 = 50000. Each virus is independent of each other, i.e., a new number is generated using the Marsenne Twister for each virus in each segment. We see that our analytical solution is in full agreement with the Monte Carlo simulation of the system for large N 0 . 2) Mucus Flow Rate: As it can be seen in Fig. 9 , mucus flow rate has a significant impact on the reach of the virus population. If the patient suffers from another condition causing nasal drip or any other faster mucus flow, the virus spends less time in the upper respiratory system. Therefore, ACE2- virus bindings in the upper respiratory tract is limited, causing the bulk of the virus population to migrate to the lower parts of the respiratory tract, especially bronchioles and alveoli. The virus population in the alveoli is 20-folds more if the mucus drop rate is v = 100µms −1 compared to v = 25µms −1 . This causes the virus to take hold in the alveoli before an immune response can be launched. 3) Nasal ACE2 Receptor Density: Fig. 10 shows us the impact of the ACE2 receptor concentration in the nasal cavity. Assuming distribution of the ACE2 receptors in the other parts of the respiratory tract is the same for different age groups, the difference in the ACE2 levels in nasal cavity has a significant effect on the virus population reaching the lower respiratory tract. The impact of ten-fold increase in ACE2 receptor concentration is six-fold increase in virus concentration in the lower respiratory system. 4) Mucus Thickness: Our model suggests an impact of the mucus thickness. Since we assume that the virus can move freely in the mucus layer via diffusion, thicker mucus implies that there is less chance for the ACE2-virus binding. Fig. 11 shows the effect of the mucus thickness. The virus population in the alveoli is 4.45 times more in the four times thicker mucus compared to the base mucus level.@story_separate@In this study, we analyze SARS-CoV-2-ACE2 receptor binding event to determine bound virus population at the different regions of respiratory system. To this end, we develop a molecular communication model of SARS-CoV-2 transmission through the human respiratory tract, which reduces the inherent complexity of the respiratory system. We perform an analysis of the developed model using mucus flow rate and ACE2 receptor densities, which are calculated based on the realistic data of respiratory tract surface parameters. Based on the analysis, we reach that higher mucus flow rate results in virus migration to the lower respiratory tract, which is compatible with the experimental results found in the literature. Our model will be useful to describe the travel of the virus through the respiratory tract and to simulate the effect of interventions (e.g. antivirals) to decrease the viral load.","Severe Acute Respiratory Syndrome-CoronaVirus 2 (SARS-CoV2) caused the ongoing pandemic. This pandemic devastated the world by killing more than a million people, as of October 2020. It is imperative to understand the transmission dynamics of SARS-CoV2 so that novel and interdisciplinary prevention, diagnostic, and therapeutic techniques could be developed. In this work, we model and analyze the transmission of SARS-CoV2 through the human respiratory tract from a molecular communication perspective. We consider that virus diffusion occurs in the mucus layer so that the shape of the tract does not have a significant effect on the transmission. Hence, this model reduces the inherent complexity of the human respiratory system. We further provide the impulse response of SARS-CoV2-ACE2 receptor binding event to determine the proportion of the virus population reaching different regions of the respiratory tract. Our findings confirm the results in the experimental literature on higher mucus flow rate causing virus migration to the lower respiratory tract. These results are especially important to understand the effect of SARS-CoV2 on the different human populations at different ages who have different mucus flow rates and ACE2 receptor concentrations in the different regions of the respiratory tract."
"There is a growing emphasis on the inclusion of patients and other stakeholders in healthcare research, in order to produce evidence that matters to patients and families and to meaningfully impact dissemination and uptake of findings [1, 2] . A continuum of involvement can be seen in the literature from patients and the public serving as advisors, helping to select the research questions, assisting with the conduct of the study as members of the research team, and even to serving as co-authors on manuscripts [3] [4] [5] . Despite the importance of patient-engaged research, persons living with dementia and their families continue to face barriers to active engagement with researchers [6] . Bethell et al. [7] conducted a scoping review to describe the extent of patient/family engagement in research related to dementia and found that, while there is a growing number of research teams that engage persons living with dementia (PLWD) and family caregivers, researchers continue to perceive barriers that include the added costs and time as well as the lack of training required to adapt to a shared decision-making process. The growing number of families impacted by Alzheimer's disease and related dementias (ADRD), coupled with the lack of disease-modifying therapies [8] , underscores the urgent need for research that addresses priorities in care for families. Bringing the perspective and expertise of families who have the lived experience of dementia to our research is important to the creation of evidence that matters and is beneficial to patients and families [9] . What is important to families often contrasts with professional perspectives; interventions will be impactful only when they make a difference on patient-and caregiver-reported outcomes [10] . Yet, a recent review examining family-centered dementia care research reports limited conceptualization in the literature around family-centered research [11] . The unique perspectives of stakeholders, including those impacted personally by dementia and social/health care professionals as well as researchers who provide care for families impacted by dementia, conduct research, and influence policy, must be included in discussions about supportive care and the priorities for research in this area. The purpose of this study was to partner with stakeholders to identify gaps in care and from this list, identify priorities for dementia care research. A second purpose was to describe the priorities for dementia care research by stakeholder group and examine for differences by group. To accomplish this, we conducted a survey to capture multiple perspectives, importantly PLWD, family caregivers to PLWD, and health/social care professionals who serve families affected by ADRD. This research was conducted in partnership with a Stakeholder Advisory Council (SAC).@story_separate@To identify priority dementia care research topics, surveys about the importance of research topics identified by the SAC were administered to community members impacted by dementia. A detailed description of the methods we applied follows. This project was submitted to the Institutional Review Board and deemed to be exempt (HSC20180659N). All SAC members provided verbal consent for their participation in identifying topics for research that would serve as survey items and in helping to identify priorities. Participants who completed the surveys to rank the survey items were provided with an information sheet about the survey, which was completed electronically or through an interview conducted on Zoom. The SAC was convened in November 2018 and continues to meet monthly as of March 2021 to address research topics relevant to patient/family-centered dementia care. The SAC consists of 15 members, including 2 PLWD, 4 family caregivers, 7 health/social care professionals, and 2 researchers. Professional members include geriatricians, a nurse with expertise in palliative care, social service program providers, and a member from the faith community. The SAC was co-led by the Principal Investigator for this research (CW), in partnership with MF, who lives with dementia. Co-chairs of the SAC consulted on agenda items each month. Monthly meetings lasted for one to one and a half hours each, with an average of 10 members in attendance. With the beginning of the pandemic and need for social distancing, the SAC transitioned to virtual zoom meetings as of March 2020. SAC meetings were designed to build capacity among members to participate in patientcentered outcomes research. Specifically, we reviewed principles of ethical conduct, discussed patient-centered outcomes research with a focus on supportive care, stakeholder engagement in research, and the role of the SAC in identifying priorities for dementia care research. After participating in generating the list of dementia care research topics, SAC members helped to reduce the list by removing redundant items and those that could not be addressed by research, refining the data collection instrument, participating in recruitment as well as data collection, and providing their interpretation to the results. Additional information about the SAC and the experience of participating in this council are described in Masoud et al. [12] . Identifying priorities for dementia care research was an iterative process. SAC members described gaps in care from their different perspectives and potential areas of research relevant to these topics, with ethnographic notes taken at each monthly SAC meeting with members' consent. SAC members were encouraged to discuss dementia-related research topics with others in their personal networks, documenting notable topics to share at the SAC meetings. Through our web-based portal, questions were posted about gaps in care and research topics to address these gaps. From the postings on the portal from the SAC and the wider network, topics for research were also collected. Discussions took place at the monthly meetings around these different topics. From the meeting notes, research topics were extracted and compiled into a list, which was shared with the SAC. An in-person symposium was held in June 2019, with a focus on stakeholder engagement in research. Through small group discussions, led by SAC members, research topics around dementia care were discussed and added to the list. From these different sources, a long list of 86 questions/topics was generated. A sub-group of SAC members, consisting of 2 researchers, a family caregiver, and 2 clinicians, met over 3 meetings to review the long list. Common ideas were merged, duplicates were removed, and wording was revised for clarity. An underlying principle in designing the survey instrument was to ensure that it would be accessible to all stakeholders, including PLWD. We followed the 'dementia friendly' design principles recommended by Morbey et al. [13] in their project of developing a core outcome set for evaluating interventions for PLWD. The SAC was also consulted on the design of the survey instrument. The survey instrument included the use of 'plain language', enlarged font, contrasting font and background color, and application of alternating background colors for each question on the survey collection tool. Response options were also abbreviated to 3 response alternatives about the importance of the question, as compared with typically used 5-point Likert scales, to limit decision burden among PLWD. The shortened list was presented to the SAC during a monthly meeting for their review, with minor changes recommended. Edits included changing the items to topics rather than questions, as they found that they were trying to answer each question rather than rate it for importance, as well as design and wording changes to make the instrument more suited for families impacted by dementia. These changes were integrated into the survey instrument and reviewed again by the SAC prior to distribution in August 2020. Figure 1 summarizes this process. Those eligible to complete the survey were individuals who were personally or professionally impacted by dementia. This included PLWD, family caregivers, and professionals who worked directly or indirectly with families affected by dementia. A snowball sampling technique was used to recruit participants, wherein members of the SAC were asked to distribute the survey to potentially eligible individuals in their respective communities. Community members who administered survey questionnaires were trained to assent PLWD, including processes to attain verbal assent, how to recognize signs that an individual withdrew assent, and how to identify when a participant's level of cognitive functioning may detrimentally affect their ability to provide meaningful survey responses [14] . Surveys were completed both through an electronic link to the Qualtrics survey and 'face-to-face' via Zoom. Face-to-face surveys were offered to PLWD and were administered by community members and members of the research team. Participants were asked to rate the importance of each research topic area in the final list of priority research topics consisting of 46 separate topics, grouped into 7 overarching research domains (Fig. 2 ). Participants were asked to identify as a PLWD, family caregiver, health or social care provider, researcher, or other; those selecting the ""other"" option were asked to write in their role. We also collected data about participants' ethnicity and gender. Survey items were listed as research topics (e.g., How to manage, anxiety, fear, and other emotions related to a diagnosis of dementia). Participants could respond that items were ""Not so important,"" ""Important,"" or ""Very important."" This wording was selected because all items were considered to be important given that they were identified by the SAC and other stakeholders, and thus there was no option to indicate a topic as being 'unimportant', but instructions indicated to think about the relative importance each particular item held for them. Survey data were analyzed using descriptive and bivariate statistics. We examined the distributions of the responses to the research topics using frequencies and proportions of responses in rating the topics. We also examined those items rated as ""Very Important"", stratified by stakeholder group (PLWD, family caregiver, health/social care professional which also included 'other'). To summarize findings, we present results from the five items with the highest proportion of ""Very important"" and the 5 items with the lowest proportion of ""Very important"" for each subgroup. After examining expected frequencies of responses for each sub-group according to importance ratings, we completed bivariate analyses. For those responses wherein expected frequencies were < 5, we applied a Fisher's exact test. For all other bivariate comparisons, we used a Pearson Chisquared statistic. Differences with a p-value of < 0.05 were considered to be statistically significant in the distribution of responses by subgroup. Analyses were completed in Stata 15.1. A total of 186 participants completed the survey from August through October 2020, including 23 (12.4%) PLWD, 101 (54.3%) family caregivers, and 62 (33.3%) classified as health/social care professionals/other. Of this latter group, the majority (n = 41, 66.1%) selfidentified as health and social care professionals, 4 healthcare students, and 17 as ""other"", which included paid caregivers, researchers, and elder lawyers, among others. Participants identified as non-Hispanic White (50%), Hispanic (35%), Black/African American (8.3%), and other race (6.7%). Three-quarters were female (75.6%) and 24.4% were male. See Table 1 for demographic characteristics by stakeholder group. Supplemental Table 1 shows the 46 topics for research included in the final list and the percentage for each topic selected as ""Very Important"" overall and by stakeholder group (PLWD, family caregiver, and health and social care professional/other). There were 10 research topics that were selected by two-thirds or more of the sample. Three of these topics fell under the theme of research topics related to the time of diagnosis, and included the resources and support that families need at the time of diagnosis (76%), the benefits of an early diagnosis (68%), and how to improve the process of a timely diagnosis (68%). Intervention topics rated as ""Very Important"" included how to support PLWD to remain in their own homes (73%), how to support families to provide care as the dementia progresses (71%), nonpharmacological interventions to manage behavioral symptoms (69%), and how to support the emotional health of PLWD and their families (68%). Other research topics rated as ""Very Important"" included interventions to support the dignity of PLWD (70%) and how we can respect individual choices of quality vs. prolonging life (66%). Finally, research that identifies the skills and knowledge needed by the health care support to support PLWD and their families was selected as ""Very Important"" by 66% of the sample. There were few significant differences across the distribution of frequencies by stakeholder group. Notable significant differences related to special populations (rural areas, those living alone, and different cultures) which were selected as ""Very Important"" by a higher proportion of health and social care professionals compared with other stakeholder groups. Relative to other stakeholder groups, a higher proportion of PLWD selected research to support individuals with younger onset dementia as well as ways for support when PLWD are unable to make decisions about health and care. Among PLWD, research focused on support for continuing to live in their own homes was ranked as the highest priority, rated by 91.3% as ""Very Important"" ( Table 2) . Among other priority topics were research around advance care planning (78.3%) and respecting individual choices (82.6%). PLWD also prioritized research that identified the benefits of an early diagnosis (73.9%), evidence about information and resources to support families following a diagnosis (73.9%), as well as support for managing the emotional consequences of a diagnosis of dementia (73.9%). The majority (78.2%) also rated the need for research that identifies the skills and knowledge health care professionals need to communication with families as ""Very Important"". High priority research areas for family caregivers included interventions to slow cognitive decline (76.3%) as well as non-pharmacological approaches to manage behavioral symptoms (74.7%). Family caregivers also prioritized evidence for resources and support for families following a diagnosis (74.2%) and support that would allow PLWD to continue living in their own homes (72%). Priority research topics for health care professionals were focused on the time of diagnosis including the benefits of an early diagnosis (71.4%), how best to provide the diagnosis (70.9%), and the supports needed following a diagnosis (78.6%). Priorities common across all stakeholder groups was the importance of research on how best to support families following a diagnosis of dementia. Furthermore, both PLWD and family caregivers prioritized effective interventions to support the PLWD to continue living in their own homes. There were more commonalities than differences across stakeholder groups in research topics rated as low priority based on the proportion of participants rating them as ""Very Important"" (Table 3) . For example, across stakeholder groups, examining how adult day programs support quality of life was given low priority. Identifying work or volunteer opportunities for family caregivers, drawing on what they have learned from their role was scored as ""Very Important"" by around one-third of PLWD (34.7%), family caregivers. (32.6%) and health/social care professionals (39.2%). A low percentage of PLWD selected culture-specific approaches to supporting at-risk communities (31.8%), research about interventions for financial support (34.8%), and interventions for people with dementia who live alone (41%) as ""Very Important"". Identifying effective strategies to engage PLWD and family caregivers in research was also rated as a lower priority by both family caregivers (37.8%) and health/social care professionals (44%). We compared the top priorities selected by PLWD with the percentage of family caregivers and health/social care professionals selecting those research topics as ""Very Important"" (Table 4 ). There were no significant differences in the distribution of the responses except for the topic that concerned research to support PLWD who are unable to make decisions about their health and health care. This topic was selected as ""Very Important"" by 73.9% of PLWD, 53.8% of family caregivers and 56.9% of health and social care professionals (p = 0.04). There was a trend for significant differences on research about the services and supports that are needed to help people living with dementia continue living in their own homes, 73.6 How the process of receiving a diagnosis of dementia can be more personalized for the individual and the family. Information and resources needed to help individuals and their families following a diagnosis of dementia. Services and supports that are needed to help people living with dementia continue living in their own homes. 72.0 Identify how to support people living with dementia and their families who are vulnerable and at higher risk for poor health outcomes. How an early diagnosis benefits the person with the diagnosis and their family. How to manage anxiety, fear, and other emotions related to a diagnosis of dementia. Ways to support communication and decisions about medications and care among people living with dementia, their families, and health care providers. Ways to support people living with dementia who are unable to make decisions about their health and care. with 91.3% of PLWD selecting this as ""Very Important"" compared with 72% of family caregivers and 66.7% of health/social care professionals (p = 0.07). This paper reports on the priority areas for dementia care research identified by stakeholders. To add to the science of dementia-specific supportive care, it is important that the voices of all stakeholders are included. To this end, SAC members as well as the larger network accessed through the symposium, webinars, and the web portal contributed, in an iterative fashion, to identifying and prioritizing topics for dementia care research. Findings are intended to inform those who conduct research  Overlapping priorities among groups are bolded PLWD Person Living with Dementia, HCP health and social care professionals, researchers, other and those who fund research about which research topics stakeholders believe are most important and thus have greatest potential to improve the quality of life among PLWD and their families. The findings from this project are consistent with a study conducted by Bethell et al. [15] to identify dementia research priorities among stakeholders in Canada. They conducted a workshop with stakeholders for final prioritization of their short list of 23 questions. Our initial plan was also to discuss the prioritization of the topics for research at an in-person symposium, but we needed to transition to an online survey related to the COVID-19 pandemic. Despite the different processes, there are consistent themes among the priorities for dementia care research, including supports and services after diagnosis, non-pharmacological management of behavioral symptoms, dementia-related knowledge and skills needed by health-care providers, and care that will support the dignity of the PLWD. The James Lind Alliance also conducted a dementia priority setting partnership to identify priorities related to prevention, diagnosis, treatment, and care related to dementia [16] . Similar priorities include the impact of an early diagnosis, supports to continue living at home, as well as a focus on palliative and end of life care. And consistent across all three studies from three different countries is the need for further community support and education for families living with dementia. We examined the high and low priority topics by stakeholder group. PLWD and family caregivers bring their unique perspectives, drawn from their lived experiences, to identify gaps in care and topics for research. Although there were few significant differences by stakeholder group across the 46 topics for research, there were differences in the ordering of priorities, with PLWD and family caregivers selecting items around diagnosis, cognitive interventions, and the emotional consequences of the diagnosis among their top priorities, while health and social care professional priorities included the process of giving the diagnosis and how a diagnosis impacts the relationship for the PLWD. One reason for these differences may be that professionals are more likely to encounter families who struggle to get a diagnosis and who experience relationship challenges, even if these issues may not affect all families living with dementia. Topics related to diagnosis and the time following diagnosis were among the priority topics for research across stakeholder groups, including strategies around a timely diagnosis. Challenges with receiving a timely diagnosis may be related to the ongoing beliefs and understanding about aging and cognitive decline. Surprisingly, in a survey conducted by Alzheimer's Disease International with almost 70,000 participants in 155 countries, 62% of health care professionals still believe that dementia is part of normal aging [17] . Participants within the SAC meetings discussed the gaps in care around diagnosis, with little guidance on what the diagnosis even means and where to go for information, education, and resources. A recent scoping review highlights the limited support for family caregivers during this period of transition into the caregiving role, with their needs following diagnosis including knowledge and information about the progression of the disease, emotional and psychological support, and assistance with care planning [18] . The review identified only 4 interventions tailored to the period surrounding the diagnosis. This suggests an important focus for research, with interventions that could proactively improve long-term health outcomes for family caregivers. Furthermore, interventions aimed at those in early stage of dementia, including information about the progression of the condition and support for advance care planning, identified as a priority by PLWD, could enhance dignity and their quality of life. Within the priority topics, there were topics for which there are already available evidence-based interventions. In a systematic review, Gitlin and colleagues reported that over 200 interventions have been tested for supporting family caregivers, but few have been translated into practice, remaining inaccessible to most of the more than 16 million family caregivers of PLWD in the US [19] . For example, there are programs with good evidence from randomized controlled trials to help family caregivers in managing the behavioral challenges associated with dementia such as the Tailored Activities Program [20, 21] , the Care of Persons with Dementia in their Environments [22] , and the Savvy Caregiver Program [23, 24] to name a few, yet most family caregivers do not have access to these programs. There are currently over 50 million people worldwide with a diagnosis of dementia, projected to increase to 152 million by 2050 [25] . There is an urgent need to address this growing crisis, delivering patient-and familycentered care. It is critical that we study what matters most to these individuals and families impacted by dementia, including translational research that disseminates evidence-based interventions, so they are accessible to the community of PLWD and family caregivers. Crowe et al. [10] reported on the continuing mismatch between what patients, families, and clinicians rank as important topics for research and the research that is actually conducted. In using the data from the James Lind Priority Setting Partnerships and comparing what topics were selected as priorities by stakeholder groups with a random selection of clinical trials, they reported marked differences, with drug studies accounting for the majority of commercial and non-commercial trials, while they accounted for only a small proportion of the priorities selected by the stakeholder groups. Addressing this mismatch necessitates an increased focus of engagement of PLWD and family caregivers in our research, beyond setting priorities, to functioning as active members of the research team. Yet, results from this project show that engagement in research was rated as ""Very Important"" by only 41% of participants and was not significantly different by stakeholder group. This result requires further investigation, including how stakeholders perceive ADRD research. Despite this growing emphasis in including stakeholders in all phases of the research, there are still few examples of this being operationalized within research settings. This resistance often comes from researchers and research institutions, with resistance related to the added time to the research to build and maintain trusting relationships [26, 27] . There are several models that can serve as examples of including PLWD and family caregivers in co-producing research and ensuring that research is studying what is relevant to families [4, 28] . When interpreting the results, the limitations of this study should be considered. Participants who completed the survey were from an urban center and thus do not necessarily reflect the priorities for research among those in rural areas. Participants were recruited through email and the survey was completed electronically or through Zoom so may not reflect those without access to technology, particularly underserved populations. We also acknowledge that our sampling technique which included members of the SAC reaching out to their networks to participate in the priority-setting could have led to bias as participants may not be independent of one another. This was, however, just one technique used for recruitment and the diversity of the SAC may have contributed to more diversity within the sample. We included only those with early-stage dementia, so the priorities might not be the same for those in later stages of dementia. Further, surveys to PLWD were interviewadministered over Zoom to ensure they were accessible compared with online completion by other participants and this could lead to potential differences in responses. We utilized a 3-point rating scale recommended from the work of Morbey et al. [13] to ensure that the survey was accessible to PLWD. This most likely limited the variability in rating the different topics. Despite this, our findings are consistent with previous priority-setting projects among dementia stakeholders [15, 16] . The low representation of African American/Black individuals (8.3%) impacts on conclusions we can draw for this group. For example, specific needs of African American caregivers to PLWD may be reflected in differences in their priorities for dementia care research [29] . Given the disparities in dementia and dementia care for this racial group [30] , there is an urgent need to ensure that future research captures their priorities for dementia care research.@story_separate@This project draws on the strengths of its multistakeholder perspective and the aim of generating patient-centered outcomes research questions was accomplished. These results offer directions for researchers and funding agencies across the trajectory of the condition, including diagnosis, resources and supports needed as the dementia progresses, including supports needed to support the family and to help keep the PLWD in the community. These findings lend support to the importance of recent policy efforts to adopt evidence-based models of support for families impacted by dementia, such as the RAISE (Recognize, Assist, Include, Support, and Engage) Family Caregiver Act [31] . This project was focused on supportive care for families impacted by dementia and the priorities identified here do not preclude the important need for continued research that investigates other areas of importance to dementia, including mechanisms of the disease, risk factors, prevention, and disease-modifying therapies.","OBJECTIVES: The purpose of this study was to partner with stakeholders to identify gaps in care for persons living with dementia and their family caregivers and from this list, identify priorities for dementia care research. METHODS: Using a community-engaged research approach, a Stakeholder Advisory Council (SAC) consisting of diverse membership including persons living with dementia and family caregivers was convened. Through our work with the SAC, along with input from the wider network through a symposium, webinars, and an online learning community, gaps in dementia care and a list of topics for dementia care research was generated. This list was reduced to 46 topics for dementia care research and sent to stakeholders (persons living with dementia, family caregivers, and health/social care professionals in dementia care) to be prioritized by rating each of the 46 topics as “Not so important,” “Important,” or “Very important.” Priorities for dementia care were summarized by frequencies and proportions. RESULTS: A total of 186 participants completed the survey from August through October 2020, including 23 (12.4%) persons living with dementia, 101 (54.3%) family caregivers, and 62 (33.3%) health/social care professionals. Consistent across stakeholder groups was the focus on research on how best to support families following a diagnosis of dementia. Among persons living with dementia, research focused on support for continuing to live in their own homes was ranked as the highest priority, rated by 91.3% as “Very Important”. High priority research areas for family caregivers included interventions to slow cognitive decline (76.3%) as well as non-pharmacological approaches to manage behavioral symptoms (74.7%). The highest priority research topics for health/social care professionals were focused on the diagnosis including benefits of an early diagnosis (71.4%), how best to deliver the diagnosis (70.9%), and supports needed following a diagnosis (78.6%). CONCLUSIONS: This project draws on the strengths of its multi-stakeholder perspective to support patient-centered outcomes research. Findings are intended to inform those who conduct research and those who fund research about which research topics stakeholders believe are most important and thus have greatest potential to improve the quality of life among people living with dementia and their families. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1186/s41687-021-00325-x."
"Collegium Telemedicus is a not-for-profit organization, which provides free technical support to groups conducting humanitarian or noncommercial telemedicine services in low resource settings. The organization offers the infrastructure necessary to set up and operate a store-and-forward telemedicine service. The Collegium system is provided under a software-as-a-service model and is designed to be easy to use and to be able to service requests from increasing numbers of users (see https://collegiumtelemedicus.org for further details). The Collegium system was first made available in 2012, and over the subsequent six years, 46 networks were established. The majority of the networks were set up to provide a clinical service (72%), with six networks (13%) designed for education and training, and the remainder (15%) for test or administrative purposes [1] . The two most active networks handled almost 12,000 cases during that period. Since the initial review conducted in 2018, the Collegium system has continued to be widely used by different organizations. While some networks ceased their activity when their telemedicine work came to an end, new networks have begun operations. At the time of writing (July 2020), over 23,000 cases have been managed by the networks. In December 2019, a new severe acute respiratory syndrome coronavirus 2 was identified. It began to spread rapidly around the world and the World Health Organization (WHO) declared the outbreak a global pandemic in March 2020. Although the case fatality rate of SARS-CoV-2 is lower than that of SARS-CoV and MERS-CoV, COVID-19 has resulted in more fatalities than other coronaviruses [2] . Major infection-control measures were put into place around the world during 2020 in an effort to reduce the spread of the disease. Between March and April 2020, over nine-tenths (91%) of the world's population were living in countries with travel restrictions, and over half of the world population was under some form of stay at home/quarantine order [3] . Against this background, several of the well-established Collegium networks began to prepare for the effect of the pandemic on their telemedicine operations in early 2020. For example, one network operated by a large humanitarian organization anticipated that the numbers of telemedicine cases would increase rapidly, and that additional resources, such as extra case coordinators and specialist expertise, would be required. Other networks were more sanguine about the size of any expected change in their numbers of telemedicine cases. Organizational changes in advance of an expected global event are likely to have resource implications: more resources expended on telemedicine are likely to mean fewer resources for conventional methods of health care. Yet there is little published information about telemedicine planning for expected global events. The closest report of which we are aware concerns the planning for a telemedicine minorinjuries network to support expected patient numbers during a solar eclipse in 1999 [4] . Because of the dearth of information about the likely effect of the coronavirus pandemic, we surveyed three wellestablished telemedicine networks to assess any changes during the first half of 2020 compared to previous years.@story_separate@We examined three Collegium telemedicine networks (networks A, B, and C) which were managing clinical cases from low-resource settings during 2020 and had been active for at least three years. In accordance with the Conditions of Use of the Collegium system (https://www.collegiumtelemedicus .org/ct/conditions.php?lang=en), only those networks which provided permission are identified here. We analysed various characteristics of their operation under eight main headings: (1) Network characteristics In each area, various quantitative performance indicators were evaluated. For example, case mix was defined in terms of the types of queries (i.e., the expertise of the specialists who were consulted) involved in the management of the cases. Case complexity was approximated by the numbers of messages per case, the numbers of queries per case, the proportion of unanswered queries, and the dialogue time. Case management was defined in terms of various performance statistics, such as the allocation delay. See Table 1 for a full description. One important index of network demand is the referral rate, i.e., the number of new cases submitted in unit time. The referral rate was calculated for each network, for the months January to June 2020. The baseline, which was used as a comparator, was the average of the monthly rates from January to June 2018 and January to June 2019 in the same network. We analysed the performance of each network in terms of the number of cases received and the delay in answering them. In addition, we examined the user feedback which had been provided by the referrers in the course of their use of the system. Three weeks after each case was submitted, the referrer was automatically sent an invitation to complete a multiple choice questionnaire about the case. The questionnaire contained 12 questions, of which three in particular were relevant to network performance: Q6 ""Did you find the advice helpful?"" (yes/no/do not know) Q8 ""Do you think the eventual outcome for the patient will be beneficial?"" (yes/perhaps/no/do not know) Q9 ""Was there any educational benefit to you in the reply?"" (yes/no) Finally, we examined the network efficiency, in terms of the outputs produced and the input resources consumed. Making the assumption that each case submitted to a network was dealt with to the satisfaction of the referrer, for which there is some evidence [5] , then the ""output"" of the network can be taken to be the number of cases that was managed by it during the study period. To handle a telemedicine case, the networks depended on case coordinators for overall case management, and panels of specialists who provided expertise to the referrers, i.e., these represent ""input"" resources. The network efficiency can be approximated as the output produced divided by the input resources. 3.1. Network Characteristics. Two of the three networks surveyed (A and B) provided telemedicine services for any type of medical or surgical case, while the third (network C) handled only pediatric radiology cases. All networks operated in Africa, but networks A and C also provided services in other resource-constrained regions. Two of the networks (networks B and C) used local staff to submit referrals, while network A relied mainly on its expatriate staff, who were 2 International Journal of Telemedicine and Applications The dialogue time is the interval (h) from case receipt until the last message from the referrer or specialist (excluding any progress report) The proportion of cases that were not allocated automatically (i.e., by the computer) to specialists The allocation delay is a measure of the performance of the coordinator(s) during the period in question. Every case will result in at least one query. The interval between the arrival of the case and the first time it is allocated for reply represents the allocation delay. This is true even if say a case results in two queries, and the first goes unanswered. The allocation delay, which is measured in hours, is defined as the delay before the first query was sent out, irrespective of whether that query was actually answered. If automatic allocation is in use, then cases on which it is used will have zero allocation delay. The number of messages sent by the coordinator(s) Answer delay (h) The answer delay is a general measure of network performance, as perceived by the referrer. The answer delay, which is measured in hours, is defined as the delay after a case has been submitted before the first reply is received from a specialist. If queries are sent to several specialists (e.g., if the case is allocated to an expert group), then, the answer delay is measured from case submission to the earliest reply received. Number of completed questionnaires Proportion of questionnaires completed (%) Q6 ""did you find the advice helpful?"" (% yes) Q8 ""do you think the eventual outcome for the patient will be beneficial?"" (% yes) Q9 ""was there any educational benefit to you in the reply?"" (% yes) 3 International Journal of Telemedicine and Applications based at the hospitals concerned for short periods ( Table 2) . Two of the networks (networks A and B) employed staff to act as case coordinators, while network C relied on volunteers. During the first half of 2020, there were 319 referrer accounts on network A, 257 on network B, and 15 on network C. The proportion of these referrers who actually submitted a case during the first half of 2020 was 43%, 23%, and 20%, respectively, see Table 3 . There were 640 potential referring sites on network A, 135 on network B, and 21 on network C. The proportion of sites from which cases had actually been submitted was 18%, 31%, and 14%, respectively. During the first six months of 2020, a total of 1203 cases was received on Network A. The rates in previous years were somewhat higher, Table 4 . In contrast, the referral rates on both networks B and C were higher in 2020 than in previous years. 3.3. Case Mix. The two general networks, A and B, handled a wide range of query types. In 2020, the most common type of case managed on network A was pediatrics (47%, Figure 1 (a)), while the most common category on network B concerned internal medicine (36%, Figure 1(b) ). There was some year-to-year variation in the query types. The mean ages of patients on network A were 21.1 years (n = 1176) and were 30.6 years (n = 844) on network B. In contrast, network C, which handled only pediatric radiology cases, had a case mix that was exclusively radiol-ogy, both in 2020 and in previous years. The mean age of the patients was 5.7 y (n = 58), see Table 5 . On network A, there were approximately equal numbers of male and female patients, while on network B, there were more females (57%), and on network C, there were more males (62%). Complexity. The mean number of messages per case on network A was 12.4. In contrast, the mean number of messages per case was lower on networks B and C (5.2 and 5.4, respectively). The same pattern was observed for the mean number of queries per case, the mean number of unanswered queries, and the dialogue time-all were similar between network B and C, but much higher on network A (Table 6 ). Cases were allocated manually on networks A and B (100% of cases and 99.9% of cases), while on network C, only 40% of cases were allocated manually, i.e., 60% of cases were allocated automatically without requiring intervention from the case coordinator. The mean allocation delay was lower on networks A and B (0.23 and 0.70 h, respectively) than on network C (5.6 h). The mean numbers of messages sent by coordinators were highest on network A at 5.6 per case. It was lower on networks B and C (2.2 and 1.2, respectively) ( Table 7) . 3.6. Network Performance. The mean answer delay on network A was 17 h (SD 29, n = 1186). The mean answer delays and 24 h (SD 33, n = 59), respectively. The proportion of the user-feedback questionnaires completed by the referrers was 17% on network A, 13% on network B, and 31% on network C. The majority of respondents (95% or higher) reported that they had found telemedicine advice to be useful. The respondents were more cautious about predicting a beneficial outcome for their patient following the telemedical interaction: 65-96% of respondents answered yes or perhaps. A higher proportion, 85-95%, felt that the telemedicine replies had been of educational benefit to them personally, see Table 8 . In network A, 416 specialists were available to respond to queries during the first half of 2020, of whom 80% were actually sent a query. There were fewer specialists available in networks B and C (120 and 35, respectively), but a roughly similar proportion were actually sent a query about a case (69% and 60%), see Table 9 . In network A, there were 25 case coordinators available during the first half of 2020, of whom about half (14) actually allocated queries. In networks B and C, there were fewer case coordinators available (6 and 3, respectively), but a roughly similar proportion of them actually allocated queries (50% and 33%). Efficiency. For various reasons, not all case coordinators managed cases, and not all specialists were asked to respond to cases, during the study period. In networks A and C, the case coordinators managed similar numbers of cases, 86 and 65 per coordinator. In network B, the number of cases managed per coordinator was about four times higher, at 285, see Table 10 . Also, in networks A and C, the number of cases sent to each specialist was similar, at 3.6 and 3.1. In network B, the number of cases sent to each specialist was about three times higher, at 10.3. Network B therefore appeared to be working some 3-4 times more efficiently than networks A and C. 3.9. Changes in Referral Rates during 2020. All three networks had relatively stable referral rates during 2018 and 2019. During the first half of 2020, the numbers of referrals received on Network B increased substantially, while in contrast, the numbers of referrals on network A declined (Table 11) . The steep rise in referral rates on network B occurred in May 2020, against a relatively constant baseline rate. The fall in referral rates on network A occurred during the second quarter of 2020, against a relatively constant baseline rate ( Figure 2 ). There has been little published information on the effect of the COVID-19 coronavirus pandemic on telemedicine referral rates in low-resource settings. In May 2020, Helou et al. surveyed physicians in Lebanon who reported a modest increase in the use of WhatsApp, telephone calls, and email for telemedical purposes [6] . We have studied referral rates in three well-established telemedicine networks-formal networks-during the first half of 2020 and also in the two preceding years. There were some similarities between the networks, in the sense that they delivered telemedical management advice to referrers based in Africa and elsewhere, using specialists many of whom were volunteers. There were also similarities in the way that the referrers rated the quality of the teleconsultations. On all three networks, 95% or more of responders rated the advice they received as helpful. On all three networks, 85% or more of responders rated the advice they received as being of educational benefit to themselves. A slightly lower proportion, 65% or more of responders, felt that the eventual outcome for the patient was likely to be beneficial. These perceptions are similar to those in larger studies [7] . That is, the principal users of the service found that the telemedicine advice they received was helpful, changed diagnosis and management, and/or reassured the patient. Despite the similarities, there were also differences between the networks in terms of their basic characteristics, such as the resources available to them, i.e., numbers of specialists and numbers of case coordinators. Network C was a relatively small network, specialising in pediatric radiology, while networks A and B were much larger and handled cases of a general nature. Network A was operated by a large international organization, and the resources available such as the numbers of case coordinators and specialists were some 3-4 times more than those available to network B. Perhaps because network C was relatively less wellresourced than the others and relied on volunteer case coordinators, allocation delay was longer than on networks A and B. Resources may also explain the differences between the networks in their average length of time between the International Journal of Telemedicine and Applications submission of a case and the first response from a specialist. The answer delay was lowest in network A during 2020 and greater in networks B and C. Nonetheless, the answer delays were similar to those reported in other large networks such as that of Brazil [8] and French Guiana [9] . The number of queries per case remained largely constant on all three networks during the three years studied. However, they were rather different between the networks-more than twice as many queries per case for network A than for network B (Table 6 ). This could mean that the cases on network A were more complex and therefore required more specialist opinions, or it might simply reflect a different workflow that was more bureaucratic and required input from a larger number of team members. There was some evidence for the latter, since the proportion of unanswered queries was higher on network A, i.e., this could be interpreted as administrative colleagues being copied into the dialogue about a case. On the other hand, there were other indicators of complexity in the case mix on network A, and the average age of the patients was lower (16.9 years on network A vs. 30.3 years on the network B). So increased case complexity may explain, at least partly, why the number of queries per case was much higher in network A than in the others. There were also differences between the networks in terms of their efficiency. One measure of network efficiency is the number of cases per coordinator, which on network B was about three times higher than on network A. Similarly, the number of cases per specialist was some three times higher on network B (Table 10 ). This could indicate that network B was operating more efficiently. Nonetheless, network A achieved rapid case allocation, and its average delay in providing the first response to the referrer was lower than the other networks, so in these respects, the quality of the service provided was relatively higher. Furthermore, it is not known whether high network efficiencies, which are reflected in higher case loads per specialist, are sustainable in the longer term. Rates in 2020. Telemedicine referral rates in the first half of 2020 were compared with those in previous, prepandemic years. Activity on network C, a small network specialising in pediatric radiology, remained in 2020 much as it had done before. However, substantial differences were observed in the referral rates on the two general networks, A and B. 17 13 31 Q6 ""did you find the advice helpful?"" (% yes) 95 96 100 Q8 ""do you think the eventual outcome for the patient will be beneficial?"" (% yes) 52 93 20 Q8 ""do you think the eventual outcome for the patient will be beneficial?"" (% yes or perhaps) 76 96 65 Q9 ""was there any educational benefit to you in the reply?"" (% yes) 89 95 85 In network B, referral rates increased substantially during 2020. This network had expanded its referral network during 2020 by increasing the number of staff in Kenya. These contracted clinical officers served as ambassadors who educated, trained, and supported fellow clinical officers in the use of the telemedicine services offered by the network in rural Kenya. In January 2020, there was one such individual, but by June 2020, there were five. The coronavirus pandemic did not affect this work because the staff members remained local and were not subject to any travel restrictions imposed at the international level. In the other general network, however, referral rates fell during the first half of 2020. The reduction occurred specifically during the second quarter of 2020 and coincided with the increase in global COVID-19 cases. For convenience, the incidence of COVID-19 cases during 2020 (based on data from the WHO [10] ) is plotted alongside the referral rate for network A in Figure 3 . The decline in referrals corresponds closely with the increase in global COVID-19 cases. Since telemedicine ser-vices in network A were heavily reliant on expatriate staff, it is unsurprising that international travel restrictions adversely affected the number of international staff who could be deployed. This in turn can be presumed to have driven the decline in their telemedicine referral rates. Although the pandemic represents an obvious reason for the changes observed, additional factors should be considered. For example, it is possible that the nature of the telemedicine cases on network A changed during 2020, perhaps being restricted to more complex cases than before. However, although the number of cases decreased, the types of specialists required to manage them did not. Neither was there any substantial change in the case management, in terms of allocation delay or overall case-dialogue time. Therefore, the differences in the reliance on local referrers versus deployed expatriates probably explain much of the observed decline in the referral rates. This suggests that the development of sustainable telemedicine in low-resource settings is likely to rest on proper ""stakeholder engagement,"" a factor, which is frequently identified as important in studies of the readiness for e-health operations in developing countries [11] . The data used to support the findings of this study are available from the corresponding author upon request, subject only to consideration of any legal and ethical concerns. The authors declare no conflict of interest in respect of this work. RW chairs the Collegium Telemedicus steering group and is the custodian of the data. HO and MM are both senior managers of Collegium networks (the nonprofit Addis Clinic and the World Federation of Pediatric Imaging, respectively) which were studied in the present work.@story_separate@The three networks studied operated in rather different ways, with differing resources available to them. All three networks delivered a service that was rated highly by the referrers. One network operated at relatively high efficiency compared to the other two, although it is not known if this is sustainable. Our findings show that there were changes in the telemedicine service demand during 2020, which were very different in the three networks studied. Networks which were more reliant on local referrers saw little reduction-or even an increase-in submitted cases, while the network that had more dependence on international staff saw a big fall in submitted cases. This was probably due to the effect of international travel restrictions on the deployment of staff. We conclude that organizations wanting to build or expand their telemedicine services should consider deliberately empowering local providers as their referrers. Not only will this reduce the fragility of the service when international travel restrictions are imposed but it might be politically advantageous to the organization in the sense of allowing it to leave behind a virtual presence when their field staff is withdrawn at the end of a finite project. That is, the legacy of the work on the ground could be a ""virtual hospital"" which continues to be supported by the organization concerned.","We surveyed three well-established store-and-forward telemedicine networks to identify any changes during the first half of 2020, which might have been due to the effect of the COVID-19 coronavirus pandemic on their telemedicine operations. The three networks all used the Collegium Telemedicus system. Various quantitative performance indicators, which included the numbers of referrals and the case-mix, were compared with their values in previous years. Two of the three networks surveyed (A and B) provided telemedicine services for any type of medical or surgical case, while the third (network C) handled only pediatric radiology cases. All networks operated in Africa, but networks A and C also provided services in other resource-constrained regions. Two of the networks (networks B and C) used local staff to submit referrals, while network A relied mainly on its expatriate staff. During the first half of 2020, the numbers of referrals received on network B increased substantially, while in contrast, the numbers of referrals on network A declined. All three networks had relatively stable referral rates during 2018 and 2019. All three networks delivered a service that was rated highly by the referrers. One network operated at relatively high efficiency compared to the other two, although it is not known if this is sustainable. The networks which were more reliant on local referrers saw little reduction—or even an increase—in submitted cases, while the network that had the most dependence on international staff saw a big fall in submitted cases. This was probably due to the effect of international travel restrictions on the deployment of its staff. We conclude that organizations wanting to build or expand their telemedicine services should consider deliberately empowering local providers as their referrers."
"Financial performance has been widely used as an indicator for business performance (Ezzemel, 1992; Ezzemel & Hart, 1989; Rappaport, 1986) . Heiman (1988) argued that there are possibly various indicators of a company's financial success, but the company's stock price is considered to be the most important. The basic stock valuation model asserts that stock price is equal to the present value of expected future dividend payments, which is formulated as: where P t is the stock price at time t, D t is the dividend payment at the end of time t, and r is the discount rate used to bring the future dividends to present value. Assuming that dividends are a linear function of earnings (E) and D t ¼ q Á E t , where q is the constant payout ratio, we can write Eq. (1) as: Eq. (2) demonstrates that stock prices reflect investors' expectations about future corporate earnings (Chaney, Devinney, & Winer, 1991; Choi, Hauser, & Kopekcy, 1999; McWilliams & Siegel, 1997) . Since business conditions usually impact corporate earnings and dividends, it is often observed that stock prices tend to fluctuate with business conditions (Barro, 1990; Beckers, Grinold, Rudd, & Stefek, 1992; Chen, Roll, & Ross, 1986; Fama, 1981; Fama & French, 1988; Ferson & Harvey, 1993; Chen, 2005) . The efficient markets theory further suggests that a firm's stock price should reflect its real-market value or true financial performance (Bodie, Alex, & Alan, 2001; Mishkin & Eakins, 2003) . Thus, some interactions are expected to exist between business conditions and firm financial performance. Business managers and policymakers are concerned with the overall business condition because a good business condition or development can generally have a positive economic effect on business firms (Jeon, Kang, & Lee, 2004) . A positive economic impact usually leads to an increase in sales and hence income (and earnings), which in turn can improve the performance of business firms. On the other hand, corporate earnings and dividends are most likely to decrease if business conditions are expected to deteriorate. This reduction in expected earnings and dividends will in turn cause a decrease in the stock price or the firm's performance (Harvey, 1991 ). Conversely, financial success of business firms can lift business conditions by generating job opportunities, business turnover, and government taxes (Jeon et al., 2004) . Many studies in economics and finance literature have examined the relationship between business or economic progress and financial sector development (Arestis, Demetriades, & Luintel, 2001; Atje & Jovanovic, 1993; Fase & Abma, 2003; Harris, 1997; King & Levine, 1993a,b; Levine, 1991; Levine & Zervos, 1996 ,1998 Mazur & Alexander, 2001) . Empirical evidence suggests a strong positive correlation between the development of financial markets and economic growth. The direction of the causality, however, is still largely under debate. Given the above illustration, this study attempts to examine the dynamic interactions between business conditions and financial performance, using tourism firms in China and Taiwan. To date, no empirical work has been devoted to the examination of long-run relationship and causality of the two variables in the hospitality and tourism industry. Thus, this research should add significant contributions to the hospitality and tourism literature by shedding some light on the interaction between business conditions and financial performance of tourism firms. Specifically, we test the following two hypotheses. The first hypothesis proposes that there is a long-run relationship between business conditions and financial performance of tourism firms. The second hypothesis tests if there exists at least one-way causality between the two variables. This hypothesis can be further broken down into three propositions. Proposition 1. A unidirectional causality runs from business conditions to financial performance of tourism firms. In other words, improved business conditions can enhance financial performance of tourism firms. Proposition 2. A unidirectional causality runs from financial performance of tourism firms to business conditions. Therefore, financial success of tourism firms can energize business conditions. Proposition 3. A bi-directional causality between the two variables exists. That is, an improvement in business conditions strengthens financial performance of tourism firms and healthy financial performance of tourism firms can lift business conditions. The paper is organized into seven sections. The next section briefly reviews the literature. Section 3 describes the data and their unit root test results. This section is followed by the examination of cointegration between business conditions and financial performance of tourism firms. Section 5 presents causality tests between business conditions and financial performance of tourism firms. Managerial implications generalized from this empirical research are provided in Section 6. The final section contains concluding remarks.@story_separate@In the hospitality and tourism literature, Nicolau (2002) uses stock price as the proxy for business performance measure to analyze the impact of the announcement of a new hotel opening on the performance of its chain in Spain. Results based on this event study shows that the announcement has a positive effect on the performance of its chain. Chen and Bin (2001) examine the effect of legislation events on stock performance of US casino gaming firms. They find that different types of casino companies show different reactions to gaming legislation announcements. Chen, Jang, and Kim (2006) , using the event study approach, test the effect of the severe acute respiratory syndrome (SARS) outbreak in 2003 on Taiwan hotel stock prices. They detect a significantly negative mean abnormal return in Taiwan hotel stocks on and after the day of the SARS outbreak. Choi, Olsen, Kwansa, and Tse (1999) develop a model to capture business cycle for the US hotel industry, which offers useful guidelines for practitioners and researchers in the hotel industry. Aimed to cover hotel activities as broadly as possible, the developed hotel industry cycle model illustrates the magnitude of growth in the US hotel industry. They find that the hotel industry experienced high growth every 4 or 5 years over a 28-year period from 1966 to 1993. Specifically, the average contraction period is about 2 years, while the average expansion period is about 3 years. They further show that the hotel industry leads general business cycle peaks by approximately 0.75 years on average and leads troughs in the general business cycle by an average of 0.5 years. Several papers examine stock valuation and risk of hotel and restaurant firms. Rushmore (1992) investigates seven stock valuation techniques used for the acquisition and appraisal of hotels in the US and discusses the strength and weakness of each technique. Borde (1998) uses the ordinary least square (OLS) regression to test the risk diversity across restaurants in the US He illustrates that the investment risk of restaurant companies can be predicted by some financial characteristics such as liquidity, dividend-payout ratio, operating returns, and growth opportunities. Elgonemy (2000) studies the pricing of US lodging stocks. He argues that the traditional valuation measures are obsolete in the new economy. Sheel and Nagpal (2000) analyze a long-run equity performance of acquiring firms in the US hospitality industry. Negative performance of equity value of acquiring hospitality firms is evidenced in their study over the period of 1980-2000. Gu and Kim (2002) , following Borde (1998) , re-examine determinants of systematic risk of US restaurant firms. They, using data covering a time period from 1996 to 1999, report that systematic risk is negatively correlated with asset turnover but positively correlated with quick ratio for restaurants. Kim and Gu (2003) investigate the risk-adjusted performance of three restaurant sectors, full-service, economy/buffet, and fast-food restaurants. Their finding suggests that in the US, fast-food restaurants perform the best, followed by full-service restaurants and economy/ buffet restaurants. However, the performance of all three sectors is inferior to that of the market portfolio. Jeon et al. (2004) examine the impact of the persistence in abnormal earnings on book value of equity and earnings. They find that the persistence in abnormal earnings is higher for hotels than for manufacturing companies. Chen (2006b) tests the mean reversion behavior of tourism stock prices towards their fundamental values, namely earnings and dividend per share. The results indicate that earnings per share serve as a better proxy than dividends per share for fundamental values of tourism stock prices. Moreover, while tourism stock prices diverge away from their fundamental values from time to time, there exists an error correction mechanism in the market that allows tourism stock prices to revert their mean towards their fundamental values. Barrows and Naka (1994) , Chen, Kim, and Kim (2005) and Chen (2006a) investigate the impact of economic forces on hospitality stock returns. Barrows and Naka (1994) use the OLS regression to assess the effect of five selected macroeconomic variables on stock returns of US hospitality firms. They demonstrate that growth rates of money supply and domestic consumption have positive effects on hotel stock returns and expected inflation rate has a negative impact on hotel stock returns. Chen et al., (2005) examine the influences of both economic and non-economic forces on hotel stock returns in Taiwan. Empirical results show that two economic variables, money supply and unemployment rate, significantly affect hotel stock returns. Moreover, non-macroeconomic factors, namely presidential elections, the 9/21 earthquake, the outbreak of SARS, sports mega-events, the Asian financial crisis, and the 9/11 terrorist attacks, have significant effects on hotel stock returns. Chen (2006a) extends the examination of the relationship between macro and non-macro explanatory variables and hotel stock returns to the emerging stock market of China. In consideration of a tremendous growth of tourism in China, Chen includes growth rate of total foreign tourist arrivals, in conjunction with other commonly used macroeconomic variables, as another critical macro factor that may affect Chinese hotel stock returns, in his study. Evidence shows that Chinese hotel stock returns are more sensitive to general macro variables. The impact of growth rate of tourist arrivals is positive but not significant. Several non-macro forces, financial crises, natural disas-ters, political events, sports mega-events, terrorist attacks and wars, are also found to be significant to Chinese hotel stock returns. Nonetheless, no work to date in the hospitality and tourism literature empirically investigate the long-run relationship and causality between business conditions and financial performance of tourism firms. Thus, the objective of this study is to fill in this gap. The empirical evidence from China and Taiwan can shed light on the unexplored interactions between business conditions and financial performance of tourism firms. In specific, we examine whether financial performance of tourism firms is important to business development and whether business conditions matter to financial success of tourism firms. Following Ezzemel (1992) , Nicolau (2002) , and Chen and Bin (2001) , we use stock prices of tourism firms as the proxy for their respective financial performance. Based on the Taiwan Stock Exchange (TSE) classification for tourism industry sector, six tourism firms, Ambassador Hotel, First Hotel, Grant Formosa Regent Taipei, Hotel Holiday Garden, Leofoo Corporation, and Wan-Hwa Corporation, are currently listed on the TSE. A firm has to have complete trading data available and an established trading period of at least 70 months to be included in the sample. As a result, the stock of Grant Formosa Regent Taipei, listed on the TSE since March 9, 1998, is excluded from the study. On the other hand, to broaden the range of tourism firms and increase sample size, China Airlines, satisfying the selection criterion above with its stocks being traded on the TSE since February 1993, is included in this study. Table 1 presents the descriptive financial statistics of tourism firms in Taiwan. All monthly stock prices of these firms are obtained from the financial database of the Taiwan Economic Journal (TEJ). The stock exchanges in China are relatively new. Their launch was part of the economic reform intended to transform the Chinese economy from a centrally planned system to a socialist-market system. Under the new system, the Chinese government seeks to use the newly launched market to further develop the economy, while maintaining some socialist characteristics. Two stock exchanges, the Shanghai Stock Exchange and the Shenzhen Stock Exchange, exist in China. They are founded in December 1990 and July 1991, respectively. Both are expanding quickly in terms of market capitalization, trading volumes, and number of stocks listed since their inception. There are two types of shares, A and B, listed on the two Chinese stock exchanges. A shares are denominated in the local currency (Renminbi, RMB Table 2 . All monthly tourism stock prices are also taken from the TEJ database. While individual stock prices enable us to examine the interactions between business conditions and financial performance of individual tourism firms, a value-weighted price index allows us to explore the interactions between business conditions and financial performance of tourism firms as a whole. Thus, for the case of Taiwan, a valueweighted tourism price index (VWTI) is derived from the use of stock prices of six individual Taiwanese tourism firms. For the case of China, three VWTIs are calculated. The first index (VWTI_SH) is computed based on stock prices of four tourism firms that are traded on the Shanghai Stock Exchange. We derive the second index (VWTI_SZ), using stock prices of the five tourism firms traded on the Shenzhen Stock Exchange. The third index (VWTI_SHSZ) is generated with stock prices of all nine Chinese tourism firms. Quarterly data on gross domestic product (GDP) are commonly used as the proxy for business conditions. In addition to GDP, industrial production (IP) is another popular measure (Chen, 1991; Estrella & Hardouvelis, 1991; Fama & French, 1989; Miffre, 2001; Chen, 2005) . IP measures business development that is more narrowly focused on the manufacturing side of the economy. The advantage of using IP data is that IP is a monthly measure, which in turn can provide more observations. Given that time series data of stock prices of tourism firms in both China and Taiwan are only available over a limited time span in this study, we hence use both IP and GDP to carry out the empirical examination. Both Taiwanese IP and GDP data are taken from the TEJ database and Chinese IP and GDP data are obtained form various issues of the China statistical yearbook. All IP and GDP data used in the examination are seasonally adjusted and the selected time periods of IP and GDP are identical to the time periods of stock prices of tourism firms for both cases of China and Taiwan, respectively. To provide a comparison of results between the two sample countries, we further create a sub-sample for the case of Taiwan, covering the period from January 1998 to 2004. Accordingly, we can conduct all empirical tests for China and Taiwan over the same time periods. Before testing a long-run equilibrium relationship between business conditions and financial performance of tourism firms, we use the Augmented Dickey and Fuller (1981) and Phillips and Perron (1988) unit root tests to examine the degree of integration of all variables in their natural logarithms. Let LIP, LGDP, and LSPI denote industrial production, gross domestic product, and various hospitality stock prices in natural logarithms, respectively. The coefficient in the log function simply implies a percentage change in the dependent variable given a percentage change in the independent variable. Monthly and quarterly results of the Augmented Dickey-Fuller and the Phillips-Perron unit root tests are reported in Tables 3 and 4 , respectively. Test results generally indicate that the null hypothesis of one unit root cannot be rejected for levels of monthly LIP, monthly LSPI, quarterly LGDP, or quarterly LSPI, but is rejected for their first differences. One exception is that the null hypothesis is rejected for both level and first difference of quarterly LSPI of the Huatian hotel in China. In other words, quarterly LSPI of the Huatian hotel is I(0) and all other time-series data of LIP, LGDP, and LSPI are Ið1Þ for their levels; however, all variables are I(0) for their first differences. Therefore, we proceed with the long-run equilibrium analysis using, the Johansen's (1988) cointegration techniques. For the Huatian hotel, quarterly LSPI and LGDP are of different integration orders and hence cannot be cointegrated. We apply the Johansen cointegration test to the examination of the long-run relationship between business conditions and financial performance of tourism firms. The maximum likelihood procedure by Johansen has good large-and finite-sample properties (Cheung & Lai, 1993 ; Note: The symbol ( ÃÃ ) indicates rejection of the null hypothesis of a unit root at the 5% level, based on MacKinnon (1991) critical values. The ADF and PP test equations include an intercept but no time trend since the inclusion of a time trend generates no significantly different results. The optimal lags selected for the ADF and PP tests, using the Akaike Information Criterion (AIC, Judge et al., 1985) and Schwartz Bayesian Criterion (SBC, Schwarz, 1978) , are in brackets. Gonzalo, 1994; Phillips, 1991) . Cheung and Ng (1998) reported that the Johansen approach is more efficient than the two-step approach of Engle and Granger (1987) . Indeed, these distinct characteristics of Johansen approach are pretty useful for this study, especially considering the fact that stock prices of tourism firms are only available over a limited time period of 7-10 years for the case of China. If cointegration test results show that LIP and LSPI are cointegrated, we can state that the business condition and financial performance of tourism firms tend to move together in the long run, while experiencing short-run transitory deviations from this long-run relationship. To illustrate the vector autoregression (VAR)-based cointegration test of Johansen, consider that X t is a k-vector of non-stationary I(1) variable. An k-dimensional VAR of order n can be expressed as a vector error correction model where L is a vector of constants that allows for a deterministic drift, D is the difference operator, E is a white noise vector, the coefficient matrix P ¼ P k i¼1 A i À I, and G i ¼ À P k j¼iþ1 A j . The Johansen's method determines the cointegrating rank r, 0orankðPÞ ¼ rok and the long-run relationship, G ¼ ab 0 . The k Â r matrices b and a represent the long-run coefficients and error-correction estimates, respectively. The vector error correction model is a VAR that builds on cointegration by incorporating error correction terms that account more fully for short-run dynamics and hence if the long-run equilibrium condition (cointegration) exists, it explains short-run fluctuations in variables X. Two likelihood ratio test statistics, the trace statistic, l trace , and the maximum eigenvalue statistic, l max , are computed as: ln 1 Àl i and wherel i is the ith largest eigenvalue of the G n matrix and T is the sample size actually used for estimation. Osterwald-Lenum (1992) provides a complete set of asymptotic critical values for the Johansen's test. The optimal lag n is determined with the smallest Akaike Information Criterion (AIC, Judge, Griffiths, Hill, Lutkepohl, & Lee, 1985) and Schwartz Bayesian Criterion (SBC, Schwarz, 1978) . Respective monthly and quarterly results of the cointegration tests between business conditions and financial performance of tourism firms are shown in Tables 5  and 6 . Note: Same as in Table 3 . Table 5 . For tourism firms in Taiwan, both trace and maximum eigenvalue tests in Table 5 indicate the existence of one cointegrating equation between LIP and LSPI of the Ambassador Hotel, First Hotel, Leofoo Corporation, Hotel Holiday Garden, Wan-Hwa Corporation and VWTI at the 5% level, one cointegrating equation between LIP and LSPI of the China Airlines at the 1% level, but no existence of cointegration between LIP and LSPI of the First Hotel. It is also found that there exists at least one cointegrating equation between LGDP and LSPI of all firms, based on both test results in Table 6 . Moreover, we also detect that there is one cointegration between LIP and LSPI at the 5% level and two cointegrating equations between LGDP and LSPI at the 5% level over the recent sample of VWTI. For all stock price indexes of tourism firms in China, both test results indicate at least one cointegrating equation between LIP and LSPI at the 5% level. We also find that there is at least one cointegration between LGDP and LSPI of the Century Plaza Hotel, Cyts Tours, Dong Feng Hotel, Jinjiang Hotel, Eastern Airlines, VWTI_SH, VWTI_SZ, and VWTI_SHSZ, but no cointegration between LGDP and LSPI of the Huangshan Tourism, Jingxi Tourism and Xian Tourism. In summary, the Johansen cointegration test generally support the first hypothesis that there is a long-run equilibrium relationship between business conditions and financial performance for the majority of tourism firms and for the tourism group as a whole in both Taiwan and China. If two time-series variables are cointegrated, then at least a Granger-causation in one direction exists (Engle & Granger, 1987; Granger, 1988) . The existence of a long-run relationship between business conditions and tourism firms' financial performance implies that two variables are causally related at least in one direction. In a cointegrated set of variables, Granger (1988) proposes that the short-run causal links between these variables should be tested within the framework of the vector errorcorrection model. Thus, to examine the direction of causation, we perform the Granger causality tests augmented with an appropriate error-correction term derived from the cointegrating relationship between business conditions and financial performance of tourism firms. The appropriate formulation of a Granger-type test of causality is given as where a is the deterministic component, D is the difference operator, LBC denotes IP or GDP in natural logarithm, LSPI is SPI in natural logarithm, e t are white noises, and ECT is the error-correction term derived from the cointegrating equation between LBC and LSPI. The optimal lag m is selected with the smallest AIC and SBC. Granger (1988) notes that a vector error-correction model provides two channels through which the causality can be detected. This procedure hence has the advantage that source of causation can be identified by either shortrun dynamics or disequilibrium adjustment. First, the evidence of causality is identified through short-run dynamics. Based on this channel, statistics of the Wald test are computed under the null hypothesis that all the coefficients of l 1i and b 2i in Eqs. (5) and (6) are equal to zero. In other words, we support the Proposition 1 of oneway causality from business conditions to tourism firms' we accept the Proposition 2 of one-way causality from tourism firms' financial performance to business conditions ðDLSPI ) DLBCÞ if l 11 ¼ l 12 ¼ Á Á Á ¼ l 1m ¼ 0 is rejected; there exists a two-way causality between business conditions and financial performance of tourism firms ðDLBC3DLSPIÞif both Second, the causality is identified through the disequilibrium mechanism, that is, through the lagged ECT term. Independent variables ''Granger-cause'' the dependent variable if the error-correction term in Eqs. (5) and (6) is statistically significant. For example, DLBC Granger causes DLSPI ðDLBC ) DLSPI Þif y 1 is significantly different from zero in Eq. (5). Similarly, DLSPI Granger causes DLBC ðDLSPI ) DLBCÞ if y 2 is statistically significant in Eq. (6). Following the same argument, the two variables Granger cause each other ðDLBC3DLSPIÞ if both y 1 and y 2 are significantly different from zero. According to Granger (1988) , at least one variable in Eqs. (5) and (6) should move to bring the relation back into equilibrium if there is a true economic relation, and thus at least one of the coefficients (y 1 and y 2 ) of the error correction terms has to be significantly different from zero. Note when LBC and LSPI are of different integration orders or LBC and LSPI are not cointegrated, the examination of the direction of causation is based on the standard Granger (1969) causality tests: The causality will be tested only through the first channel, using the Wald test. Results of causality tests, along with Wald test statistics and t-statistics on error-correction term, are reported in Tables 7-10 and are summarized as follows. First, empirical results associated with the case of Taiwan covered in Table 7 demonstrate that there is a bidirectional causality between DLIP and DLSPI in general. We find that this two-way causality exists in five out of six individual tourism firms and no causality, neither in the short-nor in the long-run, is detected for the case of the First hotel. The two-way causality consistently holds for tourism firms as a whole for both the full and recent samples. Second, mixed results are detected in Table 8 , which covers the case of Taiwan also and utilizes a bivariate system of DLGDP and DLSPI. There is a one-way longrun causality running from DLSPI to DLGDP for three firms (the Ambassador hotel, First hotel, and Hotel Holiday Garden), a one-way causality running from DLGDP to DLSPI for the Wan-Hwa corporation, and a bi-directional causality between the same two factors for the China Airlines. By taking all tourism firms as a whole, we still find a two-way causality for both the full and recent samples. In sum, empirical findings imply that good financial performance of tourism firms can improve business conditions and healthy business conditions can enhance financial performance of tourism firms in Taiwan. Table 9 shows that a bi-directional causality between DLIP and DLSPI exists in two out of four tourism firms listed on the Shanghai Stock Exchange and the other two firms exhibit a one-way causality from DLSPI to DLIP. A two-way causality is evidenced for the four tourism firms as a whole. As for the five Chinese tourism firms listed on the Shenzhen Stock Exchange, empirical results presented in Table 9 support a bi-directional causality between DLIP and DLSPI in three of them and a one-way causality from DLSPI to DLIP in the other two. For all five firms as a group, a one-way causality running from DLSPI to DLIP is evidenced. Moreover, a two-way causality between DLIP and DLSPI is revealed for nine tourism firms all together. Similarly, tests based on DLGDP and DLSPI presented in Table 10 for individual firms produce mixed results. Taking as a whole all individual tourism firms listed on the Shanghai Stock Exchange and Shenzhen Stock Exchange, respectively, we find a one-way causality from DLGDP to DLSPI for both value-weighted tourism indexes. However, when all nine firms are taken as a whole, tests result shows a bi-directional causality between DLGDP and DLSPI . Given all results covered in this section, we conclude that business conditions and financial performance of tourism firms enhance each other in both Taiwan and China. Furthermore, when taking all tourism firms as a group in both Taiwan and China, we generally find that estimates of y 1 y 1 and y 2 , the speed of adjustment to restore equilibrium in the dynamic model, are statistically significant and negative. These results imply that the estimated long-run links between business conditions and financial performance of tourism firms are indeed structural and the two variables would tend to revert back to their long-run relationship although they may temporarily deviate from the long-run relationship.  This section shows the relative importance of each variable in explaining the other variable by using impulseresponse functions and variance decompositions. Consid-ering Eqs. (5) and (6) or Eqs. (7) and (8) in the model, a change in one of the random innovations will immediately change the value of the dependent variable and hence the future values of the other variable, through the dynamic structure of the system. The analysis of impulse response functions can illustrate the response of one of the Note: Same as in Table 7 . endogenous variables to a one-unit shock (one standard deviation) in one of the other variables in the model, while the analysis of variance decompositions breaks down the forecast-error variance of one variable in each future period and determine the percentage of variance that variable in the model explains. Figs. 1-4 plot the resulting impulse-response functions and Table 11 reports the variance decompositions. As shown in Fig. 1 for the case of Taiwan, changes in business conditions ðDLIPÞ show higher response to a oneunit shock in their own innovations. Similarly, changes in financial performance of tourism firms (DLSPI) also show higher response to their own innovation shocks. Effects of unit shocks are found to be small and to decay fairly quickly. Similar results for the case of China are evidenced in Fig. 2 . Both DLIP and DLSPI show higher response to  Note: Same as in Table 7 . their own innovation shocks and effects of unit shocks are again small and decay fairly rapidly. Figs. 3 and 4 demonstrate that both DLGDP and DLSPI show higher response to their own innovation shocks as well. While effects of unit shocks are still small, they decay relatively slowly. Panel A in Table 11 reports the percentage of forecasterror variance of changes in business conditions explained by changes in financial performance of tourism firms, whereas panel B presents the proportion of forecast-error variance of changes in financial performance of tourism firms that are attributable to changes in business condition. Results in Table 11 show that changes in financial performance of tourism firms ðDLSPI Þ forecast a small portion of changes in business conditions ðDLIPÞ in both China and Taiwan. Similarly, the percentage of forecast variance of DLSPI explained by DLIP is also small. In contrast, DLGDP plays an important role in forecasting variance of DLSPI in China and Taiwan. For the case of Taiwan, the proportion of forecast variance of DLSPI attributed to DLGDP is 14.3% in the first period, 18.6% in the second period, 27.3% in the fifth period, and converges to about 30% thereafter. In China, DLGDP explains 30.2-39.6% of the forecast variance of DLSPI. Convergence is fairly fast as we can see in Table 11 , where the proportion explained in the first period is 30.2%. The empirical evidence presented in this study for China and Taiwan supports that business expansion and financial success of tourism firms reinforce each other. This finding follows our hypotheses in the introduction section, in which we propose that a sound business condition can enhance financial success of business firms through more sales and earnings, while financial success of business firms can also contribute to business conditions by generating job opportunities, business turnover, and/or government taxes. This finding is also in line with . They show that the US hotel industry performance follows the US business cycle. We further detect that gross domestic product plays a more critical role than industrial production in forecasting variance of financial performance of the tourism group as a whole in both China and Taiwan the manufacturing sector, whereas gross domestic product covers both manufacturing and other service sectors. Thus, this finding suggests that overall financial performance of the tourism industry has close links to not only the manufacturing sector but also other service sectors. Empirical findings in this study offer valuable information for tourism business managers and government tourism policymakers. Causality test results can help the government set priorities regarding where and how to allocate limited resources to improve overall business conditions. If one-way causality runs from tourism firms' financial performance to business conditions, more resources should be allocated to the tourism industry than to other sectors. If one-way causality from business conditions to tourism firms' financial performance is detected, the government should allocate resources to leading industries so that the overall economy will be improved, which would in turn boost the tourism industry. Resources should be equally allocated to tourism and other major industries if a two-way causality exists between the two variables. On the other hand, using enthusiastic tourism promotion or a strategic plan to improve the tourism industry and hence the overall economy will not be as effective as it is generally assumed, if there is no causality between business conditions and financial performance of tourism firms. Recently, the tourism industry has been seriously damaged by the outbreak of SARS in China and especially in Taiwan. For example, after the Chinese Ministry of Health first reported that there had been 300 cases of SARS on February 11, 2003, the number of visitors to China decreased from 8,484,300 in January to 7,376,000 in February (a 6.08% drop), to 5,649,200 in April (a 17.66% drop since January), and to 5,436,000 in May (a total of 35.93% drop in four months). Chen (2006a) also reports that the SARS outbreak has a significantly negative impact on hotel stock returns in China. For the case of Taiwan, after the SARS outbreak on April 22, 2003, the number of visitors to Taiwan significantly dropped from 258,023 in March to 110,632 in April (a 57% drop) and to 40,250 in May (an 84% drop in 2 months). The China Post (2003) reported that the tourism-related industries such as hotels, restaurants, and theme parks were the most severely affected groups by the outbreak of SARS in Taiwan. The severe, negative impact of the SARS outbreak on the tourism industry was well reflected on its financial performance (stock prices). As shown in Fig. 5 , the tourism industry, among all industries, experienced the highest monthly percentage drop in the share price index in the month after the SARS outbreak. Chen et al., (2005) further shows that SARS outbreak has much more damaging impact than any other non-macro events, such as the earthquake, wars, financial crisis, and the 9/11 terrorist attacks, on hotel stock returns in Taiwan. Moreover, Waugh (2003) estimates that Taiwanese economic growth in 2003 was cut by SARS by 0.5%. Given the finding that business expansion and financial success of tourism firms significantly reinforce each other in China and Taiwan, the local government authority should equally allocate resources to tourism and other major industries to enhance the tourism industry and the overall economy. The International Olympic Committee members selected Beijing as the host city for the Games of the XXIX Olympiad in 2008 on July 13, 2001. The announcement was a momentous occasion for China. It is equivalently significant to China's inbound tourism industry. Over the years, Olympics host cities and countries have organized various forms of tourism promotion in association with the Games. The Games would have a significantly positive impact on the tourism industry and, consequently, on the overall business climate and the national economy. However, Chen (2006a) finds that the announcement of the 2008 Beijing Olympics has a negative impact on hotel stock returns in China. Thus, the Chinese government and tourism authorities need to cautiously prepare for the Games, instead of expecting that a beneficial influence of the Olympics on both the tourism industry and overall economy would just happen automatically. Chen et al. (2005) report that international sports megaevents, such as the 2000 Sydney Olympics and 2002 Japan/ Korea World Cup, have a negative impact on hotel stock returns in Taiwan. As they noted, the negative impact of the Sydney Olympics is understandable, considering the geographical distance between the two nations. They also argue that Taiwan could have benefited from the 2002 Japan/Korea World Cup because of its proximity to Japan and South Korea and conclude that the negative effect might be attributed to the lack of sufficient efforts made to attract international visitors. Because evidence shows that financial success of tourism firms and business development promote each other in Taiwan and private tourism organizations and marketers should strive to take advantage of the 2008 Beijing Olympics in China. For shopping lovers, Taiwan can be regarded as an excellent stopover destination because it provides various eatery choices and products. If enthusiastic tourism promotion or activities can attract international tourists to make a trip to Taiwan during the 2008 Olympics game period, the financial performance of tourism firms and the industry as a whole would be positively influenced, which in turn can boost the overall economy.@story_separate@While interactions between business conditions and financial performance of tourism firms are generally assumed on the basis of the fundamental stock valuation model, no empirical studies on this topic have been conducted. Whether robust financial performance of tourism firms can significantly promote business development and whether the improvement of business conditions can lead to financial success of tourism firms remain unknown. This study attempts to address these questions by examining the cointegration and causality between business conditions and financial performance of tourism firms in China and Taiwan. The cointegration test results generally support a longrun relationship between business conditions and financial performance of tourism firms in both China and Taiwan. Causality tests results also show that there is a bidirectional causality between business expansion and financial performance of tourism firms as a whole for both cases. Therefore, financial success of tourism firms and business development enhance each other in China and Taiwan. In conclusion, the significant contribution of financial success of tourism firms to business development is empirically documented in this study. This result also implies that financial performance of tourism firms can serve as a leading indicator of overall business development in China and Taiwan. However, it is worth noting that empirical findings in this study should be interpreted with caution since stock prices of tourism firms are only available over a limited time span, 7-10 years for the case of China and about 14 years for the case of Taiwan. More general conclusions can be drawn, if this research can be replicated with data from different countries. The long-run relationship and causality between business conditions and financial performance of tourism firms may be fundamentally different from country to country. Lastly, the interactions between business conditions and financial performance of tourism firms may be linked to some macroeconomic factors. For instance, Chen et al. (2005) illustrate that macroeconomic forces such as monetary policy and unemployment rate significantly impact financial performance of Taiwanese hotels. Chen (2006a) finds that Chinese hotel stock returns are significantly associated with growth rates of industrial production, growth rates of imports, changes in discount rates, and changes in yield spread. In the future, tourism researchers can carry out the examination, using those economic factors.","This study contributes to the fields of hospitality and tourism by examining interactions between business conditions and financial performance of tourism firms in both China and Taiwan. We investigate whether the improvement of business conditions enhances financial performance of tourism firms and whether financial success of tourism firms matters to business development. In general, cointegration test results support a long-run equilibrium relationship between the two variables, business conditions and financial performance of tourism firms, and Granger causality test results show that these two factors reinforce each other in both China and Taiwan. This study therefore documents significant contributions of financial success of tourism firms to business development and strengthening of financial performance of tourism firms by business conditions. Some managerial implications are also discussed."
"Interprofessional education (IPE) is widely recognized as a collaborative valuable education for healthcare students [1] . American University of Health Sciences (AUHS) IPE's hosted health fair such as screening for hypertension aligns with the 2016 update of the core competencies for Interprofessional Collaborative (IPEC) such as interprofessional practice, roles/responsibilities, communication and teamwork [2] . High blood pressure (HBP) or hypertension remains the leading risk factor of cardiovascular disease (CVD) and stroke according to the recent American Heart Association (AHA) 2019 report [3] . In 2019, CVD remains the major cause of death in both men and women in the United States of America [3] . Based on the new blood pressure (BP) thresholds from the 2017 American College of Cardiology (ACC)/AHA guidelines, the age-adjusted prevalence of HBP in an American adult is approximately forty six percent (46%) which is an estimate of 116.4 million of adults aged greater than or equal to 20 years [4] . A patient is hypertensive whenever, his or her systolic blood pressure (SBP) is greater than or equal to 130 mmHg and/or his or her diastolic blood pressure (DBP) is greater than or equal to 80 mmHg, based on the 2017 ACC/AHA guidelines. HBP is generally nicknamed the silent killer because it is symptomless unless until it has made significant damage to the heart and arteries [5] . The mortality rate of homeless people is 3-4 times greater than the Unites States' general population [6] . Similarly, to the US general population, CVD associated death is remarkably high in the US homeless population [7, 8] . The purpose of this study is to estimate the age adjusted direct standardized prevalence of hypertension in a homeless community during a dual cross-sectional study and an IPE community-based health fair event in Long Beach, California.@story_separate@Homeless participants were recruited between August 2019-September 2019. AUHS students and faculty provided health education to the homeless community on the risk's factors associated with HBP. Then, participants received a consent form prior to their participation in this study. Participants are homeless adults ranging 18-80 years old. Explanation on the early screening for hypertension was provided to the participants prior to the measurement of the blood pressure. All participants signed a free and informed consent form. The size of the participants was 477/1894 (25.2%) of the total homeless population living in Long Beach, CA [9] . The total number of people experiencing homelessness was estimated to 1894 people based on the 2019 Point-in-Time Homeless Count Results in Long Beach, CA [9] . Participants self-reported demographic information (e.g., age, ethnicity, gender, education level), disability status, residential status, health insurance status, marital status, and reason for coming biweekly to the IPE Site location (Second Samoan Congregational Church of Long Beach CA). Then, faculty, nursing, and pharmacist students measured the sitting blood pressure (BP) of the participants following a standardized procedure using Omron BPN series Upper Blood Pressure monitor with cuffs. AUHS Institutional Review Board reviewed and approved the study. Exclusion criteria were failures to report age or screening for hypertension. A total of 390/477 participants (81.8%) of the total cohort satisfied the inclusion criteria and was used for the prevalence and the multivariate analysis. SBP/DBP were measured three times and the results were presented as a means (SD). The results were reported as numbers (and their corresponding percentages) for categorical variables. Age was direct-standardized based on the 2018 United States of America census population [10] . The directly standardized prevalence of hypertension (DSP) was given by the following equation [11] : where, p i was the prevalence of hypertension in the ith age group of the population. n i is the mid-year population in the ith age group. p = y∕p , with y the number of participants having hypertension, and p, the sample size. p is the crude (unadjusted) prevalence of hypertension. The confidence interval (CI) for the prevalence of hypertension was expresses as followed [12] : where, z = 1.96, is the value from the standard normal distribution for a 95% confidence level. A paired t-test was used to compared means. The 2019 United States prevalence of hypertension was used as a control. Binary logistic regression in IBM SPSS statistics version 26.0, was used to assess factors (age, race/ethnicity, gender, shelter status and, health insurance) association with hypertension set as a categorical dependent variable (hypertensive participant = 1, non-hypertensive participant = 0). The binary logistic regression was fitted and all independent factors with a p-value less than 0.25 were used in the model [13] . Demographic information comprised age stratified, sex, gender, race/ethnicity, education, disability, marital status, shelter status and reasons for visiting the Church. Means of continuous variable were compared with t-test. A p-value below 0.05 was considered statistically significant and warranted rejection of the null hypothesis. Seven pharmacy students, nine nursing students, two registered nurses, five registered pharmacists, and two medical doctors collaboratively provided health education to the homeless community and screened their sitting blood pressure and explained the importance for earlier screening for high blood pressure. The average age, median age and mode age were 51 ± 13, 53, and 57 years, respectively. Moreover, 70.8% of the participants had 45 years old or greater. As shown, in Fig. 1 , the following variables age (p = 0.000), race/ethnicity (p = 0.119), gender (p = 0.195), and insurance status (p = 0.234) were included in the fitted model and age (p = 0.000) was strongly associated with hypertension based the binary logistic analysis. In addition, as shown, in Fig. 1, age Table 1 . Out of 320 participants who reported their education level, ~ 54.75% of participants had high school level or less. Participants were mostly male with high representation of Black ethnicity (48.7%). The DBP ranged from 49.0-118.7 mmHg. The overall mean, median and mode of the DBP were 84.02 ± 12.38, 82.7 and 75.0 mmHg, respectively. Likewise, the SBP ranged from 79.7-199.5 mmHg. The overall mean, median and mode of the SBP were 132.41 ± 20.00, 130, and 127 mmHg, respectively. According to the median value of the SBP/DBP (130/82.7 mmHg), more than half of the participants had hypertension. A flyer was provided to participants who needed immediate medical attention due to SBP/DBP greater than or equal to 180/120 mmHg. Age and racial/ethnicity were factors associated with hypertension based on the binary logistic analysis and t-test. The prevalence of hypertension according to sociodemographic characteristics is shown in (Table 2 ). In this study, the prevalence of hypertension among the homeless population in Long Beach, CA was elucidated during a dual cross-sectional study during the IPE health fair community-based event. Students and faculty used the blood pressure threshold to educate the homeless participants on hypertension associated with cardiovascular disease or stroke events. They performed basic clinical blood pressure monitoring and asked participants their medication history. Nursing and pharmacy students and faculty displayed excellent teamwork. The prevalence of hypertension of this cohort of homeless population 61.5 (95% CI 56.7-66.4) was extremely high compared to that of the 2019 US general population 46% (95% CI 44-48) [3] . This result was consistent with the finding of Giuliano Tocci et al. who demonstrated that the prevalence of hypertension for a cohort of homeless people in Italy was greater than that of the housed people [14] . Many factors explained the higher rate of hypertension. For instance, the lack of physical activity, aging of the population, poor diet and stress are major risk factors of hypertension and could explain the higher prevalence of hypertension in this homeless population [15] . Seven participants had SBP/DBP ≥ 180/90 mmHg suggesting that Fig. 1 Binary logistic regression analysis predicting factors associated with hypertension they were in a state of hypertensive crisis and were strongly advised to seek immediate medical assistance. Hypertension potentiates current COVID-19 patient risk of death [16, 17] . Thus, there is a critical need to provide emergency shelter and health support [18] for this vulnerable homeless community to prevent the spread of COVID-19 among this underserved and vulnerable population [16, 19] . Most importantly, the availability of affordable permanent housing which is intrinsically income-dependent will ultimately be a sustainable and durable solution to end homelessness and health disparities associated with homelessness [7] . There were several limitations in this study. The size of the cohort was only 477/1894 (25.2%) total of the Long Beach, CA homeless population [9] . Participants failed to answer all the questionnaire resulting in only 81.8% response rate. A future direction involves the use of the eight-item Morisky Medication Adherence Scale (MMAS-8) to monitor the blood pressure of hypertensive participants over a long period of time [20] . The adherence to prescribed BP medications has shown to be an effective mean to reduce hypertension [14] .@story_separate@There was a high prevalence of hypertension in the homeless community. Interprofessional education-based health fair is a steppingstone to facilitate collaboration and teamwork among future health care workers. This work sheds new light on an issue of major public health significance and points to the need for fostering IPE community-based health fairs intervention program for the US homeless population.","INTRODUCTION: Hypertension (SBP/DBP > 130/80 mmHg) is a leading risk factor for cardiovascular disease worldwide. AIM: To determine the prevalence of hypertension in a homeless community during an interprofessional education (IPE)-based health fair. METHODS: Homeless participants were recruited between August 2019–September 2019. Faculty, nursing, and pharmacist students, educated 477 participants, aged 18–80 years, on the risk factors associated with untreated hypertension. Then, participants self-completed the consented demographic survey questionnaire. Finally, the sitting blood pressure (BP) was recorded three times based on a standardized procedure, using Omron BPN monitor with cuff. RESULTS: Seven pharmacy students, nine nursing students, two registered nurses, five registered pharmacists, and two medical doctors collaboratively provided health education to the homeless community and screened their sitting BP. 390/477 (81.8%) of participants satisfied the inclusion criteria. Participants (54.7%) of the reported education level was at the high School level or less. More than the half of the participants (average age of 51 ± 13 years) had hypertension (median SBP/DBP ≥ 130/82.7 mmHg), respectively. The prevalence of hypertension for the overall cohort was 61.52% (95% CI, 56.59–66.35). Age (p value = 0.000) was significantly associated with hypertension based on the binary logistic analysis. CONCLUSION: This study demonstrated a high prevalence of hypertension in the homeless community in Long Beach, California with high risk of cardio-vascular events or strokes. This works sheds new light on an issue of major public health significance and points to the need for fostering IPE community-based health fairs intervention program for the US homeless population."
"T he coronavirus disease 2019 (COVID-19) pandemic and stay-at-home orders and social distancing measures have dramatically changed daily life for much of the world's population. Although such containment strategies have been successful in reducing the impact of COVID-19 on mortality, they have also brought severe economic consequences and have been associated with a rise in isolation and psychological distress, which may contribute to increased alcohol consumption. 1 In the United Kingdom (UK), pubs, bars, and restaurants and other nonessential businesses were closed for >3 months between late March and early July 2020. During this period, retail sales of alcoholic beverages rose sharply raising concerns that alcohol misuse may have increased. 2, 3 This is particularly concerning because alcohol misuse is linked to subsequent polydrug use, 4 can reduce immunity to viral infections, and may amplify the risk of severe illness for those infected by 5 However, to date, evidence of the potential impact of the COVID-19 crisis on drinking behavior is limited. 6 In initial studies, a substantial portion of sampled Chinese (32%), Australian (26.6%), German (34.7%), and UK (28%) adults reported drinking more alcohol during the COVID-19 pandemic and lockdown than previously, 6−9 but these studies did not use validated measures of alcohol use and relied on retrospective recall. Furthermore, a cross-sectional survey of UK adults found a higher prevalence of high-risk drinking during lockdown than before the pandemic, 10 and a longitudinal study of UK and U.S. adults identified a marked increase in drinking alcohol ≥4 times a week and heavy episodic drinking from before to during the COVID-19 crisis. 11 Although these studies are suggestive of an increase in problem drinking, there is a need for research that examines longitudinal changes in alcohol consumption using validated measures of high-risk drinking. This study examines drinking behavior among middle-aged adults in 2016−2018 and May 2020 when lockdown restrictions were in place throughout the UK.@story_separate@This study used data from the 1970 British Cohort Study (BCS), a prospective cohort study of 17,000 children born in Britain in 1970. In May 2020, a total of 10,458 BCS participants were invited to take part in a COVID-19 online survey and 4,223 participated (response rate of 40.4%). This study draws on data from those who took part in both the COVID-19 survey and the 2016−2018 wave of the BCS (N=3,358). The BCS study received ethical approval from the National Research Ethics Service. In all analyses, survey weights were applied to align the characteristics of the COVID-19 survey respondents with those of the representative sample of BCS participants born in 1970. 12 The Alcohol Use Disorders Identification Test (AUDIT) for detecting hazardous drinkers in primary care settings (AUDIT-PC) 13 was used to screen for potentially harmful alcohol consumption. 14 The AUDIT-PC correlates very strongly with the 10item AUDIT (r≥0.95) 14 and has shown high levels of sensitivity and specificity in detecting alcohol use problems (0.84−0.94). 13, 15 For the COVID-19 survey, the AUDIT-PC was adapted by adding Since the start of the Coronavirus outbreak to the start of each item. Those who indicated that they did not consume an alcoholic drink in response to the first AUDIT-PC question (nondrinkers) were assigned a score of 0. Participants were classified into 2 groups: (1) nondrinkers and unproblematic drinkers (scores of 0−4 on the AUDIT-PC) and (2) high-risk drinkers (scores of ≥5). All analyses were adjusted for baseline characteristics: sex, marital status (married versus not married), educational qualifications (third-level qualification such as a diploma, degree, or higher degree versus no third-level qualification), whether participants have been diagnosed with a chronic mental or physical health condition, and the year the 2016−2018 baseline survey was completed. First, the change in high-risk drinking levels from 2016−2018 to May 2020 was examined in a logistic regression model with SEs clustered by the individual participant identifier. Next, the survey wave variable was interacted with each background characteristic to identify whether changes in high-risk drinking differed by subgroup. Finally, changes in individual AUDIT-PC item responses between 2016−2018 and May 2020 were examined using multinomial logistic regression. Overall AUDIT-PC scores increased from 3.17 (SD=2.46) to 3.34 (SD=2.77) between baseline and follow-up (p=0.003). An increase in the prevalence of high-risk drinking from baseline to follow-up was observed across all examined groups (Table 1) . Logistic regression analyses showed that high-risk drinking increased from 19.4% to 24.6% between 2016−2018 and May 2020, a statistically significant increase of 5.2 percentage points (95% CI=2.5, 8.0, p<0.001), which equates to a 27% increase. This increase was not moderated by participant's sex, marital status, and educational attainment, the presence of a chronic health condition, or the year the baseline survey was completed. An examination of the individual AUDIT-PC items identified a 13.5 percentage point (p<0.001) increase in the prevalence of drinking ≥4 times per week (Table 2) from 12.5% to 26%, which equates to a 108% increase. There was also a small increase in the frequency of not being able to stop drinking on a weekly (1.9% increase) or daily (1.3%) basis and a shift toward consuming 1−2 alcoholic drinks (9.1%) ( Table 2) . When both the frequency of drinking and the number of alcoholic drinks typically consumed when drinking were considered simultaneously, there was an overall increase in alcohol consumption from baseline (mean=2.71) to follow-up (mean=2.82) (b=0.11, SE=0.03, p<0.001). This study examined changes in high-risk alcohol consumption before versus during the COVID-19 crisis in a nationally representative cohort of middle-aged British adults. High-risk drinking increased between 2016−2018 and May 2020, as did the prevalence of drinking ≥4 times a week. Likewise, there was evidence of an increase in the frequency of being unable to stop drinking. Rises in highrisk drinking were observed to a similar degree among men and women and those with a lower versus higher education level as well as in married versus unmarried and those with versus those without chronic illness. These longitudinal findings corroborate a body of largely cross-sectional research that has suggested that alcohol consumption may have increased because of the COVID-19 pandemic 6−10 and one other longitudinal study examining both UK and U.S. adults. 11 Given that drinking is often driven by coping motives 16 and middle-aged adults are already at increased risk of adopting high-risk drinking patterns 17,18 but often fail to identify these practices as health damaging, 18 the stress of the COVID-19 pandemic may be exacerbating problem drinking in this group. 19 As such, investment in both mental health treatment programs and the provision of support for alcohol reduction is crucial. This need is highlighted by recent evidence indicating that despite an increase in high-risk drinking during the lockdown in the UK, there has been a decline in the use of evidence-based support for alcohol reduction and no compensatory uptake of remote supports (telephone support, websites, smartphone applications). 10 Limitations Strengths of the present study were the use of representative longitudinal data and a widely used and validated measure of problem drinking. Limitations are (1) the reliance on a 20 and has been shown to be highly stable across the months of the year in the UK. 17 This suggests that the size of the change in drinking observed in this study (e.g., doubling of the prevalence of drinking ≥4 times a week) is unlikely to be explained by anticipated year-to-year trends or seasonality effects associated with the gap between baseline and follow-up or the assessment month, respectively. In line with this, there was no evidence that the size of the increase in high-risk drinking was related to the year baseline survey was completed (2016/2017/2018). We are grateful to the University College London Centre for Longitudinal Studies for their management of the 1970 British Cohort Study and to the United Kingdom Data Archive for making the data available. The Centre for Longitudinal Studies is funded by the United Kingdom Economic and Social Research Council. These organizations bear no responsibility for the analysis or interpretation of the data. MD and ER conceived the study, and MD analyzed the data; all authors contributed to the interpretation of the data and drafting and revising the article. Contents of the article have not been presented or published elsewhere. No financial disclosures were reported by the authors of this paper.@story_separate@In a large sample of middle-aged British adults, the initial period of the COVID-19 crisis and lockdown restrictions was associated with an increase in high-risk drinking patterns and especially frequent drinking. It is important that potential long-term changes in drinking habits are monitored throughout the COVID-19 pandemic.","INTRODUCTION: Emerging evidence suggests that the COVID-19 pandemic and associated lockdown restrictions may have influenced alcohol consumption. This study examines changes in high-risk alcohol consumption from before to during the COVID-19 crisis in an established cohort of middle-aged British adults. METHODS: Participants consisted of 3,358 middle-aged adults from the 1970 British Cohort Study who completed the Alcohol Use Disorders Identification Test for detecting hazardous drinkers in primary care settings in 2016–2018 (when aged 46–48 years) and May 2020 (aged 50 years). Multivariable logistic regression analysis was used to examine changes in high-risk drinking (scores of ≥5), and multinomial regression was used to compare responses with individual test items in 2016–2018 and May 2020. RESULTS: Among middle-aged British adults, high-risk drinking increased by 5.2 percentage points from 19.4% to 24.6% (p <0.001) between 2016–2018 and May 2020. The increase in high-risk drinking was not moderated by sex, marital status, educational attainment, the presence of a chronic illness, or the year the baseline survey was completed. The prevalence of drinking ≥4 times a week doubled from 12.5% to 26% from before to during the pandemic (p<0.001), and there was also evidence of an increase in the frequency of being unable to stop drinking. CONCLUSIONS: This study provides evidence linking the COVID-19 crisis and associated lockdown restrictions to an increase in high-risk drinking patterns and particularly frequent drinking in British adults. Potential long-term changes in drinking habits should be monitored following the emergence of the COVID-19 pandemic."
"Hydrogen peroxide (H 2 O 2 ) mouthwashes have been used for a long time [1] . ey have been utilized in an attempt to complement mechanical plaque control methods as well as to prevent/control oral infections [2] . However, the evidence supporting its use is not unequivocal even though it is still used by a number of professionals. In 2011, Hossainian et al. [3] published a systematic review to evaluate the effect of H 2 O 2 mouthwashes on the prevention of plaque and gingival inflammation. e focused question of such review was as follows: ""what are the effects of oxygenating mouthwashes on plaque accumulation and gingival inflammation parameters in adults, when compared with positive or negative controls mouthwashes or no oral hygiene, when used as a monotherapy or as an adjunct in daily oral hygiene?"" Surprisingly, the number of included studies was relatively low (n � 12, in which only 5 were specifically formulated with H 2 O 2 ). e other 7 studies were related to other oxygenating agents. e results of the review demonstrated that mouthwashes containing H 2 O 2 do not consistently prevent plaque accumulation when used as a short-term monotherapy. mouth rinses are being recommended as a preprocedural rinse, as well as a regular rinsing solution with the aim of diminishing contamination possibilities by the new coronavirus. A number of associations, including the American Dental Association, are recommending the use of H 2 O 2 mouth rinses as prerinses prior to procedures [4] [5] [6] . However, the evidence for the use of H 2 O 2 for oral antiviral purposes is virtually nonexistent. Most of the evidence only demonstrates the potential of H 2 O 2 to disinfect surfaces [7] . New studies have been conducted and published after the systematic review of Hossainian et al. [3] . Hence, the existing review [3] could be broadened to consider these additional studies, especially in this particular moment. erefore, the aim of this study is to systematically review the literature, assessing the effects of H 2 O 2 mouth rinses in controlling dental plaque, gingival inflammation, and oral microbiota.@story_separate@e focused question of the present study was as follows: ""what is the effect of hydrogen peroxide, in comparison to chlorhexidine or to a placebo solution, in oral microbiota control, dental plaque, and gingival inflammatory outcomes?"" In order to be included, the study must fulfill all of the following inclusion criteria: (i) Clinical trials with humans of any age. (ii) Test group: individuals that used, at least one time per day, hydrogen peroxide mouthwash. Any concentration of hydrogen peroxide was accepted. (iii) Control group: individuals that used, at least one time per day, a placebo or chlorhexidine mouthwash. Any concentration of chlorhexidine was accepted. (iv) Outcomes: any oral microbiological, plaque index, or gingival index analysis. No restriction to language or date of publication was imposed. Studies that used both chlorhexidine and hydrogen peroxide in the same group were excluded. Studies that involved outcomes assessed in dental implants were also excluded. A search strategy was performed, up to April 23, 2020 An adaptation of the abovementioned search strategy was performed in the other databases. Two researchers independently selected the studies and extracted the data in a spreadsheet specifically developed for this study (CKR and FWMGM). Regarding study selection, both screening (title and abstract analyses) and eligibility (full-text analysis) phases were performed independently. All discrepancies were solved between the researchers by discussion. In addition, hand search was performed in the list of references of the included studies and in previously published literature reviews [3] . Studies from the last 30 years were hand searched in the following journals: Journal of Clinical Periodontology, Journal of Periodontology, Journal of Periodontal Research, and Journal of Dental Research. Searches for grey literature were also performed in the Clinical Trials (clinicaltrials.gov) and Google Scholar databases, using an adaptation of the abovementioned search strategy. All the corresponding authors of the included studies were contacted by e-mail in order to detect other potentially relevant clinical trials. In addition, manufactures were contacted to supply information about other published, unpublished, or ongoing research studies using H 2 O 2 . e risk of bias of all randomized clinical trials was assessed by the RoB2 tool, as recommended by Cochrane [8] . Randomization process, deviations from the intended interventions, missing outcome data, measurement of the outcome, selection of the reported results, and overall risk of bias were assessed by two reviewers (FWMGM and CKR). In case of disagreements, a consensus was made between reviewers. A positive sign was given for an item when sufficient information was available, indicating low risk of bias, and a negative mark was used, for high risk of bias, when information was lacking. When risk of bias could not be assessed, the item was classified as unclear. For the nonrandomized trials, the ROBINS-I tool was used [9] . Several sources of bias were assessed, such as confounding, selection of participants, classification of interventions, deviations from intended interventions, missing data, measurement of outcomes, selection of the reported result, and overall bias. e search strategy and flowchart of articles retrieval is demonstrated in Figure 1 . e reasons for exclusion of the identified studies are reported in Table S1 . It should be noted that five databases were searched. e additional search strategies did not add any study to the present review, except for one additional study identified in the Google Scholar database [10] . e retrieved studies were very distinct in all aspects, preventing the possibility of a meta-analysis. erefore, descriptive information will be given, according to design and outcome (experimental gingivitis or not; plaque, gingival inflammation, or microbiological parameters). Figure 2 demonstrates the analysis of risk bias of the randomized clinical trials included in this review according to the RoB2 instrument. It may be detected that only one study presented low risk of bias in all criteria analyzed [11] . Four other studies presented an overall high risk of bias [10, [12] [13] [14] . e criteria randomization process showed unclear risk of bias in almost all included studies. e other sources of bias comprised mainly lack of information of reproducibility. e risk of bias for the nonrandomized trials included in the present review is demonstrated in Table 1 . Bias due to confounding was critical in all studies. e other analyses mostly demonstrate moderate to low risk of bias. (1) Nonexperimental Gingivitis Studies. e plaque index measurements are demonstrated in Table 2 . It should be highlighted that 10 studies evaluated plaque parameters, of which six were performed as clinical trials allowing mechanical plaque control [11, 12, 15, [19] [20] [21] . In one, it was not possible to determine if mechanical plaque control was possible [10] . All studies that used H 2 O 2 as adjunct to mechanical oral hygiene were performed with the concentration of 1.5% . ose studies demonstrate a higher antiplaque efficacy of chlorhexidine in comparison to H 2 O 2 , except two, in which H 2 O 2 presented similar efficacy to chlorhexidine [10, 21] . Generally, very little differences from negative controls were detected. (2) Experimental Gingivitis Studies. Among the studies that evaluated plaque parameters, 3 used the experimental gingivitis model [16, 18, 22] . e information coming from these studies gives an idea of efficacy of the mouth rinses in undisturbed dental biofilms. is enhances the proofs of principle of the antiplaque effect, which would give useful information, e.g., for areas where mechanical plaque control is not effective. (1) Nonexperimental Gingivitis Studies. e results related to gingival inflammatory parameters are presented in Table 3 . Nine studies were included in this outcome [10-12, 15, 16, 18-21] , six allowed mechanical plaque control [11, 12, 15, [19] [20] [21] . In one study, it was not clear if mechanical plaque control was allowed [10] . In these parameters, H 2 O 2 mouth rinse performs better than negative controls, however less than chlorhexidine. A possibility of a decrease in inflammation could be raised since it seems that H 2 O 2 performs better in terms of gingivitis than it does in relation to plaque. (2) Experimental Gingivitis Studies. Among the studies that evaluated gingival inflammation, two were based on the experimental gingivitis model [16, 18] . e experimental gingivitis model provides information on the effect of the mouth rinse in areas in which plaque control is not adequate. Also, it rules out the eventual confounding effect of the adjunct plaque control in the study of chemical substances. (1) Nonexperimental Gingivitis Studies. e results related to microbiological parameters are demonstrated in Table 4 . Six studies were included with these outcomes International Journal of Dentistry 3 [11, 13, 14, [16] [17] [18] . Four studies allowed mechanical control of biofilm [11, 13, 14, 17] . Better results with mouth rinses containing H 2 O 2 when compared to a placebo were detected. (2) Experimental Gingivitis Studies. Two studies performed microbiological analysis using an experimental gingivitis design [16, 18] . e information coming from such studies supports the quality/quantity of different germs when plaque is accumulating overtime. It also rules out the effect of the uncontrolled mechanical plaque removal. Results-Safety. Among the 13 included studies, only five of them assessed for side effects. All of these five studies reported no side effects in individuals that used H 2 O 2 mouthwashes [11, [14] [15] [16] 19] . Additionally, no side effects were reported in those that used chlorhexidine [14, 16] . Conversely, an increased tendency for desquamation of the mucosal lining was reported in individuals that used a placebo solution [16] . e other studies that used a negative control group reported no side effect in this group [11, 14, 15, 19] . Excluded articles Hydrogen peroxide was not used = 8 Hydrogen peroxide and others substance in the same group = 7 Absesnce of control group = 2 In vitro study = 4 Mouthwash was not used = 2 Outcomes were not assessed = 4 Study with dental implants = 1 Review = 2 Not found = 11 Study protocol (data not provided or not available) = 2 International Journal of Dentistry International Journal of Dentistry  (i) (ii) (iii) (iv) (v) (vi) (vii) (viii) (ix) (x)Age Baseline N(M)/N(F) End N(M)/N(F) CHX group (concentration) Rinsing protocol N (smokers) Age Baseline N(M)/N(F) End N(M)/N(F) Control group Rinsing protocol N (smokers) Age Baseline N(M)/N(F) End N(M)/N(F) International Journal of Dentistry which also performed better than distilled sterile water International Journal of Dentistry 13 Regarding the risk of bias of both randomized and nonrandomized clinical trials, it is important to highlight that most of the included studies presented an unclear or high risk of bias. Only one study demonstrated an overall low risk of bias [11] . is randomized clinical trial demonstrated a superior antigingivitis efficacy of H 2 O 2 mouthwash in comparison to a placebo solution. However, no significant difference was observed for the antiplaque efficacy. e overall high risk of bias must be put into perspective when interpreting the results of the present study. is means that the use in clinical practice should be indicated with caution and not performed routinely since the support is not robust. e strengths of the present systematic review were based on the importance of the topic, especially because the mouth is a very contaminated cavity and mouth rinses are used to reduce different degrees of contamination. In addition, with the COVID-19 pandemic, the use of mouth rinses has been considered an additional way for reducing all sorts of contamination. e limitations are related to the quality of the evidence. erefore, the information contained herein should be cautiously interpreted. Also, in an attempt to decrease the time for publication of this information, no registration was performed and it was not possible to make a post hoc registration. Initially, the focused question included both a negative and a positive control group. e negative control could be either placebo, water, or no solution, whereas the control group should include the gold standard in terms of oral rinsechlorhexidine. e results of this systematic review should be put into the perspective that H 2 O 2 is widely used in oral care despite the lack of a large number of studies, especially in some of the aforementioned indications. We looked at the systematic review published by Hossainian et al. [3] that critically appraised the evidence until the beginning of this decade. Such work led to the conclusion that H 2 O 2 does not consistently prevent plaque accumulation in short-term periods. erefore, we expanded the search criteria, not restricting age, including microbiological parameters, updating the publication year to 2020, and including five databases instead of the two previously searched databases. Due to the higher usage of H 2 O 2 , we restricted the search to only include this substance and not any other oxygenating agent. H 2 O 2 has been used clinically for more than a century, and recently, H 2 O 2 containing mouth rinse are being recommended, especially due to a possible antiviral effect and the pandemic of COVID-19. To the best of the authors' knowledge and making a systematic search in the same databases, no studies have observed any antiviral effect of H 2 O 2 in the mouth. However, associations are supporting its use [5, 6] . e present systematic review used the most strict quality criteria for retrieving the studies. However, the interpretation will be contextualized in the moment that the world is facing a pandemic in which any kind of effort should be at least collated to make the sense of any preventive guideline. In terms of plaque, one study [15] was performed in adolescents and the others in adults. One of them also included handicapped individuals [20] . Four of the six studies that allowed oral hygiene compared 1.5% H 2 O 2 with a negative control [11, 15, 19, 20] and 2 of them with chlorhexidine [12, 21] , and one of them was also compared to a negative control [12] . Among the studies that used the experimental gingivitis model [16, 18, 22] , two were compared with a negative control [18, 22] and the other included a positive control [16] . In one study, the effect of H 2 O 2 was compared to chlorhexidine, but it was not possible to determine if mechanical plaque control was allowed [10] . In these studies, different concentrations of H 2 O 2 were used. It is clear from the encountered results that 1.5% H 2 O 2 is the most studied concentration in the formula of a mouth rinse. is result is in accordance with the previously published review [3] . For the publications evaluating the effect of H 2 O 2 on plaque, only one study (which evaluated the antiplaque effect over an 18-month time period) demonstrated improved results when compared to a placebo [15] . e other studies, which evaluated the effect over shorter periods, did not find statistically significant differences. Also, in the studies that used the experimental gingivitis model, only one study demonstrated the superiority of H 2 O 2 in comparison to placebo [18] . e same cited publications that evaluated plaque also evaluated the effect of H 2 O 2 on gingival inflammation. Although only a single study demonstrated the antiplaque benefit of H 2 O 2 , more studies clearly point to a better antigingivitis effect of H 2 O 2 mouth rinses as compared to placebo [11, 15, 20] . In fact, for one of the studies, no difference was observed between the H 2 O 2 mouth rinse and the positive control [12] . Because the participants of these studies were allowed for routine mechanical oral hygiene, an effect on clinical inflammation alone (without having the associated plaque reduction benefit) should be highlighted. ese results suggest that H 2 O 2 might perform differently in terms of plaque and gingivitis, which is of great clinical interest. Also, it is of high importance to evaluate the effect of mouth rinses on the oral microbiome. is includes not only bacteria but also other germs, such as viruses and fungi. However, despite completing a broad search of the literature, no studies were identified that evaluated the effect of H 2 O 2 oral microorganisms other than bacteria. e comparisons of the effect of rinses on oral bacteria with H 2 O 2 and with the positive control generally demonstrate a better effect of the latter. However, the differences in terms of the composition of the oral microbiome when H 2 O 2 is compared to placebo are clear in a variety of bacterial species. e present study evaluated risk of bias both for the nonrandomized and randomized trials. As expected, the randomized clinical trials presented a higher quality, with decreased risk of bias. e nonrandomized studies in general present a higher risk of bias. is is inherent to the chosen design. Randomized studies tend to present a lower risk of bias. 16 International Journal of Dentistry A systematic review was recently published by Marui et al. [23] describing the effect of preprocedural rinses with different substances on dental office-generated aerosols. ey demonstrated that rinses with chlorhexidine, essential oils, and cetylpyridinium chloride are effective. No studies with H 2 O 2 were included. Research. Meanwhile, taking into consideration the precautionary principle [24] , even without the qualified evidence, due to the high levels of morbimortality, it is of interest to see other potentials of the use of H 2 O 2 . In such conditions, the use of ""collateral evidence"" is recommended, always with a surveillance look. erefore, in the present moment, further studies including oral rinses with H 2 O 2 and other substances are warranted. Studies with the antiviral effect of H 2 O 2 are also needed. e data supporting the current study are available from the corresponding author upon request. @story_separate@In conclusion, rinsing with 1.5% H 2 O 2 has demonstrated an antigingivitis effect as compared to placebo, with also greater reductions in oral bacteria. Chlorhexidine has demonstrated, up to now, the best antiplaque and antigingivitis effect on the oral microbiome.","BACKGROUND: Hydrogen peroxide (H(2)O(2)) has been used for more than a century clinically to control plaque and gingival inflammation, with unclear supporting evidence. AIM: The aim of the present systematic review of the literature is to assess the effect of mouth rinses with H(2)O(2) on dental plaque, gingival inflammation, and oral microorganisms. METHODS: Five databases (PubMed, Scopus, Embase, Cochrane Library, and Web of Science) were searched with the following focused question: what is the effect of hydrogen peroxide, in comparison to chlorhexidine or to a placebo solution, in oral microbiota control, dental plaque, and gingival inflammatory outcomes? Two independent examiners retrieved the articles and evaluated the evidence. RESULTS: The majority of included studies were performed with 1.5% H(2)O(2). Results related to plaque accumulation generally demonstrate a slightly better effect of H(2)O(2) as compared to placebo mouth rinses, however with a lower performance as compared to chlorhexidine. In terms of gingival inflammation, H(2)O(2) performs better than placebo and more clearly demonstrates an anti-inflammation effect. No studies evaluated the effect of H(2)O(2) against viruses or fungi. In terms of bacteria, H(2)O(2) demonstrates an antibacterial effect. CONCLUSION: Rinsing with H(2)O(2) has the potential to affect plaque, gingivitis, and oral bacteria, as compared to placebo. However, the antibacterial results are not comparable to the performance of chlorhexidine."
"Since March 11, 2020, wherein the World Health Organization declared the novel coronavirus disease (COVID-19) a pandemic, healthcare providers around the world developed guidelines on the management of individuals with underlying diseases, including cancer [1] [2] [3] [4] [5] . Given the immune-compromised status of patients with cancer, they are susceptible to higher morbidity and mortality during COVID-19 pandemic [6, 7] . To reduce the exposure of patients with cancer during the era of COVID-19, several recommendations are presented as guidance for prioritizing the treatment plan for subsets of patients. According to these guidelines, clinicians should consider chance of cure, chance of palliation of symptoms, risk of tumor local recurrence, tumor biology, and its proliferating behaviors [8, 9] . Rectal cancer is an emerging healthcare issue among the Asian countries. As the third most lethal cancer, it causes about 8% of all cancer mortalities [10, 11] . Standard management of locally advanced rectal cancer (LARC) is a multidisciplinary approach to achieve optimal outcomes by reducing recurrence rate and preserving the anal sphincter. In the following sections, first, we have provided the common practice followed by our considerations during the COVID-19 era for the staging and management of LARC based on the usual approach of neoadjuvant, surgical, and adjuvant phase. Thereafter, we present the consensus of Shohada-e Tajrish Hospital for the management of LARC during the COVID-19 pandemic.@story_separate@Considering the limitation of widespread coronavirus testing and low sensitivity in asymptomatic patients, every patient with cancer must be assumed to be the carrier of SARS-CoV-2 during the pandemic [9] . Therefore, several considerations must be addressed during the staging workup of patients with LARC over the COVID-19 era. Considering the possible existence of SARS-CoV-2 in the digestive tract [12] , effective personnel protective equipment is needed for the digital rectal examination of patients. For the locoregional staging, pelvic magnetic resonance imaging (MRI) is superior to transrectal endoscopic ultrasound, as the latter is in direct contact with the gastrointestinal secretions. For the systemic staging, obtaining abdominal MRI concurrent with pelvic MRI seems to be an appropriate choice as it decreases the patients' trip. The instruments and surfaces of the endoscopic and imaging rooms must be strictly sterilized using specific disinfectants upon each examination, for example, by vaporized hydrogen peroxide for at least 2 h [13] . According to the American Joint Committee on Cancer (AJCC), LARCs are T3-4 and/or clinically node-positive tumors [11] . Given the high probability of residual after primary resection of LARC, the National Comprehensive Cancer Network (NCCN) recommends three approaches as the neoadjuvant options, including (a) neoadjuvant long-course RT (i.e., 45-50.4 Gy in 25-28 fractions) with concomitant fluoropyrimidine-based chemotherapy (LCCRT), with higher doses (i.e., up to 54 Gy) for unresectable tumors; (b) shortcourse hypo-fractionated RT (SCRT) (i.e., 25 Gy in 5 fractions); and (c) induction chemotherapy followed by chemoradiation, known as total neoadjuvant therapy (TNT). The third approach was added to the 2015 version of the NCCN guideline as an acceptable approach. The advantages of LCCRT are more downstaging of the tumor and sphincter preservation and less toxicity. The advantages of SCRT are its simplicity, no chemotherapy toxicities, and cost-effectiveness. The advantages of TNT are early eradication of micrometastasis, a higher rate of pathologic complete response (pCR), decrease in time of surgery, and improvement of chemotherapy completion [14] [15] [16] . The SCRT is a non-inferior alternative to LCCRT, in terms of locoregional recurrence, distant recurrence, and overall survival [1, 10, 15, 17] . The value of LCCRT over SCRT is questioned during the pandemic for its increased risk of myelosuppression and infection. Moreover, SCRT minimizes patients' exposure to medical surroundings [17] . Therefore, during the COVID-19 outbreak, considering SCRT would be an appropriate choice for the neoadjuvant chemoradiation of LARC. If LCCRT was selected, capecitabine is recommended as an alternative to intravenous fluorouracil (5-FU) to decrease patients' exposure. While the data is still evolving, TNT may be considered an appropriate choice during COVID-19 due to the facilitation and delay of surgery. The Society of Surgical Oncology (SSO) prioritizes the TNT over others and selects the SCRT as the radiotherapy phase of TNT [18] . According to the phase 2 trials, two regimens are available as the induction chemotherapy of TNT, the CAPOX (capecitabine and oxaliplatin) and FOLFOX regimen (fluorouracil, leucovorin, and oxaliplatin) [19, 20] . During COVID-19 pandemic, the CAPOX regimen is recommended as the induction chemotherapy of TNT to decrease the patients' exposure. According to the European Society for Medical Oncology (ESMO) guideline, several clinical settings are in priority for RT during the COVID-19 pandemic, including patients who developed severe complications due to disease progression (e.g., compressive organ failure and massive bleeding) [21] . In summary, the most accepted neoadjuvant therapy for patients with LARC is LCCRT. However, in the coronavirus outbreak, the definition of ""standard of care"" was changed fundamentally in many oncology centers around the world. Lowering the frequency and duration of the patients' exposure is the basis for the management of immunosuppressed patients with cancer [8] . An additional 10-20 Gy using intraoperative RT (IORT) or brachytherapy (BT) boost is recommended in cases of T4, recurrence tumors, or close/positive margins [22] . The available choices for boost RT following EBRT of rectal cancer are IORT, BT, and contact therapy. The IORT may not be an applicable choice during the pandemic due to the limitations of operations during the COVID-19 era. Endorectal BT is applicable because it does not need general anesthesia. The accessibility of contact X-ray BT (Papillon technique) is not as endorectal BT but it is an appropriate alternative for outpatient setting [24] . Optimizing the pathologic complete response with R0 resection is reported on the implementation of BT boost following EBRT [23, 24] . Consolidative chemotherapy after SCRT may improve the results. In 2014, Glynne-Jones et al. have shown the positive effect of four courses of FOLFOX regimen on downstaging of tumor administered as a consolidative chemotherapy after SCRT [25] . During the COVID-19 outbreak, CAPOX regimen looks like a better choice as consolidative chemotherapy to decrease patients' stay in the hospital. In March 2020, Simcock et al. proposed the delay in radiotherapy in specific situations during the COVID-19 outbreak [26] . However, this approach is not recommended for patients with LARC. Considering the natural history of LARC, delay in neoadjuvant chemoradiation may switch the non-surgical situation to an emergent situation through rectal obstructing or perforation. Therefore, timely management of LARC should be considered. The longer mean operative time of proctectomy encounters the patients with rectal cancer to further risk to develop morbidities and mortalities of COVID-19 [27] . Notably, due to the putative spread of SARS-CoV-2 via carbon dioxide aerosolization, the consensus-based guideline from the SSO recommends evaluating the risk-to-benefit ratio before application of laparoscopic procedures [18] . To reduce the risk of infection, the rate of elective surgeries has been reduced during the COVID-19 outbreak. The ESMO prioritizes several clinical settings for surgical management of patients with rectal cancer, including emergent surgical circumstances (e.g., hollow viscus perforation, obstruction, peritonitis, massive gastrointestinal bleeding), post-surgical complications (e.g., perforation, anastomotic leak), and bone fracture due to metastasis [21] . If surgery was selected upon the neoadjuvant treatment, it should be delayed no longer than 12 weeks. This may increase the radiation-associated fibrosis that led to technically more difficult operations [28] . During the surge times, delayed operation at 6-8 weeks after SCRT seems to be an appropriate choice [19] . This approach improves the tumor shrinkage and facilitates the surgery and, therefore, decreases the rate of postoperative complications and hospitalization time. Notwithstanding the lack of clinical trial evidence, ""watch and wait"" may potentially be an available approach for a subset of patients with clinical complete response (cCR) to neoadjuvant therapy during COVID-19. There is not yet adequate evidence to recommend a ""watch and wait"" approach over surgery, but a non-surgical option may likely be available to patients in the future. The benefit of adjuvant oxaliplatin-based over fluorouracilbased regimens for rectal cancers is still controversial [29] . Only one out of eight available clinical trials demonstrated the superiority of FOLFOX over 5-FU and leucovorin in terms of disease-free survival in patients with LARC who received preoperative CRT and TME [29] . Therefore, it is advised to choose fluoropyrimidine-based chemotherapy as a postoperative chemotherapy of patients with LARC to decrease the hematologic toxicities during the COVID-19 pandemic. If an oxaliplatin-based regimen was the choice, the FOLFOX and CAPOX are the two available regimens, extrapolating from clinical trials of colon cancer [30] . In this regard, CAPOX is considered more appropriate during the viral pandemic for minimizing patients' exposure. Elderly patients need special attention in the era of COVID-19. Several studies have presented recommendations for better management of elderly patients with cancer during the COVID-19 outbreak. They concluded that BT is a feasible alternative to surgery in elderly patients upon well downstaging of the tumor by neoadjuvant (chemo)radiation [31] . The criteria of frail elderly patients according to comprehensive geriatric assessment (CGA) are dependence in at least one activity of daily living; presence of at least three grade 3, or at least one grade 4 comorbidity (based on CIRS-G); or presence of at least one geriatric syndrome. Guidelines recommend SCRT plus BT for frail elderly patients. Patients with comorbidities or older than 80 years need further attention due to the five times higher risk of death [31, 32] . Eventually, frail unresectable cases were considered a separate categorization using those algorithms. BT boost following EBRT may substitute surgery for frail elderly patients and in cases of distal or inoperable tumors [25, 31, 32] . Notably, in these cases, delay in BT up to 6-8 weeks after EBRT provides more downstaging of tumor that optimizes dose delivery and decreases the toxicity [25] . Considering the aforementioned guidelines, a consensus is proposed for management of patients with LARC during the coronavirus pandemic. Scheduled treatment is varied based on the clinical stage, distance from mesorectal fascia (MRF-D), possibility of resection, and performance status. For patients with T1-2N+/T3N ± (with > 2 mm MRF-D) rectal cancer, SCRT is recommended. To obtain more clinical response, delayed surgery to 6-8 weeks after completion of SCRT is an appropriate option during the COVID-19 pandemic. Delayed surgery is non-inferior in terms of overall survival [33, 34] . For patients with T3N ± (with ≤ 2 mm MRF-D)/T4 disease, we suggest the TNT approach including LCCRT for two reasons: TNT allows the surgery to be safely delayed and LCCRT enhances the clinical response. To minimize patients' exposure during the COVID-19 era, CAPOX is preferred over FOLFOX and capecitabine over intravenous 5-FU for adjuvant/induction and concomitant chemotherapy, respectively. Our choice for the duration of postoperative chemotherapy of patients with T1-2N1/T3N1 disease is 3 months, as it showed non-inferiority results in terms of the 3-year disease-free survival in comparison with 6 months of chemotherapy [35] . In other cases, the standard 6 months of adjuvant chemotherapy is chosen at our institution. Patients who are not candidates for surgery need higher radiation doses to provide more local control. Being such, we add BT with a dose of 10-20 Gy in 2-4 fractions upon SCRT. Thereafter, chemotherapy for 3-6 months is used for control of the systemic disease. Table 1 summarizes the strategy of clinicians of the Shohada-e Tajrish Hospital in the management of patients with LARC during the COVID-19 outbreak.@story_separate@The current COVID-19 pandemic has affected the management of patients with cancer. Similar to the so-called emergency plan of radiotherapy departments for urgent situations, oncology centers need institutional guidelines for optimal management of patients during the COVID-19 pandemic. This article presented the consensus of the Clinical Oncology department of Shohada-e Tajrish Hospital for better management of patients with locally advanced rectal cancer during the COVID-19 pandemic. The recommendations discussed here were developed based on our experience in the most advanced oncology center in Iran. During COVID-19, individualized management of LARC should be considered. Priority should always be given to the life-threatening conditions. We hope that these recommendations could provide reasonable references for oncologists worldwide facing the COVID-19 pandemic.","PURPOSE: Today, the rapid outbreak of COVID-19 is the leading health issue. Patients with cancer are at high risk for the development of morbidities of COVID-19. Hence, oncology centers need to provide organ-based recommendations for optimal management of cancer in the COVID-19 era. METHODS: In this article, we have provided the recommendations on management of locally advanced rectal cancer during the COVID-19 pandemic based on our experience in Shohada-e Tajrish Hospital, Iran. RESULTS: We recommend that patients with locally advanced rectal cancer should be managed in an individualized manner in combination with local conditions related to COVID-19. CONCLUSION: Our recommendation may provide a guide for oncology centers of developing countries for better management of locally advanced rectal cancer."
"Tuning forks are frequently used as stable frequency references for musical and timing purposes. Their mechanical and acoustical properties can be found in many papers about linear resonance, 1 vibration modes 2 and sound field radiation. 3 Nowadays, the quartz tuning forks (QTF) have become the core components of various sensors and microscopies 4 such as SNOM (scanning near-field optical microscopy), 5 AFM (atomic force microscope) 6 are not readily connected to appropriate mathematical models; thus many aspects of their nonlinearity remain unclear. In this paper, we use laser Doppler interferometry to measure the tuning forks principle resonance tuning curve. We find typical nonlinear behaviors such as the softening tuning curve and jump phenomena. In Sec. II, we discuss the nonlinear behaviors of the tuning forks and apply an oscillation model containing both quadratic and cubic terms in the restoring force. In Sec. III, we introduce the double-grating Doppler interferometry experiment that achieves measurement accuracy within ten microns. In Sec. IV, we observe the softening resonance curve and jump phenomena in the principle mode of a tuning fork. Our experiment studies the nonlinear dynamics of a tuning fork using laser Doppler interferometry, providing an integrated experiment for intermediate-level students. It also serves as a basis for senior research projects to further explore the phenomena.@story_separate@The current model of tuning forks that approximates the tines as cantilever or free-free beams provides satisfactory explanation for the vibration modes and their corresponding frequencies. For in-plane symmetric modes, the modal frequencies are, 2 where a is the thickness of the forks beam, L is the length of the tines, E is the Young elastic modulus and ρ is the density of the forks material. Note that the frequency of the second symmetric mode predicted by Eq. (1) is over 6 times ( 2.988 2 1.194 2 times) the principal mode f 0 , much higher than small harmonic frequencies 2f 0 , 3f 0 , etc. that exist in a tuning forks spectrum. Apparently, these harmonics should be attributed to nonlinearity. In fact, the motion of each tine of a fork resembles that of a cantilever beam only in linear regime. A vibrating cantilever beam is symmetric and usually modeled by the Duffing equation which contains a cubic restoring force 11 . In a cantilever beam model, the beams principal mode shows hardening spring behaviors, and its vibration spectrum has a prominent third harmonic component 12 . However, these predictions based on a cantilever beam model are contradictory to experimental observations. In Section IV, our experiment shows that the principal mode exhibits softening rather than hardening spring behaviors. We also measured the free vibration spectrums radiated by a 528 Hz steel tuning fork, see Fig. 1 . We recorded the sound with the microphone on a pair of EarPods and processed the audio signal with Matlab. We were unable to use the same tuning fork described in Section IV, because this part of experiment was done at home due to the COVID-19 pandemic. The second harmonic component can be seen with a normal blow and becomes obvious with a hard blow on the tuning fork. However, the third harmonic component is very weak even under a hard blow. The tines of a real-world tuning fork are geometrically and physically asymmetric. To describe the asymmetry of the restoring force, we apply a model containing both quadratic and cubic terms. The equation for free vibration is, where ω 0 is the natural angular frequency under linear approximation, y is an effective displacement for the equivalent oscillator of a vibration mode, and α and β are nonlinear quadratic and cubic stiffness parameters. This nonlinear model was solved in Landaus textbook 13 . For the free vibration, the amplitude for second harmonic component is αb 2 6ω 0 2 , i.e. the amplitude is proportional to the square of fundamental amplitude b. In experiment, the amplitude of the strong second harmonic component is nearly proportional to the square of fundamental amplitude 2 , which clearly indicates the existence of such quadratic nonlinearity. Now consider forced vibrations; the oscillator is governed by the following equation: where F and Ω are the amplitude and angular frequency of the harmonic driving force respectively, and λ d is the damping coefficient. In our experiment, y is the displacement of the point on the fork tine where the grating is attached. The amplitude frequency response for Eq. (3) is given by where b is the amplitude of the forced vibration, ε = Ω − ω 0 , and κ = 3β 8ω 0 − 3α 2 8ω 0 3 . The resonance frequency shift derived from this model is We see that a positive κ corresponds to a hardening spring behavior, a negative κ corresponds to a softening spring behavior. Typical frequency response curves for κ < 0 are plotted in Fig. 2 according to Eq. (4). From κ = 3β 8ω 0 − 3α 2 8ω 0 3 , we know that the quadratic term always contributes to the softening effect regardless the sign of α. A tuning fork shows softening behavior when the quadratic term dominates, even though the cubic nonlinearity is a hardening one.  . F k is the critical driving amplitude for a bistable state to appear. As shown in Fig. 2(b) , when driving amplitude F > F k , the amplitude frequency response curve becomes multivalued. For each value of frequency ε in the interval between the frequencies ε 1 and ε 2 , there are three steady-state solutions. Among them the middle one on CD (dotted line) is unstable, while the other two (BC and DE) are stable. Hence, as the driving frequency decreases quasi-statically from point A, the amplitude increases along the curve ABC until frequency ε 1 is reached. A slight frequency decrease at frequency ε 1 causes a spontaneous jump from C down to E. As the frequency continues to decrease beyond ε 1 , the amplitude decreases along the curve EF. When the driving frequency increases quasistatically from point F, the amplitude increases along the curve FED until frequency ε 2 is reached. A slight frequency increase at frequency ε 2 causes a spontaneous jump from D up to B. As the frequency increases further beyond ε 2 , the amplitude decreases along the curve BA. In Section IV, we will verify these resonance properties and determine the parameters κ and λ d experimentally. In our experiment, we use double-grating Doppler interferometry [14] [15] [16] to measure the vibration of a tuning fork, see Fig. 3 . Two identical diffraction phase gratings are placed parallel to each other. One is stationary, and the other is attached to one of the fork tines. When a beam of light is incident normally to the stationary grating plane, the light is diffracted and the n-th order diffracted angle θ n satisfies d sin θ n = nλ, where λ is the wavelength of the laser and d is the grating constant. Then the beams of the diffracted light illuminating the moving grating will be diffracted again and their frequencies will shift due to the laser Doppler effect. The frequency shift is given by 17 where ν is the frequency of the light and c is the speed of light; θ is the angle of incidence to the moving grating, which equals the diffraction angle of the stationary grating; ϕ is the angle of diffraction of the moving grating; and u is the velocity of the moving grating that is parallel to the grating plane. In the experiment, the photodetector is placed in alignment with the incident light such that only light with angle ϕ = 0 can be received. For a grating with diffraction angle ϕ = 0 and incidence angle θ, the n-th diffraction maxima also satisfies d sin θ n = nλ. Because the two gratings have the same grating constant and the same θ n , they have the same diffraction order n. We refer to the beam with diffraction order n as beam 1. The frequency shift of beam 1 is ∆ν n = νu sin θn c = u d n. Similarly, the frequency shift of an adjacent order (beam 2) is u d (n+1). Since the spacing between the two gratings is small, the overlapping facula of the adjacent orders interferes. The interference of beam 1 and beam 2 with different frequencies produces an optical beat whose frequency is ν b = u(t) d . Since the beat frequency as a function of time is proportional to the velocity of the vibrating grating, the phase of the optical beat is Suppose that the motion of the grating is harmonic with angular frequency ω and amplitude b, i.e., y (t) = b cos(ωt), then the light intensity received by the photodetector is where I 0 is the intensity of the optical beat, and ϕ o is the initial phase. A theoretical image of a typical harmonic motion detected by double-grating Doppler interferometry is shown in Fig. 4(a) . The curve is periodic: the half period from A to B, where dϕ dt = 0, corresponds to half period of the vibration from t k ≡ kπ ω to t k+1 ≡ (k+1)π ω . The number of extrema N within each half period corresponds to phase change of πN ≈ Thus the vibration amplitude can be measured by counting the extrema The measurement accuracy is about d/4, reaching the range of 10 micron in our experiment. Figs.4(b) and 4(c) are typical experimental images with different amplitudes. The relative uncertainty is smaller for larger amplitudes. In the experiment, we use a 512-Hz steel tuning fork. The grating constants for the gratings are d = 5.0 × 10 −5 m. One grating is attached to the end of one tine of the tuning fork to measure its transverse vibration. We put an electromagnetic coil near the middle of the same tine to drive the steel tuning fork with a sinusoidal current with angular frequency Ω. The sinusoidal current was biased to avoid frequency doubling of the driving force. For simplicity, we approximate the driving force as F cos Ωt. = Ω − ω 0 . The natural angular frequency we measured from the sound spectrums is ω 0 = 2π × 510.89Hz which is lower than the labeled value after the grating is attached. In Fig. 5(a) , the data plotted in red squares and black dots are results at driven amplitudes At driving amplitude F 1 the theoretical curve fits the data points well, but at 25F 1 , the solid theoretical curve deviates from the data points systematically. In the figure, the broken curve fitted for 25F 1 with κ = −3.66mm −2 s −1 and λ = 0.49s −1 fits the data much better. The variation of coefficients might be caused by the driving force applied to the tines which was simplified as a linear force. The resonance curves show that obvious softening spring behaviors and jump phenomena are observed when the driving amplitude exceeds 25F 1 . The detail near the resonant peak is plotted in Fig. 5 (b) for driving amplitude 25F 1 . The data points in the inset correspond to the bi-stable region near BCED in Fig. 2(b) . In this region, the data points as the driving frequency increases and decreases are indicated with red triangles and blue squares, respectively, and the curve is the theoretical result fitted for κ = −3.66mm −2 s −1 and λ = 0.49s −1 . When the driving frequency increases from point F, the amplitude increases along the curve and experiences a jump from D up to B above frequency -0.73Hz (Ω/2π = 510.16Hz). When the frequency decreases from point A, the amplitude increases along the curve and experiences a downward jump from C to E below frequency -0.75Hz (Ω/2π = 510.14Hz). The amplitude response curve forms a hysteresis loop BCED as indicated by the arrows in Fig. 5(b) . Even though the frequency region for the bi-stability is narrow, the amplitude jump phenomenon is impressive to the observer indeed. The jump up and jump down are accompanied with a sudden change in the loudness and a perturbation on oscilloscope signals. It is difficult to determine the coefficients α, β or κ and λ d from the physical parameters in our mathematical model. Nevertheless, we can still investigate resonance properties and determine these parameters experimentally. The fact that the second harmonic component is much stronger than the third harmonic component indicates that the quadratic term prevails in our tuning fork. Quantitative results may be obtained through more detailed measurement. However, in a forced vibration system like this, nonlinearity has various origins, such as geometrical, mechanical or electromagnetic nonlinearity. The parameters are sensitive to minor altering in the symmetry physically or geometrically. For example, bending the tines inward inhibits the second harmonic 2 . Driving or detecting components such as coils or electrodes coupled to the tines might also introduce additional nonlinearity to the restoring force, the driving force as well as to the damping. In these cases, the signs and magnitudes of coefficients might be altered, and the behaviors of the oscillator might change as well. In our experiment, the nonlinearity parameter and damping coefficient change as the driving force increases. By adjusting the driving or detecting parameters such as the amplitude or the bias of the driving current appropriately, the nonlinear coefficients could be controlled according to practical needs.@story_separate@In conclusion, we investigated the nonlinear behaviors of a regular tuning fork. In theory, we apply an oscillation model containing both quadratic and cubic terms in the restoring force. The strong second harmonic component and the softening spring behaviors observed in experiment can be attributed to quadratic nonlinearity. In experiment, we apply the doublegrating Doppler interferometry and achieve measurement accuracy within ten microns. We have observed the softening tuning curve and jump phenomena. In practical applications, the nonlinear frequency shift and jump phenomenon should be addressed especially for micro-resonators like quartz tuning forks (QTF) in various sensors and microscopies. Our experiment setup is inexpensive and easy to operate. It provides an integrated experiment for intermediate-level students and a basis for senior research projects.","Tuning fork experiments at the undergraduate level usually only demonstrate a tuning fork's linear resonance. In this paper, we introduce an experiment that can be used to measure the nonlinear tuning curve of a regular tuning fork. Using double-grating Doppler interferometry, we achieve measurement accuracy within ten microns. With this experiment setup, we observe typical nonlinear behaviors of the tuning fork such as the softening tuning curve and jump phenomena. Our experiment is inexpensive and easy to operate. It provides an integrated experiment for intermediate-level students and a basis for senior research projects."
"Infective endocarditis (IE) is a serious bacterial infection of the endocardium and/or heart valves that carries considerable morbidity and mortality. Often presenting with non-specific symptoms, such as fever, chills, fatigue, and shortness of breath, this disease presents several challenges to the physician in the emergency department (ED). Patients may lack well-known risk factors such as intravenous drug use, previous history of IE, and prosthetic heart valves, which can make it more difficult to consider in the differential diagnosis, especially during the influenza season and novel coronavirus disease 2019 (COVID-19) pandemic in which there is an abundance of patients presenting with flu-like illness. It is important for the emergency medicine (EM) clinician to be aware of the multiple manifestations of IE on history and examination, especially in the absence of such risk factors. The foundation of establishing a diagnosis requires blood cultures and definitive echocardiographic findings, neither of which is considered during the patient's time in the ED. Early consideration, recognition, and treatment of IE are crucial for improved outcomes. Obtaining timely blood cultures and administering appropriate empiric antibiotics in the ED coupled with early transthoracic/transesophageal echocardiogram (TTE/TEE) are essential to guiding therapy once the patient is admitted to the hospital.@story_separate@A 47-year-old male with no pertinent medical history presented to the ED complaining of shortness of breath. He stated that his symptoms had been persistent for the last three weeks and he also reporting malaise and fatigue. He was seen at a different hospital previously for the same symptoms and was discharged home with an albuterol inhaler, with no improvement. A few days prior to arrival, his symptoms worsened and he started to experience subjective fevers, which led to his second ED visit. While in the ED, his vitals showed a low-grade fever of 99.2°F, tachycardia ranging from 102-144, a normal blood pressure of 145/78, and oxygen saturation of 98%. The ED team pursued a diagnostic workup for suspected COVID-19. Physical examination was notable for an ill-appearing and diaphoretic male. Cardiac examination revealed tachycardia without any obvious murmurs. Pulmonary examination was notable for tachypnea but otherwise clear to auscultation. The abdomen was soft and non-tender. His lab workup was notable for a leukocytosis of 29.1 x 109/L (normal range: 3.6-11.0 x 109/L) and elevation in his inflammatory markers. He had an initial C-reactive protein of 18.7 mg/dL (normal range: 0-0.1 mg/dL), ESR of 65 mm/hr (normal range: 0-10 mm/hr), lactate dehydrogenase of 267 units/L (normal range: 84-246 units/L), and D-dimer of 2,361 ng/mL (normal range: 0-316 ng/mL), which are all elevated. His lactic acid was within the normal range. Of note, his troponin was moderately elevated at 0.367 ng/dL (normal range: 0.000-0.043 ng/dL). The chest X-ray was unremarkable and the computed tomography (CT) angiogram of the chest was negative for any infectious etiology, pulmonary embolism, or dissection. CT of the abdomen/pelvis with IV contrast revealed a 7-cm hypodensity within the spleen, concerning for abscess versus infarct ( Figure 1 ). He denied any trauma or IV drug use. Follow-up ultrasound was ordered which characterized the hypodensity as a splenic abscess. The patient received a ""sepsis bundle"", which included the administration of IV fluids, ceftriaxone, and azithromycin to cover empirically for COVID co-infection; blood cultures were drawn in the ED and he was subsequently admitted to the ""COVID rule-out"" medical floor of the hospital. The patient tested negative for COVID-19 within a few hours. The infectious disease service was consulted and adjusted the antibiotic regimen to vancomycin and piperacillin-tazobactam. An echocardiogram was recommended to rule out IE and the cardiology service was consulted. The TTE was performed on hospital day 2, which showed minimal mitral valve thickening with mild mitral regurgitation. The interventional radiology (IR) service was consulted for the splenic abscess with plans to perform CT-guided drainage. An IR drain was successfully placed on hospital day 3. On the same day, blood cultures grew Klebsiella pneumoniae. On hospital day 5, the patient was transferred to the ICU due to respiratory distress and suspected new empyema formation. A CT of the chest was ordered at that time, which showed the development of a leftsided pleural effusion, as seen in Figure 2 . The patient had also been persistently tachycardic and febrile, with high leukocytosis and gradually worsening respiratory distress since admission. TEE was scheduled but was postponed due to worsening respiratory status. At this point, the cardiothoracic surgery service was consulted. They scheduled the patient for left-sided lung decortication the following day. While still intubated from the procedure, the patient was scheduled for TEE the following morning, which revealed IE of the mitral valve. Findings were discussed with cardiothoracic surgery service that initially recommended medical management. Repeat TEE was scheduled five days later, which showed mitral regurgitation and increased size of the mitral vegetation despite aggressive antibiotic therapy. Two days later, the patient was scheduled for mitral valve repair by CT surgery. Postoperatively, the patient's tachycardia resolved, leukocytosis improved, and his temperature normalized. He continued to improve during his hospital stay and was discharged 10 days later without complications.  Splenic abscess is a fairly uncommon disease. Some data suggest it has an incidence as low as 0.2-0.7% [1] . There are a variety of etiologies in which a patient can develop a splenic abscess, but most commonly, after trauma, and in patients who are immunocompromised, it develops secondary to disseminated infection [1] . It is important to note that up to 5% of IE cases are complicated by splenic abscess formation [2] . A splenic abscess most commonly presents with various non-specific symptoms such as fever, abdominal pain, and chills; this can make the diagnosis difficult based on clinical presentation alone. It should also be considered in patients who have recurrent or persistent fevers despite adequate antibiotic regimen. Fortunately, the increasing use and accuracy of ultrasonography and CT have aided in confirming the diagnosis. A review of the literature helps illuminate some of the more common presentations of splenic abscess. Specifically, fever has been noted by some sources to be present in as high as 92.6% of patients [1, 3] . Additionally, 55.6-66.7% had abdominal pain and 61.1-89% had a leukocytosis [1, 3] . Of note, some data suggest that up to 88% of patients had left-sided pleural effusions that were noted on chest X-ray [1] . In terms of microbiology, sources have mentioned that Streptococcus viridans, Staphylococcus aureus, and Klebsiella Pneumoniae tend to frequently be identified as the causative agents [1, 2] . Mortality is consistently cited at 14-16% [1, 3] . Currently, there is no gold standard approach to the management of splenic abscesses. The three current modalities of treatment are IV antibiotics alone, percutaneous drainage, and splenectomy. The literature shows they have similar survival rates, and the recommendation is that treatment should vary on a case by case basis [3] . When reviewing our case, the patient had both classic and subtle presentations of splenic abscess. Despite having respiratory symptoms for two weeks, the primary reason he presented to the hospital was due to the onset of new fevers. He was febrile and tachycardic, with a significant leukocytosis. He continued to have fevers despite antibiotic therapy and IR drainage of the abscess. After the development of a left-sided pleural effusion and empyema formation, which has been well documented to co-exist with splenic abscesses, he required decortication of the left lung. Despite these interventions, the patient remained febrile and tachycardic until he had the infected valve repaired. Blood cultures were positive for Klebsiella pneumoniae, which is uncommon but has been noted in the literature when reviewing splenic abscesses. Despite this, he did not fit into any high-risk population. He denied any trauma preceding his symptoms. Furthermore, he lacked any immunocompromising conditions such as HIV, end-stage renal disease, or diabetes. The patient was a previously healthy 47-year-old male with no hospitalizations prior to this one. He adamantly denied any drug use, which was confirmed by a negative urine drug screen. The absence of IV drug use history, negative TTE, lack of immunocompromising condition, and blood cultures that grew gramnegative rods all pointed away from IE as the primary etiology. IE is an infection of the endothelial layer of the heart and can disseminate rapidly. This disease continues to be diagnostically challenging for EM physicians to diagnose primarily due to a variety of presentations, breadth of acuity, and often absence of identifiable risk factors. The modified Duke's criteria, as shown in Figure 3 , are the standard for which clinicians diagnose IE. Recent literature updates demonstrate that these criteria have an 80% sensitivity in establishing a diagnosis [4] . Unfortunately, very few aspects can be readily applied in the ED. Positive blood cultures and echocardiographic findings are the hallmark tests for diagnosing IE. While non-invasive bedside TTE is somewhat sensitive if the vegetation is large, the blood culture results are not available when the patient is presenting acutely ill in the first few hours of their ED hospitalization. An article was written by Delaney regarding IE in the ED in which she established guidelines to increase detection and determine who may benefit from hospitalization and further workup, specifically, febrile elderly patients in the absence of a clear source, patients with prolonged fever of more than two weeks without clear etiology, patients with evidence of vasculitis or embolization with a new murmur or change in old murmur, patients with fever plus known structural heart disease or recent instrumentation without a clear source, patients with fever and with cardiac prosthesis or any duration of malaise, vasculitis, or new murmur, and, finally, any febrile IV drug users [5] . Given the increasing prevalence of structural heart disease that is associated with aging, elderly patients with fever lasting more than two weeks should undergo workup that includes IE. Additionally, they are more susceptible to bacterial illnesses. An individual with fevers longer than two weeks is unlikely to have a benign etiology and should be further evaluated. Signs of vasculitis, embolization, and new murmurs should lead to an increased index of suspicion as they can be signs of acute pathology and disseminated infection. Predisposing conditions such as structural heart disease, prosthetic heart valves, and IV drug use are significant risk factors for IE, and anyone with an associated fever without a clear source should be further evaluated [5] . Although quite dated, the article illustrates significant key features that can aid the EM physician in considering IE. Of note, other significant risk factors include previous diagnosis of IE, immunocompromised state, indwelling lines for venous access, and chronic kidney disease [4] . Echocardiography remains a critical element in diagnosing IE. TTE tends to be the first image modality due to its convenience and non-invasiveness. Despite this, the sensitivity of TTE in diagnosing IE is merely 71% [6] . There is a direct correlation between the size of the vegetation and the detection on TTE. The literature demonstrates that TTE has a sensitivity of 25%, 70%, and 84% for vegetations < 5 mm, 6-10 mm, and >10 mm, respectively [6] . Although there is a lack of literature comparing POCUS (point-of-care ultrasound) in the ED to a gold standard, it is reasonable to assume that sensitivities may be more variable than the standard of care TTE, depending on the EM providers proficiency with bedside ultrasound. Any patient with a high degree of suspicion for IE and a negative TTE should be followed up with a TEE when possible, as they have a sensitivity of 90% [4] . IE is associated with significant morbidity and mortality, making early diagnosis with appropriate therapy paramount. Some studies have noted a 30-day mortality of 30% [4] . A Korean study compared the difference between early surgical intervention and conventional medical treatment, with primary outcomes being hospital death or embolic events within six weeks. The research showed a statistically significant difference between the two groups respectively (3% vs 23%) [7] . The main difference is observed in the reduced embolic rates noted in the early surgery group [7] . The study reinforces the importance of early intervention, which requires a timely diagnosis. When reviewing our case, establishing the diagnosis of IE proved to be exceptionally complicated, especially in the setting of the COVID-19 pandemic. The most notable challenge was having a high index of suspicion for IE despite the lack of risk factors. The patient was a previously healthy 47-year-old male with no medical problems. He had no immunocompromising conditions, no congenital/structural heart defects, and normal dentition. Of note, he adamantly denied drug use. On examination, he was ill-appearing, febrile, and tachycardic, with laboratories significant for high leukocytosis, elevation in inflammatory markers, and mild troponin elevation, which are non-specific. The key feature of the disease process was that CT of the abdomen/pelvis revealed an acute splenic abscess. In the absence of other risk factors for abscess, disseminated disease from IE had to be considered. Even after a non-diagnostic TTE, a follow-up TEE would have ideally been performed next. Unfortunately, the patient's respiratory status deteriorated after the development of a left-sided empyema, which delayed the procedure, diagnosis, and, ultimately, the treatment. The management of IE in the setting of an associated splenic abscess requires certain considerations. For example, some suggest splenectomy/drainage should take place before valve repair to prevent secondary infection [1] . Depending on the stability of the patient's condition, there is literature to suggest that splenectomy/drainage can occur simultaneously with valve repair as a one-stage process [1] . Our patient's course was complicated by ongoing fevers and tachycardia and the development of a left-sided empyema even after drainage of the splenic abscess, which delayed TEE until after his lung decortication. His clinical picture began to improve after his mitral valve was repaired, and the remainder of his hospital course was uneventful.@story_separate@IE continues to be a clinical diagnostic challenge for physicians, especially in the ED, due to the lack of diagnostic criteria such as immediate positive blood cultures or ease of visualized vegetations on bedside echocardiographic studies. IE has a wide gamut of presentations with different levels of acuity. Diagnosis is more straightforward when patients present with obvious risk factors, but, in many cases, such as this one,","Infective endocarditis (IE) is a serious bacterial infection of the endocardium and/or heart valves that carries considerable morbidity and mortality. Often presenting with very non-specific symptoms, this disease presents many challenges to the emergency medicine practitioner. A 47-year-old male with no pertinent medical history presented to the emergency department complaining of shortness of breath. He stated that his symptoms had been persistent for the last three weeks and were associated with malaise and fatigue. CT of the abdomen/pelvis with IV contrast revealed a 7-cm hypodensity of the spleen concerning for abscess versus infarct. He denied any trauma or IV drug use. Follow-up ultrasound was ordered, which characterized the hypodensity as a splenic abscess. An echocardiogram was recommended for possible IE, and cardiology was consulted. The transthoracic echocardiogram was performed on hospital day 2, which showed minimal mitral valve thickening with mild mitral regurgitation. The interventional radiology (IR) service was consulted for the splenic abscess in order to perform CT-guided drainage. An IR drain was successfully placed on hospital day 3. On the same day, blood cultures grew Klebsiella pneumoniae. On hospital day 5, that patient was transferred to the ICU for possible empyema formation with signs of respiratory distress. The patient underwent CT of the chest that showed the development of a left-sided effusion. The patient had also been persistently tachycardic and febrile, with high leukocytosis since admission and worsening respiratory status. Transesophageal echocardiogram (TEE) was scheduled but put on hold due to worsening respiratory status. Repeat TEE was scheduled five days later, which showed mitral regurgitation and increased size of the vegetation despite antibiotic therapy. Two days later, he was scheduled for mitral valve repair. When reviewing our case, the patient had both common and uncommon aspects of splenic abscess or IE. First, despite having respiratory symptoms for two weeks, the primary reason he came to the hospital was due to the new onset of fevers. He was febrile, tachycardic, and with significant leukocytosis. He continued to have fevers despite antibiotic therapy and IR drainage of the abscess. With no history of IV drug use history, negative transthoracic echocardiography, lack of immunocompromising condition, and blood cultures with gram-negative rods, IE became less likely of a diagnosis. Establishing the diagnosis of IE proved to be exceptionally complicated, especially in the setting of a COVID-19 pandemic. The most notable challenge was having a high index of suspicion despite any risk factors. The patient was a previously healthy 47-year-old male with no medical problems. IE continues to be a clinical challenge for physicians, especially in the emergency department, due to the lack of diagnostic criteria such as positive blood cultures or vegetations visualized on echocardiographic studies. IE has a wide gamut of presentations with different levels of acuity. Diagnosis is more straightforward when patients present with obvious risk factors, but, in many cases, such as this one, those risk factors may be absent. A high index of suspicion is required, especially in patients with additional findings such as splenic abscess, embolic phenomenon, focal neurologic deficit, mycotic aneurysm, decompensated heart failure, new murmurs, or pleural effusions."
"Development of acute kidney injury (AKI) appears common in COVID-19, however geographical differences in incidence and severity have been described (1) (2) (3) (4) (5) (6) . In a metaanalysis of 20 cohorts worldwide, overall prevalence during admission was 17% ranging from 0.5% to 80.3% (7) . This variation likely reflects differences in sample sizes and cohort characteristics including in monitoring and detection of AKI (8) . Few studies have reported timing and severity of AKI, or temporal relationships to interventions including renal replacement therapy (RRT) (9) . Short follow-up periods have also limited analysis of longerterm survival and kidney recovery. Importantly, more severe AKI has been strongly and consistently associated with short term risk of death (10, 11) , making AKI a potentially pivotal complication in the deteriorating COVID-19 patient. Better understanding of risk factors involved may enable early recognition and use of preventative and therapeutic measures to limit AKI development or progression reducing overall morbidity and mortality. The mechanisms involved in kidney damage in COVID-19 are likely to be multifactorial. These potentially include direct viral effects on the kidney (12, 13) , systemic immune dysregulation, and hypercoagulability (11, 14) as well as traditional risk-factors for primary tubular injury including hypovolaemia, sepsis related haemodynamic instability, and consequences of intensive care unit (ICU) therapies (8, (15) (16) (17) . Patient risk factors for COVID-19 AKI are common to those of AKI in other settings, including older age and comorbid diseases (1, 18) . Importantly while racial and socioeconomic background have been identified as potential risk factors for SARS-CoV-2 infection and adverse outcomes in COVID-19 disease, the influence of these factors on the incidence of COVID-19 associated AKI is unclear despite pre-existing evidence for racial and socioeconomic disparities leading to higher AKI incidence and poorer outcomes outside of COVID-19 (19) . Accordingly, we aimed to define the incidence and severity of AKI during hospital admission, examine trajectories of AKI progression and kidney recovery following hospital discharge, and identify risk factors for AKI and adverse outcomes including the influence of ethnicity and measures of socio-economic status.@story_separate@We included all patients aged 16 years or over admitted to one of five acute hospitals within Barts Health NHS Trust between 1st January and 13th May 2020 with SARS-CoV-2 infection confirmed on PCR (20) . The first emergency admission encompassing the first positive SARS-CoV-2 test or within two weeks of positive outpatient testing was defined as the index admission. For this analysis, patients with no available creatinine data and patients with prior history of end stage kidney disease (ESKD) were excluded. Follow-up data were available up to 1 st December 2020. AKI was defined and severity staged using the Kidney disease Improving Global Outcomes (KDIGO) creatinine criteria, namely an acute ≥0.3mg/dl rise in serum creatinine within a 48h window or a ≥1.5 fold increase in creatinine above pre-morbid baseline value (21) . Baseline creatinine was set using the median result from all available blood tests in the 7 to 365 days before admission. If no prior results were available, baseline creatinine was imputed based on an eGFR of 75ml/min/1.72m 2 or admission creatinine, whichever was lower. Early AKI was defined as AKI criteria being met within the first 7 days of hospital admission. In patients who survived to day 7, early AKI was further subdivided based on trajectory during admission: Recovered (absence of AKI criteria by day 7 and no further AKI); Relapsed (absence of AKI criteria by day 7, but subsequent re-occurrence of AKI later in admission); and Persistent (persistence of AKI criteria at day 7 or requirement for RRT). Late AKI was defined as AKI occurring only after day 7. Ethnicity was defined using the NHS ethnic category codes. Relative measures of socioeconomic deprivation were assessed using the English Indices of Deprivation 2020 by matching patient postcode to national index of multiple deprivation (IMD) divided into quintiles within our study population (22) . Baseline comorbid diseases and Hospital Frailty Risk Score (HFRS) were identified by mapping to ICD-10 coding (23, 24) . Pre-existing chronic kidney disease (CKD) defined as baseline eGFR below 60ml/min/1·72m 2 was calculated using the last creatinine value available from results earlier than 7 days before hospitalisation using the CKDEpi formula (25) . ESKD was defined by ICD-10 coding (26) . Rockwood Clinical Frailty Scoring (RFS) was assessed by the admitting medical team and recorded in the electronic medical records (27) . The median CRP value for the cohort was used to a cut-off for high and low groups. Full definitions of AKI categories and baseline variables are detailed in the online supplement. The primary outcome was survival to 30-days from time of index COVID-19 hospital admission. We assessed the composite outcome of death, new dialysis, and worsened renal function constituting the major adverse kidney event (MAKE) outcome at 90 days (MAKE90) as a secondary outcome (28) . In 90-day survivors worsened renal function was defined as eGFR <70% of baseline using the last serum creatinine value before 90 days. Other secondary endpoints were 90-day survival, ICU admission, ICU length of stay, duration of organ support on ICU, need for mechanical ventilation, ICU and hospital length of stay, and discharge destination if discharged alive from hospital. A prospective statistical analysis plan was developed (29) . Baseline characteristics are presented as mean and standard deviation, median and interquartile range, or number and percentage, as appropriate. We compared proportions using Pearson's Chi-square test or Fisher's exact test and continuous variables using 2-sample t-test or Wilcoxon rank-sum or signed rank (for paired data) tests, as appropriate. Multivariable logistic regression was carried out to assess risk factors for development of early AKI. Cox proportional hazards models were used to assess survival comparing AKI to no AKI adjusted for age and sex. A further multivariable Cox model was developed to assess the effect of pre-defined risk factors described associated with adverse outcomes in COVID19: ethnicity, IMD quintile, smoking status, body mass index (BMI), diabetes, hypertension (HTN), and CKD. We assessed the effects of an interaction between ethnicity and early AKI on 30-day survival. Logistic regression modelling for the association between early AKI and ICU admission, as well as treatment using mechanical ventilation and RRT were carried out. Survival at 90-days comparing trajectory of AKI was assessed using logistic regression as relapsed-and late-AKI are not baseline hazards. Effect measures are presented as hazard ratios (HR) or odds ratios (OR) with 95% confidence intervals (CI). All analyses were performed using R version 4.0.2 (R Core Team 2020). Subgroup analyses were carried out including only patients with known baseline creatinine. STROBE reporting guidelines were followed (Table S1 ). A total of 1996 patients were included in our cohort, 28 without creatinine results available during their admission and 113 with pre-existing ESKD were excluded from analyses ( Figure S1)  Of 333 patients who developed early AKI and still alive at day 7 after admission: 134 (40.2%) recovered, 47 (14.1%) relapsed, and 152 (45.6%) had persistent AKI at day 7. A total of 105 (8.2%) patients who did not have early AKI and still alive at day 7 developed late AKI after this timepoint. Baseline characteristics are presented in Table S3 . Patients with persistent or relapsed trajectories were younger (median age 65 years persistent, 72 relapsed, 74 recovered). Compared to other ethnic groups, a greater proportion of Asian patients with early AKI had persistent or relapsed AKI than recovered ( Figure S6 ). Patients with persistent or relapsed AKI had higher CRP values (median 330.0 mg/L persistent, 199.0 relapsed, 179.0 recovered). Baseline risk factors for early AKI were also more frequent in patients with late compared to no AKI. In multivariable logistic regression, survival to 90 days was worst for persistent AKI (OR 7.57 [4.50-12.89], p<0.001) ( Figure S7 ). There was no clear evidence for any interaction between ethnicity and early AKI on 30 Associations with survival were similar although in these analyses, the risk of death associated with AKI stage 2 was comparatively greater. In our cohort of first wave hospitalised patients, one in four COVID-19 patients developed early AKI and a further 8% developed late AKI. Over a third of early AKI cases were of stage 3 severity using KDIGO grading. Importantly, adverse outcomes were strongly associated with increased severity and persistence of AKI. The majority of early AKI was persistent meaning 45.6% of early AKI patients surviving to day 7 fulfilled criteria for acute kidney disease (AKD) by the continued presence of a serum creatinine ≥1.5 fold baseline or need for RRT (21) . Importantly, persistent AKI was most strongly associated with ongoing risk of death. In addition, after day 7, relapsed AKI after initial recovery and late AKI were both associated with worse outcomes compared to recovered early AKI. Rates of early COVID associated AKI in this study are similar to previous studies in Europe and the US (8, 15, 30) . As in many previous studies early AKI was associated with poorer short-term survival (7, (30) (31) (32) , however severity and duration of AKI are clearly interlinked with aspects of the underlying COVID-19 disease process. This supports a comparative study of COVID-19 and non-COVID-19 hospitalised patients which demonstrated a higher incidence of AKI and a disproportionate burden of severe AKI in COVID-19 patients (33) . Baseline risk factors for early AKI such as older age, male sex, and diabetes in this study support those of previous studies for both COVID-19 and non-COVID-19 AKI (1, 11, 15, 34) . However, we found that different demographic and comorbidity profiles were associated with higher risk of COVID-19 AKI. In particular male sex and diabetes were associated with greater adjusted-risk, suggesting that AKI could be a complication partially mediating risk of death in these subgroups previously identified as of higher risk of death in COVID-19 disease. We also demonstrate that patients of Black ethnicity were at higher risk of developing AKI and more severe AKI shown in other cohorts, this may be linked to a higher prevalence of CKD in black populations as this association disappeared in favour of CKD when only patients with known baseline creatinine were considered (9) . Similarly, more persistent or relapsing AKI trajectories were also seen in Asian patients who developed early AKI, reflecting a less benign course of AKI in this group. Furthermore, the prevalence of diabetes and CKD were significantly greater at younger ages in Asian and Black patients respectively, potentially increasing risk of AKI in these populations. These observations suggests that patients of Black and Asian backgrounds could in part be at greater risk of death in COVID-19 disease due to greater prevalence of measured and unmeasured risk factors for development and progression of AKI. Our finding that high CRP values were associated with development, greater severity, and worse trajectories of AKI supports hypotheses underlying hyperinflammatory and cytokine release syndrome mediated damage to kidneys in COVID-19 (9, 14) . Some of this dysregulated immune response may be mediated through lung-kidney cross-talk and associations with other markers of systemic disease such as ferritin, d-dimer, and measures of severity of pulmonary disease could help elucidate pathophysiological mechanisms involved (35) . Of particular importance would be if these processes were modifiable through management strategies such as lung protective ventilation (15) . As the impact of COVID-19 continues with surges of cases, limited hospital resources including dialysis availability need to be taken into consideration. Identifying and classifying AKI by timing and trajectory may help in tailoring patient-monitoring, initiation of AKI-prevention and treatment strategies and timely consideration of patients who will benefit from RRT (30, 31). Little is known currently about long-term outcomes and kidney recovery after COVID-19 AKI, with only a small number of studies reporting post-hospital outcome (36) . Reported recovery of kidney function and dependence on RRT following hospital admission has been variable, with recovery at 41% to 65% and RRT dependence at 8% to 33% (37) (38) (39) . While the majority of patients in our study who received RRT and survived recovered independent kidney function, and the number of patients with worsened kidney function at follow-up was small, assessment of kidney function in COVID-19 survivors is likely to be complex. In individual patients there was considerable variability in serum creatinine at follow-up, with some, both with and without early AKI, developing new significantly elevated creatinine compared to baseline. Conversely many other patients, in all AKI categories, had creatinine levels substantially lower than baseline both at hospital discharge and afterwards. This suggests that both worsened renal function and substantial loss of muscle mass (and creatinine generation) may be occurring in COVID-19 survivors potentially confounding assessment of kidney function by serum creatinine in COVID-19 survivors. Longer-term follow-up is required to better understand recovery mechanisms for COVID-19 AKI which may differ from other forms of AKI (30) . This may require methods for assessing renal function less confounded by the effects of prolonged major illness, such as cystatin-c (40) . This study comprises a comprehensive analysis of the epidemiology and timing of COVID-19 associated AKI in a large and ethnically diverse urban population. The detailed dataset allowed inclusion of multiple demographic and baseline risk factors into multivariable analyses. However, more data on differences in clinical management, medication use including pre-existing renin-angiotensin system (RAS) blockade, ventilation, and haemodynamic parameters could help to identify specific AKI causes. The follow-up period was longer compared to the majority of previous reports, however, important long-term outcomes such as development and progression of CKD and ESKD over the following years are not yet available. Definitions of AKI were based only on biochemistry results and there was no data on urine output or other markers of kidney health including proteinuria. Furthermore, imputation of baseline creatinine values was required for 675 (36.4%) patients lacking pre-admission results, however this was a much smaller proportion compared to many other AKI studies and we carried out a sensitivity analysis excluding those with unknown baseline. Finally, while AKI is associated with adverse outcomes and we hypothesise that it could be an important pathway in mediating risk of death and severity of illness in COVID-19 disease, the direction of causality in these associations cannot be proven in an observational study. No funding was received for conducting this study. All authors have completed the ICMJE uniform disclosure form and have no relevant financial or non-financial interests associated with the submitted work to disclose. The results presented in this paper have not been published previously in whole or part, except in abstract form. The statistical analysis plan can be accessed online. The authors will be happy to consider additional analyses of the anonymised dataset on request. The need for stringent measures to prevent re-identification of individuals within a discrete geographical location and limited time-period however preclude sharing of patient level dataset in a GDPR compliant form.  Study approval was obtained from the Health Research Authority and protocols reviewed by the Research Ethics committee. No patient level consent was required for this study as it involved the aggregated analysis of an anonymised dataset collated by members of the direct care team. Table 1 . Cohort baseline characteristics grouped by early AKI status, n (%) unless otherwise stated. Total n=1,855 unless otherwise stated. P values based on Chi-square (for categorical) or Kruskal-Wallis test (for continuous) comparing early AKI all or by stage to no early AKI. SD: standard deviation, IQR: interquartile range, IMD: index of multiple deprivation, CKD: chronic kidney disease, HTN: hypertension, COPD: chronic obstructive pulmonary disease, CRP: C-reactive protein, ICU: intensive care unit, RRT: renal replacement therapy. @story_separate@Characterisation of COVID-19 AKI by timing and trajectory could help risk stratification and target monitoring, prevention and treatment strategies. This study highlights continued need for vigilance and monitoring of AKI throughout all stages of COVID disease, especially in subgroups with higher risk of death in COVID-19 disease. Finally, longer-term follow-up and better understanding of kidney recovery after COVID-19 is warranted as outcomes are currently unclear.","BACKGROUND: Acute kidney injury (AKI) is a common and important complication of COVID-19. Further characterisation is required to reduce both short and long-term adverse outcomes. METHODS: We examined registry data including adults with confirmed SARS-CoV-2 infection admitted to five London Hospitals from 1(st) January to 14th May 2020. Prior end-stage kidney disease was excluded. Early AKI was defined by Kidney Disease Improving Global Outcomes (KDIGO) creatinine criteria within 7 days of admission. Independent associations of AKI and survival were examined in multivariable analysis. Results are given as odds ratios (OR), or hazard ratios (HR) with 95% confidence intervals. RESULTS: Amongst 1855 admissions, 453 patients (24.5%) developed early AKI: 220 (44.0%) stage 1, 90 (19.8%) stage 2, 165 (36.3%) stage 3 (74 receiving renal replacement therapy). The strongest risk factor for AKI was high CRP (OR 3.35 [2.53-4.47], p < 0.001). Death within 30 days occurred in 242 (53.2%) with AKI compared to 255 (18.2%) without. In multivariable analysis, increasing severity of AKI was incrementally associated with higher mortality: stage 3 (HR 3.93 [3.04-5.08], p < 0.001). In 333 patients with AKI surviving to day 7, 134 (40.2%) recovered, 47 (14.1%) recovered then relapsed, and 152 (45.6%) had persistent AKI at day 7; an additional 105 (8.2%) patients developed AKI after day 7. Persistent AKI was strongly associated with adjusted mortality at 90-days (OR 7.57 [4.50-12.89], p < 0.001). CONCLUSIONS: AKI affected one in four hospital in-patients with COVID-19 and significantly increased mortality. Timing and recovery of COVID-19 AKI is a key determinant of outcome."
"The mathematical models we are considering in this paper are based on differential equations whose solutions preserve some underlying geometric structure. The design and analysis of numerical integrators that preserve the qualitative features of the underlying differential equations is the subject of Geometric Numerical Integration (Blanes & Casas 2016 , Hairer, Lubich & Wanner 2010 , Iserles 2009 , Sanz-Serna & Calvo 1994 . We are not only concerned with the accuracy and stability of numerical schemes but also with their geometric properties, which reflect important features of the phenomena being modelled. This endows the integrators with an improved qualitative behaviour, but also typically leads to significantly more accurate results.@story_separate@Numerical integration of mathematical models is an essential step in the implementation and analysis of population models: chemical reactions (see for example (Edsberg 1974 , Sandu 2001 or ), biochemical systems (Bruggeman, Burchard, Kooi & Sommeijer 2007) , and the evolution of epidemics (Kermack & McKendrick 1927 ) (see also (Giordano, Blanchini, Bruno, Colaneri, Di Filippo, Di Matteo & Colaneri 2020) and references therein). Such models are usually formulated as a system of Ordinary Differential Equations (ODEs) where f (t, y) is, in the context of this paper, consistent with two requirements of the application being modelled. Firstly, the solution satisfies the condition (with y = (y 1 , . . . , y d ) ⊤ , y 0 = (y 0 1 , . . . , y 0 with c a constant (we may assume without loss of generality that c = 1). This is referred to as mass preservation. Second, if y 0 i ≥ 0, i = 1, . . . , d then we have positivity preservation: y i (t) ≥ 0 ∀t, i = 1, . . . , d. Although the focus of this article is ODEs, positivity preservation is a much wider challenge. For example, the Nobel prize winning Black-Scholes model in finance is a stochastic differential equation with positive solutions, but standard numerical solvers, such as the Euler-Maruyama method, fail to preserve that positivity. The Kolmogorov Lecture at the Ninth World Congress in Probability and Statistics concerned methods for preserving positivity in the setting of the stochastic Langevin equations (Leite & Williams 2019) . A useful way to envisage mass and positivity preservation is that for every t ≥ 0 the state variable y(t) is a discrete probability distribution of d species. As we will show in Proposition 1, these properties can be preserved if the vector field can be written in the form f (t, y) = A(t, y)y where the matrix A : R × R d → R d×d is an integrable graph Laplacian. Definition An n × n real matrix A is a graph Laplacian if it has the following properties: Property 1 (pattern of signs) A k,ℓ ≥ 0 for k, ℓ = 1, . . . , n, k = ℓ, A k,k ≤ 0 for k = 1, . . . , n and Property 2 (zero column sum) n k=1 A k,ℓ = 0 for ℓ = 1, . . . , n. We denote the set of all n × n graph Laplacians by L n . The same term 'graph Laplacian' is used with different meanings in the literature -in our work, we allow the graph Laplacian to be nonsymmetric. For simplicity, we consider the autonomous case. (The general nonautonomous case can be considered similarly, as we show by example in (2.6).) For the remainder of this article, we focus on the solution of the nonlinear ODE y ′ = A(y)y, y(0) = y 0 ∈ R d . (1.1) 1 ⊤ y ′ (t) = 1 ⊤ A(y(t))y(t) = 0 ⊤ y(t) = 0 implies that 1 ⊤ y(t) ≡ const = 1 ⊤ y 0 = 1. To prove the statement about positivity, we consider any t * ≥ 0 such that there exists k * ∈ {1, 2, . . . , d} with y k * (t * ) = 0 and such that y(t * ) 0 -clearly, unless such t * exists, y(t) stays forever in the nonnegative cone. Note that it is perfectly possible for t * to be zero, also it is possible that several components of y(t) vanish at t = t * , this makes no difference to our argument. We note that, by (1.1), A k * ,ℓ (y(t * ))y ℓ (t * ) ≥ 0, because A is a graph Laplacian, so off-diagonal entries are nonnegative. Therefore y k * cannot change sign at t * , and it must stay in the nonnegative cone. ✷ Remark. Note in the proof of Proposition 1 that property 1 of the definition of the Laplacian gives mass preservation, and that property 2 of the definition of the Laplacian (pattern of signs) gives positivity. In particular, if the matrix A(y) has the same pattern of ± signs as a Laplacian (but we make no assumption on the column sums of A(y)), then it is still true that solutions of y ′ = A(y)y, preserve positivity. This matters later, for example, for our Theorem 7. This also matters in many, but not all, applications to chemical kinetics: compare, say, the Robertson's reactions we give in (1.2) to the oscillatory example related to the MAPK cascade that we give in (1.7), or the atmospheric chemistry example (4.1) we give later. The Robertson's reactions fit our Laplacian model (1.1) where the matrix A(y) satisfies both of the defining requirements of a Laplacian (i.e. it has both the correct pattern of signs, and also zero column sum), whereas our MAPK cascade example (1.7), and our atmospheric chemistry example (4.1), are an example of a matrix A(y) that has the same pattern of signs as the Laplacian but does not satisfy the zero column sum requirement. Let us now consider some properties of graph Laplacian matrices that allow us to deduce additional qualitative properties of the solution of (1.1). Theorem 2 Let A ∈ L n . Then it has an eigenvalue at the origin, which is simple if A is irreducible, and all its other eigenvalues reside in C − = {z ∈ C : Re z < 0}. Proof Since 1 ⊤ A = 0 ⊤ , it follows that 0 ∈ σ(A). To locate the remaining eigenvalues we use the Gerschgorin theorem, applying it to columns (typically it is applied to rows, but this makes no difference). Thus, letting we have σ(A) ⊂ n ℓ=1 S ℓ . By the definition of graph Laplacian, all Gerschgorin discs live in cl C − and adjoin iR only at the origin. Therefore σ(A) \ {0} ∈ C − . It remains to prove that 0 is a simple eigenvalue. Let α = min k=1,...,n A k,k , then the entries of B = A − αI = O are all nonnegative. Therefore, according to Frobenius-Perron theory (Berman & Plemmons 1979) , irreducibility implies that the largest in modulus eigenvalue of B is positive and simple. Since this is −α, it follows that 0 is a simple eigenvalue of A. ✷ Incidentally, one of the less well-known formulations of the Gerschgorin theorem states that if A is irreducible then an eigenvalue might be on the boundary of one Gerschgorin disc only if it is on the boundary of all Gerschgorin discs -this is certainly the case with 0. Proposition 3 Assume the matrix A ∈ L n is symmetric. Then d y(t) 2 / dt ≤ 0. where α + (B) is the spectral abscissa -the eigenvalue of the matrix B with the largest real part (which in the case of A is real because of the Perron-Frobenius theory). This is true because α + (B) ≥ v ⊤ Bv/ v 2 for any square matrix B and a nonzero vector v. Since our A(y) is graph Laplacian, it follows at once from the Gerschgorin theorem that α + (A(y(t))) ≤ 0 and, since 0 ∈ σ(A(y(t))), we deduce that d y(t) 2 / dt ≤ 0. ✷ Letŷ be the eigenvector corresponding to the simple eigenvalue 0. In the symmetric case, it is clear that y(t) −ŷ 2 is a monotonically decreasing functionusing the fact that A(y(t))ŷ = 0, and we continue as before. In the nonsymmetric case, the issue of stability needs more discussion. The two defining properties of the graph Laplacian together ensure that the columns of the matrix exponential are probability vectors, so that when A is a constant matrix, in the 1-norm we always have exp(tA) = 1, t ≥ 0. These matrices are sometimes known as W-matrices in the statistical physics literature and, by studying the adjoint z ′ (t) = A ⊤ z(t) -with arguments similar to those of our Proposition 1 -it is known that the minimum of the solution z is increasing, and that the maximum is decreasing. In the 2-norm, a sufficient condition for strong stability of y ′ = Ay(t) with solution y(t) = exp(tA)y(0), is that (A + A ⊤ ) be negative definite. Note that this condition is more restrictive than merely the assumption that the eigenvalues of A have negative real part (because then it would still be possible that (A + A ⊤ ) had a positive eigenvalue). This issue of stability is related to 'the hump' in the classical literature on the numerical analysis of the matrix exponential, and to the lognorm, and also to the subject of pseudospectra. Nonsymmetric graph Laplacians exhibit significant pseudospectra, manifesting in various ways, such as a more subtle stability analysis, and the failure of standard eigenvalue algorithms (Iserles & MacNamara 2019 , MacNamara 2015 , Macnamara, Blanes & Iserles 2020 . A sufficient condition for stability of operator splitting methods is that each part separately be strongly stable, although this may be too pessimistic in practice. For operator splitting methods, the graph Laplacian can sometimes be expressed as the sum of two matrices, each of which is separately a graph Laplacian, as we consider later in (2.1), with a physical interpretation (MacNamara, Bersani, Burrage & Sidje 2008a). In general, operator splitting, such as in (2.4), does not preserve the steady state (Speth, Green, MacNamara & Strang 2013 ) -so it is worth pointing out that the novel splitting methods that we introduce in this work, for example later in (2.8), in our numerical experiments, do have the desirable property that they preserve the steady state. In the nonautonomous case, it can still be shown that the difference of any two solutions is decreasing in the 1-norm, but the issue of stability is much more delicate (Earnshaw & Keener 2010a ). To illustrate our analysis we consider several simple models from the literature. y ′ 1 = −0.04y 1 +10 4 y 2 y 3 , y 1 (0) = 1 y ′ 2 = 0.04y 1 −10 4 y 2 y 3 −3 · 10 7 y 2 2 , y 2 (0) = 0 y ′ 3 = 3 · 10 7 y 2 2 , y 3 (0) = 0, (1.2) that, rewritten in a vector form, read d dt where the matrix is graph Laplacian. This example fits into the framework of Theorem 7, which comes later. Note that the system can also be written in many different ways, for example d dt 04 0 10 4 y 2 0.04 −3 · 10 7 y 2 −10 4 y 2 0 3 · 10 7 y 2 0 where now the matrix is no longer a graph Laplacian. As we will see, it is crucial to write properly the equations for the numerical solutions to preserve their qualitative properties. Example 2: The SIR model. The Susceptible-Infected-Recovered (SIR) model describes the temporal epidemic evolution in terms of three variables for the population: S(t): (Susceptible), I(t) (Infected) and R(t) (Recovered). It is usually asssumed that the total population does not change during the infection period. S, I and R denote the fractions with respect to the total population: S(t)+I(t)+R(t) ≡ 1. This model was proposed by Kermack & McKendrick (1927)  where R 0 > 0 is the basic reproduction number, and the system can be written in the form d dt where the matrix is evidently a graph Laplacian. Example 3: Laplacian dynamics on graphs (autonomous and linear). Graph Laplacian dynamics, x ′ = L(G)x, where L(G) is a constant matrix, representing the Laplacian of a directed graph G, gives rise to a large class of applications in biochemical kinetics, including Michaelis-Menten enzyme kinetics, allosteric enzymes, G-protein coupled receptors, ion channels, and gene regulation (Gunawardena 2012, equation (3)). Discussion of conditions under which such linear systems always converge to a steady state, and discussion of the senses in which that might be considered unique is given in (Mirzaev & Gunawardena 2013) . That linear setting x ′ = L(G)x is a special case of the more general framework here where we focus on the exact nonlinear model in (1.1). Example 4: Cardiac ion channels (nonautonomous and linear). Nonautonomous Laplacian systems, y ′ = A(t)y, have many important applications, including cardiac ion channel kinetics (Earnshaw & Keener 2010a , Earnshaw & Keener 2010b . In special cases, there are also exact solutions for the dynamical solutions, such as the explicit Magnus formulae in (Iserles & MacNamara 2019) , and closely related invariant manifolds of binomial-like solutions. Example 5: MAPK cascade (autonomous and nonlinear). The mitogen activated protein kinase (MAPK) cascade is fundamental in cell signalling biology and cancer biology, and it is modelled by eighteen differential equations where rates are given by the Law of Mass Action, together with some linear conservation laws (Qiao, Nachbar, Kevrekidis & Shvartsman 2007). By our Theorem 7 that comes later, this MAPK model fits our framework of (1.1), subject to the remarks we make following Proposition 1. The Laplacian dynamics mentioned in the above constant coefficient and linear examples, where convergence to a steady state is common (Mirzaev & Gunawardena 2013) , makes it tempting to conjecture that the model we focus on here in (1.1), likewise always converges to a steady state. However, a counter example is provided by the MAPK cascade, which can be modelled by our nonlinear Laplacian dynamics (1.1), and which has been shown by numerical simulations to exhibit both bistability and also oscillations (Qiao et al. 2007) . We have taken the model of (Hadač, Muzika, Nevoral, Přibyl & Schreiber 2017, (17)), which is closely related to the MAPK cascade, and rewritten it here in the form of our model (1.1), to show that it is clearly an example of the Laplacian dynamics that we study in this paper: (1.7) Try for example, rate constants k 1 = 100 3 , k 2 = 1 3 , k 3 = 50, k 4 = 1 2 , k 5 = 10 3 , k 6 = 1 10 , k 7 = 7 10 , and initial state y(0) = [0.1, 0.175, 0.15, 1.15, 0.81, 0.5] ⊤ . Note that we have y ′ = A(y)y, where the matrix A(y) has the same pattern of signs as a Laplacian, but that a column of A(y) does not always sum to zero, so this fits our framework of (1.1), subject to the remarks we make following Proposition 1, and this is also an example of our later Theorem 7. This model still possesses two conservation of mass laws, namely both y 1 + y 4 + y 6 and y 2 + y 3 + y 4 + y 5 are constants, which have physical interpretation in terms of the total enzyme of two types of kinases. Those two conservation laws correspond to w 1 = [1, 0, 0, 1, 0, 1] ⊤ , and w 2 = [0, 1, 1, 1, 1, 0] ⊤ , respectively. Note that we have It should be possible to use methods based on matrix exponentials (such as the methods proposed in this paper) to respect the second conservation law, because w ⊤ 2 A(y) = 0, so w ⊤ 2 exp(tA(y)) = w ⊤ 2 . However, because w ⊤ 1 A(y) = 0, it will be difficult (and probably impossible) to maintain exactly the first conservation law by methods that compute matrix exponentials. 2 Something similar to this situation arises with the stratospheric reaction example (4.1), in the sequel. This is typical of applications in chemical kinetics, and for example, the famous Michaelis-Menten enzyme kinetics model (which always converges to a unique and simple steady state) also fits the framework, with a matrix that has the same pattern of signs as a Laplacian, but that does not have zero column sum, and the model still has two simple well-known linear conservation laws. Significantly, by numerical simulation, it has been shown that solutions of this model (1.7) show oscillations (Hadač et al. 2017, Fig 5) . To sum up, the solution of a mathematical model given by (1.1) with y 0 0 and where A(y) is a graph Laplacian matrix (assuming y 0) always preserves mass and always preserves positivity. Often, the model (1.1) is also stable and converges to a steady state. These features correspond to the phenomenological desiderata in epidemilogical models. In theory, there are always exact formulae for the right eigenvector corresponding to the zero eigenvalue of a nonsymmetric graph Laplacian matrix A, via the Matrix-Tree Theorem (Gunawardena 2012) . This is the steady state of the corresponding linear Laplacian dynamical system, and in special cases, there are also formulae for the dynamical solutions, such as mentioned in the above examples (Earnshaw & Keener 2010a , Earnshaw & Keener 2010b , Iserles & MacNamara 2019 . Unfortunately, in general, the exact solution of these dynamical systems is unknown, so we need to resort to numerical algorithms. A numerical method can be seen as the exact solution of a perturbed model (backward error analysis). Unless the numerical method is chosen carefully, the numerical solutions are highly unlikely to respect the important special structure of (1.1). For example, in (Giordano et al. 2020 ) the authors consider a mathematical model for the COVID-19 epidemic in Italy, while paying much attention so that the proposed model has the structure of (1.1), but then numerically solve it using the first order explicit Euler method where h is the time step and y n ≃ y(t n ) with t n = t 0 + nh. This method can be seen as the exact solution of the perturbed problem and f ′ denotes the Jacobian matrix. We easily see that and then the mass is preserved (this is also the case for most standard methods like Runge-Kutta or multistep methods). However, f 2 (y) = A 2 (y)y with A 2 being a graph Laplacian matrix, so positivity is not assured and in general will fail. In addition, the method is only conditionally stable. In (Bolley & Crouzeix 1978) it is shown that within the class of linear multistep and Runge-Kutta methods unconditional positivity restricts the order of the method to just one. 3 For non-stiff problems and for relatively short time integration, an Euler method, or any other standard method, can provide sufficiently accurate, satisfactory results. However, if a mathematical model is stiff (this is typical to equations of chemical kinetics e.g. (1.2)) or need be solved for long time intervals, standard methods may give negative solutions or become unstable. While the stiffness in chemical kinetics equations can be dealt with using implicit methods and mass is preserved by most numerical methods, positivity remains an outstanding challenge. The most efficient solvers considered in for low to medium accuracy in the numerical solution of stiff kinetic equations are Rosenbrock methods. In addition, they are among the simplest implicit schemes to be implemented in a code, yet they fail to preserve positivity. Note that there exist exponential Rosenbrock-type methods (Hochbruck, Ostermann & Schweitzer 2008/09) that involve the computation of the exponential of the Jacobian. However, in general, this Jacobian is not graph Laplacian and positivity cannot be guaranteed. The objective of preserving mass and positivity in numerical integration, in particular within the context of chemical kinetics, received a measure of attention, although perhaps less than it deserves given its importance in applications. An obvious device to avoid the solution from becoming negative is clipping: the practice of converting a negative component to zero. This, of course, interferes with the preservation of mass but the latter can be recovered using laborious optimization procedure in every time step (Sandu 2001) . The effects of this costly algorithm on long-term accuracy and stability are unknown. Another approach toward preservation of mass and positivity are the Runge-Kutta-Patankar methods (Burchard, Deleersnijder & Meister 2003 , Kopecz & Meister 2018 , Patankar 1980 . The idea is to use Runge-Kutta-like methods for productiondestruction systems in chemical kinetics, of the form Although this is a second-order method, it is outside the scope of the established theory of numerical ODEs. Its stability and, indeed, convergence are unknown. In this work we present new schemes that are assured to preserve mass and positivity and which have good stability properties. Let us first consider the particular case in which the matrix A is constant. Then the exact solution is given via the exponential of a graph Laplacian matrix: It is a consequence of Theorem 1 that σ(e tA ) ⊂ {z ∈ C : |z| ≤ 1}, hence the solution is stable (subject to the discussion of stability we gave earlier, in the nonsymmetric case). The exponential of a graph Laplacian matrix is fundamental to the work of this paper, and this calls for a more detailed study of its qualitative properties. We need the following elements of the Perron-Frobenius theory (Berman & Plemmons 1979, p. 26-27) . Let B ∈ R d×d , B O. Then ρ(B) is an eigenvalue of B and we can choose the corresponding eigenvector v such that v 0. Moreover, if in addition B is irreducible then ρ(B) is a simple eigenvalue and v is the only eigenvector of B with nonnegative entries. Let a * = min i=1,...,d A i,i < 0 and setÃ = A − a * I. Then e tA = e ta * I+tÃ = e ta * e tÃ . SinceÃ O, all its nonnegative powers are also nonnegative and we deduce that e tÃ O. Therefore e tA O. Indeed, the Mittag-Leffler matrix function of a graph Laplacian, E α (At α ), is likewise a stochastic matrix, i.e. E α (At α ) O, and all entries are positive, and columns sum to unity (Macnamara, Henry & Mclean 2017) . Here the Mittag-Leffler function E α (z) = ∞ k=0 z k /Γ(αk + 1) is a one-parameter generalisation of the exponential, and the exponential is recovered as the special case once α → 1. Furthermore, when A is Laplacian then the pattern of signs in the resolvent, and the properties of M -matrices, show that for large n, all entries of the matrix I − t n A −n are nonnegative. This suggests the results we derive here may be extended to more general settings. Additionally, once A is irreducible and we denote by w 0 the left eigenvector of A corresponding to the zero eigenvalue, then w ⊤Ã = −a * w ⊤ . Since a * < 0, we deduce that |a * | = ρ(Ã) and w = 1. Proof By the Gerschgorin theorem applied to the columns ofÃ (or the standard Gerschgorin theorem applied toÃ ⊤ ) and because A j,i ≥ 0 for i = j, we have and the proof follows. ✷ Therefore 1 ⊤ e tA = 1 ⊤ . Proposition 5 Let R d ∋ p 0 be such that 1 ⊤ p = 1 and set q = e tA p. Then for every t ≥ 0 q 0 and 1 ⊤ q = 1. Proof We deduce at once that q 0 because e tA ≻ O. Moreover, 1 ⊤ q = 1 ⊤ e tA p = 1 ⊤ p = 1, concluding the proof. ✷ Remark Note that all previously stated results apply to maps of the form z = e σS x where x 0, S is a graph Laplacian and σ is a non-negative constant. Since S can have large negative eigenvalues, taking negative values of σ is likely to lead to a poorly conditioned problem and this compels us to avoid this choice. In the sequel we propose several methods that involve maps of the form e σS with S being graph Laplacian, and we will see that condition σ > 0 limits the order of the methods to two in the time step, an order barrier. A splitting of a separable system Suppose the system (1.1) can be written in the form where A 1 , A 2 are graph Laplacian matrices (for y 0) and t (y 0 ), respectively, or can be easily and efficiently solved numerically. Then the map provides a first order method in the time step h that preserves all qualitative properties, while the symmetrised Strang splitting corresponds to a symmetric second order method. Both methods are simple and highly efficient means toward the solution of non stiff problems. Example Let us consider the SIR model that for moderate values of R 0 is a non-stiff problem. We can split it, for example, in the form where both matrices are graph Laplacian and their exponentials have a simple explicit expression. Similar splitting can be considered for the Robertson's equations (1.2) but, since this problem is very stiff, very small time steps are necessary to get accurate results: doing this efficiently is an open problem that requires further investigation and is outside the scope of this paper. There exist higher order methods in the literature, but all of them necessarily have either time steps that involve complex numbers (and there is not yet a good numerical method in the literature with time steps being complex), or negative time steps that correspond to backward time integration (Blanes & Casas 2005) , whereby neither the preservation of qualitative properties nor stability can be in general guaranteed. In the non-autonomous case we can convert the independent variable t in the vector field into two dependent variables in the form d dt where y 1,t (t 0 ) = y 2,t (t 0 ) = t 0 , thereby separating (2.5) into two autonomous systems with the correct solution for y(t). Splitting for a general system For stiff problems it is more convenient to proceed as follows (Blanes 2019) . Let us consider the following system in the extended space where x(t) = y(t) = z(t). The system is separable into two solvable parts A : We solve the system with the symmetric second order Strang splitting method (2.4), i.e. advance half a step with A followed by a step with B and conclude with another half a step with A: (2.7) x 1 = e 1 2 hA(z1) x 1/2 . Note that x 1 and z 1 correspond to symmetric second order approximations: z 1 can be seen as the exponential midpoint and x 1 as the exponential trapezoidal rule. Since z 0 ≻ 0, the frozen matrix A(z 0 ) is a graph Laplacian, therefore x 1/2 0 and preserves the 1-norm, and similarly for z 1 and x 1 . A more accurate result is obtained with the smoothing technique, i.e. taking the solution for the next step as the average where again the 1-norm is preserved and all components of y 1 are nonnegative. The Lie group structure is not preserved by this linear combination, but this is not a property that concerns us in the present context. In addition, the difference x 1 − z 1 can be taken as an estimate of local error, using the scheme as a variable time-step algorithm in order to get more accurate results. The non-autonomous case Let us now consider the non-autonomous system y ′ = A(t, y)y, y(0) = y 0 . This occurs, for example, when the chemical reaction takes place at variable temperature and the coefficients k i (t) are time dependent or when the parameter R 0 in the SIR model changes due to political decisions, variations in behaviour or the evolution of a pathogen. In this case we duplicate the system, but taking the time as two dependent variables where x(t) = y(t) = z(t) and x t (t) = z t (t) = t: the system is now autonomous and separable into solvable parts: the outcome is an algorithm similar to (2.7), (t0+h,z1) x 1/2 , and finally y 1 = 1 2 (x 1 + z 1 ). (2.9) A more general procedure to construct higher-order methods is to consider Magnus integrators. Let A : R d → R d×d be integrable. We consider the equation (2.10) and suppose that y n ≈ y(t n ). One approach toward the solution of (2.10) is , therefore y [1] (t) = e (t−tn)A(yn) y n and we obtain the first-order method y n+1 = e hnA(yn) y n . (2.12) Letting m * = 2 leads to a second-order method y [2] ′ = A(e (t−tn)A(yn) y n )y [2] , whose Magnus solution truncated to the first term that provides second order approximations in the time step is A(y n ) + A(e (t−tn)A(yn) y n ) y n (note that the approximation of the integral with the trapezoidal rule is fully consistent with second order). This results in the second-order method y n+1 = exp h n 2 A(y n ) + A(e hnA(yn) ) y n y n . If we consider instead the midpoint rule we have y n+1 = exp h n A(e 1 2 hnA(yn) y n ) y n . (2.13) Note that this scheme coincides with z 1 in (2.7) for the first step. This method requires only two exponentials but it is not time symmetric. If it is important to preserve time symmetry, the three-exponential method (2.7) should be used, otherwise this simple and cheaper scheme suffices. Continuing in this vain, and a fourth-order Magnus reads The temptation is now to discretise using standard Magnus quadrature at Gauss-Legendre points but this does not work because the definition of A 2 itself contains an integral. Moreover, the critical issue is the dependence of A 2 on t, not on y n . We approximate The simplest solution is to approximate that integral also by two-point Gauss-Legendre (note that the interval of integration in the inner integral is of length ( 1 2 − √ 3 6 )h n and we need to adjust quadrature points), whereby Brief explanation: the first integral is in the interval [t n , t n +( 1 2 − √ 3 6 )] and the Gauss-Legendre nodes 1 2 ± √ 3 6 need be multiplied by the length of the interval. Ditto in the second interval, [t n , t n + ( 1 2 + √ 3 6 )] and we are saved a single function evaluation because, by happy coincidence, real numbers commute and ( 1 2 − √ 3 6 ) = 1 6 . Thus, altogether we need three function evaluations, one more than standard Magnus. Note moreover that the integration in A 1 is explicit, A 1 (t) = A(e (t−tn)A(yn) ). We note that B i , i = 1, 2, are graph Laplacians, but this need not be the case for their commutator [B 1 , B 2 ]. This problem can be bypassed using commutatorfree Magnus integrators (Alvermann & Fehske 2011 , Blanes, Casas & Thalhammer 2017 . We describe briefly, using an example, the construction of commutation-free integrators, based upon the work of (Blanes et al. 2017) . We approximate the solution across a single time step by 3 and the algorithm is given by 6 )h n A(y n ) y n , A 1,1 = A(x 1 ), x 2 = exp 1 6 h n A(y n ) y n , A 1,2 = A(x 2 ), x 5 = exp ( 1 4 + √ 3 12 )h n (A 1,2 + A 1,3 ) y n , B 2 = A(x 5 ), x 6 = exp 1 2 h n (αB 2 + βB 1 ) y n , y n+1 = exp 1 2 h n (βB 2 + αB 1 ) x 6 . (2.14) This is a seven-exponential method that might be useful when highly accurate results are desired and the cost of each exponential is not excessive. It preserves positivity for moderately stiff problems since it is conditionally positivity preserving. Specifically, positivity is preserved as long as the matrices αB 2 + βB 1 , βB 2 + αB 1 are graph Laplacians (Macnamara et al. 2020) . Unfortunately, β < 0 but α/|β| = 7 + 4 √ 3 ≃ 14, and unless B 1 , B 2 drastically change in a short time interval or their sparsity structure is 'unlucky', their linear combinations are likely to inherit graph-Laplacian structure. In other words, while preservation of graph Laplacians for this third-order method is not assured, it is highly likely in practice. Finally, we present the Magnus integrators to be used on non-autonomous problems. The first-order method is, obviously y n+1 = e hnA(tn,yn) y n . (2.15) The trapezoidal second-order method is given by while the corresponding second-order midpoint rule method is y n+1 = exp h n A(t n + h n 2 , e 1 2 hnA(yn) y n ) y n . (2.16) A third-order commutator-free method can be obtained following the same approximations as previously and taking, for example, the midpoint rule when approximating the intermediate integrals that still ensured the third order of accuracy for the method, giving the following algorithm 12 )h n (A 1,2 + A 1,3 ) y n , B 2 = A(t n + ( 1 2 + √ 3 6 )h n , x 5 ) x 6 = exp 1 2 h n (αB 2 + βB 1 ) y n , y n+1 = exp 1 2 h n (βB 2 + αB 1 ) x 6 . (2.17) Given an ODE system of the form with suitable initial conditions y(0) 0, 1 ⊤ y(0) = 1, we seek conditions so that it can be written in the form (1.1), where the matrix A(y) is a graph Laplacian, namely that for every nonnegative y such that 1 ⊤ y = 1 it is true that A k,k (y) ≤ 0 and A k,ℓ (y) ≥ 0, ℓ = k. Moreover, we seek constructive means of deriving such a matrix A, given (3.1). Our first observation is that the representation of (3.1) in the form (1.1) is additive, in the sense that if we can do so for two different right-hand sides of (3.1), we can do so for their sum. By the same token, if we can do so separately for the first sum and the second, double sum in (3.1), all we need is simply add the two representations. The first sum is trivial and corresponds to the constant-matrix representation y ′ = By, where B = (b ℓ k ) is a graph Laplacian. Consequently, the task at hand reduces to the derivation of a representation (1.1) of the system With greater generality, we may just as well consider the multinomial ODE system with initial conditions y(0) = y 0 0, 1 ⊤ y 0 = 1. Again, the challenge is to write it in the form (1.1) with a graph Laplacian A(y) and, again, we can use the same argument to split the task at hand into a sum of homogeneous problems of the form for j = 2, . . . , m -the case j = 1 is trivial. The problem, though, is that (3.2) can be written in the form (1.1) in a multitude of ways -indeed, even the coefficients a k ℓ are not unique. This can be seen in the simplest nontrivial case, d = 2 and j = 2: y ′ 1 = a 1,1 1 y 2 1 + (a 1,2 1 + a 2,1 1 )y 1 y 2 + a 2,2 1 y 2 2 , y ′ 2 = a 1,1 2 y 2 1 + (a 1,2 2 + a 2,1 2 )y 1 y 2 + a 2,2 2 y 2 2 . Therefore A(y) = a 1,1 1 y 1 + β 1,1 y 2 β 1,2 y 1 + a 2,2 1 y 2 a 1,1 2 y 1 + β 2,1 y 2 β 2,2 y 1 + a 2,2 2 y 2 , where β 1,1 + β 1,2 = a 1,2 1 + a 2,1 1 , β 2,1 + β 2,2 = a 1,2 2 + a 2,1 2 . We deduce that in this case the graph-Laplacian conditions (which must hold for all y 0) are a 1,1 1 , a 2,2 2 , β 1,1 , β 2,2 ≤ 0, a 1,1 1 + a 1,1 2 = a 2,2 1 + a 2,2 2 = β 1,1 + β 2,1 = β 1,2 + β 2,2 = 0. Six equalities (inclusive of (3.3)) and four inequalities for eight variables: impossible in some configurations, while other configurations lead to an infinity of solutions. Henceforth we let e i stand for the ith unit vector. Theorem 6 The ODE system admits the graph Laplacian representation (1.1) subject to the assumptions Proof We prove the theorem by constructing explicitly a graph Laplacian A(y), letting All that remains is to prove that A(y), as defined in (3.8), is indeed a graph Laplacian. Thus, recalling that y 1 , . . . , y d ≥ 0 and that k is computed modulo d, A k,k (y) = a 2e k k y k + a e k +e k+1 k y k+1 ≤ 0 because of (3.5) and (3.6). These two conditions also imply that A k,ℓ (y) = a 2e ℓ k y ℓ + a e ℓ +e ℓ+1 k y ℓ+1 ≥ 0, k = ℓ. Finally, it follows from (3.7) that In this paper we focus only on equations (3.4). The situation is more subtle for higher-order equations. For example, consider the case d = 2, m = 3 and y ′ 1 = −α 3,0 2 y 3 1 + α 2,1 1 y 2 1 y 2 + α 1,2 1 y 1 y 2 2 + α 0,3 1 y 3 2 , y ′ 2 = α 3,0 2 y 3 1 − α 2,1 1 y 2 1 y 2 − α 1,2 1 y 1 y 2 2 − α 0,3 1 y 3 2 . The most general way of writing it in the form (1.1) is with the matrix A(y) = −α 3,0 2 y 2 1 −β 2,1 2,1 y 1 y 2 −β 1,2 2,1 y 2 2 (α 2,1 1 +β 2,1 2,1 )y 2 1 +(α 1,2 1 +β 1,2 2,1 )y 1 y 2 +α 0,3 1 y 2 2 α 3,0 2 y 2 1 +β 2,1 2,1 y 1 y 2 +β 1,2 2,1 y 2 2 −(α 2,1 1 +β 2,1 2,1 )y 2 1 −(α 1,2 1 +β 1.2 2,1 )y 1 y 2 −α 0,3 1 y 3 2 , where β 2,1 2,1 and β 1,2 2,1 are constants. Clearly, to have a graph Laplacian for all y 0 we require α 0,3 1 , α 3,0 2 ≥ 0 and the two parameters need to satisfy β 2,1 2,1 ≥ max{0, −α 2,1 1 }, β 1,2 2,1 ≥ max{0, −α 1,2 1 }. Note that it is possible for β 2,1 2,1 < 0, say, and yet A 2,1 ≥ 0, provided that β 1,2 2,1 ≥ 0 and β 2,1 2,1 ≥ −2 α 3,0 2 β 1,2 2,1 . As an example, we can write y ′ 1 = −y 3 1 + y 2 1 y 2 + y 3 2 , y ′ 2 = y 3 1 − y 2 1 y 2 − y 3 2 in the form (1.1) with A(y) = −y 2 1 + 1 2 y 1 y 2 − y 2 2 1 2 y 2 1 + y 1 y 2 + y 2 2 y 2 1 − 1 2 y 1 y 2 + y 2 2 − 1 2 y 2 1 − y 1 y 2 − y 2 2 which, as is easy to verify, is a graph Laplacian. Note that this cannot occur for quadratic equations (3.4) because, once A k,ℓ (y) is a multilinear function of y 0, it is a graph Laplacian only if all off-diagonal coefficients are nonnegative. An important application are chemical reactions, where the rate of reaction is modelled by the Law of Mass Action. Then the model is a first-order ODE with a multivariate polynomial for the right hand side, so it can be considered an important special case of our framework. Suppose there are N reactions, where the j-th reaction is written in the form r j,1 G 1 + r j,2 G 2 + . . . + r j,M G M kj − → q j,1 G 1 + q j,2 G 2 + . . . + q j,M G M , j = 1 . . . , N . Here r i,j , q i,j are integer coefficients, G i , i = 1, 2, . . . , M are symbols for the chemical species, y i denotes the concentration of species i, and k j is the rate constant. The model is the ODE where y ∈ R M , S ∈ R M×N , p ∈ R N , and S i,j = q i,j − r i,j is the matrix of stoichiometric vectors, while is the Law of Mass Action to model the rates of reaction. This is a non-linear and autonomous differential equation (it would be non-autonomous if the rates k j = k j (t) were time-varying, for example to model fluctuating temperatures). The following theorem shows this model can always be written in the form where the matrix L(y) has the same pattern of signs as a Laplacian, i.e. off-diagonal entries are nonnegative, and negative entries can only appear on the diagonal. Theorem 7 The nonlinear ODE (3.9) can be written in the quasi-linearised form (3.10) where the negative elements of the matrix L(y) only appear on the diagonal. Proof We assume that q i,j , r i,j are non-negative integers, k j is a non-negative real number and y j ≥ 0. Then, a negative coefficient could only appear in the stoichiometric matrix S i,j if r i,j ≥ 1, and this happens in the equation for y ′ i . Since we have p j = k j M k=1,k =i y r k,j k y ri,j i with r i,j ≥ 1, we may allocate this term to the diagonal of the matrix L(y). All other components where r i,j = 0 in the right hand side of the equation for y ′ i have positive coefficients and can be allocated outside the diagonal. ✷ Remark. Note that the matrix L(y) of (3.10) need not be unique, as we previously showed by the example of the Robertsons reaction in (1.3) . The theorem shows that L(y) has the right pattern of signs to be a graph Laplacian. Similarly to the remarks following Proposition 1, this ensures positivity of the solutions, and the point we are making here is that the new numerical methods proposed in this paper can be applied, via (3.10), to this big class of important applications. The only difference between (3.10) and the primary focus of this paper in (1.1), is that in (1.1) we additionally assume that 1 ⊤ is in the left null space of A(y), but that does not prevent us from applying the numerical schemes proposed in this paper, and they will preserve positivity as required. (Although there may be issues with other conservation laws, as we show in the autonomous oscillations example (1.7), and our atmospheric chemistry example (4.1).) In this section we present some numerical experiments to illustrate the performance of the new methods on a number of examples from the literature. We denote: • ES2: The symmetric second order 3-exponential splitting method (2.8) or (2.9); • EM1: The first order 1-exponential Magnus integrator (2.12) or (2.15); • EM2: The second order 2-exponential Magnus integrator (2.13) or (2.16); • EM3: The third order 7-exponential Magnus integrator (2.14) or (2.17). We will also consider, for comparison, the following more conventional numerical solvers: • Euler: The first-order explicit Euler method; • RK4: The 4-stage fourth-order explicit RK method (as a reference method to compare); • ROS4: The 4-stage fourth-order Rosenbrock method with coefficients used by default in . Let us consider the Robertson's reaction written in the form (1.3) with initial conditions y 0 = [1, 0, 0] ⊤ and time interval t ∈ [0, 0.3] as in (Hairer & Wanner 2010, p. 157) . We numerically solve the problem repeatedly using different values for the time step and compute the 2-norm error of the solution at the final time. Here, we compare with the ""exact"" solution that is computed numerically with sufficiently high accuracy. Figure 4 .1 (left) shows the error versus the time step in double logarithmic scale. The implicit Rosenbrock method, ROS4, outperforms the explicit RK methods, Euler and RK4, but also turns unstable for moderate values of the time step (and does nots preserve positivity) while the new exponential methods preserve positivity and are unconditionally stable (the third order method, EM3, preserves positivity for all time steps considered). Notice the relatively high accuracy provided by the new schemes even when considering large time steps. The best method among the proposed schemes depends on the desired accuracy where the computational cost has to be taken into account. We have repeated the same numerical experiments using only the new exponential methods, but applied to the equations as given in (1.4), i.e. the same problem but written in a different way such that the matrix is no longer graph Laplacian. Figure 4 .1 (right) shows the results obtained. We filled a relevant circle when, during the numerical integration, a negative solution was obtained on any of the components. For small time steps the performance is quite similar (and the performance for EM1 is actually somewhat better) but the errors grow faster for large time steps (lower accuracies) and, even worse, negative solutions do occur.  We now consider a generalised SIR model (SIDARTHE) that has been used to model the evolution of the Cov-SARS-2 epidemic in Italy. That model can also be used for any other country with appropriate data or it can be even extended e.g. to age-dependent variables. The SIDARTHE dynamical system (Giordano et al. 2020) consists of eight ordinary differential equations, describing the evolution of the population in each stage over time. The equations can be written in the form where b(t) : R → R 15 is a vector function depending on 15 time-dependent parameters. The vector b was taken as a piecewise constant function, and the authors estimate the model parameters based on data from 20 February 2020 (day 1) to 5 April 2020 (day 46) and show the impact of progressive restrictions on the spread of the epidemic. For example, b is constant from day 1 to 4 (with a value of R 0 = 2.28), and changes to new constant values for the period 4 to 12 (with a value of R 0 = 1.66), and so on. Notice that since the vector field is not a smooth function (it is piecewise constant) the numerical methods considered in this work deteriorate down to order one. However, a more realistic model should consider b(t) as a smooth time-dependent function, and in this case the order of the methods is recovered. For simplicity, we take the same initial values for b and the same initial conditions, y 0 , as in (Giordano et al. 2020 ), but we take b constant for a longer period, from day 1 to 20. We observed that the model is very sensitive to the parameter associated to the first component of b, b 1 = α. That parameter was taken initially as α = 0.57, and we have analysed the solution for the first component of y (i.e. y 1 (t) = S(t), the susceptible ( Next, we take r = 1.5, corresponding to a moderately stiff problem (S(20) still has not dramatically decreased) and we compute the 2-norm error of the solution y(20) versus the time step for the new methods as well as for the explicit Euler method that was used in (Giordano et al. 2020) . The results are displayed in Figure 4 .3 (left). The new methods require to compute matrix exponentials and this can be computationally costly in some cases. It is thus interesting to study if it is possible to replace the exponential of matrices by cheaper approximations while still preserving positivity. This is not a very stiff problem and we have repeated the same numerical experiments while replacing each exponential by the second-order diagonal Padé approximation. In order to preserve positivity, we proceed as follows, given A =Ã + a * I whereÃ O, we consider the following approximation to the exponential e tA = e ta * e tÃ ≃ e ta * 1 + 1 2 tÃ 1 − 1 2 tÃ . Note that, since 1 ⊤Ã = −a * , we have 1 ⊤ e ta * 1 + 1 2 tÃ 1 − 1 2 tÃ = e ta * 1 − 1 2 ta * 1 + 1 2 ta * = 1 and mass is not preserved. This can be fixed, for example, if we also approximate the scalar function e ta * by the second-order diagonal Padé approximation, so and this approach preserves norm and positivity in the stability region. The results are shown in Figure 4 .3 (right). We observe that the schemes maintain their accuracy while being considerably cheaper. The third-order method EM3 exhibits second order accuracy (due to the second order Padé approximation) but this occurs only at higher accuracies. Unfortunately, this is not the case if we repeat the numerical experiment with the very stiff problem of Robertson's reaction. Once higher order approximations to the exponential are used, positivity is not guaranteed. Not all higher order Padé approximations preserve positivity, unlike the second order one, and this deserves further investigation.  Let us consider the basic stratospheric reaction mechanism studied in (Sandu 2001) that involves six species and whose model to obtain the evolution of the concentrations is given by the system of ODEs y ′ 1 = k 5 y 3 − k 6 y 1 − k 7 y 1 y 3 y ′ 2 = 2k 1 y 4 − k 2 y 2 y 4 + k 3 y 3 − k 4 y 2 y 3 + k 6 y 1 − k 9 y 2 y 6 + k 10 y 6 y ′ 3 = k 2 y 2 y 4 − k 3 y 3 − k 4 y 2 y 3 − k 5 y 3 − k 7 y 1 y 3 − k 8 y 3 y 5 (4.1) y ′ 4 = −k 1 y 4 − k 2 y 2 y 4 + k 3 y 3 + 2k 4 y 2 y 3 + k 5 y 3 + 2k 7 y 1 y 3 + k 8 y 3 y 5 + k 9 y 2 y 6 y ′ 5 = −k 8 y 3 y 5 + k 9 y 2 y 6 + k 10 y 6 y ′ 6 = k 8 y 3 y 5 − k 9 y 2 y 6 − k 10 y 6 with k 1 = 2.643 · 10 −10 σ 3 (t), k 2 = 8.018 · 10 −17 , k 3 = 6.120 · 10 −4 σ(t), k 4 = 1.576 · 10 −15 , k 5 = 1.070 · 10 −3 σ 2 (t), k 6 = 7.110 · 10 −11 , k 7 = 1.200 · 10 −10 , k 8 = 6.062 · 10 −15 , k 9 = 1.069 · 10 −11 , k 10 = 1.289 · 10 −2 σ(t), where σ(t) = 1 2 + 1 2 cos π 2TL−TR−TS The time is measured in seconds and it is taken as mod 24, T R = 4.5, T S = 19.5. The initial time is considered at noon, t 0 = 12 × 3600, and it is integrated for three full days, until t f = t 0 + 72 × 3600 with initial conditions given by y 0 = [9.906 · 10 1 , 6.624 · 10 8 , 5.326 · 10 11 , 1.697 · 10 16 , 8.725 · 10 8 , 2.240 · 10 8 ] ⊤ . This is a non-autonomous systems that can be written in the form with A(t, y) an explicitly time-dependent graph Laplacian matrix. We can write the vector field in terms of the production and destruction parts where P (t, y), D(t, y)y are non-negative. While the diagonal matrix D is unique in this case, we can write P (t, y) = A P (t, y)y in many different ways for the matrix A P . We have considered the following choice (other choices can be considered) for A P , with γ = k 3 + k 5 + k 4 y 3 + k 7 y 1 + k 8 y 5 . This problem has two linear mass conservation laws. Given it is true that w ⊤ 1 A P (t, y)y = w ⊤ 2 A P (t, y)y = 0. Unfortunately, it is impossible to find a matrix A P such that w ⊤ 1 A P (t, y) = w ⊤ 2 A P (t, y) = 0, and both mass conservations cannot be simultaneously preserved by our schemes. We have to decide how to choose A P to optimise the performance of our methods: this is typical to geometric numerical integration of differential equations with multiple invariants. For this particular choice we have w ⊤ 2 A P (t, y) = 0, but w ⊤ 1 A P (t, y) = 0, and then, in general, w ⊤ 1 y(t) = const. However, a good choice for A P can provide solutions where this quantity is preserved to very high accuracy. We have observed that y 1 , y 2 and y 5 take very small, but positive, values (say 10 −200 or smaller) along the integration (standard methods usually provide negative values). In that case, measuring relative error is not appropriate for these components. Figure 4 .4 shows the evolution of the concentration of the different species in a logarithmic scale. Negative values in this plot correspond to having no particles. Figure 4 .5 (left panel) shows the error in the preservation of the quantities I 1 = w ⊤ 1 y(t f ) (curves with circles) and I 2 = w ⊤ 2 y(t f ) (curves with stars). Remarkably, the error committed for I 2 is orders of magnitude smaller than the error in the actual solution, as seen the left panel in Fig. 4 .5! Left: Error in the preserved quantities I 1 and I 2 for the stratospheric reaction model. Right: The 2-norm error of the numerical solutions for the stratospheric reaction model for the components (y 3 , y 4 , y 5 , y 6 ) at the final time t f = t 0 + 3600 (one hour) versus the time step in double logarithmic scale. We have repeated the numerical experiments, integrating for just one hour (in-stead of 72 hours) and measured the two-norm relative error for the vector with componentsỹ = (y 3 , y 4 , y 5 , y 6 ) since at the final time y 1 and y 2 vanish. The reference solution is obtained numerically using the third-order method and a sufficiently small time step. Figure 4 .5 (right panel) shows the results obtained where we can observe the order of convergence of each method for this non-autonomous problem. Finally, we consider the model of (Hadač et al. 2017 , Table 3 , Fig 3, equations (12) -(17)), which is closely related to the MAPK cascade, given in (1.7) with such values for the parameters and initial conditions. The solution for each component is shown in the left panel of Figure 4 .6 for the time interval t ∈ [0, 200] (the initial conditions clearly identify each curve) where we observe that, after a transition period, the solution turns nearly periodic. Next, we have numerically solved the problem using the new exponential methods using different values of the time step and measured the two-norm relative error in the vector solution at the final time. The right panel of Figure 4 .6 shows the results obtained. @story_separate@An outstanding challenge is to approximate the exponential of matrices by diagonal Padé approximants or by other means (e.g. Krylov-subspace methods) to reduce the cost of the algorithms while still preserving positivity. Another is to explore the scope of methods, like the commutator-free Magnus integrators (2.14), which almost preserve positivity and formulate 'almost preservation' in more precise terms. Yet, perhaps the most interesting challenge is to explore the surprising success of 'almost positivity-preserving' methods, e.g. the fourth-order commutator-free Magnus method, in the examples in this paper. Recall that classical ODE solvers that preserve positivity are restricted to order one (Bolley & Crouzeix 1978) , while in this paper we have introduced second-order positivity-preserving methods in the non-classical class of Magnus integrators. It is natural to formulate the conjecture that this is as much as can be done within the realm of such methods, but equally fascinating is the remarkable almost-preservation of positivity or mass (at any rate in the examples of this paper) by some higher-order methods. For example, Figure 4 .5 (left) is concerned with two conservation laws in a stratospheric reaction: one is preserved correctly, up to roundoff error, while the other is preserved to much higher accuracy than the error committed (cf. Fig. 4 .5 right) in the solution itself. We look forward to an explanation.","Many important applications are modelled by differential equations with positive solutions. However, it remains an outstanding open problem to develop numerical methods that are both (i) of a high order of accuracy and (ii) capable of preserving positivity. It is known that the two main families of numerical methods, Runge-Kutta methods and multistep methods, face an order barrier: if they preserve positivity, then they are constrained to low accuracy: they cannot be better than first order. We propose novel methods that overcome this barrier: our methods are of second order, and they are guaranteed to preserve positivity. Our methods apply to a large class of differential equations that have a special graph Laplacian structure, which we elucidate. The equations need be neither linear nor autonomous and the graph Laplacian need not be symmetric. This algebraic structure arises naturally in many important applications where positivity is required. We showcase our new methods on applications where standard high order methods fail to preserve positivity, including infectious diseases, Markov processes, master equations and chemical reactions."
"Communities of practice (CoPs) within healthcare involve groups of people who share an interest in a particular topic, and a desire to deepen their knowledge and expertise by interacting with others regularly. 1 2 They foster mutual learning and knowledge sharing outside the silos of discipline-specific professional expertise, 3 provide a forum for developing and implementing evidencebased practice, 4 and facilitate the delivery of high quality, cost-effective care. The three main elements characterising CoPs identified by Wenger et al 1 are community (collective learning through social interactions), domain (within a particular area of interest) and practice (developing, sharing and maintaining knowledge). Examples of CoPs where professionals have sought further education, development and innovation in a particular practice area, include the promotion of a new measurement tool in child and youth mental healthcare, 5 promotion of recoveryoriented practices in mental healthcare 6 and the management of COVID-19. 7 The advantages of CoPs within healthcare include the joint analysis of practical experiences and information among their members. 3 They allow members to openly discuss concerns and acknowledge errors, encourage in-situ learning, shared decision-making and coordination of experimentation. 8 While CoPs aim to promote standardisation of practice and the establishment of interpersonal relationships that encourage knowledge sharing, there is diversity in how and why they are implemented. 9 CoPs in healthcare have been found to be complex and multifaceted. They vary in composition, intended purpose and use a variety of models for members to share their knowledge. 9 The diversity of CoPs, can be influenced by various social, cultural and individual factors, such as clinical leadership, support and commitment for quality management, regular communication, and availability of accurate and relevant Strengths and limitations of this study ► The scoping review will identify methods used to establish and maintain virtual communities of practice (VCoPs) in healthcare. ► The review will provide detailed analysis of the extent of the literature on VCoPs in healthcare published in the last 10 years. ► The review will be limited to studies in English written in the last 10 years. ► VCoPs that are purely for teaching purposes, for example, online learning, will be excluded. Open access data. 10 Their establishment requires a flexible framework that will guide their formation and ongoing operational procedures. 9 Advances in technology-based communication and the growth of the internet has led to a rapid increase in the sharing of health information globally. Health professionals can use virtual communities of practice (VCoPs) to share their knowledge. 11 12 More recently, the COVID-19 pandemic has significantly limited physical interactions and meetings for sharing of expertise, and therefore, the relevance and utility of VCoPs is more evident. 13 VCoPs provide the opportunity to stay connected and informed, by the sharing of emerging resources and dissemination of research on health issues. 13 VCoPs use a wide variety of media to establish a virtual collaborative space including social media sites, videoconferencing and websites. 14 The creation of VCoPs means that health professionals who are geographically dispersed, 15 can use virtual communities for learning, support, continuing professional education, knowledge management and information sharing. 11 12 Being a member of a VCoP can be a great opportunity for healthcare professionals to share and gain access to highly specialised knowledge. 16 They allow healthcare professionals to build a professional support network and promote the translation of evidence into daily practice, by accessing a common platform. 17 18 VCoPs have a key role in promoting interprofessional learning and collaboration, with virtual modes of communication helping to reduce professional barriers and encourage communication within and between healthcare professions. 19 The successful design and management of VCoPs depend on the characteristics of the virtual community. 8 Members of CoPs and VCoPs are likely to experience very different environments because of the primary way they interact. 14 Computer-mediated interactions are likely to make it more difficult for members to build mutual knowledge, trust, a sense of belonging and open exchange of ideas. 20 21 Factors found to affect knowledge sharing in online communities identified in the literature include individual factors, technological factors and social factors. [22] [23] [24] [25] Individual factors include the contributions of members, with active participation being essential for the VCoP to grow and develop. 16 Active participation refers to members' knowledgeexchange activities, such as posting questions on online community boards, engaging in live chats, participating in online and videoconferencing discussion sessions and providing asynchronous answers and feedback in discussion threads. 26 27 Active participation is influenced by members' motivations, personalities, time available and values. 1 16 26-28 Social factors include the social interaction among members within the group and the roles of group moderators, while technological factors involve technical and usability issues. 16 A 7-year longitudinal study by Antonacci et al 16 showed the growth of VCoPs for healthcare professionals to be related to the presence of a centralised leadership structure and the frequent rotating of leadership over time. Healthcare organisations have a responsibility to deliver high-quality, cost-effective care by implementing evidence-informed policy and practice. [29] [30] [31] Despite the growing number of clinical guidelines produced by government agencies to improve effectiveness and quality of care, 32 frequently there are gaps between research evidence and clinical practice. [33] [34] [35] [36] By providing a platform for healthcare professionals to collaborate towards a common purpose, VCoPs can bridge the gap between research evidence, policy-making and implementation of clinical guidelines. 37 To attempt to address the problems of translating falls prevention clinical guidelines into practice across multiple sites of a residential aged care organisation, one team used a webbased falls prevention CoP. 28 Member engagement with the Information Communication Technology (ICT) applications of asynchronous discussions and accessing evidence were low, with a number of barriers and facilitators to web-based CoP operation identified. 28 Barriers to sustainability included members' capabilities for using ICT applications and lack of dedicated time provided by management for web-based participation. 28 However, the operation of a VCoP in falls prevention was found to be achievable if staff were given sufficient time, and provided with suitable training and support. 38 All of these points could be considered when establishing a VCoP in falls prevention. It is essential to clarify effective methods of VCoPs for knowledge synthesis and translation into practice. Given the limited reporting of a standard approach to the design and administration of VCoPs within healthcare, a scoping review shall be conducted to determine the nature of reported VCoPs within this context in the last 10 years. It aims to identify the methods used to establish and maintain VCoPs and ascertain potential barriers and facilitators to the implementation of VCoPs. This information will then be used to develop a flexible framework that will guide the establishment and facilitation of a VCoP for healthcare professionals on falls prevention in hospitals to assist the translation of clinical guidelines into practice. Healthcare organisations have a responsibility to deliver high quality, cost effective care by implementing evidence-informed policy and practice. [1] [2] [3] Despite the growing number of clinical guidelines produced by government agencies to improve effectiveness and quality of care, 4 frequently there are gaps between research evidence and clinical practice. [5] [6] [7] [8] Communities of practice (CoP) were initially developed in business to promote the management and sharing of knowledge, and aim to stimulate innovation, and organisational value. 9 Communities of practice have been implemented within health care settings to foster mutual learning and knowledge sharing outside the silos of discipline-specific professional expertise. 10 Communities of practice within healthcare involve groups of people who share an interest in a particular topic and a desire to deepen this knowledge and expertise by interacting with others regularly, in order to refine their expertise and mastery. 11 12 Communities of practice provide a forum for developing and implementing evidence-based practice. 13 They facilitate the delivery of high quality, cost-effective care. The three main elements characterising CoPs identified by Wenger et al (2002) are community (collective learning through social interactions), domain (within a particular area of interest), and practice (developing, sharing and maintaining knowledge). 11 Examples of CoPs where professionals have sought further education, development and innovation in a particular practice area, include the promotion of a new measurement tool in child and youth mental health care, 14 promotion of recovery-oriented practices in mental health care, 15 and the management of COVID 19. 16 The advantages of CoPs within healthcare include the joint analysis of practical experiences and information among their members. 10 They allow members to openly discuss concerns and acknowledge errors, encourage in-situ learning, shared decision-making and coordination of experimentation. 10 Communities of practice, however, cover a variety of initiatives that can differ greatly in their aims, design, mode of operation and utilisation of technology. 17 and the establishment of interpersonal relationships that encourage knowledge sharing, there is diversity in how and why they are implemented. 18 CoPs in healthcare have been found to be complex, multifaceted programs that vary in composition, intended purpose and use a variety of models for members to share their knowledge. 18 The diversity of CoPs, can be influenced by various social, cultural and individual factors, such as clinical leadership, support and commitment for quality management, regular communication, and availability of accurate and relevant data. 19 Their establishment requires a flexible framework that will guide their formation and ongoing operational procedures. 18 Advances in technology-based communication and the growth of the internet has led to a rapid increase in the sharing of health information globally. Health professionals can now utilise virtual communities of practice (VCoPs) to share their knowledge. 20 21 VCoPs use a wide variety of media to establish a virtual collaborative space including social media sites, videoconferencing and websites. 22 The creation of VCoPs means that health professionals who are geographically dispersed, can use virtual communities for learning, support, continuing professional education, knowledge management and information sharing. 20 21 Being a member of a VCoP can be a great opportunity for healthcare professionals to share and gain access to highly specialised knowledge. 23 VCoPs also allow healthcare professionals to build a professional support network and promote the translation of evidence into daily practice, by accessing a common platform. 24 25 The successful design and management of VCoPs depends on the characteristics of the virtual community. 17 Members of CoPs and VCoPs are likely to experience very different environments because of the primary way they interact. 22 Computer-mediated interactions are likely to make it more difficult for members to build mutual knowledge, trust, a sense of belonging and open exchange of ideas. 26 asynchronous answers and feedback in discussion threads. 32 33 Active participation is influenced by   members' motivations, personalities, time available and values. 11 23 32-34 Social factors include the social   interaction amongst members within the group and the roles of group moderators, whilst technological factors involve technical and usability issues. 23 A 7 year longitudinal study by Antonacci et al (2017) showed the growth of VCoPs for healthcare professionals to be related to the presence of a centralised leadership structure and the frequent rotating of leadership over time. 23 By providing a platform for health care professionals to collaborate towards a common purpose, VCoPs can bridge the gap between research evidence, policy-making and implementation of clinical guidelines. 35 The problem of falls in healthcare facilities worldwide, can be used to illustrate this point. Falls are associated with marked morbidity, mortality, increased length of stay and re-admissions. [36] [37] [38] [39] [40] To ensure healthcare professional systematic translation of falls prevention clinical guidelines into practice, appropriate implementation strategies need to be employed. 41 To attempt to address these problems across multiple sites of a residential aged care organisation, one team used a web-based falls prevention CoP. 34 The operation of a VCoP in falls prevention was found to be achievable if staff were given sufficient time, and provided with suitable training and support. 42 Barriers to sustainability were identified such as members' capabilities for using ICT applications and lack of dedicated time provided by management for web-based participation. 34 All of these points could be considered when establishing a VCoP in falls prevention. It is essential to clarify effective methods of VCoPs for knowledge synthesis and translation into practice. Given the limited reporting of a standard approach to the design and administration of VCoPs within healthcare organisations, a scoping review shall be conducted to determine the nature of reported VCoPs within this context. Our scoping review will provide a new and detailed analysis of the extent of the literature on VCoPs in clinical healthcare published in the last 10 years. It aims to identify the methods used to establish and maintain VCoPs, ascertain potential barriers and facilitators to the implementation of VCoPs, determine the best methods for evaluation of VCoPs and discover the impact of VCoPs on clinical 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n  l  y   VCoPs scoping review   6 practice. This information will then be used to develop a flexible framework that will guide the establishment and facilitation of a VCoP for healthcare professionals on falls prevention in hospitals.@story_separate@The methodological structure will follow Arksey and O'Malley's framework for scoping reviews, 39 which was refined by the Joanna Briggs Institute. 40 41 The protocol will use the Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR) checklist, 42  Review stages Stage 1: identifying the research question Scoping reviews are a form of knowledge synthesis that present a broad overview of the evidence on a topic of interest, without addressing study quality, and can be used to identify key concepts for a topic area and identify any knowledge gaps. 43 The concepts underpinning a research area can be mapped by systematically searching, selecting and synthesising existing knowledge. 39 44 The primary research question is: i. What is the extent of reported research on establish- during the implementation of VCoPs? The authors are aware and prepared for themes and recommendations that arise from the literature that are beyond these research questions and will amend and update the questions as required. In a scoping review, the three elements of population, concept and context are used to establish inclusion and exclusion criteria. The population details the relevant characteristics of participants, the concept is the principal focus of the review and the context describes the setting under examination. The population of interest is any healthcare professionals who are part of a VCoP for the purposes of building and exchanging knowledge, developing individual capabilities, ensuring their practice is evidence-based and enhancing interprofessional collaboration. The concept is VCoPs for the purposes of improving clinical outcomes. CoPs that describe themselves as 'virtual', 'online' or 'web-based' are included. They should report on the establishment and maintenance of VCoPs that have been implemented in a healthcare setting for health professionals. The context is any platform used by healthcare professionals to support virtual interactions in healthcare for knowledge advancement and sharing of ideas. VCoPs that are purely for teaching purposes, for example, online learning, will be excluded. To be included, articles should be peer-reviewed and in the English language. Included articles can be existing literature on VCoPs including primary research studies of any design (quantitative, qualitative and mixed-methods), systematic reviews, meta-analyses, guideline implementation. Exclusions include grey literature, commentaries and any other opinion pieces. The articles need to be accessible as full text and published between January 2010 and October 2020. A three-step approach will be developed by the study group in collaboration with an academic librarian. The librarian will execute the searches on behalf of the study group. i. There will be an initial limited search of PubMed and Cumulative Index to Nursing and Allied Health Literature (CINAHL), to identify relevant studies to assist with search term development, based on the research questions and purpose of the study. The librarian will assist in guiding a rigorous analysis process to identify the best search terms and strategy related to VCoPs in healthcare. The process will be iterative, to ensure all relevant search terms are captured. ii. Words in the title and abstract of the initial retrieved papers and indexing terms will be analysed and used to classify the articles. iii. A second comprehensive search across PubMed, CINAHL, CENTRAL, PsycINFO, Cochrane Library and Education Resources Information Center from January 2010 to October 2020 will be conducted, to ensure VCoPs are contemporary in terms of design and content. The reference lists of all identified reports and articles will be searched for additional studies meeting the inclusion criteria. We will retrieve all supplementary files that are referred to in the included papers and any papers that are referred to in a particular study that were part of the research project. Online supplemental appendix 1 shows the initial search strategy to be executed in CINAHL and PubMed. All studies identified from the search strategy will be uploaded to the online systematic review software, Covidence. 45 Two reviewers will independently screen the titles and abstracts of retrieved papers. The full texts of identified papers will be obtained and assessed by two independent reviewers, to identify studies that meet the inclusion criteria. Discrepancies will be resolved through discussion and if necessary, consensus will be achieved via a third reviewer. The results of the search will be presented in a PRISMA-ScR flow diagram (see figure 1 ). 42 Stage 4: data charting Data from eligible studies will be charted independently by two researchers using a data extraction Open access chart developed in Covidence. 45 The chart will capture the relevant information on key study characteristics (eg, year of publication, country of origin, type of research, setting, study population of those in the VCoP), objectives, terminology used, development (activities undertaken at the inquiry, design and launch stages), barriers and facilitators to VCoP development, outcomes and key findings related to the review questions. This process will be iterative and variables may be identified following complete review of the full texts. The data extraction form will be trialled by two reviewers on a random sample of 10 included articles to ensure that all relevant results were able to be captured, and modifications will be made as required. After this, the same two reviewers will independently chart the data for all included studies, and then compare and merge the data into a final dataset. Conflicts at the data merging stage will be resolved by discussion until consensus is reached. If a consensus cannot be reached, a third study group member will be consulted. The synthesis of extracted data will include thematic analysis for qualitative data. Quantitative data will be summarised using frequency analysis, with the counts and percentages of articles for each category calculated. Data synthesis will be an iterative process with new categories and themes identified through ongoing analysis. For the qualitative analysis, two reviewers will categorise the key components independently in Excel. Through discussion, they will develop a coding framework. The coding framework will be piloted on a random sample of 10 included articles by the two primary reviewers and modifications will be made as required. One of the primary reviewers will then code the remaining articles according to the final framework. Quantitative results will be summarised in tables, charts and diagrams as indicated by the data, to allow for easy comparison. Following synthesis and analysis of the data best practice methods to establish and maintain VCoPs, barriers and facilitators to establishing VCoPs, approaches to evaluation and the impact of VCoPs on clinical practice, will be identified. The results of this scoping review will highlight the best ways to design and manage VCoPs in healthcare organisations. The findings will be presented at relevant stakeholder workshops, conferences and published in peer-reviewed journals. Ethics approval is not required for this scoping review. Open access SUMMARY VCoPs are becoming increasingly popular, yet the best methods of how to establish them have not been realised. The proposed scoping review will follow an updated, five-step rigorous methodology for conducting scoping reviews as described by the Joanna Briggs Institute. The review will provide a new and detailed analysis of the extent of the literature on VCoPs in healthcare published in the last 10 years. It will highlight the best methods for establishing and maintaining VCoPs within a healthcare setting. It will also outline any potential barriers and facilitators to developing a VCoP in a healthcare setting. The findings will inform the development of a standardised but flexible framework for the translation of falls prevention clinical guidelines into practice. Twitter Debra Kiegaldie @DKiegaldie Contributors LS was involved in study conception, preliminary literature review, writing and editing of the protocol, scoping review framework and analysis, design of the search strategy and content expert input. DJ was involved in study conception, editing of the protocol, content expert input and preliminary literature review. MM and DK were involved in editing of the protocol, provided general guidance to the research team, were involved in study conception and content expert input. All authors have made substantive intellectual contributions to the development of this protocol. All authors read and approved the manuscript. Funding This scoping review is being conducted as a part of an NHMRC funded public-private partnership (#GNT1152853) which aims to use implementation science principles to enable both clinicians and patients to better mitigate future risk of hospital falls and to reduce falls rates. The partnership is between the Healthscope private hospital network, Holmesglen Institute and Australian universities. Competing interests None declared. Patient consent for publication Not required. Provenance and peer review Not commissioned; externally peer reviewed. Supplemental material This content has been supplied by the author(s). It has not been vetted by BMJ Publishing Group Limited (BMJ) and may not have been peer-reviewed. Any opinions or recommendations discussed are solely those of the author(s) and are not endorsed by BMJ. BMJ disclaims all liability and responsibility arising from any reliance placed on the content. Where the content includes any translated material, BMJ does not warrant the accuracy and reliability of the translations (including but not limited to local regulations, clinical guidelines, terminology, drug names and drug dosages), and is not responsible for any error and/or omissions arising from translation and adaptation or otherwise. Introduction: Virtual communities of practice (VCoPs) use a common online platform to provide healthcare professionals with the opportunity to access highly specialised knowledge, build a professional support network, and promote the translation of research evidence into practice. There is limited reporting of how best to design and administer VCoPs within healthcare organisations. The primary aim of this scoping review is to identify the best methods used to establish and maintain VCoPs. We also aim to ascertain potential barriers and facilitators to the implementation of VCoPs, determine the best methods for their evaluation, and discover the impact of VCoPs on clinical practice. Findings shall be used to develop a flexible framework to guide the establishment and facilitation of a VCoP for healthcare professionals. A five stage scoping review process will be followed based on Arksey and O'Malley's framework and refined by the Joanna Briggs Institute Methodology. An initial limited search of PubMed and CINAHL will identify relevant studies and assist with search term development. This will be followed by a search of 5 online databases to identify papers published from January 2010 until November 2020. Papers will be independently screened by two reviewers, and data extracted and analysed using a reporting framework. Qualitative data will be analysed thematically and numerical synthesis of the data will be conducted. The results of this scoping review will highlight the best ways to design and manage VCoPs in healthcare organisations. The findings will be presented at relevant stakeholder workshops, conferences and published in peer-reviewed journals. Ethics approval is not required for this scoping review. -We will identify methods used to establish and maintain VCoPs in healthcare and shed light on the facilitators and barriers to implementation. -The findings will guide the establishment and facilitation of a VCoP for health professionals on falls prevention in hospitals -This review will be limited to studies in English written in the last 10 years. The methodological structure will follow Arksey and O'Malley's framework for scoping reviews, 43 which was refined by the Joanna Briggs Institute. 44 The protocol was drafted using the PRISMA-ScR To be included, articles should be peer-reviewed and in the English language. Included articles can be any existing literature on VCoPs including primary research studies, systematic reviews, meta-analyses, guideline implementation, grey literature and commentaries. They should report on any aspect of VCoPs that have been implemented in a healthcare setting. The articles need to be accessible as full text, and published between January 2010 and October 2020. Search strategy: A three step approach will be developed by the study group in collaboration with an academic librarian. The librarian will execute the searches on behalf of the study group. (i) There will be an initial limited search of PubMed and Cumulative Index to Nursing and Allied Health Literature (CINAHL), to identify relevant studies to assist with search term development, based on the research questions and purpose of the study. The librarian will assist us to guide a rigorous analysis process to identify the best search terms and strategy related to VCoPs in healthcare. The process will be iterative, to ensure all relevant search terms are captured. (ii) Words in the title and abstract of the initial retrieved papers and indexing terms will be analysed and used to classify the articles. (iii) A second comprehensive search across PubMed, CINAHL, CENTRAL, PsycINFO and Education Resources Information Center (ERIC) from January 2010 to October 2020 will be conducted, to ensure VCoPs are contemporary in terms of design and content. The reference lists of all identified reports and articles will be searched for additional studies meeting the inclusion criteria. We will retrieve all supplementary files that are referred to in the included papers and any papers that are referred to in a particular study that were part of the research project. The search for unpublished studies will include Trove and ProQuest Theses and Dissertations Global. We will also search for grey literature using Google and Google Scholar. Appendix 1 shows the initial search strategy to be executed in CINAHL and PubMed. All studies identified from the search strategy will be uploaded to the online systematic review software, Covidence. 47 Two reviewers will independently screen the titles and abstracts of retrieved papers. The full texts of identified papers will be obtained and assessed by two independent reviewers, to identify studies that meet the inclusion criteria. Discrepancies will be resolved through discussion and if necessary, consensus will be achieved via a third reviewer. The results of the search will be presented in a PRISMA flow diagram. 45 Data from eligible studies will be charted independently by two researchers using a data extraction chart developed in Covidence. 47 The chart will capture the relevant information on key study characteristics (for example, year of publication, country of origin, type of research, setting, study population of those in the VCoP), objectives, terminology used, development (activities undertaken at the inquiry, design and launch stages), evaluation methods, outcomes and key findings related to the review questions. This process will be iterative and variables may be identified following complete review of the full texts. The same two reviewers will compare and merge the data into a final dataset. Conflicts at the data merging stage will be resolved by discussion until consensus is reached. If a consensus cannot be reached, a third study group member will be consulted. The data extraction form will be piloted on a random sample of 10 included articles by the two primary reviewers and modifications will be made as required. The synthesis of extracted data will include thematic analysis for the qualitative data. Quantitative data will be summarised using frequency analysis, with the counts and percentages of articles for each category calculated. Data synthesis will be an iterative process with new categories and themes identified through ongoing analysis. For the qualitative analysis, two reviewers will categorise the key components independently in Excel. Through discussion they will develop a coding framework. The coding framework will be piloted on a random sample of 10 included articles by the two primary reviewers and modifications will be made as required. One of the primary reviewers will then code the remaining articles according to the final framework. Quantitative results will be summarised in tables, charts and diagrams as indicated by the data, to allow for easy comparison. Following synthesis and analysis of the data best practice methods to establish and maintain VCoPs, barriers and facilitators to establishing VCoPs, approaches to evaluation, and the impact of VCoPs on clinical practice, will be identified.  This scoping review does not require ethics approval as data will be obtained through review of existing published literature. Study findings will be presented at relevant consumer stakeholder meetings, 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n  l  y conferences and public forums, and published in peer-reviewed journals. The findings will inform the future direction of the development and evaluation of a VCoP to promote best practice falls prevention in hospitals. Authors' contributions: LS was involved in study conception, preliminary literature review, writing and editing of the protocol, scoping review framework and analysis, design of the search strategy and content expert input. DJ was involved in study conception, editing of the protocol, content expert input, and preliminary literature review. MM and DK were involved in editing of the protocol, provided general guidance to the research team, were involved in study conception and content expert input. All authors have made substantive intellectual contributions to the development of this protocol. All authors read and approved the manuscript. Funding: This scoping review is being conducted as a part of an NHMRC funded public-private partnership (#GNT1152853) which aims to utilise implementation science principles to enable both clinicians and patients to better mitigate future risk of hospital falls and to reduce falls rates. The partnership is between the Healthscope private hospital network, Holmesglen Institute and Australian universities. Data sharing: Data from this study will be available by emailing the lead author, Louise Shaw: louise.shaw@holmesglen.edu.au Patient and public involvement: As this study is a scoping review of existing literature, no patients or public will be involved. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  Describe the rationale for the review in the context of what is already known. Explain why the review questions/objectives lend themselves to a scoping review approach. Objectives 4 Provide an explicit statement of the questions and objectives being addressed with reference to their key elements (e.g., population or participants, concepts, and context) or other relevant key elements used to conceptualize the review questions and/or objectives. Indicate whether a review protocol exists; state if and where it can be accessed (e.g., a Web address); and if available, provide registration information, including the registration number. This is a protocol paper Eligibility criteria 6 Specify characteristics of the sources of evidence used as eligibility criteria (e.g., years considered, language, and publication status), and provide a rationale. Information sources* 7 Describe all information sources in the search (e.g., databases with dates of coverage and contact with authors to identify additional sources), as well as the date the most recent search was executed. Search 8 Present the full electronic search strategy for at least 1 database, including any limits used, such that it could be repeated. Selection of sources of evidence † 9 State the process for selecting sources of evidence (i.e., screening and eligibility) included in the scoping review. Data charting process ‡ 10 Describe the methods of charting data from the included sources of evidence (e.g., calibrated forms or forms that have been tested by the team before their use, and whether data charting was done independently or in duplicate) and any processes for obtaining and confirming data from investigators. Data items 11 List and define all variables for which data were sought and any assumptions and simplifications made. 8-9 Critical appraisal of individual sources of evidence § If done, provide a rationale for conducting a critical appraisal of included sources of evidence; describe the methods used and how this information was used in any data synthesis (if appropriate). Provide a general interpretation of the results with respect to the review questions and objectives, as well as potential implications and/or next steps. Describe sources of funding for the included sources of evidence, as well as sources of funding for the scoping review. Describe the role of the funders of the scoping review. N/A for protocol JBI = Joanna Briggs Institute; PRISMA-ScR = Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews. * Where sources of evidence (see second footnote) are compiled from, such as bibliographic databases, social media platforms, and Web sites. † A more inclusive/heterogeneous term used to account for the different types of evidence or data sources (e.g., quantitative and/or qualitative research, expert opinion, and policy documents) that may be eligible in a scoping review as opposed to only studies. This is not to be confused with information sources (see first footnote). ‡ The frameworks by Arksey and O'Malley (6) and Levac and colleagues (7) and the JBI guidance (4, 5) refer to the process of data extraction in a scoping review as data charting. § The process of systematically examining research evidence to assess its validity, results, and relevance before using it to inform a decision. This term is used for items 12 and 19 instead of ""risk of bias"" (which is more applicable to systematic reviews of interventions) to include and acknowledge the various sources of evidence that may be used in a scoping review (e.g., quantitative and/or qualitative research, expert opinion, and policy document). PubMed and CINAHL will identify relevant studies and assist with search term development. This will be followed by a search of 5 online databases to identify papers published from January 2010 until November 2020. Papers will be independently screened by two reviewers, and data extracted and analysed using a reporting framework. Qualitative data will be analysed thematically and numerical synthesis of the data will be conducted. The results of this scoping review will highlight the best ways to design and manage VCoPs in healthcare organisations. The findings will be presented at relevant stakeholder workshops, conferences and published in peer-reviewed journals. -The scoping review will identify methods used to establish and maintain VCoPs in healthcare. -The review will provide detailed analysis of the extent of the literature on VCoPs in healthcare published in the last 10 years -The review will be limited to studies in English written in the last 10 years. -VCoPs that are purely for teaching purposes, for example on-line learning, will be excluded. The advantages of CoPs within healthcare include the joint analysis of practical experiences and information among their members. 3 They allow members to openly discuss concerns and acknowledge errors, encourage in-situ learning, shared decision-making, and coordination of experimentation. 8 Whilst CoPs aim to promote standardisation of practice and the establishment of interpersonal relationships that encourage knowledge sharing, there is diversity in how and why they are implemented. 9 CoPs in healthcare have been found to be complex and multifaceted. They vary in composition, intended purpose and use a variety of models for members to share their knowledge. 9 The diversity of CoPs, can be influenced by various social, cultural and individual factors, such as clinical leadership, support and commitment for quality management, regular communication, and availability of accurate and relevant data. 10 Their establishment requires a flexible framework that will guide their formation and ongoing operational procedures. 9 Advances in technology-based communication and the growth of the internet has led to a rapid increase in the sharing of health information globally. Health professionals can utilise virtual communities of practice (VCoPs) to share their knowledge. 11 12 More recently, the COVID-19 pandemic has significantly limited physical interactions and meetings for sharing of expertise, and therefore, the relevance and utility of VCoPs is more evident. 13 VCoPs provide the opportunity to stay connected and informed, by the sharing of emerging resources and dissemination of research on health issues. 13 VCoPs use a wide variety of media to establish a virtual collaborative space including social media sites, videoconferencing and websites. 14 The creation of VCoPs means that health professionals who are geographically dispersed, 15 can use virtual communities for learning, support, continuing professional education, knowledge management and information sharing. 11 12 Being a member of a VCoP can be a great opportunity for healthcare professionals to share and gain access to highly specialised knowledge. 16 They allow healthcare professionals to build a professional support network and promote the translation of evidence into daily practice, by accessing a common platform. 17 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n  l  y VCoPs scoping review 5 the growth of VCoPs for healthcare professionals to be related to the presence of a centralised leadership structure and the frequent rotating of leadership over time. 16 Healthcare organisations have a responsibility to deliver high quality, cost effective care by implementing evidence-informed policy and practice. [29] [30] [31] Despite the growing number of clinical guidelines produced by government agencies to improve effectiveness and quality of care, 32 frequently there are gaps between research evidence and clinical practice. [33] [34] [35] [36] By providing a platform for health care professionals to collaborate towards a common purpose, VCoPs can bridge the gap between research evidence, policymaking and implementation of clinical guidelines. 37 To attempt to address the problems of translating falls prevention clinical guidelines into practice across multiple sites of a residential aged care organisation, one team used a web-based falls prevention CoP. 28 Member engagement with the ICT applications of asynchronous discussions and accessing evidence were low, with a number of barriers and facilitators to web-based CoP operation identified. 28 Barriers to sustainability included members' capabilities for using ICT applications and lack of dedicated time provided by management for web-based participation. 28 However, the operation of a VCoP in falls prevention was found to be achievable if staff were given sufficient time, and provided with suitable training and support. 38 All of these points could be considered when establishing a VCoP in falls prevention. It is essential to clarify effective methods of VCoPs for knowledge synthesis and translation into practice. Given the limited reporting of a standard approach to the design and administration of VCoPs within healthcare, a scoping review shall be conducted to determine the nature of reported VCoPs within this context in the last 10 years. It aims to identify the methods used to establish and maintain VCoPs and ascertain potential barriers and facilitators to the implementation of VCoPs. This information will then be used to develop a flexible framework that will guide the establishment and facilitation of a VCoP for healthcare professionals on falls prevention in hospitals to assist the translation of clinical guidelines into practice. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n l y The methodological structure will follow Arksey and O'Malley's framework for scoping reviews, 39 which was refined by the Joanna Briggs Institute. 40 The review stages Scoping reviews are a form of knowledge synthesis that present a broad overview of the evidence on a topic of interest, without addressing study quality, and can be used to identify key concepts for a topic area and identify any knowledge gaps. 43 The concepts underpinning a research area can be mapped by systematically searching, selecting, and synthesising existing knowledge. 39 44 The primary research question is: The authors are aware and prepared for themes and recommendations that arise from the literature that are beyond these research questions and will amend and update the questions as required. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 Participants: The population of interest is any healthcare professionals who are part of a VCoP for the purposes of building and exchanging knowledge, developing individual capabilities, ensuring their practice is evidence-based, and enhancing interprofessional collaboration. The concept is Virtual Communities of Practice for the purposes of improving clinical outcomes. Communities of Practice that describe themselves as 'virtual', 'on-line' or 'web-based' are included. They should report on of the establishment and maintenance of VCoPs that have been implemented in a healthcare setting for health professionals. The context is any platform used by healthcare professionals to support virtual interactions in healthcare for knowledge advancement and sharing of ideas. VCoPs that are purely for teaching purposes, for example on-line learning, will be excluded. To be included, articles should be peer-reviewed and in the English language. Included articles can be existing literature on VCoPs including primary research studies of any design (quantitative, qualitative and mixed methods), systematic reviews, meta-analyses, guideline implementation. Exclusions include grey literature, commentaries and any other opinion pieces. The articles need to be accessible as full text, and published between January 2010 and October 2020. Search strategy: A three step approach will be developed by the study group in collaboration with an academic librarian. The librarian will execute the searches on behalf of the study group. (i) There will be an initial limited search of PubMed and Cumulative Index to Nursing and Allied Health Literature (CINAHL), to identify relevant studies to assist with search term development, based on the research questions and purpose of the study. The librarian will assist in guiding a rigorous analysis process to identify the best search terms and strategy related to VCoPs in healthcare. The process will be iterative, to ensure all relevant search terms are captured. (ii) Words in the title and abstract of the initial retrieved papers and indexing terms will be analysed and used to classify the articles. (iii) A second comprehensive search across PubMed, CINAHL, CENTRAL, PsycINFO, Cochrane Library and Education Resources Information Center (ERIC) from January 2010 to October 2020 will be conducted, to ensure VCoPs are contemporary in terms of design and content. The reference lists of all identified reports and articles will be searched for additional studies meeting the inclusion criteria. We will retrieve all supplementary files that are referred to in the included papers and any papers that are referred to in a particular study that were part of the research project. Appendix 1 shows the initial search strategy to be executed in CINAHL and PubMed. All studies identified from the search strategy will be uploaded to the online systematic review software, Covidence. 45 Two reviewers will independently screen the titles and abstracts of retrieved papers. The full texts of identified papers will be obtained and assessed by two independent reviewers, to identify studies that meet the inclusion criteria. Discrepancies will be resolved through discussion and if necessary, consensus will be achieved via a third reviewer. The results of the search will be presented in a PRISMA-ScR flow diagram (see Figure 1 ). 42 Data from eligible studies will be charted independently by two researchers using a data extraction chart developed in Covidence. 45 The chart will capture the relevant information on key study characteristics (for example, year of publication, country of origin, type of research, setting, study population of those in the VCoP), objectives, terminology used, development (activities undertaken at the inquiry, design and launch stages), barriers and facilitators to VCoP development, outcomes and key findings related to the review questions. This process will be iterative and variables may be identified following complete review of the full texts. The data extraction form will be trialled by two reviewers on a random sample of 10 included articles to ensure that all relevant results were able to be captured, and modifications will be made as required. After this, the same two reviewers will independently chart the data for all included studies, and then compare and merge the data into a final dataset. Conflicts at the data merging stage will be resolved by discussion until consensus is reached. If a consensus cannot be reached, a third study group member will be consulted. The synthesis of extracted data will include thematic analysis for qualitative data. Quantitative data will be summarised using frequency analysis, with the counts and percentages of articles for each category calculated. Data synthesis will be an iterative process with new categories and themes identified through ongoing analysis. For the qualitative analysis, two reviewers will categorise the key components independently in Excel. Through discussion they will develop a coding framework. The coding framework will be piloted on a random sample of 10 included articles by the two primary reviewers and modifications will be made as required. One of the primary reviewers will then code the remaining articles according to the final framework. Quantitative results will be summarised in tables, charts and diagrams as indicated by the data, to allow for easy comparison. Following synthesis and analysis of the data best practice methods to establish and maintain VCoPs, barriers and facilitators to establishing VCoPs, approaches to evaluation, and the impact of VCoPs on clinical practice, will be identified. The results of this scoping review will highlight the best ways to design and manage VCoPs in healthcare organisations. The findings will be presented at relevant stakeholder workshops, conferences and published in peer-reviewed journals. Ethics approval is not required for this scoping review. VCoPs are becoming increasingly popular, yet the best methods of how to establish them have not been realised. The proposed scoping review will follow an updated, five step rigorous methodology for conducting scoping reviews as described by the Joanna Briggs Institute. The review will provide new and detailed analysis of the extent of the literature on VCoPs in healthcare published in the last 10 years. It will highlight the best methods for establishing and maintaining VCoPs within a healthcare setting. It will also 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  This scoping review does not require ethics approval as data will be obtained through review of existing published literature. Study findings will be presented at relevant consumer stakeholder meetings, conferences and public forums, and published in peer-reviewed journals. The findings will inform the future direction of the development and evaluation of a VCoP to promote best practice falls prevention in hospitals. Authors' contributions: LS was involved in study conception, preliminary literature review, writing and editing of the protocol, scoping review framework and analysis, design of the search strategy and content expert input. DJ was involved in study conception, editing of the protocol, content expert input, and preliminary literature review. MM and DK were involved in editing of the protocol, provided general guidance to the research team, were involved in study conception and content expert input. All authors have made substantive intellectual contributions to the development of this protocol. All authors read and approved the manuscript. Funding: This scoping review is being conducted as a part of an NHMRC funded public-private partnership (#GNT1152853) which aims to utilise implementation science principles to enable both clinicians and patients to better mitigate future risk of hospital falls and to reduce falls rates. The partnership is between the Healthscope private hospital network, Holmesglen Institute and Australian universities. Data sharing: Data from this study will be available by emailing the lead author, Louise Shaw: louise.shaw@holmesglen.edu.au 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 Provide a structured summary that includes (as applicable): background, objectives, eligibility criteria, sources of evidence, charting methods, results, and conclusions that relate to the review questions and objectives. Describe the rationale for the review in the context of what is already known. Explain why the review questions/objectives lend themselves to a scoping review approach. Objectives 4 Provide an explicit statement of the questions and objectives being addressed with reference to their key elements (e.g., population or participants, concepts, and context) or other relevant key elements used to conceptualize the review questions and/or objectives. Indicate whether a review protocol exists; state if and where it can be accessed (e.g., a Web address); and if available, provide registration information, including the registration number. This is a protocol paper Eligibility criteria 6 Specify characteristics of the sources of evidence used as eligibility criteria (e.g., years considered, language, and publication status), and provide a rationale. Information sources* 7 Describe all information sources in the search (e.g., databases with dates of coverage and contact with authors to identify additional sources), as well as the date the most recent search was executed. Search 8 Present the full electronic search strategy for at least 1 database, including any limits used, such that it could be repeated. Selection of sources of evidence † 9 State the process for selecting sources of evidence (i.e., screening and eligibility) included in the scoping review. Data charting process ‡ 10 Describe the methods of charting data from the included sources of evidence (e.g., calibrated forms or forms that have been tested by the team before their use, and whether data charting was done independently or in duplicate) and any processes for obtaining and confirming data from investigators. Data items 11 List and define all variables for which data were sought and any assumptions and simplifications made. 8-9 Critical appraisal of individual sources of evidence § If done, provide a rationale for conducting a critical appraisal of included sources of evidence; describe the methods used and how this information was used in any data synthesis (if appropriate). 13 Describe the methods of handling and summarizing the data that were charted. 9 Give numbers of sources of evidence screened, assessed for eligibility, and included in the review, with reasons for exclusions at each stage, ideally using a flow diagram. Characteristics of sources of evidence 15 For each source of evidence, present characteristics for which data were charted and provide the citations. Critical appraisal within sources of evidence 16 If done, present data on critical appraisal of included sources of evidence (see item 12) . Results of individual sources of evidence 17 For each included source of evidence, present the relevant data that were charted that relate to the review questions and objectives. Synthesis of results 18 Summarize and/or present the charting results as they relate to the review questions and objectives. Summarize the main results (including an overview of concepts, themes, and types of evidence available), link to the review questions and objectives, and consider the relevance to key groups. Limitations 20 Discuss the limitations of the scoping review process. N/A for protocol Describe sources of funding for the included sources of evidence, as well as sources of funding for the scoping review. Describe the role of the funders of the scoping review. N/A for protocol JBI = Joanna Briggs Institute; PRISMA-ScR = Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews. * Where sources of evidence (see second footnote) are compiled from, such as bibliographic databases, social media platforms, and Web sites. † A more inclusive/heterogeneous term used to account for the different types of evidence or data sources (e.g., quantitative and/or qualitative research, expert opinion, and policy documents) that may be eligible in a scoping review as opposed to only studies. This is not to be confused with information sources (see first footnote). ‡ The frameworks by Arksey and O'Malley (6) and Levac and colleagues (7) and the JBI guidance (4, 5) refer to the process of data extraction in a scoping review as data charting. § The process of systematically examining research evidence to assess its validity, results, and relevance before using it to inform a decision. This term is used for items 12 and 19 instead of ""risk of bias"" (which is more applicable to systematic reviews of interventions) to include and acknowledge the various sources of evidence that may be used in a scoping review (e.g., quantitative and/or qualitative research, expert opinion, and policy document). 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 @story_separate@Provide a general interpretation of the results with respect to the review questions and objectives, as well as potential implications and/or next steps.","INTRODUCTION: Virtual communities of practice (VCoPs) use a common online platform to provide healthcare professionals with the opportunity to access highly specialised knowledge, build a professional support network and promote the translation of research evidence into practice. There is limited reporting of how best to design and administer VCoPs within healthcare organisations. The primary aim of this scoping review is to identify the best methods used to establish and maintain VCoPs. Findings shall be used to develop a flexible framework to guide the establishment and facilitation of a VCoP for healthcare professionals to ensure the translation of falls prevention clinical guidelines into practice. METHODS AND ANALYSIS: A five-stage scoping review process will be followed based on Arksey and O’Malley’s framework and refined by the Joanna Briggs Institute Methodology. An initial limited search of PubMed and Cumulative Index to Nursing and Allied Health Literature will identify relevant studies and assist with search term development. This will be followed by a search of five online databases to identify papers published from January 2010 until November 2020. Papers will be independently screened by two reviewers, and data extracted and analysed using a reporting framework. Qualitative data will be analysed thematically and numerical synthesis of the data will be conducted. RESULTS AND DISSEMINATION: The results of this scoping review will highlight the best ways to design and manage VCoPs in healthcare organisations. The findings will be presented at relevant stakeholder workshops, conferences and published in peer-reviewed journals."
"The coronavirus SARS-CoV-2, the cause of the COVID-19 pandemic, 1 encodes several enzymes that are essential to its ability to replicate. 2, 3 After cell entry, viral RNA is translated by host ribosomes into two polyproteins that are cleaved to produce the viral proteins that are needed for assembling new virions. As potential targets for discovery of therapeutic agents, the two cysteine proteases that are responsible for cleaving the polyproteins have been highlighted, namely, the chymotrypsinlike or main protease, known as 3CL pro or M pro , and the papain-like protease, PL pro . 2, 3 Following the disease outbreak in 2002 from SARS-CoV, these proteins have received much attention for characterization of their structural biology and development of inhibitors. 4, 5 The high sequence homology between the proteins from the two coronaviruses, 83% for PL pro and 96% for M pro , has allowed the prior studies to provide a solid foundation for current efforts targeting the new isoforms. Thus, crystal structures of M pro from SARS-CoV-2 have quickly emerged along with initial reports of inhibitors. 6−11 As for the earlier virus, the designed inhibitors have largely been peptide-like with incorporation of a reactive warhead that covalently binds to the catalytic cysteine, Cys145. These features are generally not optimal for drug development owing to potential proteolytic degradation, limited antiviral activity, and toxicities from off-target covalent modification of other biomolecules. 5 In contrast, our efforts have been directed to discovery of nonpeptidic, noncovalent inhibitors of SARS-CoV-2 M pro that are drug-like and show both high inhibitory and antiviral activity. Specifically, the present effort started from a virtual screen of ca. 2000 known, approved drugs that led to identification of 14 drugs as inhibitors of SARS-CoV-2 M pro with IC 50 values as low as 5 μM in a kinetic assay. 12 As stated, the goals were to identify possible drugs for repurposing and to provide clearly drug-like hits for lead optimization to yield highly potent antiviral agents. 12 We now report successful execution of the latter strategy starting from a weak hit, the anti-epileptic drug perampanel, in the kinetic assay. The optimization proceeded extraordinarily rapidly owing to the use of free-energy perturbation (FEP) calculations to guide the choices of structural modifications. 13−17 ■ RESULTS AND DISCUSSION Structural Analysis, FEP Calculations, and Initial Designs. Perampanel (1) showed activity in the enzyme assay, though only a rough IC 50 of 100−250 μM could be established owing to interference of the compound's fluorescence with the product of the assay. 12 However, its relatively simple structure is amenable to synthesis of analogues and its docked structure from the work in ref 12 was compelling ( Figure 1 ). The structure reflects a cloverleaf motif for active compounds with the three leaves occupying the binding pockets referred to as S1, S1′, and S2, 10 as identified in Figure 1A . The phenyl, cyanophenyl, and pyridinyl groups of 1 are predicted to reside in the three pockets with the central pyridinone ring acting as the connecting hub. The catalytic residues Cys145 and His41 are located at the bottom of the site, as drawn, and other key surrounding residues are noted in Figure 1B . The analyses began by close examination of the docked complex with locations of some notable interaction points highlighted in Figure 1B by the circled letters. (a) The backbone NH of Glu166 is directed at the pyridinone but it does not form a hydrogen bond. (b) The pyridine nitrogen is directed toward the solvent so it is not helpful to binding. (c) The pyridine ring makes an edge-to-face aryl−aryl interaction with His41. It appears that there might be room for an additional small group in the meta position. (d) The cyano group of 1 is directed well at the NH of Cys145. In the docked structure the N···N distance is 3.94 Å, which shortens to 3.46 Å upon conjugate-gradient optimization of the complex using the MCPRO program 18 with the OPLS-AA/M force field for the protein 19 and OPLS/CM1A for the ligand. 20, 21 However, the carbonyl group of the pyridinone ring does not participate in a hydrogen bond and is blocked from solvation by the side chain of Asn142. In addition, the C3−C4 edge of the cyanophenyl ring is proximal to the opposing backbone oxygen and NH of Thr26. (e) The phenyl ring in the S1 pocket appears mismatched with the polar environment, which includes the side chains of Ser1B, His163, and Glu166. It is noted that a meta-CH is well directed at N ϵ of His163 with a C···N separation of 3.38 Å. Considering these features, several modifications of 1 to enhance binding seemed reasonable to pursue: switching the carbonyl group from C2 to C6 to form a hydrogen bond with the NH of Glu166, removing the pyridine nitrogen and adding a small group at C3 of the pyridine ring, leaving the cyano group and/or introducing a hydrogen bonding edge at C2−C4 of the cyanophenyl ring, and replacing the phenyl ring in S1 with a heterocycle that could hydrogen bond with His163. FEP calculations were used to explore the possible benefits of such changes. The necessary structures were built with the BOMB program 13 and the FEP calculations were carried out using standard protocols with the MCPRO program and the previously mentioned force fields. 13, 14 Relative free energies of binding, ΔΔG b , are obtained by mutating the ligand from structure A to structure B for both the protein−ligand complex in water and the unbound ligand. The configurational sampling for the systems was carried out at 25°C with Monte Carlo simulations including the 242 protein residues nearest to the active site and 1250 and 2000 TIP4P water molecules 22 for the ligand-bound and ligand-free calculations. Briefly, starting from perampanel, switching the carbonyl group from C2 to C6 was predicted to be very favorable (ΔΔG b = −4.7 ± 0.3 kcal/mol) with formation of the hydrogen bond with Glu166; replacement of the S1 benzene ring by 2-, 3-, or 4-pyridine, 2,4-pyrimidine, 2,4,6-triazine, and 4-pyridine-N-oxide showed no benefit except for the 3-pyridine (ΔΔG b = −3.6 ± 0.2 kcal/ mol), which gave a hydrogen bond with His163; and a chlorine scan for benzene in the S2 site predicted significant benefit for a meta-Cl directed inward toward His41, neutral effects for a Cl at the exposed ortho and meta positions, and strong disfavoring (4−6 kcal/mol) for a Cl at the para and inward-ortho positions. The combination led us to focus immediately on 2 as a target ( Figure 2 ). Additional model building with BOMB/ MCPRO for numerous heterocycles replacing the cyanophenyl group in the S1′ site also led us to 3 for which the central HNCO of the uracil is expected to form hydrogen bonds with the Thr26 backbone. 3,5-Dichloro analogues such as 4 were also anticipated to be viable in view of the FEP results and the expected factor-of-two benefit for binding due to the added symmetry and the predicted strong preference for the chlorine atoms in 2 and 3 to be directed inward. Initial Designs are Confirmed by an M pro Inhibition Assay and Crystallography. As detailed in the Supporting Information, 2, 3, and 4 were synthesized. Inhibition of proteolytic activity was tested using recombinant SARS-CoV-2 M pro , which was expressed and purified as previously described. 8, 12 For the kinetic assays, 100 nM M pro in reaction buffer (20 mM Tris, 100 mM NaCl, 1 mM DTT, pH 7.3) was incubated with or without compound in DMSO at varying concentrations to a final DMSO concentration of 6% for 15 min with shaking at room temperature. The reaction was initiated by addition of substrate (Dabcyl-KTSAVLQ↓ SGFRKM-E(Edans-NH2); GL Biochem) in reaction buffer, which is cleaved by M pro , generating a product containing a free Edans group. Fluorescence was monitored at an excitation wavelength of 360 nm and emission wavelength of 460 nm. Baseline subtraction controlled for intrinsic fluorescence of each compound as well as intrinsic fluorescence of the uncleaved FRET substrate. All tested compounds had purity of at least 95% based on HPLC, and all measurements were performed in triplicate and averaged. As reflected in Table 1 , the initial results were gratifying with IC 50 values for 2, 3, and 4 of 10.0, 6.4, and 4.0 μM showing striking improvement over the >100 μM for perampanel (1) . Both the cyanophenyl and uracilyl alternatives are viable, with a small preference for the uracil 3. Furthermore, addition of the second chlorine atom to 2 in going to 4 did provide the expected ca. factor-of-2 enhancement in inhibitory activity. Fortunately, it was also possible to obtain a high-resolution (1.6 Å) X-ray crystal structure for the complex of 4 with SARS-CoV-2 M pro . As shown in Figure 3 , the crystal structure fully confirmed the expectations from the modeling. There are three protein−ligand hydrogen bonds between the pyridinone oxygen and Glu166 nitrogen (2.84 Å), nitrile nitrogen and nitrogen of Cys145 (3.14 Å), and pyridine nitrogen and N ϵ of His163 (2.92 Å). In addition, a chlorophenyl edge packs well against the imidazole ring of His41 in the S2 pocket with no indication of space for expansion. The overall structure of the protein is essentially identical with that used for the original modeling (PDB ID: 5R82 23 ) with an rms deviation of 0.62 Å between the protein C α atoms. Lead Optimization in S3−S4 Delivers 20 nM Inhibitors. After this initial advance, consideration turned toward growth into the S3−S4 region ( Figure 1A ) to obtain increased potency. Model building and the crystal structure for 4 made it clear that it should be possible to replace the metachlorine near Gln189 with a variety of alkyl or alkoxy groups that would terminate in the hydrophobic S4 site. Again, FEP calculations were executed to obtain ΔΔG b values for replacing the chlorine with 11 alternatives. In all, 15 FEP calculations were executed as above to link the alternatives in sequences such as CH 3 13 The resultant ΔΔG b values are reported in Table 2 . The results were promising with expectation for improvements especially with alkoxy groups containing 4 or 5 non-hydrogen atoms. In these cases, a CH 2 or CH 3 group is being placed in the hydrophobic S4 site. Past experience with this FEP methodology has indicated that the range of the computed ΔΔG b values is larger than observed by experiment, but that improvements in activity are almost always found when ΔΔG b is more favorable than 2−3 kcal/mol. 13 Thus, the propoxy 5 and methoxyethoxy 6 analogues of 4 were synthesized, and they were found to have IC 50 values of 0.14 and 0.47 μM, respectively ( Figure 4 , Table 1 ). The FEP results were again nicely predictive, and the factor of ca. 30 improvement in the potency for 5 over 4 is striking. It was also possible to obtain a crystal structure for the complex of 5 with SARS-CoV-2 M pro at 1.8 Å resolution, as shown in Figure 5 . In this case, the asymmetric unit contains two M pro monomers and two copies of 5. The binding sites are nearly identical in both copies with minor width variation in the S4 region, which may arise from crystal packing. The structure shows little change from that for 4 with the close packing of the chlorophenyl fragment and His41, and the three protein− inhibitor hydrogen bonds. The notable addition is the propoxy group, which extends to place the terminal methyl group in the hydrophobic region at the juncture of Met165, Leu167, and Pro168 in the S4 site. The electron density for 5 is very welldefined ( Figure S1 ) and shows that the terminal OCCC dihedral angle is gauche to allow contact of the methyl group with terminal methyl groups of Met165 and Leu167. The packing in this region is illustrated in Figure 5b . It is also noted that the CC ipso OC anisole fragment is planar and directed toward Glu166, as expected from the modeling and the steric blockage for projection in the opposite direction toward Gln189 (Figure 5b) . Many other possibilities for the S3−S4 appendage were modeled by building structures of the complexes with BOMB including ones that incorporated phenyl or heterocyclic rings. Both benzyloxy and phenethyloxy groups appeared promising, as illustrated in Figure 6 , and synthetic access to these and substituted analogues from the common phenolic precursor was also an attractive feature. The benzyloxy analogue is predicted to replace the ethyl terminus of 5 with a phenyl edge that occupies the S4 site ( Figure 6A ), while the higher homologue is fully extended and shows striking face-to-face contact between Pro168 and the phenyl ring ( Figure 6B) . A potential drawback of the latter structure is that the backbone carbonyl of Glu166 is likely blocked from hydrogen-bonding with a water molecule, which is observed in the crystal structures for 4 and 5. In the balance, it was decided to synthesize 7 and 8 (Figure 4) , which yielded IC 50 values of 0.28 and 0.51 μM, respectively, in the enzyme inhibition assay. Thus, they are competitive in potency with 5 and 6, and modeling of substituted analogues was auspicious and is considered further below. At this point, before turning back to the uracil series, curiosity arose for preparation of 9, the 5-pyrimidinyl analogue of 4. The thoughts were that the second nitrogen atom would be solvent exposed or possibly form a hydrogen bond with the side chain amino group of Asn142, which itself is solventexposed; however, there would be the factor-of-two symmetry gain for binding the pyrimidine over the pyridine. On the other hand, the added nitrogen atom would decrease the basicity and hydrogen-bond-accepting ability of the nitrogen atom that is hydrogen-bonded to His163. In the event, 9 was prepared and assayed yielding an IC 50 in the 1−10 μM range. So, the latter consideration appears to dominate, and it was decided to continue with 3-pyridinyl for the S1 site. The meta-methoxy uracil analogue 10 was synthesized and did give an improvement in IC 50 to 1.2 μM from the 6.4 μM for the unsubstituted 3. The 3,5-dichloro uracil analogue corresponding to 4 was not prepared, but based on the results for 2−4, it would be expected to have an IC 50 of 2−3 μM. Thus, little benefit is apparent from changing the chlorine to a methoxy group, which is consistent with the FEP prediction in Table 2 . A larger alkoxy group is needed as in 5 and 6 to extend to the S4 site. Compounds 11−13 were then prepared to explore the effects of propoxy, butoxy, and isopentoxy alternatives for the uracil series. As expected from the FEP results, the activities are significantly improved, though expansion of the alkyl ether substituent beyond the propoxy analogue 11 (0.120 μM) does not provide further benefit. The benzyloxy uracil analogue 14 was prepared and also showed good activity with an IC 50 of 0.128 μM and there was additional gain for the phenethyloxy homologue 15 at 0.110 μM. It was expected that further progress was more likely to arise by addition of small groups at the ortho and meta positions in 14, which might better fill the S4 site as suggested in Figure 6A . FEP calculations were carried out and predicted gains in free energy of binding of 2−3 kcal/mol for methyl, fluorine, or chlorine substituents, which is large enough to usually yield observed benefits. The monomethyl analogues 16 and 17 were prepared and did show small improvement over 14 to 0.10 and 0.11 μM, respectively, ( Table 1) in spite of the expected loss from the reduced symmetry. At this point, a crystal was obtained for the complex of 14, which was of particular interest since it was the first structure for the uracil series (Figure 7) . It was gratifying to see that the five anticipated hydrogen bonds between the ligand and M pro were all present: pyridinone carbonyl oxygen with Glu166 N (2.79 Å), the pyridine nitrogen with His163 (2.97 Å), and the uracil O−NH−O edge with the NH of Cys145, and backbone O and NH of Thr26 (3.55, 3.45, and 3.37 Å). No water molecules are located in the crystal structure between the protein and the ligand, while there are three hydrogen-bonded water molecules bridging between the backbone oxygen of Glu166 and the pyridinone oxygen, and between the central uracilyl NH and CO and the backbone oxygen and NH of Thr26. Furthermore, the conformation of the benzyloxy side chain is extended with a phenyl edge in the S4 site as in Figure  6A . This supported further exploration of small substituents at the ortho and meta positions, which was realized by synthesis of the fluorine and chlorine substituted analogues 18−21. The outcome was highly productive yielding inhibitors with IC 50 values of 0.018−0.037 μM with the ortho-chloro analogue 21 being the most potent ( Figure 4 , Table 1 ). As detailed in the Supporting Information, a crystal structure for the complex of 21 with M pro was obtained at 2.2-Å resolution (PDB ID: 7L13), which clearly documents ideal placement of the ortho-Cl in the pocket between Met165 and Leu167. 22 and 23, monofluoro analogues of 21, were then prepared and are also potent inhibitors at 0.036 and 0.020 μM. Simultaneously, additional inhibitors in the low nanomolar potency range were obtained by replacing the propyloxy group in the uracil 11 with cyclopropylmethyloxy (24, 0.037 μM) and 3,3,3-trifluoropropyloxy (25, 0.025 μM). These analogues modeled well and are expected to show enhanced metabolic stability through reduced oxidation compared to 11. The corresponding analogues in the cyanophenyl series (26, 27) were also prepared, but with IC 50 values of 0.170 and 0.120 μM, they showed similar potency as for 5 (0.140 μM) in contrast to the 3-fold boost in the uracil series for 24 vs 11. A crystal structure for the complex of 26 with M pro was obtained at 1.7 Å resolution (PDB ID: 7L14); it shows the C2−C3 edge of the cyclopropyl ring in close contact with Leu167, but with less ideal contact with Met165 compared to the ortho-chlorine atom in 21. Evaluation of Antiviral Activity Against SARS-CoV-2. To probe the series' potential for therapeutic value, several compounds were tested for inhibition of infectious SARS-CoV-2 replication in Vero E6 cells. Protection against the viral cytopathic effect was tested in two assays, as detailed in the Supporting Information. Due to the ability to multiplex in 96well plates and concurrently evaluate compound general cytotoxicity, a methylthiazolyl-diphenyl-tetrazolium bromide (MTT) dye 25 was used in the primary assay, while the more labor-intensive, lower-throughput viral plaque assay 26 was used to confirm antiviral activity. Previous studies have shown excellent correlation between the two assays. 25 In addition to the Vero E6 cells, compound cytotoxicity was also evaluated in normal human bronchial epithelial cells (NHBE) via MTT assays. 25 The results are summarized in Table 3 . For the MTT assays, three independent measurements were performed in triplicate to yield the indicated statistical uncertainties (±1σ). The viral plaque assay, which requires serial dilutions using 6well plates, was only performed once for each compound except 5. In that case, the results of three independent experiments provided an uncertainty of ±0.15 μM. It was found that 5 has antiviral potency against infectious SARS-CoV-2 in both the MTT and viral titer plaque assays with EC 50 values of 2.5 and 1.5 μM, a little above a reported value for the SARS-CoV-2 polymerase inhibitor, remdesivir. 24 For 14, antiviral activity was found in the viral plaque assay, but not in the MTT assay, perhaps due to infringing cytotoxicity or compound efflux. Likewise, 21 and 23 showed activity in the viral plaque assay, but lacked antiviral activity in the MTT assay, and they showed the greatest cytotoxicity. Small modifications of the uracils should be explored to seek reduced cytotoxicity. The most auspicious results are for 26, which exhibits potency near 1 μM in both antiviral assays, and it shows no cytotoxicity to the highest concentration tested (100 μM). The closely related 27, which just replaces the cyclopropylmethoxy group in 26 with trifluoropropoxy, is more active in the MTT assay at 1.1 μM, similar to remdesivir, but it is also more cytotoxic toward the NHBE cells. Other pharmacological properties of the compounds will be Examination of Drug Synergy with Remdesivir. A desirable feature for an antiviral drug candidate is synergistic behavior when used in combination with other antiviral agents. The major successes in treatment of HIV and Hepatitis C utilize a combination of viral polymerase and protease inhibitors. 27,28 Therefore, a preliminary investigation was undertaken with 5 to assess possible synergy with the FDAapproved polymerase inhibitor, remdesivir. A replicon assay with a noninfectious SARS-CoV-2 clone and a nanoluciferase reporter were utilized to evaluate combinations of 5 and remdesivir. The inhibitory data were analyzed using MacSynergy II, a 3D model for statistical evaluation of combination assays. 29 In this model, a simple additive effect results in a horizontal plane at 0% inhibition, whereas a synergistic or antagonistic effect will render a hill or depression above or below the plane. As shown in the 3D plot in Figure 8 , a range of combinations of 5 and remdesivir do provide values above the plane. This reflects statistically significant synergistic behavior with a ratio of 30.8/0 μM 2 % for the mean synergy volume/antagonism volume, as detailed in the Supporting Information.@story_separate@The Supporting Information is available free of charge at https://pubs.acs.org/doi/10.1021/acscentsci.1c00039. Details for the synthetic procedures, compound characterization, computations, assays, and crystallographic results (PDF) To a 250-mL thick walled round bottom flask 2-cyanophenyl boronic acid (2.204 g, 1.5 mmol, 15.0 equiv), 5-bromo-2-methoxypyridine (1.30 mL, 10.0 mmol, 1.0 equiv), palladium tetrakis (1.156 g, 10 mol%), Cs2CO3 (6.516 g, 20.0 mmol, 2.0 equiv) and DMF (100 mL) were added. The solution was sparged with N2 then the flask was sealed with a screw cap. The reaction was allowed to stir at 120°C for 15 hours. Once cooled to room temperature, the solvent was removed. The crude material was resuspended in EtOAc and extracted from water with EtOAc (3X) and DCM (3X). The combined organic layers were then dried over Na2SO4, filtered, and concentrated in vacuo. The crude material was then subjected to normal phase column chromatography (SNAP Ultra 100g, gradient = 0-10% EtOAc/Hex over 4 CV, then 10-15% EtOAc/Hex over 4 CV, then 15-100% EtOAc/Hex over 2 CV) to afford a white solid (1.497 g, 7.1 mmol, 71% yield). 1  A 40-mL thick walled flask equipped with a stir bar was charged with S1 (1.000 g, 4.76 mmol, 1.0 equiv), lithium chloride (1.008 g, 23.80 mmol, 5.0 equiv), ptoluenesulfonic acid (4.095 g, 23.80 mmol, 5.0 equiv), and DMF (16 mL). The flask was sealed with a screw cap and then heated to 120 °C for 2 hours. Once cooled to room temperature, the solution was quenched with water and subsequently filtered to afford a white solid (0.934 g, quant. yield). 1 H NMR (600 MHz, DMSO-d6) δ 11.98 (s, 1H), 7.96 -7.87 (m, 1H), 7.74 (td, J = 7.8, 1.2 Hz, 1H), 7.71 -7.66 (m, 2H), 7.60 (d, J = 7.9 Hz, 1H), 7.52 (t, J = 7.6 Hz, 1H), 6.51 -6.42 (m, 1H). 13 C NMR (151 MHz, DMSO-d6) δ 161. 6, 141.3, 140.4, 135.7, 133.8, 133.6, 129.4, 127.8, 119.7, 118.5, 115.4, 109.7 . HRMS (ESI) m/z: [M+H] + calcd for C12H9N2O + 197.0709, found 197.0714. To a 2-dram vial equipped with a stir bar S2 (0.196 g, 1.00 mmol, 1.0 equiv), 3pyridinylboronic acid (0.246 g, 2.00 mmol, 2.0 equiv), Cu-TMEDA (0.0464 g, 10 mol%), DMF (4.0 mL), and water (0.2 mL) were added. The reaction vessel was closed using a screw-cap with bonded septum, then affixed with an oxygen balloon and stirred under an oxygen atmosphere for 4 days. The mixture was then diluted with DCM and water. The aqueous layer was rinsed with DCM (5X). The combined organic layer was then dried with Na2SO4, filtered, and concentrated in vacuo. The crude material was purified using normal phase column chromatography (SNAP Ultra 50g, gradient = 0-6% DCM/MeOH over 5 CV, then 6-7% DCM/MeOH over 3 CV, then 7-10% DCM/MeOH over 2 CV) to afford 0.184 g (67% yield) of a white solid. 1 6 .69 (d, J = 9.5 Hz, 1H). 13  To a 2-dram vial equipped with a stir bar S2 (39.6 mg, 0.20 mmol, 1.0 equiv), pyrimidine 5-boronic acid (45.9 g, 0.40 mmol, 2.0 equiv), Cu-TMEDA (9.3 mg, 10 mol%), DCM (1.0 mL) and DMF (1.0 mL) were added. The reaction vessel was closed using a screw-cap with bonded septum, then affixed with an oxygen balloon and stirred under an oxygen atmosphere for 6 days. The solution was diluted with water and extracted with EtOAc/Hex (1:1; 3X), then DCM (3X). The combined organic layer was dried over Na2SO4, filtered, and concentrated in vacuo. The crude material was used in the following step without further purification. A 2-dram vial equipped with a stir bar was charged with crude S3a (0.273 g, 1.00 mmol, 1.0 equiv) and DMF (2 mL). While stirring at room temperature NBS (0.214 g, 1.20 mmol, 1.2 equiv) was added. After 15 hours, the reaction was quenched with ice water, and subsequently filtered to afford 0.313 g of a white solid (89% yield), which was used without further purification. Analytically pure material was obtained using normal phase column chromatography (SNAP Ultra 50g, gradient = 0-100% EtOAc/Hex over 4 CV, then 100% EtOAc/Hex over 6 CV  A 2-dram vial equipped with a stir bar was charged with crude S3b (54.9 mg, 0.20 mmol, 1.0 equiv) and DMF (1 mL  Benzyl alcohol (6.75 g, 62.48 mmol, 1.1 eq) was dissolved in anhydrous THF (120 mL) and cooled to 0 °C. Then, 60% sodium hydride (2.73 g, 68.2 mmol, 1.2 eq) was added and stirred for 30 min at 0 °C. Next, 5-bromo-2fluoropyridine (10 g, 56.8 mmol, 1.0 eq) was added and the mixture was heated at 70 °C overnight. The reaction was quenched by the dropwise addition of water, then brine and more ethyl acetate were added for extraction. The organic layer was dried over Na2SO4 and the solvent removed under reduced pressure, the residue was recrystallized in hexane to afford a white solid (12 g, 80% yield  To a 250 mL round bottom flask, (S5, 7 g, 26.5 mmol, 1.0 eq), (2,4dimethoxypyrimidin-5-yl)boronic acid (6.34 g, 34.45 mmol, 1.3 eq), Cs2CO3 (17.3 g, 53 mmol, 2.0 eq) and bis(triphenylphosphine)palladium(II) chloride (0.93 g, 1.3 mmol, 0.05 eq) were added and suspended in DMF (150 mL). The mixture underwent three cycles of vacuum/filling with N2, then stirred at 80 °C for 5 h. The mixture was concentrated in vacuo and the residue was resuspended in water (80 mL) and extracted with dichloromethane (2× 80 mL). The combined organic layer was concentrated in vacuo and the crude product was purified using silica gel chromatography with an ethyl acetate/hexanes gradient (0 -10 %) to afford a white solid (6.2 g, 72% yield).  To a 250 mL round bottom flask, S6 (6.2 g) and palladium on activated carbon (10%, 500 mg) were suspended in methanol (150 mL) and water (10 mL). The mixture underwent 3 cycles of vacuum/filling with H2 and then stirred at 40 °C for 4 h. After the reaction was complete, dichloromethane was added to dissolve the solid product, then the mixture was filtered. The filtrate was concentrated in vacuo to give the desired product as a gray solid (4.4 g, 99% yield).  To a 250 mL round bottom flask, S7 (4.4 g, 18.9 mmol, 1.0 eq), 3pyridylboronic acid (4.65 g, 37.8 mmol, 2.0 eq), cupric acetate (3.43 g, 18.9 mmol, 1.0 eq) and N,N,N',N'-Tetramethylethylenediamine (4.4 g, 37.8 mmol, 2.0 eq) were suspended in anhydrous DMF (120 mL). Dry air was bubbled through the mixture and the solution was then stirred at room temperature for 4 days. After the reaction was complete, the mixture was concentrated in vacuo and the residue was diluted with aq. ammonium (5%, 40 mL) and the organic layer was extracted with dichloromethane (3× 40 mL). The combined organic layer was concentrated in vacuo and the crude product was purified using silica gel chromatography with a methanol/ dichloromethane gradient (0-5 %) to yield the desired product as a white solid (  To a 250 mL round bottom flask, S8 (4.3 g 13.9 mmol, 1.0 eq) was dissolved in anhydrous DMF (80 mL). The mixture underwent 3 cycles of vacuum/filling with N2, then N-bromosuccinimide (9.9 g, 55.6 mmol, 4 eq) was added and the solution was stirred at room temperature for 4 h. Then the reaction was quenched with aqueous sodium thiosulfate solution (1 M, 50 mL) at 0 °C and stirred at this temperature for 2 h. The aqueous layer was extracted with dichloromethane (3× 60 mL) and then the combined organic layer was concentrated in vacuo at low temperature (20 °C). The residue was further dried using a vacuum pump, then the crude product was purified using silica gel chromatography with a methanol/ dichloromethane gradient (0 -5 %) to afford an orange solid (3.7 g, 69% yield  To a solution of 3-bromo-5-chlorophenol (1.04 g, 5.0 mmol, 1.0 eq) and K2CO3 (1.38 g, 10 mmol, 2.0 eq) in DMF (30 mL) the requisite alkyl halide (5.1 mmol, 1.02 eq) was added and the solution was stirred at 80 °C for 1 h. Once complete, the mixture was concentrated in vacuo and the residue was resuspended in water (40 mL) and extracted with dichloromethane (2× 40 mL). The combined organic layer was concentrated in vacuo and the crude product was purified using silica gel chromatography (100% hexane) to yield the desired compound. Compound S10a was prepared according to General Procedure A described using 1-bromo-2-methoxyethane. S-11 Compound S10b was prepared according to General Procedure A using 3-bromo-5-chlorophenol and (2-bromoethyl)benzene. 1 H NMR (400 MHz, CDCl3) δ 7.35 -7.29 (m, 2H), 7.27 -7.22 (m, 3H), 7.08 (s, 1H), 6.94 (s, 1H), 6.82 (t, J = 1.8 Hz, 1H), 4.13 (t, J = 7.0 Hz, 2H), 3.08 (t, J = 6.9 Hz, 2H). Compound S10c was prepared according to General Procedure A using 1bromobutane.  Compound S10d was prepared according to General Procedure A using 1bromo-3-methylbutane.  Compound S10e was prepared according to General Procedure A using 1-  Compound S10f was prepared according to General Procedure A using 1-(bromomethyl)-3-methylbenzene.  Compound S10g was prepared according to General Procedure A using 1-  S-12 Compound S10h was prepared according to General Procedure A using 1-(bromomethyl)-3-fluorobenzene. 1 H NMR (400 MHz, CDCl3) δ 7.36 (td, J = 7.9, 5.9 Hz, 1H), 7.19 -7.09 (m, 3H), 7.07 -7.00 (m, 2H), 6.91 (t, J = 2.0 Hz, 1H), 5.02 (s, 2H). Compound S10i was prepared according to General Procedure A using 2-(bromomethyl)-1,3-difluorobenzene. 1 H NMR (400 MHz, CDCl3) δ 7.41 -7.31 (m, 1H), 7.16 -7.12 (m, 1H), 7.06 (d, J = 1.7 Hz, 1H), 7.00 -6.91 (m, 3H), 5.09 (s, 2H). Compound S10j was prepared according to General Procedure A using 1chloro-2-(chloromethyl)benzene.  Compound S10k was prepared according to General Procedure  Compound S10m was prepared according to General Procedure A using 5bromo-3-chloro-2-fluorophenol and 1-chloro-2-(chloromethyl)benzene. 1  Compound S10l was prepared according to General Procedure A using  3-Bromo-5-chlorophenol (1.04 g, 5.0 mmol, 1.0 eq), 3,3,3-trifluoropropan-1-ol (0.63 g, 5.5 mmol, 1.1 eq) and PPh3 (1.97 g, 7.5 mmol, 1.5 eq) were dissolved in dry THF (60 mL). The mixture was cooled to 0 °C and S-13 underwent 3 cycles of vacuum/filling with N2. Diisopropyl azodicarboxylate (1.52 g, 7.5 mmol, 1.5 eq) was added dropwise and the mixture was warmed to room temperature slowly while stirring for 30 min. Then, the mixture was heated to 80 °C and stirred overnight. After the reaction was complete, the mixture was concentrated in vacuo and the residue was redissolved in dichloromethane, and washed with saturated aqueous NH4Cl, water, then brine. The combined organic layer was dried over MgSO4, and the crude product was purified using silica gel chromatography (100 % hexanes) to afford S10n (0.68 g, 45% yield). 1 H NMR (400 MHz, CDCl3) δ 7.14 (t, J = 1.6 Hz, 1H), 6.97 -6.93 (m, 1H), 6.84 (t, J = 2.0 Hz, 1H), 4.16 (t, J = 6.5 Hz, 2H), 2.62 (qt, J = 10.4, 6.5 Hz, 2H). Aryl bromide (1.0 equiv), boronic acid (2.0 equiv), palladium tetrakis (10 mol%), cesium carbonate (2.5 equiv) and DMF (0.07 M) were added to a 2-dram vial equipped with a stir bar. The solution was sparged with N2, then the vial was sealed with a Teflon cap and heated to 120°C for 15-20 hours. Once cooled to room temperature, the DMF was removed, and the crude material was purified via column chromatography to afford the desired product. Aryl bromide (1.0 equiv), boronic acid (1.5 equiv), PdCl2(PPh3)2 (10 mol%), potassium carbonate (2.0 equiv) were suspended in DMF (10 mL). The mixture underwent three cycles of vacuum/filling with N2, then stirred at 120 °C for 1 h. After the reaction was complete, the mixture was concentrated in vacuo, then the residue was extracted with DCM (2× 40 mL) and water (40 mL). The combined organic layer was concentrated, and the crude product was purified using silica gel chromatography with a dichloromethane / ethyl acetate / methanol (92% / 5% / 3%) gradient to afford the desired product as a white solid. Aryl bromide (1.1 equiv), bis(pinacolato)diboron (1.15 equiv), KOAc (2.2 equiv), and (PPh3)2PdCl2 (20 mol%) and DMF (0.04 M) were added to a 2-dram vial equipped with a stir bar. The solution was sparged with N2, then the vial was sealed with a Teflon cap and heated to 80°C for 1-3 hours. Once cooled to room temperature, methanol (0.4 mL) was added to scavenge excess of pinacolborane. Pyridone bromide (1.0 equiv) and K2CO3 (2.0 equiv) were added to the solution, which was then sparged with N2, sealed with a Teflon cap, and heated to 120 °C for 1-15 hours. Once cooled to room temperature, the DMF was removed, water was added, and the solution was extracted with DCM (3X). The combined organic layer was dried with Na2SO4, filtered, and then concentrated in vacuo. Uracil compounds were purified using purified using silica gel chromatography with a dichloromethane / ethyl acetate / methanol (92% / 5% / 3%) gradient to afford the target compound as a white solid. The purification of the cyanophenyl compounds is specified for each compound below. Substituted 2,4-dimethyoxypyrimidine (1.0 equiv) was dissolved in DMF (10 mL) along with LiCl (10 equiv) and p-toluenesulfonic acid (10 equiv). The solution was then stirred at 80 °C for 30 min. After completion, the mixture was concentrated in vacuo. The residue was suspended in saturated aqueous NaHCO3 (15 mL), then the mixture was filtered. The solid was washed with S-14 saturated aqueous NaHCO3, water, and then hexanes. The material was then dried with a lyophilizer to yield the desired compound as a pale yellow solid. General Procedure C was employed using S9 and (3-chloro-5propoxyphenyl)boronic acid to afford the title compound as a white solid (130.6 mg, 68% yield  General Procedure D was employed using S9 and 1-bromo-3butoxy-5-chlorobenzene (S10c) to afford the title compound (66 mg, 67% yield  General Procedure D was employed using S9 and 1-bromo-3-chloro-5-(isopentyloxy)benzene (S10d) to afford the title compound as a white solid (73.9 mg, 73% yield  General Procedure D was employed using S9 and 1-bromo-3-chloro-5-phenethoxybenzene (S10b) to afford the title compound as a white solid (65.9 mg, 61% yield  General Procedure D was employed using S9 and 1-bromo-3chloro-5-((2-methylbenzyl)oxy) benzene (S10e) to afford the title compound as a white solid (83.2 mg, 77% yield  General Procedure D was employed using S9 and 1-bromo-3chloro-5-((3-methylbenzyl)oxy) benzene (S10f) to afford the title compound as a white solid (76.7 mg, 71% yield  General Procedure D was employed using S9 and 1-bromo-3chloro-5-((2-fluorobenzyl)oxy) benzene (S10g) to afford the title compound as a white solid (62 mg, 57% yield  S-16 General Procedure D was employed using S9 and 1-bromo-3chloro-5-((3-fluorobenzyl)oxy) benzene (S10h) to afford the title compound as a white solid (71.8 mg, 66% yield  General Procedure D was employed using S9 and 2-((3-bromo-5chlorophenoxy)methyl)-1,3-difluorobenzene (S10i) to afford the title compound as a white solid (60.7 mg, 54% yield). 1  General Procedure D was employed using S9 and 1-bromo-3chloro-5-((2-chlorobenzyl)oxy)benzene (S10j) to afford the title compound as a white solid (124.2 mg, 55% yield).  General Procedure D was employed using S9 and 1-((3-bromo-5-chlorophenoxy)methyl)-2-chloro-4-fluorobenzene (S10k) to afford the title compound as a white solid (58.9 mg, 51% yield  General Procedure D was employed using S9 and 5-bromo-1chloro-3-((2-chlorobenzyl)oxy)-2-fluorobenzene (S10l) to afford the title compound as a white solid (77.4 mg, 67% yield  General Procedure D was employed using S9 and 1-bromo-3-chloro-5-(cyclopropylmethoxy)benzene (S10m) to afford the title compound as a white solid (73.5 mg, 75% yield  General Procedure D was employed using S9 and 1-bromo-3chloro-5-(3,3,3-trifluoropropoxy) benzene (S10n) to afford the title compound as a white solid (66 mg, 62% yield  S-18 General Procedure B was employed using S4a (12.3 mg, 0.035 mmol, 1.0 equiv) and (3-chlorophenyl)boronic acid (10.9 mg, 0.070 mmol, 2.0 equiv). Purification was accomplished using normal phase column chromatography (SNAP Ultra 25g, gradient = 0-80% EtOAc/Hex over 9 CV, then 80% EtOAc/Hex over 2 CV, then 80-100% EtOAc/Hex over 2 CV) to afford 13  General Procedures C and E were employed using S9 and (3chlorophenyl)boronic acid to afford 3 (81.5 mg, 52 % yield  General Procedure B was employed using S4a (26.0 mg, 0.074 mmol, 1.0 equiv) and (3,5-dichlorophenyl)boronic acid (28.2 mg, 0.15 mmol, 2.0 equiv). Purification was accomplished using normal phase column chromatography (SNAP Ultra 25g, gradient = 0-100% EtOAc/Hex over 10 CV, then 100% EtOAc/Hex over 3 CV) followed by reverse phase column chromatography (SNAP Ultra C18 30g, gradient = 30-68% MeCN/H2O over 5 CV, then 68% MeCN/H2O over 2 CV, then 68-100% MeCN/H2O over 5 CV) to afford 9.3 mg (30% yield) of a pale pink solid.  General Procedure B was employed using S4a (26.4 mg, 0.075 mmol, 1.0 equiv) and (3-chloro-5-propoxy-phenyl)boronic acid (32.2 mg, 0.15 mmol, 2.0 equiv). Purification was accomplished using normal phase column chromatography (SNAP Ultra 25g, gradient = 0-90% EtOAc/Hex over 9 CV, then 90% EtOAc/Hex over 2 CV, then 90-100% EtOAc/Hex over 1 CV) followed by reverse phase column chromatography (SNAP Ultra C18 30g, gradient = 0-85% MeOH/H2O over 7 CV, then 85% MeOH/H2O over 2 CV, then 85-88% MeOH/H2O over 2 CV, then 88-100% MeOH/H2O over 2 CV) to afford  General Procedure D was employed using S10a (39.8 mg, 0.15 mmol, 1.1 equiv) and S4a (50.0 mg, 0.14 mmol, 1.0 equiv).  General Procedure B was employed using S4a  General Procedure D was employed using S10b (48.7 mg, 0.15 mmol, 1.1 equiv) and S4a (50.0 mg, 0.14 mmol, 1.0 equiv). Purification was accomplished using reverse phase column chromatography (SNAP Ultra C18 60g, gradient = 0-100% MeOH/H2O over 8 CV, then 100% MeOH/H2O over 3 CV), then normal phase column chromatography (SNAP Ultra 25g, gradient = 0-80% EtOAc/Hex over 7 CV, then 80% EtOAc/Hex over 3 CV, then 80-100% EtOAc/Hex over 2 CV) to afford 31.8 mg (44% yield over 2 steps) of a white solid.  General Procedure B was employed using S4b (15.0 mg, 0.04 mmol, 1.00 equiv) and (3-chloro-5-propoxyphenyl)boronic (18.2 mg, 0.08 mmol, 2.00 equiv). Purification was accomplished using reverse phase column chromatography (SNAP Ultra C18 30g, gradient = 0-87% MeOH/H2O over 8 CV, then 87 MeOH/H2O over 2 CV, then 87-100% MeOH/H2O over 1 CV, then 100% MeOH/H2O over 3 CV), then normal phase column chromatography (SNAP Ultra 25g, gradient = 0-8% DCM/MeOH over 8 CV) to afford 8.0 mg (43% yield) of a white solid.  General Procedures C and E were employed using S9 and (3-chloro-5methoxyphenyl) boronic acid to afford 10 (54 mg, 64% yield  General Procedure E was employed using S11a to afford 11 as a pale yellow solid (105 mg, 93% yield  General Procedure E was employed using S11b to afford 12 (56.5 mg, 91% yield  Vero-E6 were cultured in Dulbecco's Modified Eagle Medium (DMEM) with 10% heatinactivated fetal bovine serum (FBS), and 1% Penicillin/Streptomycin unless otherwise indicated. For Vero-E6, 5 μg/mL of puromycin (GIBCO) and 5 μg/mL blasticidin (GIBCO) were added as appropriate. Normal human bronchial epithelial cells (16HBE140-) were a kind gift of Dr. Marie Egan, Yale University School of Medicine. Cells were grown in T75 flasks coated with fibronectin, BSA, and Collagen ECM at 37°C in a humidified incubator with 5% CO2. Cells are grown in minimum essential media containing 10% FBS and 1% penicillin /streptomycin (Gibco). To generate viral stocks, Vero-E6 cells were inoculated with the SARS-CoV-2 isolate USA-WA1/2020 (BEI Resources #NR-52281) at an MOI of 0.01 for three days to generate a P1 stock. The P1 stock was used to inoculate Vero-E6 cells for three days at approximately 50% cytopathic effects. Virus titer was determined by plaque assay using Vero-E6 cells. Vero-E6 cells were seeded at 4 × 10 5 cells/well in 12-well plates and infected for 1 hour with the SARS-CoV-2 isolate USA-WA1/2020 at an MOI of 0.01. The cells were washed twice to remove residual unattached virus. Serial dilutions of each compound (0.1% DMSO in 2% FBS in DMEM media) were added to the cells and incubated at 37 °C (2 dpi). After 2 dpi, the supernatant containing virus was cleared from cell debris at 1000 rpm for 10 min and frozen until analysis via plaque assay. For the plaque assay, Vero-E6 cells were seeded at 7.5 × 10 5 cells/well in 6-well plates. The following day, the media was removed and replaced with 100 μL of 10-fold serial dilutions of previously frozen viral supernatant. Plates were incubated at 37°C for 1 hour with gentle rocking. Subsequently, overlay media (DMEM, 2% FBS, 0.6% Avicel RC-581) was added to each well. At 2 dpi for SARS-CoV-2 plates were fixed with 10% formaldehyde for 30 min, stained with crystal violet solution (0.5% crystal violet in 20% ethanol) for 30 min, and then rinsed with deionized water to visualize plaques. The antiviral activity of compounds was examined by evaluating the cytopathic effect in Vero-E6 cells grown at 37°C in a 5% CO2 atmosphere for 72 h using 96 multi-well plates (50,000 cells/ well) using 3-(4,5-dimethyl-2-thiazolyl)-2,5-diphenyl-2H-tetrazolium bromide (MTT; Sigma-Aldrich) method according to the manufacturer's instructions. Cells were challenged with SARS-CoV-2 at a multiplicity of infection (MOI) of 0.01. The virus was added together with the compound(s) under investigation and incubated in DMEM supplemented with 2% FBS and using 0.1% DMSO with no inhibitor as a control. To assess in vitro antiviral activity, serial dilutions of compounds in (0.1% DMSO in 2% FBS in DMEM media) were made in a concentration range of 0.1 M to 25 M. Optical densities were measured at 560/620 nm with a Spectramax Plate Reader. Three independent experiments with triplicate measurements were performed. Data were analyzed by a four-parameter curve-fitting from a dose-response curve using GraphPad Prism (version 7.00) to calculate the EC50 (concentration of the compound that inhibited 50% of the infection) based on the MTT method. Concurrently in this experiment, general cellular cytotoxicity in the absence of virus was determined. The MTT assay was also used to assess compound cytotoxicity in human normal bronchial epithelial cells. Statistical significance was determined as p < 0.05 using GraphPad Prism 7 unless otherwise indicated. Experiments were analyzed by unpaired two-tailed t tests, Mann-Whitney test, or ANOVA, as indicated. A SARS-CoV-2 replicon was generated by replacing the Spike gene with a Nano luciferase (Nluc) gene within a full-length infectious cDNA clone of the viral genome stably maintained within the yeast artificial chromosome (YAC) pCC1BAC-HIS3/SARS-CoV-2 (PMID: 32365353). Briefly, the Nluc gene was amplified to add flanking SARS-CoV-2 sequences and remove an internal EagI site in two steps. First, the 5´ end of the Nluc gene was amplified by using Q5 DNA polymerase (New England Biolabs) with primers YO-3778 (5´-GAG TTG TTA TTT  CTA GTG ATG TTC TTG TTA CAG TTC CAA TTG TGA AGA TTC TCA TAA ACA AAT CCA TAA GTT CGT TTA CGC  CAG AAT GCG TTC GCA CA-3´ ). The full-length Nluc gene was then amplified by using Q5 DNA polymerase with primers YO-3778 and YO-3779, and inserted into BamHI-linearized pCC1BAC-HIS3/SARS-CoV-2 by contransfection into yeast VL6-48N (PMID: 9207100) and selection of homologous recombinants on histidine-deficient media. YACs were recovered from liquid cultures by treatment with zymolase (Zymo Research) and ZymoPURE plasmid midiprep kits (Zymo Research), then transformed into Epi300 bacterial cells (Lucigen). Chloramphenicolresistant colonies were picked and grown in liquid media with CopyControl induction solution, then used to prepare amplified YAC by using the ZymoPURE plasmid midiprep kit. The repliconbearing YAC was sequence verified by whole plasmid sequencing (Massachusetts General Hospital Genome DNA Core) and linearized by overnight digestion with EagI. Linearized transcription templates were purified by treatment at 55°C with 0.5% (W/V) SDS and 3 units proteinase K followed by two rounds of phenol/chloroform extraction and ethanol precipitation. Replicon RNAs were transcribed from purified transcription templates with the T7 Ribomax kit (Promega) and anti-reverse cap analog (New England Biolabs). RNAs were purified by treatment with RQ1 DNase (Promega) and RNA Clean & Concentrator-25 kit (Zymo), eluted into 2 mM sodium citrate (pH 6.4), aliquoted in 1 µg portions, and stored frozen at -80°C. RNAs were transfected into BHK cells engineered to express a human codon-optimized SARS-CoV-2 nucleoprotein gene by electroporation (PMID: 9371625). Cells were then seeded in multiple replicates on 96-well plates containing serial dilutions of 5 and/or remdesivir. The following day, S-59 Nluc activity was measured by using NanoGlo reagents (Promega) with a CentroXS3 LB 960 microplate luminometer (Berthold). The combination inhibitory effects of compound 5 and remdesivir were tested in a 2-drug combination using replicon assay. The combination inhibitory data were analyzed using MacSynergy II 3D plots by Prichard and Shipman. 3 The resulting surface plots of the data reflect the difference between the experimental dose-response surface and the predicted additive surface. On a 3D model, a simple additive effect will result in a horizontal plane at 0% inhibition, whereas a synergistic or antagonistic effect will render a peak or depression above or below the horizontal plane. The volumes of the peaks/depressions were then calculated to quantify the effect of the drug combination on antiviral activity [synergy/antagonism volumes (μM 2 %)]. For these studies, synergy volumes are divided into the following categories: minor but significant synergy: 25-50 μM 2 %; moderate synergy: 50-100 μM 2 %; strong synergy: over 100 μM 2 %; minor antagonistic: −50 and −100 μM 2 %; and strong antagonistic: less than −100 μM 2 %. S-60 Recombinant SARS-CoV-2 M pro with native N-and C-termini was expressed and purified as previously described. 4, 5 Pure protein was buffer exchanged (20 mM Tris, 150 mM NaCl, 1 mM EDTA, 1 mM DTT, pH 7.8) and stored at -80 °C at 20 mg/mL. Samples were thawed on ice and subsequently incubated at 37 °C with 2 mM compound in DMSO for 30 minutes prior to centrifugation at 10,000xg. The supernatant was used to set up crystallization screens with the commercially available PEGRx1 and PEGRx2 screens (Hampton Research). Crystal screens were set up manually with 2 L drops with a 1:1 protein/reservoir solution ratio to equilibrate with 80L reservoir solution using the sitting-drop vapor-diffusion method at 18 °C. Plate-like or small, threedimensional crystals appeared overnight with most compounds in several conditions. Crystals were harvested, cryo-protected with 15% glycerol and flash frozen in liquid nitrogen. Diffraction data was collected at the 24-ID-E and 24-ID-C beamlines at the Advanced Photon Source and AMX beamline at the National Synchrotron Light Source II. Datasets were indexed using the XDS software. 6 Molecular replacement was performed using PHASER with previously solved structures from the PDB. 7 6Y2F was used as the search model for the structure of Mpro:Mpro4 (7L10), 6Y2E was used as the search model for the structure of Mpro:Mpro-14 (7L12), and 6Y2G was used as the search model for Mpro:Mpro5, 21, and 26 (7L11, 7L13, and 7L14, respectively). 4 Models were built using COOT and successive rounds of refinement performed with Phenix Refine. 8, 9 Diffraction data processing and refinement statistics are found in Table S1 . Crystallography software was compiled by SBGrid. 10 Table S1 . Diffraction data and refinement statistics. @story_separate@Our FEP-guided approach has led to a series of antiviral agents that has the potential to yield new therapies for treating SARS-CoV-2 infections. Rapid progress was made since experimental studies could only be initiated in our laboratories in June 2020. Following expression and purification of the target protein M pro and assay implementation, a virtual screen of ∼2000 known drugs led to the discovery of 14 drugs with micromolar inhibitory activity. 12 One of the weaker hits, perampanel, was chosen for redesign and optimization based on analyses of the predicted structure of its complex with M pro (Figure 1 ). FEP results favored selection of 3-pyridinyl and 3-chlrophenyl groups for the S1 and S2 pockets and repositioning of the pyridinone carbonyl group. These ideas were embodied in preparation of 2−4, which showed striking improvement in potency to the ca. 5 μM level. Further computational analyses combined with acquisition of multiple crystal structures for the complexes led to numerous inhibitors of M pro with ca. 20 nM potency. The reported compounds are highly notable as inhibitors of M pro since they are structurally novel, nonpeptidic, and noncovalent with low nanomolar activity. Initial results from technically challenging antiviral cellular assays using infectious SARS-CoV-2 confirmed the promise of the series with compound 26 emerging as particularly interesting with 1 μM antiviral activity and no cytotoxicity. Moreover, initial drug combination studies show synergistic behavior of 5 and the FDA-approved drug remdesivir. The series will continue to be pursued in our laboratories, while other investigators should benefit from the reported data and crystal structures.","[Image: see text] Starting from our previous finding of 14 known drugs as inhibitors of the main protease (M(pro)) of SARS-CoV-2, the virus responsible for COVID-19, we have redesigned the weak hit perampanel to yield multiple noncovalent, nonpeptidic inhibitors with ca. 20 nM IC(50) values in a kinetic assay. Free-energy perturbation (FEP) calculations for M(pro)-ligand complexes provided valuable guidance on beneficial modifications that rapidly delivered the potent analogues. The design efforts were confirmed and augmented by determination of high-resolution X-ray crystal structures for five analogues bound to M(pro). Results of cell-based antiviral assays further demonstrated the potential of the compounds for treatment of COVID-19. In addition to the possible therapeutic significance, the work clearly demonstrates the power of computational chemistry for drug discovery, especially FEP-guided lead optimization."
"The cosmopolitan protozoan parasite Cryptosporidium parvum infects humans and a range of other hosts, and is a common agent of calf diarrhea during the first four weeks of life (Gulliksen et al., 2009; Lanz Uhde et al., 2008; Millemann, 2009; Trotz-Williams et al., 2005) . The spectrum of severity of cryptosporidiosis in calves varies from subclinical infection to severe diarrhea and dehydration (Fayer et al., 1998; Klein et al., 2008; O'Handley et al., 1999) . Typically, natural and experimental infections with C. parvum in calves have a pre-patent period of 3-6 days, followed by a patent period characterized by a bell-shaped oocyst excretion curve, with the number of fecal oocysts peaking and then rapidly decreasing to undetectable levels in a matter of 7-14 days (Fayer et al., 1998; Grinberg et al., 2002) . Oocyst numbers as high as 10 7 oocysts per gram of feces have been reported at the peak of excretion (Fayer et al., 1998; Grinberg et al., 2002) . Whereas infection rates as high as 100% have been reported in calves in the first month of life, a lower infection prevalence has been consistently observed in weaned animals (Bartels et al., 2010; Brook et al., 2008; Fayer and Xiao, 2008; Fayer et al., 2000; Garber et al., 1994) . Newborn calves are therefore considered major amplifiers of potentially zoonotic C. parvum in nature, and the prevention of calf cryptosporidiosis is relevant from both animal and human health perspectives (Hunter and Thompson, 2005; Kiang et al., 2006; Smith et al., 2007; Grinberg et al., 2008; Xiao and Feng, 2008; Zhou et al., 2008) . Several features of the C. parvum life cycle make the control of cryptosporidiosis on-farms extremely difficult to be achieved by means of hygienic measures alone. Calves can excrete hundreds of millions of oocysts (Naciri et al., 1999; Grinberg et al., 2002) , and considering there is a high probability of infection with a dose as low as 50 oocysts (Moore et al., 2003) , an infected calf could produce enough oocysts to infect thousands of new animals. Attempts to interrupt the transmission of C. parvum should therefore include the immediate isolation of every infected animal, but this is rarely feasible on commercial farms. In addition, the C. parvum oocysts are insensitive to the action of numerous disinfectants (Chen et al., 2002; Quilez et al., 2005) and are excreted sporulated and fully infectious (Smith et al., 2005) , requiring daily cleaning for their removal. Therefore, in the absence of effective immunizing agents, pharmacological control strategies remain central for the prevention of cryptosporidiosis in calves. A number of compounds, such as halofuginone lactate (HL) (Lefay et al., 2001; Jarvie et al., 2005; Klein, 2008; De Waele et al., 2010; Trotz-Williams et al., 2011) , paromomycin sulphate (Fayer and Ellis, 1993; Grinberg et al., 2002) , nitazoxanide (Ollivett et al., 2009; Schnyder et al., 2009 ) and decoquinate (Moore et al., 2003; Lallemond et al., 2006) have been tested for the prevention of cryptosporidiosis in calves, with variable results. HL is a synthetic derivative of a quinazolinone alkaloid with cryptosporidiostatic activity, but a mode of action that is poorly characterized. In naturally and experimentally infected calves, the oral administration of 60 g/kg HL for seven consecutive days from the first day of life delays the onset of oocyst shedding, reduces the number of oocysts excreted, and lowers the severity of diarrhea Joachim et al., 2003; Klein, 2008; Lefay et al., 2001; Trotz-Williams et al., 2011; Villacorta et al., 1991) . A formulation containing HL (Halocur, Intervet Ltd., Republic of Ireland) is currently the only prescription drug registered for the prevention of cryptosporidiosis of calves in several countries. The use of HL has a number of limitations, including a substantial market price and a narrow therapeutic index, with toxicity observable at approximately twice the recommended dose (Villacorta et al., 1991; Naciri et al., 1993; Trotz-Williams et al., 2005 ; http://www.msd-animal-health.co. uk/products public/halocur/090 product datasheet.aspx, accessed 15 July 2012). Remarkably, notwithstanding the frequent occurrence of co-infections with Cryptosporidium and other enteropathogens in the field (De la Fuente et al., 1999; Naciri et al., 1999; Tzipori et al., 1980) , the utility of HL in the presence of such co-infections is not well understood. Indeed, with some exceptions (Klein, 2008; Lefay et al., 2001) , most anti-Cryptosporidium efficacy studies of HL did not analyze or took into account the presence of co-infections. Co-infections might modify the anti-Cryptosporidium effect of HL in different ways. The increased fluid content and intestinal motility determined by the presence of co-infecting pathogens may reduce the activity of HL by dilution, or by reducing the transit time of the drug in the intestinal tract. Furthermore, enteric infection with rotavirus or other agents may cause exfoliation of infected cells, altering intestinal cellular function (Ramig, 2004) , potentially increasing the toxicity of HL via systemic absorption. Motivated by the need of more data on the utility HL in the presence of co-infections with other pathogens, we performed a randomized controlled field trial of the anti-Cryptosporidium preventive efficacy of the compound in calves on a New Zealand farm enzootically infected with C. parvum, bovine rotavirus and Salmonella Typhimurium.@story_separate@The study was performed between July and October 2010, on a farm situated in the Taranaki District, New Zealand. Consent for this study was obtained from the farmer and the use of animals was approved by the Animal Ethics Committee, Massey University. The farm managed a seasonal (spring) calving dairy herd of approximately 400 milking cows and a smaller herd of beef cattle. It was recruited following a post-mortem investigation performed in March 2010 on two calves conducted by one of the authors (AG), which indicated the presence of Cryptosporidium oocysts in the feces of two calves. Salmonella was ruled out by culture and rotavirus and coronavirus by antigen ELISA. Sequence analysis of the Cryptosporidium 18S rRNA gene subsequently confirmed C. parvum in both animals. Notwithstanding the microbiological results performed in March, bovine rotavirus and Salmonella spp. were subsequently identified in multiple enrolled calves during the study (see below). The study was a randomized controlled field trial (see randomization procedure below). A commercial product (Halocur, Intervet Ltd., Republic of Ireland) registered in New Zealand for the prevention and treatment of cryptosporidiosis of calves was used to test the efficacy of HL. The product's recommended preventive dose is 4 mg (8 ml) for calves weighing 35-45 kg, and 6 mg (12 ml) for calves weighing 45-60 kg, for seven consecutive days, from birth. The initial aim was to assess the preventive efficacy of half the recommended dose of HL as compared with the full dose and no treatment, and the following treatment groups were established: Group 1 (full dose regime; n = 15), calves treated orally with 8 ml Halocur; Group 2 (half dose regime; n = 15), calves treated with 4 ml Halocur; Group 3 (placebo control group; n = 15), calves treated with 4 ml water delivered using the product's dispenser. The group sizes were estimated using power analysis for the detection of a difference between means, using PASS software (NCSS, Kaysville, UT). Assumptions for the calculations were a log 10 -transformed mean number of oocysts per gram of feces of 4.3 for the full dose and 6.2 for the untreated-control group at the peak of shedding, and a common standard deviation of 1.2. These means were estimated from a previously published study of efficacy of paromomycin sulphate (Grinberg et al., 2002) . Group sizes of 8 calves per group achieved 84% power to reject the null hypothesis that both group means are 6.2. Thus, assuming a conservative infection rate of ∼50%, 15 calves per group were required. Management of the enrolled calves followed the same routine procedures used on the farm. Briefly, newborn calves were left on the calving paddock for 10-24 h after birth, as commonly done in New Zealand pasture-based farms to allow for colostrum intake directly from the dam. Then the newborns were transferred to a large shed and allocated to a pen for newborn calves, of a capacity of about 10 calves. Calves were left in this pen for 2-3 days and then transferred to a new pen, in order to create space for new calves in the first pen. Subsequently, the calves progressed to new pens (often adjacent) containing 5-10 animals of the same age group every 2-3 days, until weaning. The allocation of calves to pens was done by caretakers who were not aware of the nature of the treatments given to each calf, effectively creating a commingled pen design. Adjacent pens were separated by slatted wooden fences. Sawdust was used as bedding material. The feeding regimen included the administration of complete commercial milk replacement for the first two weeks using a tank with multiple nipples, followed by feeding ad libitum using automatic feeders until weaning. Roughage was available from the second week of life. No routine vaccinations, preventive treatments or supplementations were administered to the calves during the study. However, inappetent calves could be tube-fed and severely diarrheic calves treated with oral electrolytes and/or other treatments by the caretakers. Severely sick calves were transferred to an isolation shed. The study protocol stipulated that the tube fed calves could continue in the study, but animals treated by the farmer other means or transferred to the isolation pen, were removed. Newborn calves were sequentially enrolled in the mornings, at arrival from the calving area to the rearing shed. Any calf born on the farm was eligible for enrolment, provided no congenital or pathological conditions were identified. At the day of enrolment (Day 0), calves were identified by the ear or neck tag number, given a sequential number and allocated to one of the three treatment groups. Allocation of calves to treatment groups was done using a randomized block design, with blocks of three (one calf per group), using a random list prepared in advance. The fourth calf presented to the investigator after each block of three was systematically left unenrolled and continued its normal life cycle on the farm. Treatments were administered in the morning using the commercial product (see above). Fecal specimens were collected at the time of treatment from the rectum of each calf using disposable gloves, on Days 6, 8, 10, 14 and 20. At the time of collection, each specimen was scored according to its consistency as 1, solid (specimen conserved its original shape); 2, semi-solid (specimen spread across the bottom of the container but was not liquid); or 3, liquid specimen. Specimens were transported on ice to Massey University and kept in refrigeration and analyzed between December 2010 and April 2011, as described below. The fecal specimens were analyzed by a quantitative method that estimated the number of Cryptosporidium oocysts present. Briefly, after mixing the specimen with a spatula, 1 g of feces was suspended in 10 ml tap water and strained through a tea sieve. The filtrate was centrifuged at 900 × g for 10 min and the sediment re-suspended in 4 ml of normal saline. A 10 l aliquot of this suspension was deposited as a drop on a slide using a micropipette, airdried and fixed in methanol. Fixed drops were stained using a commercial immunofluoresecent anti-Giardia and Cryptosporidium monoclonal antibody (Aqua-Glo G/C Direct Comprehensive Kit, Waterborne Inc., New Orleans, USA), according to the manufacturer's instructions. The applegreen fluorescent oocysts present on the entire area of the stained drop were counted using a fluorescent microscope using an excitation wavelength of 490 nm and a 200× magnification lens. This total number of oocysts (TON) on the slide was used for statistical analysis (see below). Samples containing >1000 oocysts were difficult to count and were re-processed by a further dilution of the fecal suspension at 10 −1 in water, followed by the staining of a 10 l drop as above. The TON present on each rediluted specimen was estimated by multiplying the result by 10. When the available fecal material was insufficient for counting, a direct fecal smear was stained and a qualitative result (presence/absence of oocysts) was obtained. These qualitative results could not be used for the statistical comparison of the number of oocysts between the treatment groups, but were used for any comparison between proportions of Cryptosporidium-positive and negative specimens. In March 2012, DNA from three Cryptosporidiumpositive specimens from each treatment group was extracted from the stored feces using a DNA extraction kit (QIAamp, DNA Stool Mini Kit, Qiagen, Hilden, GmbH), and Cryptosporidium parasites were identified by means of PCRsequencing of the 18S rRNA gene. Primers were 5-GTT AAA CTG CGA ATG GCT CA-3 (forward) and 5-CCA TTT CCT TCG AAA CAG GA-3 (reverse) (Learmonth et al., 2004) . Amplification was performed in 20 l containing 2 l 10 x PCR buffer, 1 l dNTP (2 mM), 1 l MgCl 2 (50 mM), 2 l nonacetylated bovine serum albumin (2 mg/ml) (New England Biolabs, USA), 4 picomoles of each primer, and 0.5 l of Platinum ® Taq DNA Polymerase (2 mg/mL) (Invitrogen Corporation, Carlsbad CA, USA). The PCR amplification was carried out in a thermocycler (SensoQuest, Goettingen, Germany) with initial denaturation at 96 • C for 2 min, followed by 40 cycles at 94 • C for 30 s, 55 • C for 30 s and 72 • C for 30 s. PCR products were purified using an in-house ethanol purification protocol, and bidirectional sequencing of an internal segment of the amplicon was performed using primers 5-CTCGACTTTATGGAAGGGTTG-3 (forward) and 5-CCT CCAATCTCTAGTTG GCATA-3 (reverse). Forward and reverse sequences were aligned and edited manually using Geneious software version 5.6.5 (Biomatters Ltd., http://www.geneious. com/). Distal and proximal segments that could not be verified were trimmed and the resulting edited sequences aligned with sequences deposited in Genbank using the alignment algorithm BLAST (http://blast.ncbi.nlm.nih.gov/Blast.cgi, accessed on April 2012). In addition to the analysis for Cryptosporidium, 23 fecal specimens taken haphazardly from the three treatment groups were analyzed for the presence of rotavirus, coronavirus, enterotoxigenic K99 + Escherichia coli (K99) and Salmonella spp. The analysis for rotavirus, coronavirus and K99-positive E. coli was performed by a commercial diagnostic laboratory using antigen-ELISA (Institut Pourquier, Montpellier, France). The analysis for Salmonella included parallel inoculation of fecal material into tetrathionate and Rappaport-Vassiliadis soy peptone (RVS) broths, and incubation for 24 h at 37 • C and 42 • C, respectively. This was followed by subculture onto xylose lactose deoxycholate agar plates incubated as above. Colonies consistent with Salmonella were sub-cultured into triple-sugar iron agar slopes (TSI) and l-lysine decarboxylase broth. Lysinepositive bacteria exhibiting TSI patterns consistent with Salmonella were subjected to Salmonella poly-O slide agglutination using a commercial antiserum (Institut für Immunpräparate und Nährmedien GmbH Berlin, Berlin, Germany), and agglutinating isolates were sent to the Salmonella Reference Laboratory (Institute of Environmental Science and Research, Porirua, New Zealand) for serotyping. The parasitological and clinical effects of the three treatments were statistically compared. The parasitological effects were analyzed by comparing the TONs between the groups using an analysis of variance for repeated measurements (rmANOVA) implemented by the PROC MIXED procedure of SAS (Statistical Analysis System, 2001, SAS Institute, Cary, NC, USA). TONs were log-transformed as log 10 (TON + 1) and analyzed using mixed models. The first model (Model 1) included the fixed effects of treatment (variable 'treatment Group'), the day of sampling as repeated factor (variable 'sampling Day'), and the interaction of treatment Group and sampling Day. Model 2 included the fixed effects as in Model 1 plus the fixed effect of sex of the calves, and Model 3 included the same factors as in Model 2 plus the effect of breed of the calves as a co-variable (variable 'Breed'; Friesian calf = 1; non-Friesian calf = 0). Fixed effects for variable Breed were not included as a class due to unbalanced designs deriving from the uneven distribution of breeds across treatments. Finally, Model 4 was similar to Model 2, but did not include the data obtained from non-Freisian calves, effectively removing any variability due to the breed. All the models included the random effect of calf, to account for the within-calf variability. Using the Akaike's information criterion, an unstructured error structure was determined as the most appropriate residual covariance structure for repeated measures over time, within animals. Finally, the parasitological efficacy was also assessed by comparing the decline of the proportion of parasitologically negative calves as a function of time between the three groups using the nonparametric Kaplan-Meier method (KM) (Kaplan and Meier, 1958) . Also in this case, the analysis was repeated after omitting all the data from the non-Friesian calves. The KM analysis was implemented using a code available in ""R"" (Terry Therneau, 2012; A Package for Survival Analysis in R package version 2.36-14). The clinical effects of the treatments were assessed by comparing the fecal consistency scores (considered ordinal data) between the groups on each sampling day, using the non-parametric Kruskal-Wallis test. These tests were performed using the codes available in ""R"" (Kruskal-Wallis: http://stat.ethz.ch/R-manual/R-patched/library/stats/ html/kruskal.test.html; Wilcoxon: http://stat. ethz.ch/ R-manual/R-patched/library/stats/html/wilcoxon.test. html). Comparisons between proportions of interest were performed using two-tailed Fisher's exact tests, which were interpreted using Bonferroni-adjusted critical P-values for multiple testing. Finally, the existence of association between the fecal consistency and the number of oocysts shed was tested by means of logistic regression, using the log 10 (TON + 1) as independent variable and the presence/absence of a liquid specimen (fecal score 3) as outcome variable. This analysis was performed using a code available in ""R"" (http://www.ats.ucla. edu/stat/r/dae/logit.htm; R package version 2.36-14). All the calves were enrolled within 24 h from birth, over a 26-day period, between 16 July and 06 August 2010. A total of 24/45 (53.3%) calves were females and there were no statistical differences between the proportion of males and females among the treatment groups. There were 27 (60%) Friesian, 11 (25%) Angus, 6 (11%) Hereford calves and one (2%) Jersey × Angus crossbred calf (Table 1) . Six calves haphazardly selected at enrolment and weighted using electronic scales had bodyweights between 35 and 40 kg (mean = 36.5, standard deviation = 1), indicating under/over-dosing of calves was unlikely to have occurred. One calf that died on the same day of enrolment (calf number 1; Table 1 ) was substituted by the subsequent calf presented to the investigator (calf number 46; Table 1 ). All the enrolled calves remained in their pens during the study and no calf was withdrawn due to concomitant treatment. Thirty eight calves (84.5%) were followed up for the entire observation period, and seven (15.5%) died at different stages during the study. The causes of death were defined by the farmer as following: two calves from Group 1 died on Days 6 and 7, two from Group 3 on Days 7 and 9, and one from Group 2 on Day 13, from severe diarrhea. Two calves (Group 1 and Group 3) died from improper tube-feeding (possibly, milk inhalation) on Day 7 and Day 13. There were 3 deaths in Group 1, three in Group 3 and one in Group 2, and these rates were not statistically different (Table 1) . Only 6/225 (2.6%) fecal specimens could not be retrieved and were not analyzed (Table 1) . A total of 41/45 (91%) calves were Cryptosporidiumpositive by immunofluorescence at some stage during the study. Two calves belonging to Group 2, that were able to be followed up for the entire observation period, remained parasitologically negative throughout the study (Table 1) . Sequence analysis of the 18SrRNA gene of the 9 fecal specimens analyzed by PCR-sequencing indicated the presence of C. parvum in all cases. Out of 23 specimens analyzed for other enteropathogens, 21 (91%) were positive for rotavirus and 3 (13%) for Salmonella Typhimurium. As expected for calves of this age, no E. coli K99-positive specimens were identified. Although analysis for Giardia spp. was not the subject of this study (as the parasite it is not widely considered pathogenic for calves), we note for completeness that Giardia cysts were observed in 20 calves by immunofluorescence. The number of Cryptosporidium-negative/positive fecal specimens stratified by the treatment Groups and sampling Days. On the same Day, similar superscripts indicate comparisons between proportions resulting in two-tailed Fisher's exact test P < 0.01 (less than Bonferroni-adjusted critical probability). Sampling day Group 1 (full dose) Group 2 (half dose) Group 3 (placebo-control) Group 1 (full dose) Group 2 (half dose) Group 3 (placebo-control) Day 6 13/0 13/2 12/2 7/0 6/2 10/2 Day 8 9/3* 10/5 + 1/13* + 5/2 4/4* 0/12* Day 10 7/5* 4/11 1/11* 4/3 2/6 3/9 Day 14 1/11 2/12 1/11 2/5 1/7 3/9 Day 20 5/7* 12/2* 10/2 5/2 6/2 10/2 The calves in Group 3 (control Group) showed an oocyst shedding curve which was typical for natural cryptosporidiosis, with the majority of the animals becoming parasitologically positive by Day 8 and again negative by Day 20 (Tables 1 and 2) . Group 3 started shedding earlier than the other two groups. In this group, the highest mean log 10 (TON + 1) was observed on Day 10, and on Day 20 oocysts were observed only in two calves (Table 1 ). In contrast, in Group 1 the peak mean log 10 (TON + 1) was lower and occurred later (Day 14) than in Group 3, and on Day 20, the calves in this group were shedding more oocysts than the other two groups. However, when only Friesian calves were considered, Groups 1 and 3 showed a very similar oocyst shedding curve, which peaked in both groups on Day 10. Comparisons between the crude mean log 10 (TON + 1) of the three treatment groups at the various sampling days are reported in Fig. 1 . Model 1 (which included the effects of treatment Group, sampling Day and their interaction term), indicated a significant difference between the three treatments (P = 0.04). Post hoc comparisons identified a significant difference between Group 1 (full dose) and Group 3 (control) (Group 1 < log 10 (TON + 1) than Group 3; P = 0.01), and no significant difference between Group 2 and the other two groups. In Model 2, which included also the fixed effect of sex, the significance between the treatment Groups was preserved (P = 0.02), but there was no significant effect of sex on the outcome (P = 0.17). When variable 'Breed' was introduced as a covariate in Model 3, the Pvalue of the effect of treatment increased to P = 0.098, and the covariable 'Breed' was also significant (Friesian < log 10 (TON + 1) than non-Friesian; P = 0.02). This increase in the P-value was supported by Model 4 (Friesian only), which showed P = 0.73 for the effect of treatment. All these models produced similar P-values when the interaction of treatment × day was removed (not shown). In order to cross-validate these results we compared the mean log 10 (TON + 1) between the treatment groups using bivariate ANOVA for each sampling day separately (not shown), with consistent results: whereas the inclusion of all the animals resulted in statistically significant difference between treatment Groups 1 and 3 on Days 8, 10 and 20 (P < 0.05), all the P-values were > 0.05 when only Friesian calves were analyzed. The results of the Kaplan-Meier test indicated a significantly longer prepatent period in Group 1 as compared with Group 3. In fact, 13 calves (93%) in Group 3 were parasitologically positive on Day 8, whereas only three calves from Group 1 (25%) were shedding oocysts on the same Day (Tables 1 and 2 ). The duration of the prepatent period in Group 2 was intermediate, although not statistically different from the other groups. Similar results were observed when only data from Friesian calves were analyzed (Fig. 2) . The results of the Fisher's exact tests showed that on Days 8 and 10, the proportion of Cryptosporidiumpositive calves was significantly greater in Group 3 than in Group 1 (two-tailed Fisher's exact test P < Bonferroniadjusted critical value of 0.003). On the other hand, most calves (58%) in Group 1 and only two calves in Group 2 and two in Group 3 were shedding oocysts on that day (Table 2) . Twenty three calves (51%) passed at least one liquid specimen in the course of the study, and the proportion of liquid specimens on Day 6 was relatively high, and very similar in the three treatment groups (Table 3 ). The results of Kruskal-Wallis and Wilcoxon tests test did not indicate any significant difference (P > 0.05) between the fecal consistency scores of the three groups on any sampling day (not shown). Except one significant difference between Group 2 and Group 3 on Day 8, the proportion of liquid specimens did not differ between the groups (Table 3) , and these result persisted when only Friesian calves were included in the analysis (not shown). There was no association between the log 10 (TON + 1) and the presence of liquid specimen by logistic regression (P > 0.05). Halofuginone lactate is registered for the prevention of calf cryptosporidiosis in several countries, but the compound has a relatively narrow therapeutic index and a substantial market price. This study was initially designed to assess the efficacy of a reduced dosage regime of HL on a farm infected with C. parvum, with no evidence for the presence of co-infection. However, the identification of rotavirus and Salmonella Typhimurium in multiple calves provided an opportunity to collect much needed data of the utility of the compound in the presence of such common co-infections. When the study was designed, we predicted a longer prepatent period and a decreased number of oocysts and fecal consistency scores in the full dose group, as reported for HL in the absence of documented co-infections. The coinfections with rotavirus and Salmonella Typhimurium did not affect the ability of HL (full dose) to delay the onset of shedding. Whereas most calves (5/12) in Group 1 were still shedding oocysts on Day 20, the majority of the calves in the control group were already parasitologically negative by that day (Table 2 ). The prolongation of the prepatent period was not coupled with statistically significant differences between the mean log 10 (TON + 1) of the treatment Groups after controlling for the repeated measurements, the sex, and the breed of the calves (Models 3 and 4) . We hypothesize that HL suppressed the parasite's life cycle during the first days of treatment, when the diarrhea caused by the other pathogens was not yet overt, effectively prolonging the prepatent period in Group 1. Conversely, about 30% of the calves in Group 1 were passing liquid feces on Day 6, and this early diarrhea (probably caused by the co-infecting pathogens) could have shortened the intestinal transit time of HL in the last days of treatment, effectively abolishing its anti-Cryptosporidium effect. Some authors suggested that co-infections with multiple agents could cause a more severe diarrhea than mono-infections (De la Fuente et al., 1999; Garcia et al., 2000) . Consequently, it could be hypothesized that suppression of one organism could reduce the severity of the Table 3 The number of liquid feces (fecal score 3)/total specimens assessed, stratified by sampling Days, treatment Groups (the corresponding proportions are in brackets). On the same sampling Day, an asterisk indicates a two-tailed Fisher's exact test P < 0.003 (Bonferroni-adjusted critical probability). Group 1 (full dose) Group 2 (half dose) Group 3 (placebo-control) (0) diarrhea. In this study, there was no significant difference between the fecal consistency scores of the treated and untreated groups, and no association between the TON and the presence of liquid feces was found by logistic regression. Furthermore, in agreement with the results of a metaanalysis of the literature (Silverlås et al., 2009 ), the mortality rates did not differ between the groups. Therefore, the results do not allow conclusions to be drawn on a clinical benefit of HL in the presence of co-infections. Finally, the statistically significant effect of breed on the intensity of oocyst shedding observed in this study was intriguing. Other studies have previously reported lower Cryptosporidium infection prevalence in beef calves than in dairy calves (Geurden et al., 2006; Kváč et al., 2006) . One year after the completion of this study, the authors requested and obtained funding from MSD Animal Health New Zealand (the new distributors of Halocur) to perform an epidemiological study of neonatal calf diarrhea in dairy farms. The current study was performed before such funds were requested and the company was not consulted during the preparation of this manuscript.@story_separate@The anti-Cryptosporidium activity of HL was not fully preserved and the use of the drug was not associated with a clinical benefit in the presence of enzootic co-infection with rotavirus and Salmonella Typhimurium in calves. Diagnostic efforts should therefore aim to rule out the presence of other common enteropathogens in order to maximize the clinical efficacy of HL in the field.","Halofuginone lactate (HL) is registered in several countries for the prevention of calf cryptosporidiosis, but the compound's utility in the presence of co-infection with other enteropathogens is not well understood. We performed a randomized controlled field trial of the efficacy of HL for the prevention of natural calf cryptosporidiosis, in the presence of co-infection with rotavirus and Salmonella Typhimurium. Newborn calves on one farm were sequentially enrolled and allocated to a full dose (n = 15), half dose (n = 15), or a placebo control group (n = 15), using a randomized block design. The Cryptosporidium oocysts in fecal specimens collected on Days 6, 8, 10, 14 and 20 were counted and the severity of the diarrhea was assessed using fecal consistency scores (solid, semisolid, or liquid). The oocyst numbers and fecal consistency scores were statistically compared between the groups. Ninety one percent of the calves shed Cryptosporidium parvum oocysts during the trial. The full dose group had a longer prepatent period than the control group, but no statistical difference in the number of oocysts was identified between the groups after controlling for the effects of sex and breed. The fecal consistency scores and mortality rates did not differ between the groups. These results indicated that the anti-Cryptosporidium activity and clinical benefit of HL were limited. It is concluded that in order to maximize the clinical efficacy of HL in the field, diagnostic efforts should aim to rule out the presence of other enteropathogens."
"The outbreak of the Covid-19 pandemic in late December 2019 has quickly and severely impacted countries worldwide, becoming one of the most important health crisis of the 21 st century (Spinelli and Pellino 2020; Remuzzi and Remuzzi 2020) . The infectious agent, a coronavirus, identified as responsible for this disease is notoriously difficult to pin down: while it leaves many infected people without symptoms, it can lead to a common cold for some and even severe respiratory disorders to others (Clerkin et al. 2020) . So far, the Covid-19 has brought about a human tragedy, with hundreds of thousands of lives lost, as well as an economic downturn due to the lockdown of over three billion people (half of humanity) (Mahase 2020; Dudel et al. 2020) . The scale of Covid-19 effects has urged stakeholders at different levels (government, local authorities, private companies, academia, NGOs, and citizens) to plan and implement measures for addressing the virus spread. In particular, while pharmaceutical research is embarked on a long journey to develop a vaccine, non-pharmaceutical innovations are sought to contribute to responding to the outbreak. Chief among the technologies that are employed, Information and Communication Technology has been widely leveraged in various capacities across all regions and targeting broadly all levels of society. For example, news and sensitization messages have been viral thanks to the use of internetbased services such as social networks. Given the widespread use of handheld devices such as smartphones, users are keen to install applications (often referred to as apps in the mobile realm) that have specific purposes for entertainment, business, productivity, news, and social networking. Following the outbreak of the pandemic, authorities, non-governmental organizations, and independent developers have engaged in an app development race to provide readily-available digital tools to the modern citizen. We focus in this paper on the case of the Android ecosystem. With the largest market share on mobiles (86% in 2020 (IDC 2020)), Android constitutes a prime choice for developers and users alike. Initial reporting on Covid-19 related apps are focused on the problems that such apps raise: (1) have already explored the case of coronavirus-themed Android malware; (2) Google, the maintainer of the Play Store (i.e., the official Android app market) has decided to crack down on Covid-19 apps to combat misinformation, sometimes with an excessive zeal (i.e., legitimate apps can be temporarily banned just for sharing Covid-19 information (Carman 2020; Google 2020) ). Our study is of a broader and more generic dimension. It is about characterizing the applications that are related to the Covid-19 outbreak: What are they for? Who developed them? To what extent can they be considered dangerous? These are some of the questions that we undertake to investigate. To that end, we have considered 184 563 apps released in the time window of July 2019-May 2020 and collected in the AndroZoo dataset (Allix et al. 2016) . From this initial set of Android apps, by following well-defined heuristics, we were able to identify 44 Covid-related apps. Given the limita-tions of the crawling of AndroZoo, we also considered other online resources such as the GitHub search engine, specialized Covid-19 technology blogs, etc., and we finally raise the number of collected Covid-related apps to 92. We extract different features from these apps and provide summary statistics on their characterization. This paper presents our analyses exploring the use of permissions and libraries, the presence of leaks, the malicious status, the code size and complexity, the authorship, and the described purposes. Mainly, we establish that most of the apps are made for informing people, monitoring their health, and tracing users with the goal of preventing the spread of the virus. In addition, we note that Covid-related apps are not flagged by malware detectors. Yet, we found that some of them have been removed from Google Play. Then, after assessing the complexity of Covid-related apps and comparing them with standard apps, we found that on average, Covid-related apps are less complex, which has been shown to often be indicative of quality for apps (Jošt et al. 2013 ). Finally, we applied state-of-the-art security and privacy scanners in order to check for potential cryptographic API misuses and data leaks in the code. In summary, we present the following contributions: -We present the first systematic study of Covid-related apps that explores their characteristics and compare them with other non-Covid-related apps. -We build a taxonomy of Covid-related apps based on their described goals. -We apply literature analysis tools on Covid-related apps and discuss their results. All artifacts are made available online at: https://github.com/JordanSamhi/APKCOVID@story_separate@The main objective of this work is to analyze and understand Covid-related apps. To do so, we empirically observe apps characteristics by extracting features that provide insights toward understanding those apps. Hence, to accomplish this objective, we plan to answer the following research questions: -RQ 1: What are Covid-related apps used for? -RQ 2: When did Covid-related apps start to appear in Google Play? -RQ 3: Do Covid-related apps have specific characteristics? -RQ 4: Are Covid-related Android apps more complex than standard apps? -RQ 5: To what extent were Covid-related apps removed from the official Google Play and why? -RQ 6: Who are Covid-related apps' developers? -RQ 7: Are Covid-related apps flagged by privacy & security scanners? The remainder of this paper is organized as follows. First, we present an initial Android apps dataset and some background for the reader in Section 2. Then, in Section 3, we give details about the experimental setup of our study. In Section 4, we provide the results obtained from our experiments. We discuss the threats to validity of our study in Section 5. Finally, we discuss related work in Section 6 and conclude in Section 7. Initial Dataset: In order to perform our experiments and to exhaustively answer the research questions presented in Introduction, we need to rely on a comprehensive dataset. Consequently, we used the state-of-the-art largest dataset available, namely AndroZoo (Allix et al. 2016) . At the time of writing, AndroZoo contains more than 11 million Android apps (June 2020) that have been collected from different sources, such as Google Play and other third-party providers (F-Droid, Anzhi, AppChina, etc.). As it is continuously growing, researchers still heavily rely on AndroZoo for collecting apps and experimenting on it (Ranganath and Mitra 2020; Shar et al. 2020; Xu et al. 2020) . Since the Covid-19 outbreak is quite recent, we did not consider the 11 million Android apps present in AndroZoo. For our study, we instead considered and collected from AndroZoo all the apps ranging from July, 1 st 2019 to May, 25 th 2020, leading to a total number of 184 432 collected apps. Note that AndroZoo does not contain information about the release date of an app, i.e., there is no information about when an app has been uploaded on the market. For this reason, we approximate the date of an app by considering the date of the dex files in apks. Figure 1 shows the distribution of collected APKs according to the month of the date of the dex files in APKs. Dataset Augmentation: It is a known problem that the date metadata in APKs (approximated as the date of the dex files) is not always reliable (Li et al. 2018) . For this reason, we decided to augment our dataset (1) by not considering the time window from July 2019 to May 2020 and (2) by selecting any apps from AndroZoo whose package name contained the words covid-19, covid19 or coronavirus. With this heuristic, we were able to retrieve 131 apps from AndroZoo. ⇒ Finally, our initial dataset is constituted of 184 563 Android apps. Short Background on Android Apps: In general, Java is the language used by developers to create Android applications, though other languages can be used such as Kotlin, or even languages such as C and C++ thanks to the Native Development Kit (NDK). For common Java applications, the source code is compiled into Java bytecode and run into the Java virtual machine. However, within the Android operating system lies the Dalvik Virtual Machine which is an optimized version of the Java Virtual Machine. Thus, regarding mobile apps, the source code is first compiled into Java bytecode, then translated into Dalvik bytecode and written in DEX files. Still, the DEX files do not constitute an application. Indeed, an Android app is a collection of files packaged together in the so-called Android Applica-0 5000 10000 15000 20000 25000 30000 2 0 1 9 -0 7 2 0 1 9 -0 8 2 0 1 9 -0 9 2 0 1 9 -1 0 2 0 1 9 -1 1 2 0 1 9 -1 2 2 0 2 0 -0 1 2 0 2 0 -0 2 2 0 2 0 -0 3 2 0 2 0 -0 4 2 0 2 0 -0 5 Number of apps Number of apps collected from AndroZoo by month Fig. 1 : Monthly distribution of Android apps considered in this study tion Package (APK). This format is used for distributing apps on devices, not only via official and unofficial markets but also via any other channels where an independent file can be distributed. Indeed, Android devices allow users to install applications from any sources once a configuration option is toggled off. An APK is typically a zip file containing the following files: (1) Metadata files, (2) the certificate(s) used to sign the application, (3) a lib folder containing platform-dependent compiled code, (4) compiled and non-compiled resource files, (5) one or multiple DEX files, generally named classesX.dex with X an integer, (6) an AndroidManifest.xml file describing the application (package name, components, version, access rights, etc.). In this section, we describe the setup of our experiments. More specifically, in Section 3.1 we describe how we create a dataset of Covid-related apps by searching apps in both our initial dataset extracted from AndroZoo, and on the web. Then we describe in Section 3.2 how the information needed for answering the research questions was extracted. The large majority of apps contained in our initial dataset of Android apps are not related to the Covid-19 outbreak. Consequently, we need to curate this dataset. Our first idea was to rely on a clear definition of what is a Covid-related app. However, we realized that finding a correct and precise definition is not obvious. For instance, the english Wikipedia page related to Covid-19 (Wikipedia 2020) defines Covid-related apps as ""mobile software applications that use digital tracking to aid contact tracing in response to the COVID-19 pandemic, i.e. the process of identifying persons (""contacts"") who may have been in contact with an infected individual.""(accessed early June 2020). We quickly considered this definition as too restrictive since we found several Covid-related apps that are not about ""digital tracing"". As a result, rather than relying on a definition of what is a Covid-related app we (1) implemented several heuristics based on hypotheses, and (2) performed several quality checks to filter out irrelevant apps. Hypothesis 1: Our first hypothesis is that a Covid-related app contains strings (e.g., in a class name or method name) related to Covid-19. Based on this hypothesis, we defined several regular expressions that we apply on various fields on an Android app. More specifically, we proceed as follows: Let A be a set of Android apps and apk an app of this set. Let be: • C apk the set of classes names in apk, • M apk the set of method names in apk, • F apk the set of file names in apk, • S apk the set of strings contained apk, We keep apk if at least one element of C apk ∪ M apk ∪ F apk ∪ S apk matches at least one regular expressions listed in Table 1 . An app is kept if for instance, a class name contains the substring coronavirus, or if the app contains a string with pandemi as substring. ""(?i).*coronavirus"" ""(?i).*corona"" ""(?i).*sars(-?cov)"" ""(?i).*quarantin.*"" ""(?i).*lock-?down"" ""(?i).*containment"" ""(?i).*social-?distanc.*"" ""(?i).*pandemi.*"" ""(?i).*out-?break"" ""(?i).*epidemi.*"" ""(?i).*confinement"" We ended up with a set of 35 613 supposedly Covid-related apps. In Figure 2 , we can see the number of apps that were retrieved per keyword (note that several keywords can be present in a given app). Quality Check #1: We can see in Figure 2 that the keywords corona and quarantine match a significantly larger number of apps. After investigation, we found that the keyword quarantine is used in strings by developers to check if the smartphones are rooted, i.e., if the user has super-user (root) privilege. Indeed, the presence of some packages can characterize the fact that a smartphone is rooted. Among such packages, one is named com.ramdroid.appquarantine and another is named com.ramdroid.appquarantinepro. To verify the presence of these two packages, developers often use these package names as string, and thus the regular expression using quarantine matches. This explains why there is such a high number of apps containing quarantine in strings. We manually analyzed several dozens of apps and confirmed they are not real Covid-related apps (mainly music, education, news, shopping, and game apps). Regarding the corona keyword, it also appears in a substantial number of apps, especially in string feature. After manual investigation, we found that apps retrieved with this keyword use a framework called Corona developed by Coronalabs 1 . These apps are mainly games, entertainment and personalization apps. Moreover, the word corona not only refers to Covid-19, but also has uses in several unrelated contexts (in architecture, beverages, books, music, movies, and games). We decided to rule out both keywords corona and quarantine, as they revealed to mostly bring noise in our dataset. After filtering the apps gathered, i.e., not taking into account the corona and quarantine keywords, we obtained 4103 apps. Hypothesis 2: Our second hypothesis is that since AndroZoo is known to contain successive versions of the same apps (Allix et al. 2016; Li et al. 2018) , our dataset contains successive versions of the same Covid-related app. Therefore, for the subsequent analyses, we only keep the latest version of a given app. This step is performed by comparing apps version code-available in AndroZoo metadata-and keeping the highest value. After this step, our dataset contains 750 apps. Hypothesis 3: Our third hypothesis is that official Covid-related apps are released only on the official Android market, i.e., the Google Play market. Figure 3 depicts the distributions of the apps per market from where they have been crawled. Most of the apps were released in the official Google Play market. By considering apps from Google Play only, we reach the number of 619 apps. Quality Check #2: To check the quality of our dataset, we inspect the date of the dex file in the APKs. The Covid-19 outbreak was not known until the very end of 2019, so we expect to find apps from 2020 only. Figure 4 presents the distribution of those apps by the date of the dex file in the APK. First, we observe that fewer apps were released during winter 2019-2020. Then we can see that starting from March 2020, a significant amount of apps were developed, which corresponds to the containment period. Finally, Figure 4 show that our set of Covid-related apps contains a significant amount of apps developed before the pandemic, which indicates the low quality of our dataset and that an additional layer of filtering is needed to only keep Covid-related apps. Indeed, our manual investigations of those apps revealed that they contained keywords that may be overly broad. For example, it appeared that keywords such as outbreak, pandemic, containment or lockdown were popular in games, most notably in the zombies and survival games genres. 2 0 1 9 -0 7 2 0 1 9 -0 8 2 0 1 9 -0 9 2 0 1 9 -1 0 2 0 1 9 -1 1 2 0 1 9 -1 2 2 0 2 0 -0 1 2 0 2 0 -0 2 2 0 2 0 -0 3 2 0 2 0 -0 4 2 0 2 0 -0 5 Monthly distribution of our filtered set of 619 apps Hypothesis 4: Our fourth hypothesis is that the description of an app is a reliable source of information to check that an app is Covid-related. AndroZoo does not provide the app descriptions, so we queried Google Play to retrieve the description of the 619 apps. Note that it was possible to get the description for 537 apps, while 82 apps were not available anymore in Google Play at that time. Actually, AndroZoo uses a crawler to automatically download apps. It is possible that AndroZoo downloads an app at a time t 1 , but if Google Play decides to remove this app (or if the developer decides to remove it) at time t 2 , with t 1 < t 2 , it is not possible to access the description of the app anymore after time t 2 . We found that Google is actively deleting apps that are violating their policy with respect to Covid-19 (Carman 2020; Google 2020) . Manual investigations of the remaining 537 apps were conducted to qualify an app as Covid-related. We did so by analyzing the Google Play page of every app, reading the description, and looking at screenshots. We did not encounter any ambiguous case, hence it was straightforward to qualify an app as Covidrelated or not. With this method, we determined with high certainty that 44 apps taken from AndroZoo were Covid-related. Note that the set of apps from our initial dataset for which the package name contained Covid-related keywords contained 131 elements and we only retrieved 44 apps. This is explained by the fact that among those 131 apps, there were, for almost one-fifth, different versions of the same app. In addition, we were not able to analyze the Google Play page of apps that were not available in the Google Play market. Figure 5 gives two examples to show an app we discarded and an app that was considered as Covid-related. First, the game picture on the left clearly shows a game in which the user has to kill zombies, even though the game contains the string outbreak. Second, the Covid-related app on the right is explicit regarding the content delivered to the user. The title refers to Covid-19, the description gives clues about the content, i.e., information about Covid-19 and guidelines about Covid-19. Besides, the screenshots are also explicit by depicting what actions users can perform, here Covid-related actions, i.e., getting information about Covid-19, performing self-diagnosis, receiving guidelines, and news about Covid-19. It represents how unequivocally our decisions were to be made to qualify an app as Covid-related or not. Hypothesis 5: As AndroZoo is not exhaustive and all the apps in the Google Play market are not available for download from every country (An-droZoo crawls from specific countries), we felt the need to expand our research. Consequently, manual investigations were conducted on the web to search for apps that would not be available in Google Play and/or that our empirical analysis on AndroZoo did not catch. We found 48 additional apps from diverse countries from different time span, e.g. we found apps that were first seen in June 2020. Thus, our set of COVID-applications reached a length of 92 apps. ⇒ Figure 6 summarizes our dataset curation process. Dotted boxes represent filters used to refine the dataset. In this section, we expose what features were extracted from the apps considered in this study and how we extracted it. Three ways of retrieving the needed information were considered: (1) Extracted from AndroZoo, (2) Automated analysis of the apps, and (3) Manual analysis, which we describe below. Apps from AndroZoo are provided with additional metadata, i.e., a vector of length 11 representing different information. Those metadata elements are used to expose several properties of the apps under analysis. We consider the date of the dex files to verify when the Covid-related apps started to appear. The package names and version codes were useful to have some insights into the versioning of the apps and to keep only the latest version of the app considered, i.e., one version of the app will be analyzed. When available, the number of AntiVirus products reporting an app as malicious (obtained from VirusTotal) was used for qualifying the maliciousness of the apps. Finally, AndroZoo metadata also indicate the source where an app was obtained from. AndroZoo metadata being limited, we additionally leveraged existing tools and frameworks to analyze Android apps in order to obtain the information to acquire the information needed by our study. -We developed a program relying on the Androguard software package 2 . Thanks to this tool, we extract the permissions requested by the apps, as well as information about components in the applications (i.e., Activities, Services, etc.). We also automatically compute the complexity metrics described in Appendix A. -We used CogniCrypt (Krger et al. 2017 ) and its headless implementation CryptoAnalysis (CryptoAnalysis 2020) to check that they were written following best practices regarding cryptographic APIs. -As several Covid-related apps seek to acquire and process the location of users (considered a piece of sensitive information), we verify if those apps were subject to data leaks. Hence, we leverage FlowDroid-IccTA (Li et al. 2015) , the state-of-the-art static data leak detector dedicated to Android apps. While application information that can be readily obtained through automatic tools were necessary for our study, we went a step further and acquired qualitative data on the apps. By collecting and carefully reading the descriptions of apps 3 and by confirming the validity of our understanding by matching the reviews and app screenshots with the description, we were able to assemble highly-qualified data on the goals of app developers, on whether an app is developed by a state body or an individual person, etc. Finally, after leveraging the automated tools described above, their results were manually confirmed for the set of Covid-related apps (i.e., 92). This allowed us to ensure those tools did not yield false-positives, and that their results were sound and consistent. In this section, we present our experimental results and we answer our research questions. Textual descriptions of apps on markets generally provide a wealth of information on the purpose and functionalities that developers advertise. We undertake to systematically examine the descriptions of all the apps under study. Unfortunately, since Google Play is actively moderating Covid-related apps, we have faced an issue with some apps that we were able to initially collect but which were no longer available on the market at the time of analysis. Eventually, our analysis of descriptions was performed on 78 apps. In other words, from the time we read the descriptions of the apps to curate our dataset (as explained in Section 3.1) and the time we perform this more in-depth study, i.e., collecting information related to the features of the apps, 14 apps (92−78) were not able anymore on GooglePlay. A Taxonomy of Covid-related apps. After a careful analysis of information available in Google Play, we summarize for each app its general goal, i.e., which aspects of the Covid-19 crisis the app is precisely intended to address. Eventually, we identified three main categories to which each app can be associated with possible overlap between categories, i.e., an app can be associated with several categories: 1. Information broadcast (top-down) -Apps in this category aim to provide users with various types of information, from general guidelines, infection statistics to general Covid-19 news. Although such apps are not always officially released by government bodies, they often relay official information from top (authorities) down (users). 2. Upstream collection (bottom-up) -Apps in this category collect information from users and make it available to the developer and/or an official body, such as a country's health authorities. 3. Tooling -Apps in this category serve as tools with functionalities that directly deal with daily aspects of the Covid-19 (e.g., generation of certificates). [1] Information broadcast. From the collected dataset of Covid-related apps we identified several distinguishing scenarios in apps performing information broadcasting. Figure 7 overviews the related characteristics, notably based on the types of information that are made available to the user: 1. Guidelines on measures to take to minimize the risk of infection -Among such apps, some render maps highlighting high-risk areas. Other apps provide behavioral advice (e.g., how to wash hands), leveraging the whole spectrum of available media: (1) textual descriptions (for the majority of apps), (2) videos and, (3) audio clips. 2. Continuously-updated Statistics on the pandemic evolution; 3. General information about Covid-19, such as about the typical symptoms. We identified two different scenarios in the provision of general information: -Some apps present curated information, i.e., information that is somehow checked and filtered by the development team before it is shown to the public. Such information is often tagged in a way that allows interested people to find the source, and gauge its credibility. Sometimes, these apps are developed directly by an entity that itself carries credibility as a source of information, such as national healthcare authorities. An example is the MyHealth Sri Lanka app 4 developed by the national ICT Agency, which presents to the user verified information on the current Covid-19 status. -A number of apps appear to provide unfiltered information regarding Covid-19. Their developers are not always themselves entities that would traditionally be assumed to have any specific credibility on the matter. For example, the DiagnoseMe app 5 , which claims to provide the user with all the information on the virus, is proposed by an association with unrecorded expertise in health. [2] Upstream collection. Most apps in our dataset perform data collection from users. This suggests that many app providers consider data to be key in the mitigation of the Covid-19 crisis. App providers indeed collect a variety of information, including user personal information (e.g., name, age, address, etc.), some medical information (e.g., whether a user is infected with Covid-19, the therapies that are used). Some apps are even used to keep a health diary (sharing information about symptoms every day), or to report the infection of people in the app user's acquaintances. Overall, we have identified three different ways in which apps collect user data, as summarized in Figure 8 . Note that in the case of data collection and spread tracking apps, we did not try to qualify whether apps were as privacypreserving as their developers claimed they were (e.g., data is deleted after N days), nor to determine to what extent the collected data is shared with third parties. Similarly, for this paper, we did not analyze the inner workings of contacttracing apps, and we did not evaluate the merit nor the opportunity of contacttracing, this having already been-and still being to this date-discussed by security researchers (Culnane 2020; Anderson 2020; Baumgrtner et al. 2020 Several apps take inputs from the users to offer diagnoses related to Covid-19. Such apps can provide a built-in questionnaire that users have to fill within the app, or leverage a virtual assistant or chatbot. In these cases, the diagnosis can be made automatically, with no interaction nor confirmation with a trained medical practitioner. Other apps, however, provide a somewhat more traditional medical visit experience, by offering the facilities needed to remotely exchange (e.g., via instant text messages as well as voice and/or video calls) with a medical doctor. Such apps are used from home, since millions of people worldwide were confined, and were potentially reluctant or unable to visit a brick-and-mortar doctor's office. Additionally, some apps are developed to track the spread of the virus by locating the users of the apps. While a few of those apps use simple geomonitoring with GPS information for tracking users, most apps do it automatically. Nevertheless, we found a few apps that request users to provide a-posteriori the locations they have visited on a given day. We also identified one app which uses QR code scanning at the entrance of public buildings to obtain precise location information, while still being fully under users' control. With respect to tracing, a few apps promote social-distancing using the GPS location of users, the goal being to not approach other people too closely. Furthermore, several apps implement contract-tracing, i.e., the ability to retrieve who a specific person has been in contact with, providing users a way to know if they have encountered someone infected, and potentially infectious. Contact-tracing apps mainly rely on three methods, (1) Using the GPS location of users, (2) Using the Bluetooth technology to detect proximity, and (3) Using a location diary that the users have to manually fill. [3] Tooling The last category is the tooling category which includes several types of tools aimed at helping users deal with some consequences of the Covid-19 crisis (see Figure 9 ). A few apps allow users to auto-generate documents for their local authorities (e.g., travel authorization that had been made mandatory in several countries during containment). Users can also install apps offering appointment-capabilities for medical purposes, or selling Covid-related products (e.g., masks, hand-sanitizers, etc.). On the entertainment front, apps were released proposing games around the pandemic, or providing users with Covid-19-themed image filters, for example adding a virtual mask, or adding virtual decorative elements to an actual mask. Lastly, apps were also made to cater to the newly-discovered needs of massive remote education. The interested reader can inspect Tables 2, 3 and 4 for more information about the mapping between categories and the apps for which we were able to retrieve the relevant information. RQ1 Answer: Our empirical analyses show that overall, Covid-related apps are mainly developed for: 1. Providing users with information; 2. Collecting data from users; 3. Offering users Covid-related tools. 4.2 When did Covid-related apps start to appear in Google Play? In this section, we consider the 92 collected Covid-related apks 6 . As the first release date of apps is not available on Google Play, we need to rely on other sources of information to try to find the first appearance date. Therefore, in order to visualize when Covid-related apps appeared, we selected, for each app (when available), the minimum date among the AndroZoo added date, the VirusTotal first seen date and dates available in third party datasets (i.e., Koodous 7 , APKCombo 8 , APKPure 9 ). The results can be seen in Figure 10 . We can see that those apps started to appear as early as February 2020, i.e., before the pandemic was officially recognized by the World Health Organization (i.e., March, 11 th 2020) (Ghebreyesus 2020) . We can also see in Figure 10 that since February 2020, the number of new apps seen has only increased. We note that for June 2020, the lower number is not significant: Our study, and app collection process, stops in early June 2020 10 . RQ 2 Answer: Covid-related apps started to emerge on markets as early as February 2020, i.e., even before the World Health Organization officially declared Covid-19 as a pandemic. In prior work, Tian et al. (2015) have shown that specific sets of apps can have similar characteristics (e.g., similar permissions, components, size, etc.). In this section, we investigate to what extent 92 apps form one coherent group that is significantly different than other apps. To that end, for each app, we counted the number of different Android components (i.e., Activities, Broadcast Receivers, Services, and Content 6 We remind that we collected 92 apks but only 78 descriptions. So, if descriptions are required to perform a study, 78 apks are considered. If not, 92 apks are considered. 7 https://koodous.com 8 https://apkcombo.com 9 https://apkpure.com 10 We remind the reader that, while our initial app dataset built from AndroZoo contains apps until end of May 2020, our dataset manually collected contains apps from June 2020 (See Section 3.1) Comparison Dataset: For comparing the characteristics of Covid-related apps with other apps characteristics, we randomly selected 5000 apps from our initial dataset of 184 563 apps. Those 5000 apps are sampled from the same time span (i.e., they are coming from the same initial dataset) to ensure that time is not a factor in potential differences. In the rest of this section, those 5000 apps are referred to standard apps, in opposition to Covid-related apps. Android Components: Figure 11 depicts differences between standard apps and our set of Covid-related apps regarding the number of components included in the app. We notice that Covid-related apps tend to use fewer activities than standard apps and that their distributions are different. However, it is unclear whether or not there is a significant difference for the remaining components, i.e., services, broadcast receivers and content providers. Therefore, we apply the Mann-Whitney-Wilcoxon (MWW) test which is a non-parametric statistical test for assessing the significance of the difference between two distributions (Mann and Whitney 1947; Wilcoxon 1945) . We obtained the results available in Table 5 . At a 0.05 significance level, we can see that the distribution of the number of activities in standard and Covid-related apps are significantly different, likewise for providers. However, regarding the distribution for services and receivers, the difference does not seem to be much significant. Therefore, the difference between those apps seems to appear at the GUI level since Activities are the main UI building block in Android apps. This suggests that Covid-related may have less complex GUI than standard apps. Content Providers are slightly more used in Covid-related apps, which could hint at Covid-related apps being more data-centric than standard apps. p-value Activities 7.9 × 10 −27 Services 4.5 × 10 −2 Receivers 5.1 × 10 −2 Providers 1.6 × 10 −3 Dex files size: Figure 12 shows the distributions of the dex sizes of both the Covid-related and standard apps. An MWW test confirms the statistical significance of the difference between both distributions. Figure 12 also shows that the majority of Covid-related apps are smaller than typical standard apps. However, the maximum dex size value is higher than standard apps, hinting at a higher diversity amongst Covid-related apps. Permissions: In Table 6 , we can compare the permissions used by the set of standard apps and the permissions of the Covid-related apps. For this purpose, we extracted for both sets of apps the top ten most requested permissions. First, a notable difference is that Covid-related apps tend to use more the wake lock permission. This permission is used for preventing the screen of the device from being turned off, and/or to ensure an app remains active. This feature is likely used for keeping the phone awake while locating the phone. In the same way, access fine location and access coarse location tend to be used more by Covid-related apps. Figure 13 shows the distributions of the number of permissions requested by standard apps and Covid-related apps. We note that these distributions are significantly different, as confirmed by a p-value of 10 −9 for the MWW Table 6 : Top ten most requested permissions in both standard apps and Covidrelated apps. Percentage indicates the ratio of apps using the permission. statistical test. It is notable to see that the median value for Covid-related apps is half the median of standard apps, which clearly shows that Covid-related apps request fewer permissions than standard apps. This result suggests that Covid-related apps leverage fewer functionalities from the Android framework. Libraries: To compare the patterns of libraries inclusion, we measure the use of libraries by relying on a collection of well-known libraries. More specifically, we re-use two lists of libraries established in prior works (Li et al. 2019; Li et al. 2016 ): a list of 1 114 common libraries and a list of 240 advertisement libraries. Therefore, for both Covid-related and standard apps datasets, we computed the number of apps using at least one common library and one advertisement library. Table 7 presents our results. First, we notice that almost all the apps (Covid-related and standard) use common libraries, which is not surprising since Android software development-just like non-mobile software-heavily relies on reusable libraries and frameworks. However, the difference is significant regarding the advertisement libraries. Indeed, while advertisement libraries are used by more than 80% of standard apps, they only appear in less than 20% of Covid-related apps. Furthermore, only 3 out of 240 advertisement libraries are used in Covid-related apps, namely: (1) com.facebook, (2) com.startapp.android and (3) com.flurry. This strongly suggests that the primary goal of Covid-related apps is not to obtain a financial gain from advertisement, in opposition to the vast majority of standard apps. Covid-related apps are composed of less GUI-related components than standard apps, and are usually smaller. While overall they use fewer permissions, they tend to use more tracking-related permissions. With respect to libraries, Covid-related apps use common libraries just like any other apps, but significantly fewer advertisement libraries than standard apps. 4.4 Are Covid-related Android apps more complex than standard apps? App complexity is an elusive concept. Yet, in the literature, there are various studies that propose metrics to measure some form of complexity and attempt to show its correlation with app quality and maintainability (Jošt et al. 2013; Gao et al. 2019b) . We undertake to investigate our research question based on these common metrics from the literature (Chidamber and Kemerer 1994) . We provide in Appendix A the descriptions of the complexity metrics we use. Since complexity comparison can be biased by the size of apps, we propose to compare the complexity metrics of Covid-related apps with 5000 randomly selected standard apps whose size is within the same size range (0 MB < size < 20 MB). The different metrics attempt to capture the Lack of Cohesion in Methods (LCOM), the Weighted number of Methods per Class (WMC), the number of methods invoked per class, i.e., the Response For a Class (RFC), the Coupling Between Object classes (CBO) and the Number Of Children per class (NOC). Figure 14 presents the distributions of metric values. LCOM, WMC, and CBO appear to present similar distributions across standard and Covid-related apps. However, MWW test revealed significant differences between the distributions of Covid-related apps and standard apps only for the following metrics: WMC (p = 0.0012), RFC (p < 10 −7 ), CBO (p < 10 −4 ) and NOC (p < 10 −4 ). Overall, these results establish that Covid-related apps are, to some extent, less complex than standard apps. According to (Jošt et al. 2013) , this result suggests that Covid-related apps may be more maintainable and of better quality. Additionally, we note that a lower complexity could also indicate that Covid-related apps have on average less functionalities and/or are focused on more specific goals, as was already hinted above in the permission usages comparison. RQ 4 Answer: Our empirical study shows that Covid-related apps tend to be less complex than standard apps.  To what extent were Covid-related apps removed from the official Google Play and why? We have seen in Section 4.1 that during our analyses, some Covid-related apps disappeared from the official Google Play in a matter of days. Google has announced in its policy regarding Covid-related apps (Google 2020 ) that specific checks are performed on Covid-related apps. In this section, we measure the impact of this policy on the removal of Covid-related apps from Google Play. Therefore, for each app that was initially identified at the beginning of our study (i.e., one of the 92 Covid-related apps), we queried the Google Play market, at the time of writing, to check if the app is still available. Around 15% of Covid-related apps (i.e., 14 apps) have been removed from Google Play. In comparison, among 1675 standard apps taken randomly from our initial dataset (see Section 2), we found that 277 (i.e. 16.54%) apps were removed from the Google Play market. The removal rates of both app datasets are close. Actually, we expected a much higher removal rate for Covid-related apps. This relatively low ratio of removal for Covid-related apps could be explained in several ways: -Google either enforces its policy very quickly or pre-screens (i.e., before it is accepted on the market) each app that is potentially relevant to Covid-19; In that case, apps would either never make it to the market, or would be removed too quickly for AndroZoo crawlers to catch them; -App developers either rapidly adapted to Google's policy and/or very few developers proposed apps that conflict with Google's policy. We are not able to reliably answer this research question. Indeed, even if, based on our study, we could conclude that Covid-related apps are removed from Google Play at a normal rate, we suspect that Google removes very quickly Covid-related apps that do not comply with the Google policy. To reliably conclude on this question, one would need access to key data about the stream of apps submitted to Google Play validation process. 4.6 Who are Covid-related apps' developers? On Google Play, in each web page of an app 11 , there is a field developer that provides the name of the person or entity (e.g. a software company, a governmental institution, an OMG, etc.) who has released the app. After collecting this information, we detail in Table 2 (column Developer Type) the status (or the type) of the entity having released an app. Table 8 presents the number of released Covid-related apps for each type of entities. We can see that most of the app providers are governmental institutions. We indeed find Covid-related apps that are officially promoted by national governments (e.g. Government of Brazil 12 or Government of France 13 ). We also see apps released by more local governmental bodies (at the state or regional level). We have for instance apps from specific states of the USA , or from specific ""Switzerland Canton"" (e.g. Gesundheitsdepartement des Kantons Basel-Stadt 15 ). About 20% of the Covid-related apps (17 apps) are provided by companies. In order to understand why these apps have not been removed by Google, we further check the description of these apps and the descriptions of the companies. We found that: -Even if the developer is identified as a company, two apps have been developed on behalf of official bodies (Care19 16 is the official COVID-19 app for the states of South Dakota and North Dakota, COVID AP-HM 17 is an app developed for a hospital); -Seven apps are either endorsed by a ministry 18 , or working in close collaboration with medical/health actors 19 , or working in collaboration with renowned universities 20 . -Two apps are actually online shopping apps 21 . -One app is not on the market anymore 22 . -Finally, five apps related to social distancing 23 , or health 24 , or Covidrelated news 25 , have been released by companies without any explicit link to official organizations. We remind that the official Google Covid-19 policy (Google 2020) is that Covid-related apps with no explicit links with governmental bodies or health organizations cannot provide ""health claims"". We further check these 5 apps, and we confirm that they comply with the Google Covid-19 policy. For the remaining nine Covid-related apps, we noticed that 3 apps have been provided by associations. More specifically, the DiagnoseMe 26 app has been released by the Faso Civic association from Burkina Faso, the Self Shield App 27 by the Commonwealth Medical Association (through the Commonwealth Centre for Digital Health organization) and the COVID Safe Paths 28 app by a non-profit organization related to MIT. We also noticed that two apps have been developed by independent developers, and two other apps have been provided by researchers. One by a group of researchers from German Universities 29 , one by researchers from the Aga Khan University in Pakistan 30 . Finally, one app has been provided by an NGO (i.e., the Austria Red Cross), and one by a hospital (actually a group of hospitals in Paris, France). We note that among all the Covid-related apps, 71% of them have been released by entities having multiple Android apps on Google Play. Finally, we represent in the map of Figure 15 the geographical distribution of the apps over the world. We can see that Covid-related apps are provided world-wide (maybe less present in Africa). The countries in blue are the ones listed in Table 2 . Note that we also identified 16 other apps from 16 countries that we were unable to obtain; These countries are represented in red. In contrast to a recent work , which focused on dissection Covidrelated malware, our aim in this work is not to perform an extensive security analysis of these apps. Nevertheless, we propose to leverage three practical security and privacy scanners on our set of 92 Covid-related apps in order to systematically evaluate three S&P aspects: (1) the presence of privacy leaks; (2) the number of apps flagged by VirusTotal; (3) the misuse of crypto-APIs. [Privacy leaks] As we have seen in 4.1, most of the Covid-related apps are made for collecting personal and sensitive data, e.g., health data and/or the location of users. Therefore, the security and privacy aspects of these apps are crucial, and many people started to share concerns related to this topic (Page 2020; Parliament 2020; Stolton 2020) . In order to assess the privacy of Covidrelated apps, we applied the state-of-the-art data leak detector FlowDroid-IccTA (Arzt et al. 2014; Li et al. 2015) . Through static analysis, this tool is able to detect sensitive data leaks intra-component (e.g., inside an Activity) or inter-component (e.g., across Activities). FlowDroid-IccTA was able to detect 24 intra-component data leaks in 2 different apps and found no inter-component leak. The app SODI 31 contained only 1 potential leak, whereas the app Coronavirus -SUS 32 contained 23 potential leaks. Given that static analysis tools are subject to false-positives, we undertake to manually analyze every detected leak. SODI an app promoting social-distancing. The app is not originating from a government. Our manual analysis concluded, however, that the reported leak is a false-positive alarm and does not constitute a real data leak. Regarding Coronavirus -SUS, which is an official app of the government of Brazil, FlowDroid-IccTA flagged 24 potential sensitive leaks (i.e., there is a path between a source (that can access a sensitive data) to a sink (e.g. sendSMS)). We notice that four of these leaks allow the app to get the longitude and/or latitude (the sources) of the app to log it internally (the sink). However, this does not necessarily constitute a malicious behavior. [AntiVirus detection] For each of the Covid-related apps, we have collected the detection reports from over 60 AntiVirus products, thanks to the 31 com.bloomreality.sodi 32 br.gov.datasus.guardioes VirusTotal API 33 . None of Covid-related apps is flagged by any of the 60 anti-virus software available in VirusTotal at the time of writing. [Crypto-API misuses] Finally, we leverage the state-of-the-art staticanalyzer CogniCrypt (Krger et al. 2017 ) through its headless implementation CryptoAnalysis (CryptoAnalysis 2020) for detecting cryptographic API misuses in Java programs. Such misuses could indeed indicate security issues. We found that 81 apps among our set of 92 Covid-related apps use JCA 34 APIs. However, CogniCrypt did not report any cryptographic misuse. In contrast, Gao et al. (2019a) have shown that in a dataset of more than 598 000 apks, 96% of apks using JCA exhibit dangerous misuses of cryptographic APIs. With 0%, Covid-related apps seem to be totally exempt from such misuses. RQ 7 Answer: Covid-related apps do not seem to leak sensitive data; Covid-related apps are not flagged by AntiVirus engines; Covid-related apps are further free of common cryptography API misuses; Overall, the analysis results suggest that Covid-related apps do not present the same security problems as general apps. Our study bears a number of threats to validity related to the selection of apps, external factors that may impact our conclusions, and the limitations of the leveraged tools. We may have missed some Covid-related apps. Our approach relies on simple heuristics to gather applications from AndroZoo, which helped to identify 44 unique apps. Therefore, we cannot guarantee that our app collection of Covid-related apps is exhaustive. However, we leveraged AndroZoo which is the largest and continuously-updated repository of Android applications available to the research community, which we further manually supplemented with other sources. Overall, it is unlikely that we missed a number of Covid-related apps that is large enough to significantly invalidate the conclusions presented in this paper. Actually, as we have revealed, we were able to catch in time some apps that were later removed from the market. The enforcement of Google Policy on market apps may have biased our study on Covid-related apps. Our experiments were conducted until early June 2020, and we know that Google has been enforcing its policy regarding Covid-related apps since at least early April Google (2020) . Given our vantage point without insider knowledge, our observations are limited to apps that were actually released on Google Play, and our study is blind to apps that were never let in Google Play. There is thus a possibility that some of our findings are consequences of Google's policy, and not a characterization of apps that were meant to be on Google Play. Nonetheless, our study reflects closely the landscape of apps that were made available to users. On the other hand, we note that the removal of apps from the market may have been performed by either market maintainers or by developers themselves. Future studies may attempt to investigate closely the reasons of app removal. Apk date is unreliable. In our collected dataset, the earliest date of appearance of Covid-related apps goes back to February 2020, but we relied on the earliest date between the AndroZoo added date, the first submission date to VirusTotal, and several other websites (included last update date in Google Play as discussed in Section 4.2). Since AndroZoo does not provide the release date of an app (which is different from the added date), it could be the case that the apps existed before. Unfortunately, we cannot rely on the dates of the files constituting the apps since they can easily be modified (Li et al. 2018) . As discussed in Hypothesis 4 of Section 3.1, for classifying an app as Covidrelated or not, we manually analyzed the Google Play page of the apps. More specifically, we relied on the information available on the app description part of this page. There is a risk that this information could be dishonest, incomplete, or misleading. However, we note that no Covid-related app was flagged by antivirus products, which is an indication that Covid-related apps are unlikely to be malicious, leaving little incentives for apps publishers to provide deceptive information. Furthermore, we note that as long as this potential misinformation is only sporadic, it is very unlikely to significantly alter our taxonomy. Additionally, the fact that the app descriptions were still available on Google Play, suggests that Google did not detect any dishonest information. We leveraged several security scanners to evaluate different security characteristics of apps, and hence inherit their limitations. We cannot guarantee that these tools yielded accurate analyses. We mitigated the threats by first ensuring the selection of state of the art tools that are commonly used in the literature, and second by ensuring that we do not overclaim based on their results. Several prior works have conducted empirical studies on large sets of Android applications collected from app markets. Viennot et al. (2014) collected more than a million apps from Google Play and uncovered several interesting patterns in Android apps and the way they are developed. Also in 2014, Gorla et al. (2014) collected apps and their associated descriptions and automatically verified whether the descriptions actually matched the app behaviors while Qu et al. (2014) checked the descriptions against the permission usages. Other works focused on financial apps (Taylor and Martinovic 2017) , on app maintenance and prices (Carbunar and Potharaju 2015) , on malware (Zhou and Jiang 2012) , or on the quality of apps descriptions (Jiang et al. 2014) . To the best of our knowledge, the academic literature has not yet reported on a systematic study on Covid-related Android apps. however have recently discussed the case of coronavirus-themed malicious apps. In this paper, the authors analyzed 277 malicious applications related to Covid-19 among an initial dataset of 2016 applications. In our paper, we did not identify any malicious Covid-related apps which seems to be in contradiction with who collected 277 malicious Covid-related apps. However, this discrepancy could be explained by at least two reasons: -We used a more selective filtering process to collect Covid-related apps, notably because we realized that the keywords we initially used (and that also used) were too broad, i.e., they catch many apps not related to Covid-19; -Among their initial dataset of 2016 apps, only 6 are coming from Google Play. Unfortunately, the paper does not precise whether the 6 apps from Google Play are part of their 277 identified malicious apps. Overall, their results show that the vast majority (and probably all, as per our results) of malicious Covid-related apps are coming from application sources that are outside Google Play. Besides, note that in we do not have the information whether they considered different versions of a single app or not. Considering multiple versions could drastically lower the number of apps in their dataset. Their study further shows that there is a correlation between the number of Covid-19 cases in the world and the number of malicious Covid-related apps and that malicious developers did not repackage existing Covid-related apps for releasing malicious apps, contrary to a common malware practice (Zhou and Jiang 2012) . uncovered 34 different malware families used in malicious Covid-related apps. Their results further suggest that malware developers do not target specific users but target a wide range of countries. Since the emergence of the Covid-19 topic in the media, researchers are investigating its effect, not only on the medical front but also on our daily life. Related to the security perspective of mobile applications users, several security researchers and analysts publicly disclosed their findings, usually in blog posts or press articles, about the activity of malware developers trying to take advantage of the Covid-19 crisis (Doffman 2020; Buguroo 2020; Saleh 2020; Arsene 2020) . Similarly, from a privacy perspective, a common functionality of Covidrelated apps for fighting against the spread of Covid-19 is contact tracing. Tracing however carries several concerns with respect to user privacy. Indeed, Baumgrtner et al. (2020) show that although developers claim to respect privacy, it is possible to de-anonymize information about infected persons that are traced and even sabotage the tracing effort by injecting fake contacts.@story_separate@In this paper, we provide a first systematic study of Covid-related Android applications. We collected from different channels 92 apps that are manually vetted as relevant. Then, based on the apps' described goals, our study yields a taxonomy of Covid-related Android apps as a contribution to the literature. Our empirical findings reveal that Covid-related apps have mainly three purposes: (1) inform users, (2) collect data, and (3) provide tooling capabilities for users. After exploring the inner characteristics (e.g., libraries, permissions, size) and the results of security and privacy scanners, we provide first insights into the nature of Covid-related apps. Overall, our empirical study constitutes a first milestone towards understanding who, what, and how Covid-related apps are built. We expect future work in the community to go in-depth into each of the dimensions that we have explored. All artifacts are made available online at: https://github.com/JordanSamhi/APKCOVID","Due to the convenience of access-on-demand to information and business solutions, mobile apps have become an important asset in the digital world. In the context of the Covid-19 pandemic, app developers have joined the response effort in various ways by releasing apps that target different user bases (e.g., all citizens or journalists), offer different services (e.g., location tracking or diagnostic-aid), provide generic or specialized information, etc. While many apps have raised some concerns by spreading misinformation or even malware, the literature does not yet provide a clear landscape of the different apps that were developed. In this study, we focus on the Android ecosystem and investigate Covid-related Android apps. In a best-effort scenario, we attempt to systematically identify all relevant apps and study their characteristics with the objective to provide a first taxonomy of Covid-related apps, broadening the relevance beyond the implementation of contact tracing. Overall, our study yields a number of empirical insights that contribute to enlarge the knowledge on Covid-related apps: (1) Developer communities contributed rapidly to the Covid-19, with apps released as early as February 2020; (2) Covid-related apps deliver digital tools to users (e.g., health diaries), serve to broadcast information to users (e.g., spread statistics), and collect data from users (e.g., for tracing); (3) Covid-related apps are less complex than standard apps; (4) they generally do not seem to leak sensitive data; (5) in the majority of cases, Covid-related apps are released by entities with past experience on the market, mostly official government entities or public health organizations."
"The Investigator's Brochure (IB) is a multifunctional regulatory document essential for the conduct of clinical trials that summarises the physical, chemical, pharmaceutical, pharmacological, and toxicological characteristics of an investigational medicinal product (IMP) as well as any clinical experience. The ICH E6 (R2) Guideline for Good Clinical Practice recommends a standardised content structure for the IB which includes a final summary of data and guidance for the investigator (Table 1 ) [1] . One key function of the IB is to provide investigators with data about the IMP that informs them in their conduct of a clinical trial, providing information that can facilitate the understanding of potential safety concerns and appropriate precautions to minimise the risk of exposure in healthy subjects and patients receiving the IMP. Safety remains the primary concern of those conducting early phase clinical trials. However, during the early stages of development, clinical experience with the IMP is either lacking (first-in-human studies; FIH) or sparse, leaving assessment of risk dependent on non-clinical pharmacology, safety and toxicology data when transitioning from in vitro investigations and animal studies to humans. While more than 3000 FIH trials have been conducted in the European Union (EU) [2] over the last 14 years, there have been two incidents where exposure to IMP has resulted in tragic consequences: the serious systemic inflammatory responses (cytokine storm) following the first administration of TGN1412 in 2006 [3] and the serious neurologic adverse effects after multiple highdose administrations of BIA 10-2474 in 2016, leading to the death of one healthy subject [4] . In considering circumstances behind these events the European Medicines Agency (EMA) issued a revised guideline on strategies to identify and mitigate risks to subjects involved in first-inhuman and early clinical trials [5] . The guideline requires sponsors to describe the degree of uncertainty around the predicted effects of the IMP which can then be used to determine corresponding risk mitigation strategies. Those involved in the design, approval and conduct of clinical trials are then required to base any decisions they make before and during a trial on rigorous assessment of all the available data-with the Investigator's Brochure being the pivotal document where these data can be found. Consequently, the clear presentation of IMP-related data within the IB with its interpretation of potential risks and appropriate risk mitigation measures is of utmost importance for the safe and successful conduct of clinical trials.@story_separate@A discussion forum to critically debate on how to optimise the presentation of data in the Investigator's Brochures and derive a meaningful assessment of risk in early clinical trials was hosted by the German non-for profit Association for Applied Human Pharmacology (AGAH e.V.) in Bonn, Germany on November 11, 2019. Participants comprised expert stakeholders from pharmaceutical industry, clinical contract research organisations (CROs), scientific consultancy, academia, ethics committees and the German competent regulatory authority, the Federal Institute for Drugs and Medical Devices (BfArM), (for conference details, contributions, and presentations see https ://www.agah.eu/event / agah-disku ssion sforu m-2019/). After a keynote lecture entitled 'The IB-key information to support early clinical trials', the following four topics were first introduced by brief presentations and then discussed with the audience: • Deficiencies/uncertainties in IBs-quality of IB sections 1 to 6 • Guidance for the investigator-quality of IB section 7 • Reference safety information in the IB • Potential risks for human subjects associated with inadequate non-clinical safety assessment in the IB Effects in humans 6.1 Pharmacokinetics and product metabolism in humans 6.2 Safety and efficacy 6.3 Marketing experience 7 Summary of data and guidance for the investigator At the end of each discussion session, the participants were asked to voluntarily take part in an online survey using the online slido tool (https ://www.sli.do/) on their own mobile devices and answer a series of specific questions either by choosing from a panel of multiple pre-specified answers or by giving a rating from a score between 1 = very bad and 5 = very good, as applicable. The intention of the online survey was to capture the opinions and experience of the participants in a structured way. Since this survey was not regarded as medical research or a clinical study, an explicit written informed consent was not obtained from the participants. Responses were compiled electronically. The survey responses underwent real-time analysis by the slido tool to provide descriptive statistical results which comprised percentage of participants who have given a specific answer and mean values for rating scores where applicable. Forty-three participants of the discussion forum took part in the online survey. Six of them (14%) came from the pharmaceutical industry, 20 (47%) from CROs, seven (16%) from a regulatory authority, six (14%) from universities, one (2%) from an ethics committee, and three (7%) were scientific consultants. The majority of participants were based in Germany (38; 88%), three (7%) were from the Netherlands and two (5%) from Switzerland. The results presented are reflecting the discussion of the experts at the forum based on their opinions and experience. The online surveys add to this approach, but it is not intended to fulfil the characteristics of systematic research or a clinical study. The keynote lecture addressed better ways for displaying and interpreting data within IBs and focused on the recent development of a tool that can be used to integrate and display specific data [6] . The tool consists of a database in which relevant results from non-clinical studies on the IMP are entered. The tool enables data to be sorted in tabulated rows by increasing maximum systemic drug concentrations (C max ) or by increasing estimated human equivalent doses. Missing data on C max may need to be interpolated from population pharmacokinetic models or from other studies within the same species. Results from pharmacology, safety, and toxicology studies can be differentiated by colour (e.g. blank = no data, green = pharmacologically or therapeutically desirable, yellow-orange = increasingly severe reversible adverse events, red = irreversible toxicity and death). Adding an anticipated human effective maximum concentration or dose or even data from first clinical trials allows the sponsor and the investigator to get an overview on all non-clinical data for the IMP and to obtain an overview of whether the intended human doses are expected to be safe for administration in humans. Several real-world examples illustrated the practicability of the tool which is freely available for use (www.ib-deris k.org) [6] . The emphasis of this approach is on acute, potentially adversely exaggerated pharmacological effects correlating with C max . Dose-limiting acute adverse responses to substances of certain classes, such as compounds causing pharmacologically mediated functional central nervous system (CNS) effects, may directly correlate with maximal systemic plasma/serum concentrations and are a typical function of the pharmacological on-target activity [7] . However, for most drug classes, including CNS-active drugs, the full characterisation of the safety profile requires repeated dosing and exposure data in terms of area under the concentration-time curve (AUC) which are often more predictive of how effects will translate from one species to another, with the exception of acute functional effects [8] . The German Medicinal Products Act requires that 'each investigator has been informed by a scientist responsible for the pharmacological-toxicological test about the findings of the test and the foreseeable risks involved in the clinical trial' [9] . Although this information is provided in the IB, this prerequisite to conducting clinical trials with the IMP has the inherent implication that an identifiable scientist takes responsibility for the acceptability of the risk to subjects by providing the signed approval in the IB. Deficiencies observed by the BfArM refer to the use of different IB versions in parallel and to IB changes considered as nonsubstantial by the sponsor although they had to be regarded as substantial according to the assessment by BfArM. The availability of new non-clinical and/or clinical data is usually considered substantial as they could have an impact on safety of subjects in clinical trials and need to be scientifically reviewed by the national competent authority, whereas non-substantial amendments are processed administratively only. The sponsor must assess any new information to evaluate its potential impact on the established risk/ benefit profile. The BfArM requires the sponsor to provide this assessment for the IMP and the ongoing trials, justifying why either the risk/benefit assessment remains unchanged or why existing measures are considered adequate to control for any newly identified risks or previously established risks with an altered appraisal (e.g. lower NOAEL with prolonged treatment but same principal target organ). It was general consensus that similar to clinical trial protocols, the IB should include a last section on the amendment change history. Overall, it is the task of the sponsor to constantly evaluate the risk/benefit of an ongoing clinical trial, and the competent authority must be able to scrutinise this assessment objectively. The findings of the online survey asking participants about the overall content quality of the IB they have previously used for early development clinical trials revealed a mean score of 3.6 (where a score of 1 = very bad and 5 = very good). The majority of participants (55%) gave a rating of 4. More detailed participant questioning revealed mean scores of 3.1 for readability, 3.3 for comprehensibility, 3.3 for appropriateness for risk assessment, 3.6 for timeliness, 3.3 for quality of presented non-clinical information, and 3.7 for quality of presented clinical information. Important requirements for this section of the IB from the viewpoint of the investigator are completeness and correctness of data as well as clarity, comprehensibility, and relevance of the information provided [10] . It is generally agreed that this is best achieved through the logical presentation of data, indication of important missing data, and comprehensive conclusions containing guidance for dosing, warning messages on adverse reaction that might be expected with the IMP, and recommendations on appropriate measures to deal with IMP-related medical emergencies. From the point of view of the ethics committee, nonclinical studies on pharmacology should include elements essential for ensuring validity such as randomisation, sample size calculation, and blinded outcome assessments [11] . Non-clinical pharmacology studies to establish the proofof-principle for a given investigational medicinal product (IMP) are ideally undertaken in various disease models since many of them face limitations regarding their predictive value for the human condition, and a combination of studies provides a more meaningful basis to assess the relevance of the data. Typically, some studies may not show evidence of efficacy whereas others are positive. However, all pharmacology studies including those with negative outcomes should be reported in order to avoid assessment bias and to allow investigators to appraise the strength of the supporting preclinical data systematically which should be reflected in the guidance for the investigator [11] . According to the 'detailed guidance on the collection, verification and presentation of adverse event/reaction reports arising from clinical trials on medicinal products for human use' issued by the European Commission [12] and according to 'regulation (EU) no 536/2014 of the European Parliament and of the Council on clinical trials on medicinal products for human use' [13] , the IB shall include a clearly identifiable section called the 'Reference Safety Information' (RSI). The RSI shall contain information on the IMP, how to determine whether adverse reactions should be considered as expected adverse reactions, and on the expected frequency and nature of such adverse reactions [13] . The sponsor's informed opinion on the expectedness of an adverse reaction must be provided from the perspective of previously observed events rather than the basis of what might be anticipated from the pharmacological properties of the IMP [12] . More specific guidance was recently issued in the form of a Questions & Answers publication by the Clinical Trial Facilitation Group (CTFG) [14] . It was highlighted how the RSI should be used by the sponsor for the assessment of the expectedness of all suspected serious adverse reactions (SARs) occurring in clinical trials in order to assess the need for expedited safety reporting [15] . It was determined that the content of the RSI should include a clear list of 'expected SARs', i.e. SARs that could be expected following exposure to the IMP. The list should be restricted to 'suspected' SARs that had previously been observed and where, after a thorough assessment by the sponsor, reasonable evidence of a causal relationship between the event and the IMP had been established [14] . By implication, each 'expected SAR' should already have been reported as a 'suspected' SAR more than once. Where SARs turned out to be fatal and life-threatening, they would usually be considered to be unexpected even if the fatal or life-threatening SARs had been reported previously [14] . In the IB, the sponsor should clearly indicate that the RSI section outlines expected SARs for regulatory reporting purposes and that the information in the RSI section does not present a comprehensive overview of the safety profile of the IMP. Therefore, the RSI should always be provided in a clearly separated and specific section of the IB [13] . It is required that the RSI be presented in a table form where the 'expected SARs' are listed by body system organ class using preferred terms as per the latest MedDRA version followed by frequency of incidence calculated on an aggregated level based on previously observed 'suspected' SARs. Where necessary, the RSI section of the IB is to be updated annually after the Development Safety Update Report data-lock point by a substantial amendment. All other safety information comprising a detailed overview on the safety profile of the IMP should be provided in the subsection on the 'Effects in Humans' and in the 'Summary of Data and Guidance to the Investigator' (Section 7) [14, 16] . Following discussion over the RSI, participants indicated through the online survey that most (63%) were familiar with the CTFG Q&A document and that they were used to seeing this information provided as a separate section in some, though not all, of the IBs they had worked with. When debating their involvement in the production of IBs, the majority of participants (61%) indicated that they preferred to locate the RSI as a separate section, Section 8, rather than as a subsection of Section 7. These results are in line with data from a previous web-based survey that was conducted to understand current industry practices in the preparation of safety information for the IB [16] . The ICH E6 (R2) guideline for Good Clinical Practice requires that results of all relevant non-clinical pharmacology, toxicology, pharmacokinetic, and metabolism studies be provided in a summary form within the IB addressing the methodology and discussing the relevance of the findings to the IMP and the possible unfavourable and unintended effects in humans [1] . It is emphasised that where possible, data should be provided in tabular form to enhance the clarity of its presentation. It was generally agreed that provision of these data in a format that is difficult to interpret would be associated with unwarranted risk in terms of the overall conduct of the clinical trial assuming that critical information could be missed. One example where inadequate presentation might introduce risk would be investigations that had been performed but were not discussed or presented in the IB or cases where the sponsor provides insufficient discussion regarding the relevance of pharmacodynamic models, species differences, off-target effects, safety margins, or findings from non-clinical safety studies. It may also include listing of results in the absence of context and/or a lack of expert interpretation. Key objectives of the non-clinical safety programme are to identify: a. the initial safe starting dose and subsequent dose escalation schemes in humans; b. potential target organ systems for toxicity including dose dependence of adverse effects, any relation to exposure over the dose range tested (C max and AUC in each species tested including total and free drug including predicted safety margins compared to predicted efficacious exposures in humans), and the assessment of reversibility of adverse effects; c. safety parameters for clinical monitoring [5, 17] . It is the purpose of the IB to describe all these characteristics of the IMP clearly. In particular for early IBs, the results obtained from the non-clinical safety package need to be addressed in appropriate detail and context in order to support the development of a benefit-risk assessment based on the totality of the available evidence. The interpretation of critical findings such as mortality in toxicology studies should be robust and interrogated to establish whether there is any underlying cause for concern, whether it is possible to establish such an opinion with the data available and if so, based on which observations. Key considerations include but are not limited to the nature, type, and severity of the findings across species and studies, their relation to exposures over the dose range tested, the steepness of the dose-response, and predicted safety margins [8] . If any important issues are identified during non-clinical development, it is mandatory to assess critically whether they require clinical development to be put on hold and whether any risks can be mitigated and/ or explained. While findings may be deemed to have a negative impact on the risk/benefit profile but the safety of subjects in clinical trials can still be ensured with the overall risk/benefit profile remaining positive, initial measures intended to mitigate potential risks may need to be modified. Any new measures introduced to mitigate risk and address any management needs must be explained clearly and justified. All relevant data should be presented in a transparent manner to convey salient information supporting human risk assessment clearly and concisely. In debating the production of IBs, it was agreed by the participants that they represent a living document critical to the clinical development of new medicines that are constantly changing as we learn more about an IMP. As such, this places emphasis on producing documents containing the correct information and therefore should be given sufficient time and resources to produce and maintain quality documents, keeping them appropriately updated with new information from all disciplines [10] . Ongoing and interdisciplinary risk assessments must be undertaken regularly and at appropriate junctures to integrate all data, and to identify and address any emerging concerns in a timely manner. The final session of the discussion forum involved a group debate on whether it was possible for any parts of the IB to be improved and if so, how that could be delivered. The overall opinion of the participants is summarised in Table 2 . The IB section that most of the participants perceived in need for improvement was the summary of data and guidance for the investigator (Section 7), followed by the nonclinical studies (Section 5) and the effects in humans (Section 6). It was generally perceived that this represented a call to the sponsors of clinical trials and the authors of the IBs to diligently optimise the data presentation in order to ensure the best possible understanding of the IMP characteristics by the investigator and permit an optimal benefit-risk assessment which safeguards those taking part in clinical trials.@story_separate@The discussion forum on interpretation and optimisation of the IB for meaningful risk assessment of early clinical trials yielded aspects for future consideration: • The summary of data and guidance for the investigator are considered as the IB section with the highest need for improvement especially with respect to readability, comprehensibility, timeliness of data, and appropriateness for risk assessment. • The name and dated signature of at least the sponsor's scientist responsible for the content on pharmacology and toxicology of the IMP should be given in the IB. • Changes to the IB should be thoroughly considered whether they constitute substantial amendments, which need regulatory approval. • All non-clinical pharmacology studies with negative outcomes should be reported in the IB in order to avoid assessment bias. • The RSI should be depicted as a clearly separate section close to the section 7 of the IB rather than as a subsection thereof. • The IB should include a section on the amendment change history at the end of the document.","PURPOSE: A discussion forum was hosted by the Association for Applied Human Pharmacology (AGAH e.V.) to critically debate how to interpret and optimise the Investigator’s Brochure (IB) for meaningful risk assessment of early clinical trials. MATERIALS AND METHODS: Four topics were specifically discussed: deficiencies/uncertainties in IBs, guidance for the investigator, reference safety information, and potential risks for human subjects associated with inadequate non-clinical safety assessment in the IB. In each case, 43 participants took part in a real-time online survey with pre-defined questions to capture the audience’s opinion. RESULTS: The ‘Summary of Data and Guidance for the Investigator’ was considered as the section of the IB with the highest need for improvement with emphasis on readability, comprehensibility, timeliness of data, and appropriateness for risk assessment. It was suggested that the IB should at least be signed by the sponsor’s scientist responsible for the content on pharmacology and toxicology. It was agreed that sponsors should consider thoroughly whether changes to an IB constitute a substantial amendment, and that the IB should include a section on the change history. Non-clinical pharmacology studies with negative outcomes should be reported in the IB in order to avoid assessment bias. The reference safety information for expectedness assessment of suspected serious adverse reactions should be provided as a stand-alone section of the IB. CONCLUSION: The overall consensus was that an optimised presentation of data will ensure the best possible understanding of a compound’s characteristics and an optimal benefit-risk assessment which will safeguard the participants in clinical trials."
"Coronavirus infectious disease-19 (COVID-19) is thought to have originated in Wuhan, China in December, 2019, causing a serious pandemic lasting through 2020 and beyond [1] The World Health Organization (WHO) declared it a public health emergency of international concern on January 30, 2020 and gave a name for the new coronavirus disease COVID-19 on 11 February 2020 [2] . COVID-19 is characterized by the symptoms of fever, fatigue, dry cough, malaise and breathing difficulty [3, 4] . Although the virus can affect all age groups, the risk of death and severe illness is mainly related to old age and pre-existing chronic diseases such as hypertension, cardiac disease, lung disease, cancer and diabetes [5] . COVID-19 is primarily transmitted from person to person through respiratory droplets and close contacts with infected individuals [6, 7] . The virus can also be transmitted through respiratory droplets and touching a surface or object infected with the virus [7] . COVID-19 continues to cause morbidity, mortality and economic crisis all over the world. As of 7 th December, 2020, more than 65.8 million confirmed cases and over 1.5 million deaths were reported worldwide [8] . In Africa, over 1.5 million cases and 34,486 deaths were registered as of 7 th December 2020 [8] . Estimates have showed that in African countries COVID-19 will cause 1.4% Gross Domestic Product (GDP) decline and 5% in public revenue losses [9] . Estimates have also showed that by 2025 emerging and developing economies could experience drops in output of nearly 8% while oil-dependent countries such as South Sudan could decline by as much as 11% [10] . In Ethiopia, the first case was confirmed on 4 March 2020 and by 07 December 2020 had recorded 112,740 confirmed cases and 1,745 deaths [11] . Currently there is no widely-available medication for COVID-19, so the best way to tackle the virus is to practice prevention strategies. The Centers for Disease Control and Prevention (CDC) recommends the prevention of human-to-human transmission by measures such as staying home and avoiding close contact with others, wearing a facemask that covers nose and mouth in public settings, cleaning and disinfecting frequently touched surfaces, washing hands often with soap and water for at least 20 seconds, or using an alcohol-based hand sanitizer that contains at least 60% alcohol [12] . Studies have also showed that repeated handwashing can minimize the risk of viral transmission [13, 14] . Viruses that cause respiratory infection can survive on surfaces for extended periods [15] . Through contacts with these surfaces the virus can be transmitted to human skin, therefore keeping good hand hygiene helps to kill the virus since it is more inactivated from human skin than surfaces [16] . The effectiveness of hand hygiene in the prevention of respiratory infections was observed during the Severe Acute Respiratory Syndrome (SARS) outbreak in 2002-2004 [13] . A similar result has also been observed during outbreaks of influenza-like illness, that hand hygiene combined with facemasks reduced the rate of influenza-like illness [17, 18] . The effectiveness of hand hygiene was not limited in the prevention of SARS and influenza-like illness, but is also effective in preventing COVID-19 [19] . In addition to different COVID-19 prevention measures, it is crucial for people to have adequate knowledge and attitude for the successful control over COVID-19. For example, in China, awareness among the public has helped achieve the successful control of the disease [20] . Hence, the golden approach to tackle COVID-19 relies on applying appropriate prevention measures. This in turn requires having adequate knowledge to influence peoples' attitude and practices. The majority of taxi drivers may not be aware of prevention measures due to various unknown reasons. Since they have frequent close interactions with others, protecting taxi drivers helps to protect the community against COVID-19. Successfully accomplishing this goal requires assessment of the current knowledge, attitude, and the prevention practices applied by the taxi drivers. Since the outbreak in Ethiopia in early March 2020, COVID-19 has been spreading in all parts of the country. Despite measures taken to battle the virus, the country had 112,740 cases and 1,745 deaths recorded as of 07 December 2020 [8] . Although taxi drivers are especially vulnerable to the virus due to the nature of their work, they have not been among the groups that have been studied by investigators examining knowledge, attitude and hand hygiene practices during the COVID-19 pandemic among other target groups. Thus, this study aimed to determine the COVID-19 knowledge, attitude and frequent hand hygiene practices among taxi drivers in Dessie City and Kombolcha Town, Northeastern Ethiopia.@story_separate@a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 A cross-sectional study was conducted from July to August, 2020 in Dessie City and Kombolcha Town in northeastern Ethiopia to assess COVID-19 knowledge, attitude, and frequent hand hygiene practices of taxi drivers. Dessie City is found in the South Wollo Zone, on the eastern margin of Amhara regional state in north-central part of Ethiopia, at a distance of 401km from Addis Ababa. The town lies on the intersection of 11˚8 0 N and 39˚38 0 E. According to the 2007 population and housing census projection, Dessie District had a total population of 212,436 in 2014 [21] , whereas the population of Kombolcha was 102,530 in 2020 [22] . The source population for this study was all taxi drivers in Dessie City and Kombolcha Town, whereas the study population was all selected taxi drivers. The sample size of study participants was determined using single population proportion formula: n ¼ ðz a=2 Þ 2 � pð1À pÞ d 2 [23] . Since there is no published study that showed the knowledge, attitude, and frequent hand hygiene practices during COVID-19 among taxi drivers in Ethiopia, 50% of proportion was used considering 95% confidence level, 5% margin of error (d) and 10% non-response rate to get a final sample size of 422. A total of nine taxi fermatas (also called taxi stands [24] ) were selected randomly from the total fermatas found in Dessie City and Kombolcha Town (five fermatas selected in Dessie City out of the nine fermatas and 4 fermatas selected in Kombolcha town out of the 8 fermatas). Sample size for Dessie City and Kombolcha Town were proportionally allocated to determine the number of taxi drivers to be included from each fermata. Then, a simple random sampling technique was employed to select taxi drivers from the respective fermatas. There were 11 knowledge questions with ""yes = 1"" for correct or ""no = 0"" for incorrect responses to give values ranging from 0 to 11. Good knowledge was classified as a taxi driver giving correct answers equal to or above the mean out of 11 knowledge questions about COVID-19; poor knowledge was classified as a taxi driver giving correct answers below the mean out of 11 knowledge questions about COVID-19. Similarly, there were 15 attitude questions to which responses were classified into five categories ""Strongly Agree = 4, Agree = 3, Neutral = 2, Disagree = 1 and Strongly Disagree = 0"" with total values ranging from 0 to 60. Taxi drivers who gave correct answers equal or above the mean score were classified as having a positive attitude towards COVID-19 prevention, whereas taxi drivers who gave correct answers below the mean score were classified as having a negative attitude towards COVID-19 prevention. There were 6 COVID-19 frequent hand hygiene practice questions with ""yes = 1"" for correct or ""no = 0"" for incorrect responses to give values ranging from 0 to 6; those taxi drivers who answered equal or above the mean out of 6 hand hygiene practices questions were categorised as having good frequent hand hygiene practice, whereas having poor frequent hand hygiene practice was the category established for taxi drivers who answered below the mean out of the 6 hand hygiene practices questions. Hand hygiene is the compliance of cleansing hands with soap and water or with antiseptic hand rub (alcohol-based hand sanitizer) to remove transient microorganisms from hands and maintain the condition of the skin [25] . Therefore, in this study, frequent hand hygiene practice meant that frequently washing hands using with soap and water or with antiseptic hand rub to prevent COVID-19. A structured questionnaire adapted from previous published articles [26] [27] [28] , Ethiopian Public Health Institute (EPHI) and Ethiopian Ministry of Health (EMOH) COVID-19 prevention guidelines [29] and WHO guidelines [28] . The questionnaire was first designed in English, then translated to the local language (Amharic) and translated back to English to ensure consistency. The structured questionnaire consisted of four parts: Part I: questions related to socio-demographic and economic factors; Part II: questions related to knowledge about COVID-19; Part III: questions related to attitude towards COVID-19 and Part IV: questions related to frequent hand hygiene practices. A pre-test was conducted using a 5.0% sample size of the total study sample from outside the selected taxi stands of both Dessie City and Kombolcha Town to establish the validity and reliability of the questionnaire. The questionnaire was amended based on the findings of the pre-test. The reliability of the questionnaires for outcome variable measurements of knowledge, attitude and frequent hand hygiene questions was checked using Cronbach's alpha and found to be a Cronbach's alpha value of 0.973, 0.897 and 0.928. To enhance data quality, supervisor and data collectors received two days of intensive training. The training was provided by the principal investigator. The training components included information about each variable of the questionnaire, data collection procedures, how to approach study subjects, and how to ensure ethical practices during data collection. The two data collectors were BSc degree-holding environmental health professionals and one supervisor was BSc degree-holding public health officer. The data were collected by face-to-face interview and by observation of the availability of a handwashing facility with soap at each fermata and also the presence of alcohol-based hand sanitizer inside the car. As part of COVID-19 prevention measures, the data collectors and supervisor wore facemasks and kept a minimum two-meter distance from interviewees. The collected data were checked daily for completeness by the supervisor and principal investigator. In order to make the process convenient for study participants, a specific time frame for administration of the survey questionnaire was set, offering flexible times for respondents. When a study participant declined to respond to any specific questions at any time of the interview, the response was recorded as ""missing."" Furthermore, to check the reliability of the collected data, 10% of the study participants were randomly selected and re-interviewed by another interviewer. The accuracy of data entries was also checked by re-entered 10% for a randomly selected questionnaire. The collected data were checked for completeness and entered into EpiData version 4.6 and exported to Statistical Package for the Social Sciences (SPSS) version 25.0 for data cleaning and analysis. For continuous variables, mean with standard deviation were computed, whereas for categorical variables, descriptive statistics including frequencies, percentages and proportions were calculated. Factors associated with the outcomes of good knowledge, positive attitudes, and good frequent hand hygiene practices towards COVID-19 were determined using a binary logistic regression model. We used three different logistic regression models: Model I identified factors associated with good knowledge about COVID-19, Model II identified factors associated with positive attitudes towards COVID-19 prevention and Model III identified factors associated with good frequent hand hygiene practices about COVID-19. For each model, bivariable logistic regression analysis (crude odds ratio [COR]) and multivariable logistic regression analysis (adjusted odds ratio [AOR]) was performed at 95% CI (confidence interval). To control potential confounders, variables with p<0.25 were included into the multivariable analysis. Variables with a significance level at p<0.05 from the multivariable logistic regression analysis of each model were taken as statistically significant and factors significantly associated with good knowledge, positive attitude and good frequent hand hygiene practice related to COVID-19 among taxi drivers. The presence of multi-collinearity between independent variables was checked using standard error at the cut-off value of 2; we found a maximum standard error of 1.78, which indicated no multi-collinearity within independent variables. The Hosmer Lemeshow goodnessof-fit test [30] with p-value greater than 0.05 was used to check the fitness of each model; the pvalue of the Model I, Model II and Model III was 0.935, 0.896, and 0.963, respectively, and indicated that all models were fit. Ethical clearance was obtained from the ethical review committee of Wollo University College of Medicine and Health Sciences. Permission to conduct the study was obtained from South Wollo Zone Health Bureau and in turn permission was secured from Dessie City and Kombolcha Town Health Bureaus. Prior to the data collection, the purpose of the study was explained to the study participants and assurance was given that their participation in the study was voluntary. Then, informed verbal consent was obtained from each study participant. Data collectors wore facemasks and maintained social distancing per the WHO guidelines to prevent transmission of COVID-19. A facemask was provided to any study participant taxi driver who did not wear one during the data collection. The confidentiality of the study participants' responses was ensured by not disclosing any information to a third party. Of the total 422 taxi drivers, 417 participated, for a response rate of 98.0%. Nearly two-thirds 266 (63.8%) of the taxi drivers were aged �30 years, one-third 151 (36.2%) were >30 years and the mean age was 29.61 (±7.264SD). The education of more than half 247 (59.2%) of the drivers was at a secondary level or above while it was at a primary level for the rest 170 (40.8%) ( Table 1) . The mean (±SD) knowledge score among taxi drivers was 7 (±2.9) out of a possible score of 11. Based on the knowledge mean score, each study participant was classified as having good  The mean (±SD) attitude score among taxi drivers was 40 (±14.8 (Fig 2) . Overall, most of the respondents agreed that COVID-19 will be successfully controlled (84.0%). Overall, most (61.2%) of the respondents disagreed that they greet their friends and colleagues with a handshake whereas only (1.9%) agreed that they shake hands for a greeting. Almost half of the drivers (46.8%) strongly agreed that they wash their hands regularly and for sufficient length period of time and a similar proportion (43.9%) wear a facemask to protect against the risk of infection (Table 3) . The proportion of good frequent hand hygiene among taxi drivers was 66.4% [95%CI: 62.1-71] (Fig 3) . Three-fourths (75.1%) of the drivers cleaned their hands frequently before eating and two-thirds (66.7%) of them cleaned their hands frequently after using the latrine. More than half (57.8%) of the drivers cleaned their hands frequently before putting on a facemask. Nearly two-thirds 258 (61.9%) of the drivers cleaned their hands frequently using water and Table 4 ). From multivariable logistic regression analysis, it was found that educational level, place of residence and attitude towards COVID-19 were associated with knowledge about COVID-19; age, educational level, income and knowledge about COVID-19 were factors associated with attitude towards COVID-19; and educational level and attitude towards COVID-19 showed significant association with good frequent hand hygiene practices among the taxi drivers (Table 5 ). Taxi drivers with educational level of secondary (grades 9-12) or above were 7.55 times more likely to have good knowledge than individuals with a primary educational level (AOR = 7.55, 95% CI: 4.55-12.54). Place of residence also showed significant association with good knowledge about the disease. Individuals who were urban residents were 5.41 times more likely to have good knowledge than rural residents (AOR = 5.41, 95% CI: 1.4-20.08). On the other hand, drivers with a positive attitude were 1.67 times more likely to have good knowledge than those with negative attitude (AOR = 1.67, 95% CI: 1.02-2.74) ( Table 6) .  Knowledge, attitude and hand hygiene practices towards COVID-19 among taxi drivers With regard to attitude towards COVID-19, age 30 years or below (AOR = 3.01, 95% CI = 1.76-5.13), educational level of secondary or above (AOR = 3.16, 95% CI = 1.88-5.31), monthly income of 5,001.00-7,000.00 ETB (Ethiopia birr) (AOR = 3.36, 95% CI = 1.48-7.61) and monthly income of above 7,000.00 ETB (AOR = 2.12, 1.15-301) and knowledge about COVID-19 (AOR = 2.1, 95% CI = 1.21-3.54) were factors associated with positive attitude towards COVID-19 prevention. In addition, attitude towards COVID-19 (AOR = 5.5, 95% were the factors associated with good frequent hand hygiene practices (Table 7 ). The result of this study showed that 69.78% of taxi drivers had good knowledge score about COVID-19, while 30.2% of the drivers had poor knowledge about the disease. With regard to attitude towards COVID-19, the majority 282 (67.6%) had a positive attitude and one-third 135 (32.4%) had a negative attitude towards COVID-19. In this study, good knowledge about COVID-19 was significantly associated with educational level, place of residence and attitude towards COVID-19 whereas, age, educational level, income and knowledge about COVID-19 were factors associated with positive attitude towards COVID-19. We also found that the proportion of taxi drivers using good hand hygiene practices was 66.4%. In this study, the proportion of taxi drivers with good hand hygiene practices was significantly associated with educational level and attitude towards COVID-19. From our study, a good knowledge score was recorded for 69.8% (95% CI: 65.2-73.9) of taxi drivers. The result is in line with findings of some other studies conducted in Ethiopia 70% and 64.6% [31, 32] and Uganda 69% [33] . It is much higher than the findings from other Ethiopian studies 37.59%, 41.3%, 42.9%, 45.89% and 60.5% [34] [35] [36] [37] [38] , from Syria 60% [39] , and from Pakistan 54.3% [40] . But this finding was lower than in some studies in Ethiopia 93.8%, 88.2% and 74.7% [41] [42] [43] , from Nigeria 78.6%, 99.5% and 86.6% [44] [45] [46] , from Cameroon 84.19% [47] , from USA 80% [48] , from Pakistan 75.5% and 93.2% [49, 50] , from Malaysia 80.5% [51] , from China 89%, 90% and 82.34% [20, 52, 53] , from Iran 87.7% [54] and from Bangladesh 54.87% and 61.2% [55, 56] . This variation might be caused by differences in the study area and type of questions used. Our study suggests that educational level was significantly associated with knowledge about COVID-19. Taxi drivers with an educational level of secondary (grades 9-12) or above were 7.55 times more likely to have good knowledge than individuals with primary education only. This result was supported by previous studies in Northwest Ethiopia [57, 58] , in Egypt [59, 60] , in Pakistan [50, 61] , in Syria [39] , in South Korea [62] , in Nepal [63, 64] , in Iran [54] and in Bangladesh [65] . A possible explanation is that a person with higher education might make an effort to seek out more information about the virus and therefore have good knowledge about COVID-19. In this study, attitude towards COVID-19 was also found to be significantly associated with good knowledge about the disease. Taxi drivers who had a positive attitude were more likely to have good knowledge than those with a negative attitude. This was supported by previous studies in Nigeria [46] , Pakistan [61] , China [66] , Bangladesh [56, 67] , Iran [54] and in Nepal [63] . On the other hand, drivers' place of residence showed significant association with knowledge about COVID-19. Individuals who were urban residents were 5.41 times more likely to have good knowledge than rural residents. That result was similar to those of studies in Northwest Ethiopia [57] and in Egypt [59] . Our study demonstrated that 67.6% (95% CI: 63.1-72.2) of the drivers had a positive attitude towards COVID-19. This result is in line with studies in Northwest Ethiopia 66.1% and 70.65% [37, 57] , in Nigeria 64% and in Cameroon 69% [44, 47] , and in Syria 63.5% [39] . But it is lower than found in other studies from Ethiopia 94.7% and 74% [41, 42] , Nigeria 79.5% and 80.6% [45, 46] , Malaysia 83.1% [51] , Pakistan 86.5% and 75% [40, 49] , and from Bangladesh 78.9% [56] . The finding is higher than in some studies from Northwest Ethiopia 52.7% [57] , from Uganda 21% [33] , from Bangladesh 62.3% [68] and from Iran 59.3% [54] . This difference might be as a result of differences in target groups and type of questions used. Differences in scoring systems could also be the reason for the difference. In this study, positive attitude was correlated with age, educational level, income and knowledge about COVID-19. Educational level of secondary or above was significantly associated with good knowledge about the disease. This result was similar to other studies in Ethiopia [57, 69] . Drivers' income also showed significant association with positive attitude. This was supported by a previous study in Northwest Ethiopia [57] . In addition, good knowledge about COVID-19 was associated with a positive attitude towards COVID-19 prevention. This finding was supported by studies in Northwest Ethiopia [57] , Bangladesh [67] and Iran [54] . Evidence shows that hand hygiene is very important in the prevention of respiratory diseases [70, 71] . It can play a role in reduction of respiratory illness by 21% and gastrointestinal illness by 31% [72] . Although hand hygiene is the cheapest and easiest tool to prevent the spread of COVID-19 and other infections, it was found to be more widely practiced to during the period of widespread transmission of COVID-19 than during the spread of other infections. This was supported by the result of the study among Polish adolescents that hand hygiene practices showed increment from 58.4% to 68.1% during COVID-19 [27] . In our study, the proportion of good hand hygiene practice among taxi drivers was 66.4% [95%CI: 62.1-71]. This was higher than found in studies from China 42.05% [73] , Poland 58.4% [27] and Japan 58.5% [74] . The result is also lower than the studies from Jimma among health care workers 76% [41] , Jimma University medical center visitors 95.5% [35] , from Amhara region 82% [31] , from Nigeria 95.3% [44] , from Malaysia 87.8% [51] , and in United States 85.2% [75] . The difference in the proportion of good hand hygiene could be due to different target groups for the study, in that most studies were conducted among healthcare workers who have access to hand sanitizer and also it might be due to different study areas. In this study, 73.4% of taxi drivers always used alcohol-based hand sanitizer. It was lower than study findings from Jimma [41] , where the use of alcohol-based hand sanitizer among healthcare workers was 95.8%. The lower utilization of alcohol-based hand sanitizer in our study area might be either because of no access to hand sanitizer due to high cost or less attention to the hand sanitizer. A similar result was obtained in Kenya where the high cost of the hand sanitizer caused participants to practice poor hand hygiene [76] . Having a positive attitude towards COVID-19 prevention was positively associated with good hand hygiene practices among the taxi drivers. Drivers with a positive attitude towards COVID-19 were 5.5 times more likely to practice good hand hygiene than those with a negative attitude towards COVID-19. This result was supported by the two recent studies in United States [75, 77] , and in China during H1N1 influenza outbreak [78] . The possible reason might be due to the fact that a positive attitude can influence good practices. Based on our findings, educational level was found to have a direct association with good hand hygiene practices. In this study, a high proportion of good hand hygiene practices was observed among drivers with higher educational level. Similar results were obtained from previous studies that showed higher educational level was the determinant to have good hand hygiene practices [78, 79] and [73] . The possible reason for the association of higher educational level with good hand hygiene practice was that education is an important tool to improve knowledge and to create a condition to search for the knowledge on different issues including knowledge about COVID-19. In contrast to our findings, several studies revealed gender as a factor for good hand hygiene practices [73, [78] [79] [80] [81] , but it was not a factor in our study since all the study participants were male. Although the study was conducted among one of the most vulnerable groups, the study has some limitations. The limitations of the study were that the proportion of hand hygiene practices were determined based on drivers' self-report, which may increase the proportion of good hand hygiene practices reported. In addition, because of the scarcity of literature on the knowledge, attitude and hand hygiene practices among taxi drivers, the discussion was made on the basis of the findings with other target groups. COVID-19 is still causing morbidity and mortality all over the world. While social distancing, hand hygiene, and facemasks are the most important methods to prevent COVID-19, practicing them requires good knowledge and a positive attitude about COVID-19 prevention measures. Therefore, the findings of this study will have a practical application in helping to design COVID-19 prevention measures among taxi drivers.@story_separate@From this study, it can be concluded that the drivers' good knowledge, positive attitude and good frequent hand hygiene practices were relatively low at 69.8%, 67.6%, and 67.4%, respectively. The main factors that were significantly associated with good knowledge about COVID-19 were educational level, place of residence and attitude towards COVID-19. Good attitude towards COVID-19 was associated with age, educational level, income and knowledge about the disease, whereas a driver's attitude towards COVID-19 and educational level were factors significantly associated with good hand hygiene practices. Thus, it is better to provide training about COVID-19 prevention measures for taxi drivers to improve their knowledge, attitude and hand hygiene practices by making special considerations with respect to age, educational level and attitude.","BACKGROUND: Although several studies have been conducted on COVID-19 knowledge, attitude and prevention practices among healthcare workers and the general population, there has not been any study among taxi drivers in Ethiopia, including Dessie City and Kombolcha Town, the lack of which hinders providing evidence-based interventions to this target group. Thus, this study was designed to contribute to proper planning of COVID-19 intervention measures among taxi drivers in Dessie City and Kombolcha Town, Ethiopia. METHODS: A cross-sectional study was conducted among 417 taxi drivers in Dessie City and Kombolcha Town during July to August, 2020. The data was collected using a structured questionnaire and an observational checklist. The collected data was checked, coded and entered to EpiData version 4.6 and exported to Statistical Package for the Social Sciences (SPSS) version 25.0 for data cleaning and analysis. The outcome variables of this study were good or poor knowledge, positive or negative attitude and good or poor frequent hand hygiene practices towards COVID-19. Bivariate (Crude Odds Ratio [COR]) and multivariable (Adjusted Odds Ratio [AOR]) logistic regression analysis were employed to identify factors significantly associated with good knowledge, positive attitude and good frequent hand hygiene practices among taxi drivers. Significance level of variables was declared at a p < 0.05 from the adjusted analysis. MAIN FINDINGS: Out of the total 417 taxi drivers, 69.8% [95% CI: 65.2–73.9], 67.6% [95%CI: 63.1–72.2] and 66.4% [95% CI: 62.1–71.0] of the drivers had good knowledge, positive attitude and good frequent hand hygiene practices, respectively. Educational level (AOR = 7.55, 95% CI = 4.55–12.54), place of residence (AOR = 5.41, 95% CI = 1.4–20.08) and attitude towards COVID-19 prevention (AOR = 1.67, 95% CI = 1.02–2.74) were factors associated with good knowledge about COVID-19. Further, age of taxi drivers greater than 30 years (AOR = 3.01, 95% CI = 1.76–5.13), educational level of secondary or above (AOR = 3.16, 95% CI = 1.88–5.31), income (AOR = 3.36, 95% CI = 1.48–7.61), and knowledge about COVID-19 (AOR = 2.1, 95% CI = 1.21–3.54) were factors associated with positive attitude towards COVID-19 prevention. In addition, attitude towards COVID-19 (AOR = 5.5, 95% CI = 3.40–8.88) and educational level (AOR = 1.84, 95% CI = 1.15–2.95) were the factors associated with good frequent hand hygiene practices. CONCLUSION: We concluded that the rates of good knowledge, positive attitude and good frequent hand hygiene practices were relatively low among taxi drivers in Dessie City and Kombolcha Town. We strongly recommended providing training about COVID-19 prevention measures for taxi drivers that considers age, education status and attitude areas essential to improve their knowledge, attitude and frequent hand hygiene practices to prevent the spread of COVID-19."
"A newly discovered coronavirus named severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) triggered the pneumonia outbreak in China's Hubei province in December 2019. 1 The World Health Organization (WHO) termed the infection COVID-19 (coronavirus disease 2019), which has now spread beyond China and has become a fullblown pandemic. 2 To combat the virus's spread, affected countries have adopted numerous public health measures such as isolation, quarantine, regional lockdown, social distancing, restriction on people's movement, and limiting local and international traveling. Despite these preventive measures, the disease surges across countries, with more than 170 million confirmed cases with around 3.7 million deaths to date (June 9, 2021). 3 SARS-CoV-2 has a higher transmission rate than the previous two coronaviruses: severe acute respiratory syndrome coronavirus (SARS-CoV) and middle east respiratory syndrome coronavirus (MERS-CoV). 4 Ongoing research has emphasized finding therapeutic interventions and preventive vaccination, with limited success. 5 Early diagnosis is critical for successfully containing this contagious outbreak. 6 The US Food and Drug Administration (FDA) is issuing emergency use authorization (EUA) for different categories of in vitro diagnostic tests to increase testing capacity. 7 While nucleic acid and antigen-based tests can detect active infection, low viral load, and variation in test sensitivity issues increase the risk of false-negative results, limiting their use. 8, 9 In addition, many COVID-19 victims are asymptomatic with a viral load lower than symptomatic individuals. 10 These hinder these tests' usefulness and make the epidemiological evaluation of the disease complex. 11, 12 Moreover, amidst mass COVID-19 vaccination, it is essential to monitor the antibody dynamics for COVID-19 containment. 13, 14 WHO recommends systemic serosurveys to determine the whole disease spectrum implemented by many countries around the world. 13, [15] [16] [17] [18] [19] Other than assessing risk and prevalence, serology testing is vital for contact tracing, detecting immune response against the virus, identifying potential plasma donors, in some instances to be used in adjunct with a molecular diagnosis, and evaluating the success of vaccination program in place. 14, [20] [21] [22] [23] The widely used tests for serology-based diagnosis of SARS-CoV-2 include enzyme-linked immunosorbent assay (ELISA), automated chemiluminescence assay (CLIA), neutralization assay, and rapid immunoassay. 20 Except for rapid tests, all assay systems require infrastructure, longer testing time, and qualified staff to conduct the tests and interpret the results, rendering these assays inadequate when a large number of testing are required immediately. 24, 25 These limitations call for an urgent need for easy and affordable rapid point-of-care testing (POCT). The present study reports developing such a rapid POCT antibody test, based on membrane immune-concentration flow-through principle, for SARS-CoV-2 specific IgG detection. There are numerous reports on rapid lateral flow immunochromatographic assay systems for SARS-CoV-2 antibody detection. 7, [26] [27] [28] [29] However, LFIA systems have a higher possibility of the false-negative signal depending on the immobilized analyte concentration compared to the flow-through system. 30, 31 Here, we have developed a flow-through detection system with biofunctionalized colloidal gold nanoparticles (AuNP). Among numerous reporter molecules, AuNPs have wide applications due to their remarkable optical and physicochemical properties. [32] [33] [34] [35] AuNP-conjugated biomolecules are exceptional for their simplicity and high contrast visualization when coupled with a rapid immunoassay system. [36] [37] [38] This report will detail how we developed and validated a rapid dot-blot serological assay to detect SARS-CoV-2 specific IgG in human serum using seropositive and seronegative samples, characterized by other available detection systems. Moreover, compared with nucleocapsid (NCP) or receptor-binding domain (RBD) ELISA specific for SARS-CoV-2, an additive sensitivity was observed in our assay system due to co-immobilization of both antigens to capture a wide range of antibodies.@story_separate@Rapid flow-through dot-blot immunoassay (FT-DBA) is a qualitative immunoassay to detect the presence of SARS-CoV-2 specific IgG antibodies in human serum. It https://doi.org/10.2147/IJN.S313140 International Journal of Nanomedicine 2021:16 utilizes the solid phase capture technique in a membrane immune-concentration flow-through system ( Figure 1 ). The test device is a plastic cassette that contains a combo made up of an absorbent pad with nitrocellulose (N.C.) membrane with its active side on top, visible through a circular window. The test media (active side of the N.C. membrane) has two adjacent dots, designated T (test) and C (control) position to indicate test result and test kit validity, respectively. A mixture of SARS-CoV-2 antigens is immobilized on the test dot, and the control dot contains immobilized mouse IgG. Thus, the SARS-CoV-2 specific antibody, if present in the serum, is captured on the N.C. test dot, which becomes visible after the addition of AuNP conjugate. SARS-CoV and SARS-CoV-2 specific NCP, envelope (E), spike S1, spike S2, and RBD recombinant proteins were purchased from The Native Antigen (UK, Kidlington), M. P. Biomedicals (California, USA), Sino Biological (China), Fapon Biotech Inc. (China), and Creative Diagnostics (USA). Gold colloids (particle size: 10nm and 40nm) were purchased from Bhat Biotech Ltd. (India) and BBI Solutions (U.K.). PBS (Phosphate-buffered saline) tablets (pH 7.4), Tris-Buffered Saline (TBS) pH 7.2, glycerol, and sodium chloride (NaCl) were purchased from Thermo Fisher Scientific (USA). Cold-water gelatin was purchased from Sigma-Aldrich. Gold dilution and stabilization buffer (Bhat Biotech Ltd.), mouse IgG antibody (Fapon Biotech), goatanti-mouse IgG (Fapon Biotech) and, goat-anti-human IgG (Fapon Biotech) were also purchased. N.C. membranes were purchased from Ken Biotech (China), Bhat Biotech Ltd. (India), and Sartorius (France) to determine the optimal support matrix for the immunoassay. Other materials (such as plastic cassettes) were purchased from Bhat Biotech (India) and Changzhou Dengfeng (China). Gold nanoparticles have already been accepted as a remarkable diagnostic tool worldwide. 39 Therefore, the research team employed AuNP conjugated with antihuman IgG to detect SARS-CoV-2 specific IgG in human serum. Anti-human IgG-AuNP and anti-mouse IgG-AuNP conjugates were prepared according to the protocol described by Oliver C. et al. 40 Briefly, two different sized (10 nm and 40 nm) gold colloids were evaluated and to determine the optimal concentration of both proteins for conjugation, aliquots of the anti-human IgG/anti-mouse IgG solutions (5 µg/mL, 10 µg/mL, 15 µg/mL and 20 µg/mL) in phosphate-buffered saline (PBS) were prepared. About 1 mL of gold colloid solution was added to each aliquot, and the tubes were incubated for 15 mins at room temperature. The minimum amount of anti-human IgG/ anti-mouse IgG required to stabilize the conjugates was determined by assessing color change and agglomeration. The conjugates were then stabilized using 1% cold-water gelatin. Excess antibodies were removed using glycerol gradient, and the conjugates were dialyzed against TBS for 1 hr at room temperature. The final preparation was diluted with TBS and 1% cold-water gelatin and stored at 4 °C. In this assay's development and optimization, two SARS-CoV-2 positive and two negative control sera were utilized. Clinical symptoms, RT-qPCR confirmation, serostatus verification with in-house and commercial chemiluminescence assay (ROCHE, Elecsys Anti SARS-CoV-2) were considered control selection criteria. 41 Besides, the antibody kinetics of positive control individuals were analyzed longitudinally to avoid spectrum bias. 42  The clinical performance of rapid FT-DBA has been evaluated with three panels of serum samples (n=181). Two sera panels comprise single and multiple collections of SARS-CoV-2 positive serum samples (n=81) from fortyfive RT-qPCR confirmed individuals with clinical signs and symptoms of COVID-19. Panel 1 consists of twenty RT-qPCR positive samples that have been collected within two weeks from the onset of symptoms. Panel 2 (n=61) samples were also from RT-qPCR positive individuals with symptom onset of >14 days. Panel 3 samples were negative samples (n=100) collected during, i) prepandemic sera from healthy donors (n=40), ii) April to June 2020 from RT-qPCR negative individuals (n=36), and iii) pre-pandemic dengue-positive patients (n=24). The panels were characterized with SARS-CoV-2 IgG ELISA described by Sil et al against SARS-CoV-2 antigens: NCP and RBD. [41] [42] [43] Comparative analyses were carried out with these samples, between in-house ELISA assays and the kit developed in this work. Moreover, according to FDA guidelines, the seropositive and seronegative samples  based on in-house ELISA results were tested with the developed assay. We analyzed 35 samples from these 181 in-house ELISA characterized sera by US FDA approved commercial Elecsys SARS-CoV-2 assay. Among these 35 samples, 15 (>21 days after being RT-qPCR positive) were previously identified as seropositive in both NCP-IgG and RBD-IgG ELISA, and 20 were seronegative NCP-IgG and RBD-IgG. All the samples were stored at −80 °C until further use. At the development phase of the assay, each component and steps were optimized and screened. SARS-CoV-2 recombinant antigens: NCP, E, S1, S2, and RBD proteins were utilized as the potential capturing agent. Six different cocktail preparations (antigen dilution ranged from 1:10 to 1:800) from 16 antigens were analyzed. The combination generating the highest signal without cross-reaction was immobilized as a test dot. To avoid blocking the N.C. membrane, sample processing steps were optimized by diluting samples to 1:2, 1:4, and 1:8 in the commercial buffer (Bhat Biotech Ltd., India). The untreated serum sample was thawed at 37 °C and processed further for the test procedure. During testing, the sample was diluted with 2-3 drops (50-75 µL) of dilution buffer. About 50 µL of diluted serum was then added to test media following two drops of (50µL) of wash buffer. The addition of one drop of AuNP conjugate mixture, followed by two drops of (50µL) of wash buffer, completed the test. Development of control dot attests to the fact that the device is working correctly and the presence or absence of test dot specifies positive or negative results. Results were interpreted as shown in Figure 2 . An intensity scale was developed for the semi-quantitative determination of the detection limit (LOD) of the rapid immunoassay. One positive control serum with a high antibody titer was selected based on reference ELISA value. 41 Two-fold serial dilutions (1:2, 1:4, 1:8, 1:16, 1:32) of the chosen sera were prepared in standard negative serum and run through the assay. A five-point gradient scale was generated for semiquantitative detection of IgG in the sample ( Figure 3 ). Rapid dot-blot assay performance validation was designed to determine its clinical efficiency. The rapid assay's performance was analyzed with the selected samples (n=181) and characterized seropositive and seronegative samples. The selected specimens' serostatus was first evaluated with an established in-house ELISA test against SARS-CoV-2 recombinant NCP and RBD antigens. [41] [42] [43] All the samples were then tested with the rapid dot-blot assay to assess their performance. The coefficient of variation (CV) demonstrates test reproducibility and precision. The intra-assay and inter-assay variations were tested, with five replicates of two positive sera samples on the same day and in 15 different days for later. The coefficient of variation was determined using the following formula. Coefficient of variation (CV)= (Standard Deviation/ Mean) x 100% Sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV), and area under the curve (AUC) with 95% confidence interval were estimated to see the effectiveness of this rapid dot blot assay with in-house developed ELISA as well as FDA approved commercial kits. The calculation was done using a 2×2 table format with the formula shown in Table 1 . A linear regression model was used to assess the mean difference of the Ratio (O.D./cut-off) with the different intensity scale. The analysis was performed with STATA 15 (StataCorp, L.P., College Station, Texas, USA), and GraphPad Prism 8.3 was used for graphical presentation. Human participants in this study were enrolled, maintaining the national research committee's ethical standards and the 1964 Helsinki Declaration. All participants of this study were verbally explained in detail regarding the study's aims, scopes, and purpose. Thereby, researchers obtained research participants' written consent (approval) for their participation in this study, titled ""AuNP Coupled Rapid Flow-Through Dot-Blot Immuno-Assay for Enhanced Detection of SARS-CoV-2 Specific Nucleocapsid and Receptor Binding Domain IgG"". The history of the participants was noted in a questionnaire before the collection of blood samples.  Test performance of rapid dot-blot mainly depends upon the appropriate combination of four major factors: coating agent concentration, nitrocellulose membrane blocking effect, sample processing, and sample volume. Different combinations of SARS-CoV and SARS-CoV-2 antigens were evaluated as capture antigens. Our results showed that the combo, which contained Escherichia coli derived recombinant envelope and S2 proteins, cross-reacted with negative controls (Data not shown). Nevertheless, the combination containing human cell line-derived NCP and RBD showed the best results with good sensitivity and specificity with the controls. Henceforth, a combo containing 1:10 dilution of NCP and RBD proteins (Sino Biologicals) was selected to be used as a test dot immobilized on the N.C. membrane. Moreover, mouse IgG (Fapon) was immobilized as a control dot on the N.C. membrane. Direct use of sample without any processing blocked the N.C. membrane, which interfered with result interpretation. Sample processing steps were optimized to avoid blocking the N. C. membrane. Two-fold dilution was finalized among three dilutions, and 50 µL of diluted samples were used for testing. Moreover, AuNP conjugate prepared from 10 nm gold colloid gave a better resolving background than 40 nm. Thus, further evaluation conjugates of 10 nm gold colloids were chosen. A two-fold serial diluted sample (P-1) was evaluated, and a scale was generated ( Figure 3 ). Based on the results we found in our experiment, a range of 0.5 to 3 plus scale was considered positive for semi-quantitative differentiation of the dot-blot result. In contrast, negative was considered the absence of antibody. As a combination of NCP and RBD was used as capture antigen in the test platform, the serum samples of SARS-CoV-2 RT-qPCR confirmed patients (n=81), dengue positive patients (n=24), and healthy donors (n=76) were first characterized using IgG ELISA test against these two SARS-CoV-2 immunogens on previously developed inhouse ELISA. 41, 43 Data analysis showed that the sera of  COVID-19 infected individuals had a different antibody titer range (Figure 4 ). Three patterns of antibody response were observed among positive patients with blood collection in the first two weeks of infection: i) 16 patients mounted IgG response against both SARS-CoV-2 proteins and two were entirely negative, ii) two patients achieved only anti-NCP IgG, and iii) two patients mounted an only anti-RBD response. In the second group, all the patients are seropositive for both SARS-CoV-2 antigens except for two patients; one did not develop NCP-IgG, but RBD-IgG for other vice-versa result was observed (Figure 4 ). Only one sample from the dengue panel showed cross-reactivity to SARS-CoV-2 RBD protein ( Figure 4A ) but no reaction against NCP ( Figure 4B ). Other sera were negative for both SARS-CoV-2 antigens. Both the OD/cut-off of RBD ( Figure 4A ) and NCP ( Figure 4B ) of the positive cases at <14 and >14 days showed highly significant (p<0.001) with the participants who were negative against SARS-CoV-2. Besides this, the mean OD/cut-off between <14 (3.13±0.58; 2.97±0.53) and >14 days (5.81±0.33; 4.40 ±0.30) of RBD (p<0.001) and NCP (p=0.021), respectively also showed a significant difference. When 35 pre-characterized samples were tested in a commercial CLIA kit, they were found to be accurately characterized by our in-house ELISA, ie, fifteen NCP-IgG and RBD-IgG containing RT-qPCR positive samples were reactive in chemiluminescence assay. In contrast, the other twenty NCP-IgG and RBD-IgG negative samples were found non-reactive in tested CLIA ( Table 2 ). The rapid dot blot assay's performance efficiency was evaluated with the three chosen sample panels of sera (n=181). The assay detected SARS-CoV-2 infection in patients with symptoms less than 14 days with 85.0% (62.1%, 96.8%) sensitivity which increased to 100% (94.1%, 100.0%) after 14 days and (Table 4 ). To check the specificity and cross-reactivity we have run 76 and 24 sera from healthy donors and dengue positive samples, respectively. Among them only 2 samples were misdiagnosed and the overall specificity was found 98.0% (95% CI: 93.0%, 99.8%) ( Table 3 ). The overall sensitivity and specificity were calculated it was found to be 96.3% (95% CI; 89.6%, 99.2%) and 98%, (95% CI; 93.0%, 99.8%), respectively, with 97.5% PPV and 97.0% NPV (Tables 5 and 6 ). The overall test agreement was 94.4% (Kappa=0.944; p<0.001). Moreover, when the Dot-blot assay kit was evaluated with in-house ELISA (NCP and/ RBD) characterized seropositive and seronegative sera, the sensitivity and specificity in less than 14 days samples were 94.7% (95% CI; 74.0%, 99.9%) and 98.0% (95% CI; 93.0%, 99.8%), respectively, with 94.4% PPV and 98.8% NPV. As expected, the values increased for the samples collected more than 14 days of symptom onset, and sensitivity and specificity were 100% (95% CI; 94.1%, 100.0%) and 98.0% (95% CI; 93.0%, 99.8%), respectively. The PPV and NPV for this phase were 96.8% and 100%, respectively. The overall sensitivity and specificity were calculated and it was found to be 98.8% (95% CI; 93.3%, 100%) and 98% (95% CI, 93.0%, 99.8%), respectively, with 99.6% PPV and 99.0% NPV (Tables 5 and 6 ). Interestingly, when thirty-five Elecsys SARS-CoV-2 assay characterized sera by evaluated in FT-DBA, fifteen sera found reactive in CLIA showed test dots. In contrast, the other twenty non-reactive sera did not produce any dot in the test region. This indicated the sensitivity and specificity of FT-DBA were 100% (95% CI; 78.3%, 100%) and 100% (95% CI; 83.2%, 100%) with a test agreement of 100% (Kappa 1.00; p<0.001), respectively when compared with the Elecsys SARS-CoV-2 assay (Table 2) . When compared with Reference ELISA values, it was revealed that the FT-DBA could detect SARS-CoV-2 specific IgG antibodies in human serum even when the OD/ Cut-off ratio was meager. Linear regression model between the intensity scale and IgG cut-off of RBD and NCP showed significant difference. The highest intensity 3+ had the most elevated IgG (Cut off) for RBD (7.37±2.18) and NCP (5.78±2.44), which showed significant differences with the intensity scale of 2+ (p= 0.001 and 0.003), respectively. Similarly, the intensity scale of 2+ showed significant differences with the intensity scale of 1+ of RBD and NCP (p=0.003 and 0.029), respectively. Whereas a significant difference (p=0.030) was noted in RBD between 1+ and 0.5+ scales. No such difference was found in NCP ( Figure 5 ). Mean ELISA values in RBD and NCP corresponding to each intensity scale are listed in Supplementary Tables 1 and 2 . Our analysis showed no intra-assay variation in the assay, but a 9.98% coefficient of variance was found in the interassay for the sera samples used. Rapid dot-blot has been a valuable tool for the diagnosis and epidemiological survey of various viral diseases. [44] [45] [46] Currently, rapid SARS-CoV-2 antibody detection is primarily conducted through LFIA. 27, 28 Other techniques like surface-enhanced Raman spectroscopy (SERS), microfluidic immune-fluorescence assay, chromatographic digital immunoassays are evolving. [47] [48] [49] However, the flowthrough dot-blot-based approach is not yet widely available as COVID-19 immunoassay. This work includes developing and evaluating a rapid flow-through dot-blot assay (FT-DBA) to detect SARS-CoV-2 specific IgG in human serum. The Dot-blot principle provides a more reliable field-testing framework than LFIA. The latter has usual limitations, including mass transport limitation, binding kinetics of immunogen, and competitive inhibition of target analyte ( Figure 6 ). 31 The biofunctionalized theranostic agent can significantly improve targeting, imaging, and therapeutics in diagnostic and clinical settings. Carbon, gold, silver, lipid, nanoceria, bilirubin, and cerium oxide nanoparticles are widely used as theranostic agents. [50] [51] [52] [53] [54] [55] AuNP and silver nanoparticles (AgNP) are excellent theranostic agents, with a high ratio of area to the volume that allows the functionalization of these particles with various biomolecules. 35, 56 Moreover, the use of AuNP and enhancement of signal by Ag is reported to improve the sensitivity. 56 Due to their multiple nanostructures, such as nanospheres, nanorods, nanosheets, nanotriangle, nanoshells, nanostars, nanocubes, gold nanoparticles, they have a wide range of clinical and diagnostics applications. 50,57-61 Since our objective was to develop a rapid immunoassay that the naked eye can interpret, easy to maneuver, cost-effective, and widely available, we choose the AuNP in our assay. For the role in viral pathogenesis and entry into a host cell, NCP and spike are considered the two most crucial target immunogens of SARS-CoV-2. The RBD of the spike is more advantageous for having the potential to induce neutralizing antibodies. 62 Typically, an immune reaction to NCP evolves earlier than RBD, but exceptions have also been reported. 43 In that context, for increased sensitivity in the acute phase of infection, an assay was designed with dual immune capture property instead of one, which might fail to identify a seroconverted person. 63, 64 Another issue considered was the possibility of cross-reaction with other alpha and beta-coronaviruses surrounding the use of the SARS-CoV-2 serological test. The two human coronaviruses SARS-CoV and MERS-CoV, tend to pose the greatest likelihood of crossreaction. Due to the high level of sequence identity, SARS-CoV antigens are known to cross-neutralize SARS-CoV-2. 1,65 However, since the SARS-CoV epidemic, it has been seventeen years, and specific antibody response against the virus has been reported to be diminished (90%). 66 Therefore, a false-positive reaction at this point  is an unlikely event. MERS-CoV is still active in the population along with the four endemic low pathogenic human CoVs (229E-CoV, NL63-CoV, OC43-CoV, and HKU1-CoV), so most humans might bear antibody against them. 67,68 Nevertheless, their cross-reactivity against SARS-CoV-2 nucleoprotein and RBD protein is very low. Except for SARS-CoV, other human coronaviruses do not have any sequence resemblance to RBD and S1 domain of spike protein of SARS-CoV-2. 69 Moreover, the assay was designed to take into account the dengue-endemic situation of Bangladesh. 70 Since both diseases have common characteristics in the early phase and there has been a report of antigenic cross-reactivity between SARS-CoV-2 and dengue virus (DENV), there is a chance of misdiagnosis. 71 To avoid the risk, 24 prepandemic dengue-positive sera were incorporated into the evaluation panel. Another strengthening point of the assay is that it was developed using SARS-CoV-2 positive sera, studied longitudinally for antibody response in one of our previous studies, eliminating any chance of spectrum bias. 42, 72 Laboratory evaluation revealed that when challenged with RT-qPCR positive or RT-qPCR positive and seropositive samples, the developed assay's overall sensitivity was 96.3% (Table 3) for the former but increased to 98.8% (Table 5) for the latter group. The difference was that two RT-qPCR positive samples were never seroconverted in our study, as reported by others. 73 A comparative study conducted between conventional ELISA and developed assay revealed a high correlation, which others can find. 74 Dot intensity seemed to increase with corresponding ELISA value upon disease progression and antibody titer ( Figure 5 , Supplementary  Tables 1 and 2 ). Statistical analysis revealed equivalent clinical agreement between the two techniques as well as with the gold-standard method of RT-qPCR, with a Cohen's Kappa value of 0.84 (strong agreement) and 0.94 (robust agreement), respectively, in between <14 and >14 days (Table 3) . A similar significant difference was observed between the results of two-time points in both assay techniques (Table 3) . Moreover, a slightly increased sensitivity was observed in the dot assay compared to the two ELISA, which might have an additive effect of using two proteins instead of one (Table 3 ). 100% (95% CI; 78.3%, 100%) sensitivity and 100% (95% CI; 83.2%, 100%) Elecsys SARS-CoV-2 assay (NCP) is a US FDA approved CLIA assay system, which qualitatively detects COVID-19 antibodies with high sensitivity of 99.5 (95% CI; 97.0-100.0) and specificity of 99.80 (95% CI; 99.69-99.88), when samples were RT-qPCR positive samples were tested >14 days of disease onset 75. Compared with other ELISA, CLIA, and LFIA, Elecsys SARS-CoV-2 assay has always been consistent with its performance. [75] [76] [77] [78] Moreover, researchers have employed this kit to identify the convalescent groups before vaccinations. 79 We opted to use this assay to evaluate our FT-DBA kit compared to 100% similarity in sensitivity and specificity with thirtyfive samples tested (Table 2 ). Our future endeavor will focus on using both NCP and S1 versions of Elecsys SARS-CoV-2 assay kits to evaluate our FT-DBA kit with a larger sample pool. One of the limitations of our study is that it does not include infants and newborns, although recent studies have found that infants and newborns can also be infected with SARS-CoV-2. 80, 81 In addition, all the study participants were 18 years and above, who would provide informed consent without a guardian requirement. Considering all these factors, the assay system elaborated in the present study can be regarded as a more feasible option for serosurveillance study than conventional ELISA, especially for low-and middleincome countries (LMICs), including Bangladesh. Moreover, with the ongoing vaccination programs, the serological test will be essential in addressing two fundamental issues: vaccine prioritization and monitoring of protective immunity development in postvaccinated cohorts. 1. Implementing rapid antibody tests in low and middle-income countries (LMICs) would facilitate serostatus assessment after natural infection. 2. With the scarcity of vaccines in many LMICs, the antibody tests would provide a way to better implement the vaccination to those who have not been exposed or have low to no antibodies, thereby breaking the chain of SARS-CoV-2 transmission. 3. We have observed that flow-through immunoassay presented higher specificity with comparable sensitivity. 4. Implementation of RBD-specific antibody assay is necessary for observing the efficacy of vaccination. 5. Implementation of NCP-specific antibody assay will provide an insight into the previous infections. Simultaneously, the absence of NCP-specific antibodies and the presence of RBD-specific antibodies would correlate with the vaccination program's efficacy in providing protection. Mean difference in RBD and NCP specific IgG (Cut off) in contrast with intensity scale. The linear regression model was used to estimate the p-value, and the data were shown as mean with a 95% confidence interval.  All authors reviewed and approved the final version and have agreed to be accountable for all aspects of the work, including any issues related to accuracy or integrity. The authors of the current work appreciate and show their gratitude to Prof Mohammed S. Razzaque, MBBS, Ph.D. of Lake Erie College of Osteopathic Medicine (Pennsylvania, USA), for reading the manuscript and providing valuable suggestions. All authors made a significant contribution to the work reported, whether that is in the conception, study design, execution, acquisition of data, analysis, and interpretation, or in all these areas; took part in drafting, revising, or critically reviewing the article; gave final approval of the version to be published; have agreed on the Journal to which the article has been submitted; and agreed to be accountable for all aspects of the work. Mr Md. Ahsanul Haq reports a patent 10202006327W pending; Prof Dr Bijon Kumar Sil reports a patent 10202006327W pending; Dr Mohd Raeed Jamiruddin reports a patent 10202006327W pending to Intellectual Property Office of Singapore; Dr Nihad Adnan reports a patent 10202006327W pending; the authors report no other conflicts interest in this work. This research has the potential possibility for applying for patent rights. Mainul Haque declines not to be part of the patent. @story_separate@The developed kit uses AuNP technology to detect and determine the presence of SARS-CoV-2 antibodies in infected individuals. Compared to the in-house developed ELISA kit, the FT-DBA presents an overall sensitivity of 98% and a specificity of 98.8%. The specificity indicates that the kit can differentiate antibodies against SARS-CoV-2 antigens from those detecting against other coronaviruses antigens. Furthermore, the kit provides a reliable semi-quantitative result that can help determine the efficacy of the vaccination programs within the country. The convenient use and easy implementation mean that the developed system can be utilized in remote regions of the world with low healthcare penetration.","BACKGROUND: Serological tests detecting severe acute respiratory syndrome coronavirus−2 (SARS-CoV-2) are widely used in seroprevalence studies and evaluating the efficacy of the vaccination program. Some of the widely used serological testing techniques are enzyme-linked immune-sorbent assay (ELISA), chemiluminescence immunoassay (CLIA), and lateral flow immunoassay (LFIA). However, these tests are plagued with low sensitivity or specificity, time-consuming, labor-intensive, and expensive. We developed a serological test implementing flow-through dot-blot assay (FT-DBA) for SARS-CoV-2 specific IgG detection, which provides enhanced sensitivity and specificity while being quick to perform and easy to use. METHODS: SARS-CoV-2 antigens were immobilized on nitrocellulose membrane to capture human IgG, which was then detected with anti-human IgG conjugated gold nanoparticle (hIgG-AuNP). A total of 181 samples were analyzed in-house. Within which 35 were further evaluated in US FDA-approved CLIA Elecsys SARS-CoV-2 assay. The positive panel consisted of RT-qPCR positive samples from patients with both <14 days and >14 days from the onset of clinical symptoms. The negative panel contained samples collected from the pre-pandemic era dengue patients and healthy donors during the pandemic. Moreover, the sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) of FT-DBA were evaluated against RT-qPCR positive sera. However, the overall efficacies were assessed with sera that seroconverted against either nucleocapsid (NCP) or receptor-binding domain (RBD). RESULTS: In-house ELISA selected a total of 81 true seropositive and 100 seronegative samples. The sensitivity of samples with <14 days using FT-DBA was 94.7%, increasing to 100% for samples >14 days. The overall detection sensitivity and specificity were 98.8% and 98%, respectively, whereas the overall PPV and NPV were 99.6% and 99%. Moreover, comparative analysis between in-house ELISA assays and FT-DBA revealed clinical agreement of Cohen’s Kappa value of 0.944. The FT-DBA showed sensitivity and specificity of 100% when compared with commercial CLIA kits. CONCLUSION: The assay can confirm past SARS-CoV-2 infection with high accuracy within 2 minutes compared to commercial CLIA or in-house ELISA. It can help track SARS-CoV-2 disease progression, population screening, and vaccination response. The ease of use of the assay without requiring any instruments while being semi-quantitative provides the avenue of its implementation in remote areas around the globe, where conventional serodiagnosis is not feasible."
"The human body hosts a large variety of bacteria, fungus, and viruses, on and within the tissues and biofluids, and various sites including the gastrointestinal tract and oral mucosa. However, an invasion and/or change in the population of the microbes leads to infections that are detrimental to an individual's health [1, 2] . While infections are common, we have seen in the past and also present (COVID-19 caused due to the novel coronavirus SARS-CoV-2 virus) that infections have led to pandemics, with a huge loss to health, economy, and lives. The transmittance/spread of infection can occur via air, water, and contact with infected individuals, depending on the microbe. The typical approaches used to manage infections involve preventive measures such as avoiding contact, use of chemical disinfectant, UV light sterilization, personal sanitization, and protective equipment [3] . Many infections have been eradicated or kept under control through immunization by vaccines. However, the development of vaccines and remedies for new pathogens takes time and thus prevention of transmittance of infections is the key to its management. Particularly, in the case of COVID-19, a huge emphasis is rightly on disinfecting surfaces where the pathogen may be present. It has been observed that the SARS-CoV-2 virus remains active on various surfaces like metals, plastics, fabrics, or surgical masks for durations up to 7 days [4, 5] . Chemical sanitization is one of the efficacious ways to arrest the proliferation of microorganisms. However, repeated sanitization, particularly of large and open areas is practically and economically challenging. Moreover, extended interaction with such chemicals for sanitization purposes (chlorine, hydrogen peroxide, sodium hypochlorite, ethanol) may lead to the enhanced probability of contracting long-term heart and lung diseases [3] . Studies show that continual use of alcohol can lead to alcohol poisoning in children which can cause drowsiness, vomiting and in serious cases can cause respiratory arrest. In addition, the dermal contact of ethanol can be responsible for skin irritation and long-term exposure can result in dryness of skin with itching [6] . Another popular disinfection method is UV treatment, which can cause skin burns and skin cancer [3] . Given all these concerns, there is an increasing impetus on designing and developing antimicrobial surfaces which provide a benign and sustainable approach to prevent the spread of infections. The antimicrobial surfaces should be effective to prevent fouling by a broad spectrum of pathogens in one of the two ways. The first approach involves killing the microorganisms (cidal activity) to prevent their transmission, while the second approach is to prevent the attachment, survival, and growth of microorganisms and biofilm formation (static activity). These types of surfaces are of huge interest in various fields such as healthcare, public transportation systems, household hygiene, food protections, sportswear, some of which are also discussed in this paper [7] [8] [9] [10] . There are a few recent reviews on antimicrobial surfaces which focus on approaches for developing both microbicidal and microbiostatic surfaces. Ding et al. have reviewed an antibacterial surface with strategies covering both monofunctional and multifunctional antibacterial surfaces [11] . Similarly, Zou et al. have summarized antibacterial surfaces with dual functionality. They have discussed the surfaces which show simultaneously antiadhesive and bactericidal activities and those with switchable antiadhesive and bactericidal activities [12] . Wei et al. have also focused on versatile antibacterial coating which covered self-defensive coatings, synergistic antibacterial coating, and smart kill and release antibacterial coating [13] . However, all the previous literature limited their review only to bacteria. Also, some of the reviews only focused on one type of strategies such as based on their topography [7] or wettability [14] . The present review furnishes a comprehensive survey of various approaches to develop antimicrobial surfaces against bacteria, fungi, and viruses by modulating physical and chemical properties, particularly by altering wettability (superhydrophobic and superhydrophilic), surface chemistry (functionalization), and topography. It must be understood that all the microbes are different in structure and pathological behavior. Thus, they have different mechanisms of attaching to a surface and transmitting to a body and causing infections, making it difficult to fabricate a universal antifouling surface. Therefore, it becomes important to understand the cause of antifouling activity and its specificity. This review first summarizes the structure of bacteria, fungus, and viruses to facilitate a better understanding of interaction with surfaces. Further, the reports on antimicrobial surfaces are classified based on surfaces developed by physical modification (patterned surfaces), chemical modification (functionalized surfaces), a combination of physical and chemical modifications (superhydrophobic, superhydrophilic surfaces), and smart surfaces. Finally, the application-related challenges, unanswered fundamental questions, and future prospects of antimicrobial surfaces are discussed.@story_separate@This agreement sets out the terms and conditions of the licence (the Licence) between you and Springer Nature Customer Service Centre GmbH (the Licensor). By clicking 'accept' and completing the transaction for the material (Licensed Material), you also confirm your acceptance of these terms and conditions. 1. 1. The Licensor grants you a personal, non-exclusive, non-transferable, world-wide licence to reproduce the Licensed Material for the purpose specified in your order only. Licences are granted for the specific use requested in the order and for no other use, subject to the conditions below. 1. 2. The Licensor warrants that it has, to the best of its knowledge, the rights to license reuse of the Licensed Material. However, you should ensure that the material you are requesting is original to the Licensor and does not carry the copyright of another entity (as credited in the published version). If the credit line on any part of the material you have requested indicates that it was reprinted or adapted with permission from another source, then you should also seek permission from that source to reuse the material. 2. 1. You may only use the Licensed Content in the manner and to the extent permitted by these Ts&Cs and any applicable laws. A separate licence may be required for any additional use of the Licensed Material, e.g. where a licence has been purchased for print only use, separate permission must be obtained for electronic re-use. Similarly, a licence is only valid in the language selected and does not apply for editions in other languages unless additional translation rights have been granted separately in the licence. Any content owned by third parties are expressly excluded from the licence. Similarly, rights for additional components such as custom editions and derivatives require additional permission and may be subject to an additional fee. Please apply to Journalpermissions@springernature.com/bookpermissions@springernature.com for these rights. Where permission has been granted free of charge for material in print, permission may also be granted for any electronic version of that work, provided that the material is incidental to your work as a whole and that the electronic version is essentially equivalent to, or substitutes for, the print version. An alternative scope of licence may apply to signatories of the STM Permissions Guidelines, as amended from time to time.  Post on a website 12 months Presentations 12 months Books and journals Lifetime of the edition in the language purchased 4. Acknowledgement 4. 1. The Licensor's permission must be acknowledged next to the Licenced Material in print. In electronic form, this acknowledgement must be visible at the same time as the figures/tables/illustrations or abstract, and must be hyperlinked to the journal/book's homepage. Our required acknowledgement format is in the Appendix below. Use of the Licensed Material may be permitted for incidental promotional use and minor editing privileges e.g. minor adaptations of single figures, changes of format, colour and/or style where the adaptation is credited as set out in Appendix 1 below. Any other changes including but not limited to, cropping, adapting, omitting material that affect the meaning, intention or moral rights of the author are strictly prohibited. You must not use any Licensed Material as part of any design or trademark. Publications (OAP) before publication by Springer Nature, but any Licensed Material must be removed from OAP sites prior to final publication.  For content reuse requests that qualify for permission under the STM Permissions Guidelines, which may be updated from time to time, the STM Permissions Guidelines supersede the terms and conditions contained in this licence. 9. 1. Licences will expire after the period shown in Clause 3 (above). Licensee reserves the right to terminate the Licence in the event that payment is not received in full or if there has been a breach of this agreement by you. Particular microorganisms such as viruses, bacteria, and fungi may cause harm to human health and hygiene and have been known to be the causative factors for various infectious diseases. As mentioned before, they are all very different in structure and pathological behavior. Thus, before discussing the various approaches to create antimicrobial surfaces, it is useful to understand their structure and interaction with surfaces. Bacteria is a family of single-cell microorganisms that may vary in size and mass and are 10-100 times bigger than viruses. Typically, bacteria are 1-3 lm in length and are shaped like a sphere or a rod. Grampositive bacteria comprise a cytoplasmic membrane surrounded by a dense coating of peptidoglycan, whereas, gram-negative bacteria comprise a cytoplasm surrounded by three layers made up of an inner surface/membrane, a layer of peptidoglycan, and an outer membrane as shown in Fig. 1a . The outer surface is uneven and undulating with an internal leaflet holding phospholipids and an external leaflet formed by lipopolysaccharide [15, 16] . The attachment of bacterial cells to a surface occurs in two phases. The early phase of attachment is rapid but reversible and consists of hydrodynamic and electrostatic interactions. The majority of the bacterial surfaces are negatively charged, and at the beginning of the biofilm formation process, they get seamlessly linked with external surfaces having a positive charge. The next stage of this process cannot be reversed and is a long-drawn process that may continue for many hours. The interaction of the hydrophobic part of the external cell membrane to other surfaces is through van der Waals interaction. Furthermore, particular proteins are involved in the process of converting reversible cell attachment into an irreversible one. In addition, various classes of extracellular organelles such as flagella, pili, and curli fibers help in attachment to the surface using specific adhesins (proteins). Type I pili attach particularly to glycoproteins containing Alpha-D-mannose, whereas, type IV pili attach to phosphatidylethanolamine [17] . Commonly, the deposition of bacterial matter leads to the development of biofilms. The discharged extracellular polymeric substances (EPS) from cells in biofilm deposit on various surfaces protects them from wear and tear or any physical damage caused by the movement of fluids. In addition, the development of intransigence to antibiotics/antimicrobial agents by the microbes is known to be facilitated by the biofilm [17] . Fungi are multicellular eukaryotes with cells having a true nucleus and a complex internal structure as shown in Fig. 1b and are normally found as environment-resistant spores and molds. Glucans, chitin, and glycoproteins are the important building block in the cell wall of the fungus. The most essential building block in the construction of a cell wall is chitin as it is located adjacent to the plasma membrane. Species of fungi, morphotype, and growth stage of the fungi determine the configuration and the structure of the outer cover [18] . Fungi are competent to invade complex substrates very efficiently by forming tubular, thread-like cells called hyphae. These cells can penetrate the substrates upon which they nourish [19] . The development of a biofilm in fungi consists of the sequential process where, in the initial stage, the attachment of fungi to a surface subsequently becomes an accretional process. It implies that cell-to-cell adhesion results in the metamorphosis of the biofilm into structurally layered and distinguishable biomasses. In the last stage of the process, also referred to as the maturation step, there is a decrease in the yeast-like growth and there is an increase in hyphal growth. Also, an extracellular matrix covers the biofilm. Several adherence factors and transcription factors determined the development of the biofilm. The attachment step in the pathogenic yeast C. albicans, biofilm is achieved through the cell appendages like adhesive proteins in the outer layer of the cell which are generally considered in the class Glycosyl Phosphatidyl Inositol (GPI)-anchored proteins. These cell wall proteins serve as membrane anchors for many cell surface proteins. The extracellular matrix (ECM) components are capable of performing multiple functions, for example attaching to various surfaces, cell-to-cell binding, and storehouse for cell nutrients. Additionally, an extracellular matrix may serve as a protective layer for the biofilm cells from any antimicrobial substances or the influence of the defensive mechanism present in the host tissue. In such cases, it may serve as a protection from phagocytic cells and also support and strengthen the cohesion of the biofilm to obviate infiltration of toxins into the biofilm [20] . Chemical analysis of the ECM of C. albicans has led to the detection of glucose, hexosamine, protein, extracellular DNA (eDNA), and a few other components. Such an analysis was complemented through functional assays for biofilm integrity and adherence after enzymatic hydrolysis of ECM components. Overall biofilm integrity was found to depend upon multiple ECM components like b-1,3 glucan, chitin, protein, and eDNA. Additionally, b-1,3 glucan displays the greatest propensity to contribute to the substrate for cell-cell adherence [21] . Viruses are holoparasites that need a suitable living host for their survival and growth. They are the tiniest agents of infectious diseases, mostly round in shape and with their size ranging from about 20 to 200 nm in diameter. A virus comprises of DNA or RNA inside a protective protein coat known as a capsid [22] . The shape of the capsid differs for different viruses. The interaction of viral proteins to the host cell membranes is crucial for the entry of the virus into the host cell followed by replication of the viral genome and finally production of progeny particles. A virus has to cross the plasma membrane of the host cell to replicate its genome. A few types of viruses are surrounded by an outer membrane of the lipid bilayer and are known as enveloped viruses. These viruses develop their envelope from the outer layer of the plasma or the inner layer between the host cell [23] . A schematic of the enveloped and nonenveloped virus is shown in Fig. 1c . The ability to develop a biofilm has been detected among bacteria, fungi, and yeast since the initial observation recorded in 1978. This phenomenon is an important field of research even today. However, the concept of biofilm formation in viruses is still under research. Some viruses show biofilm-like assemblies, for example in the case of Human T-cell leukemia virus type 1 (HTLV-1). It is a retrovirus and its extracellular viral assemblies show apparent commonalities in their organization, composition, and dissemination with bacterial biofilms. Bacterial biofilms, as well as viral assemblies, are composite bonded blocks containing microbial colonies amalgamated with a profuse amount of carbohydrate and multimolecular network. Such a composite structure and layout of the viral network give rise to its resistance, strength, and its propensity to proliferate, which corresponds to the particular traits which are common among bacterial biofilms. In the case of bacterial biofilms, the matrix is constructed by bacteria on their own; on the other hand, in the case of viruses the viral assemblies are formed by the host cell which has been infested by the virus. This is so because of the parasitic properties of the virus which utilizes the cellular mechanism to combine the proteins present in the cells with the viral constituents to form the structure of the viral assembly as the viruses on their own do not possess the capability and the requisite functional metabolism to construct such a structure. In the case of HTLV-1, the viral infestation of the cells triggers the creation of the network of viral assembly which is governed, controlled, and synthesis of ECM and linker proteins present in the cell [24] . A major factor for viral transmission is its capability to survive in a given environment and on a surface. Depending on the type of virus, it remains viable for varying intervals of time which can range from hours to days on surfaces as a fomite. For example, the Hepatitis A virus can remain infectious on a finger pad even after 4 h, although over 60% of the viruses lose their infectivity in the first hour. Similarly, the respiratory syncytial virus remains viable on rubber gloves for a period of 90 min, clothing gown and paper towel for 30-45 min, and skin for 20 min. Influenza virus and rhinovirus are also known to survive on the skin for a long time [25] [26] [27] [28] . According to recent studies, coronavirus can remain viable on a surface for a significantly longer period compared to other viruses. For example, the coronavirus remains active on wood for 4 days, metal for 5 days, cardboard for 24 h, and plastic for 2-3 days [4] . The viability and proliferation of all microorganisms on various surfaces depend largely on the physical and chemical linkage among the outer membrane of these microorganisms and the surfaces. Microbes drift toward a solid surface leading to adsorption and colonization on the surface. This surface accumulation followed by the proliferation of microbes is known as biofilm formation [29] . During this course of time, microorganisms experience diverse biological changes and form an extracellular matrix. The constituents of ECM differ with microbe and species and also depend on the growth conditions. In bacteria, ECM mainly consists of polysaccharides, proteins, and extracellular DNA. They serve various functions for example shielding free-living bacteria and assist genetic exchange. Also, ECM is the basis of a biofilm, which encourages cell to cell and cell to surface interaction at inanimate surfaces as discussed above [7, 29, 30] . In the case of fungi, the ECM also provides antifungal resistance by binding to antifungal agents and preventing entry to their deliberate targets at the surface or within fungal cells, other than acting as a protective barrier against chemical and biological agents [31] . In the case of viruses, ECM is used for adhesion to the target cells followed by interaction with cell surface receptors permitting their entry [32] . Since microbial cell attachment to the surface is the crucial step of biofilm formation, knowledge of cellsurface interaction is very important to control biofilm development. Surface charge and wettability are the two most important factors that influence microbial cell-surface interaction [33, 34] . In general, bacteria are negatively charged. The cell wall in grampositive bacteria consists of peptidoglycan which is embedded with teichoic acids. Teichoic acids are anionic cell surface polymers that contribute to bacterial cell charge, whereas, in the case of gram-negative bacteria, the outer membrane consists of phospholipids and lipopolysaccharides which confer net negative charge to the cell surface [35] . The distribution of charge in virus particles is due to a distinct arrangement of protein within its structure, which is different for different types of viruses [36, 37] . On the other hand, the charge on fungi is because of the existence of mannoproteins found on the fungal cell wall. These mannoproteins are linked to beta-glucans via glycophosphate groups which give a negative charge to the fungal cell wall [38] . The negative cells interact strongly with the positively charged surface via electrostatic interaction. Similarly, surface hydrophobicity is one of the major factors responsible for attachment and detachment from the surface which can enhance biofilm formation. In bacteria, fimbriae contain a high content of hydrophobic amino acid residue and are responsible for cell surface hydrophobicity and attachment [39] . Also, various proteins are closely related to cell surface hydrophobicity that affects the attachment of fungus such as C. albicans to the surface. According to the studies, mycolic acid-containing organisms are more hydrophobic, and an increase in the chain length of mycolic acid increases cell hydrophobicity [40] . Given these considerations, the approaches to create antimicrobial surfaces involve tuning the surface morphology and chemistry. In the next few sections, literature on creating antimicrobial surfaces based on superhydrophobicity, superhydrophilicity, pattern, and functionalization is discussed. The chemical and physical interaction of a surface with a microbe is modulated to impart an antimicrobial nature to a surface. This involves tuning the surface through physical or chemical methods. To facilitate readability, the literature review on developing antimicrobial surfaces has been classified into four sections.  The above classification is based on the type of modifications/approaches used to develop an antimicrobial surface. The first strategy, i.e., patterned surface is a physical modification of surface in which microbes are either unable to come in contact with the surface due to steric hindrance or killed due to physical disruption of cells owing to penetration of the surface features. The second class is that of the functionalized surfaces where a purely chemical modification is done to either inhibit microbe-surface interaction or killing the microbes on interaction. The next approach is to produce superwetting surfaces which require a combination of both physical and chemical modification. It is known that for developing superhydrophobic and superhydrophilic surfaces both roughness (created by physical modification) and chemical modifications are required. However, these surfaces can only inhibit microbial adhesion and are thus treated as a separate category. It must be pointed out here that the above-discussed classes are maybe effective either in killing or inhibiting microbial adhesion. Recently, various surfaces with multiple modes of action have been sought such as surfaces with switchable properties which can both kill and inhibit the microbes simultaneously. These types of surfaces can be developed by incorporating biocidal agents and stimuli-responsive polymers as antifouling material. These approaches have been summarized as the fourth category of smart surfaces, the schematic of all the strategies is illustrated in Fig. 2 . Nature is full of examples including lotus leaf, taro leaf, and shark skin that prevent microbial attachment as a result of micro and nanostructured surfaces. Some surfaces for instance gecko skin, cicada wings, and dragonfly wings can even exhibit microbicidal activity [41] . Details of the structure and activity of some of the natural biocidal surfaces are shown in Table 1 . The biocidal efficiency of a patterned antimicrobial surface is dependent on the surface topography and the microbe species. Hasan et al. have studied the antibacterial action of the cicada wing surface on both gram-positive and gram-negative bacteria of rodshaped and coccoid shape. The surface topography of cicada wings consists of hexagonally packed uniform nanopillars of height 200 nm, base and cap diameter of 100 nm and 60 nm, respectively, and spacing of 170 nm for Psaltoda claripennis species (of cicada). It was found that only gram-negative bacteria, regardless of their shape, are significantly distorted and eventually killed by the wings of cicada [46] . This is because the cell walls of both gram-positive and gram-negative bacteria are different. The cell wall of gram-positive bacteria comprises several layers of peptidoglycan which makes it more rigid compared to that of gram-negative bacteria that have a single layer of peptidoglycan and a second layer consisting of phospholipids. Likewise, Yang et al. have reported antibacterial surfaces by developing a honeycomb pattern on a silicon wafer with varied pore sizes from 0.5 to 10 lm using photolithography and deep reactive ion etching. It was observed that the pattern of 1 lm significantly decreases the bacterial adhesion and growth and biofilm formation of S. aureus and E. coli. The reason behind the enhanced antibacterial behavior of 1 lm pattern size is due to the two key aspects. First is the accessibility of favorable attachment sites. The attachment sites must facilitate a greater contact area between cell and substrate with minimal cell damage. For 1 lm pattern, preferable adhesion sites are much lesser as compared to that of larger pattern size, as 1 lm pattern causes large cell deformation by trapping the cells inside the pores. This was observed for E. coli cells which have a similar dimension as that of the pattern. The second aspect is the physical confinement which hinders the growth and proliferation of bacteria [47] . A similar result was seen for micronsized patterns of comparable dimensions [48] . Xiang et al. have reported the effect of micro-nanopillar array on bacterial inhibition. Titania (TiO 2 ) micronanopillar with motif size of 0.6 lm reduced bacterial adhesion by 62% for S. aureus (cell size is 1 lm) and 73% for E. coli (cell size is 1-2 lm long and 0.5 lm wide) after 30 min as compared to that of flat surface [49] . Similarly, Ivanova et al. have reported the bactericidal activity of patterned black silicon. According to their findings, 500 nm height nanoprotrusions showed similar killing efficiency to that of dragonfly wings for S. aureus and B. subtilis with a killing rate of 4.5 X 10 5 cells killed cm -2 min -1 and 1.4 X 10 5 cells killed cm -2 min -1 , respectively. However, black silicon showed higher killing efficiency than dragonfly wings for P. aeruginosa [50] . Rosenzweig et al. have developed a poly (methyl methacrylate) nanopillar structure and studied their fungicidal properties. The nanostructured surface pillars were fabricated with a periodicity of 170, 320, and 500 nm. It was observed that nanopillar structure showed antifungal activity in the increasing order of periodicity 170 nm \ 320 nm \ 500 nm as shown in Fig. 3 (a) and (b) . Compared to the flat surface, spores started deforming on the nanopillared surface and the development of the germ tube is disrupted and detachment of spores starts after 16 h and 8 h for A. fumigatus and F. oxysporum, respectively. This result may appear contradictory to the observations for antibacterial activity discussed above as the sizes of the cell and mechanosensing mechanism are different [51, 52] . However, the fundamental understanding behind the mechanism of antimicrobial activity of patterned surfaces is very rudimentary at this stage and needs further work. Furthermore, Hasan et al. have reported antibacterial and antiviral Al 6063 alloy nanostructured surface prepared by wet chemical etching. The nanostructures of width 23 nm ± 2 nm were orientated randomly into parallel ridges with a root-meansquared roughness of 995 ± 114.7 nm. To check the effect of surface structure on antibacterial properties, S. aureus (gram-positive) and P. aeruginosa (gramnegative) bacteria were tested. Both types of bacterial cells were deformed on the nanostructured surface and more than 87% of attached cells were rendered nonviable. The reason for bactericidal activity was attributed to the rupturing of the bacterial cells. The effect of nanostructured surfaces on viruses was studied using Respiratory syncytial virus (RSV) and Rhinovirus (RV). Within two hours of exposure, the etched Al surface showed remarkably lower viable viruses compared to that of flat Al surface and all viruses were killed after 24 h. However, the RV virus was more susceptible to the nanostructured surface as compared to that of the RSV virus. Also, the same surface was tested against SARS-CoV-2, and virus viability was examined for different intervals of time up to 48 h. The viability of viruses was reduced after three hours of exposure, and no live viruses were observed after six hours of exposure. In comparison, on the control surface, significant depletion of live viruses was noticed only after 24 h of exposure. A period of 48 h is needed for the elimination of all the viable viruses from the control surface. The reason for virucidal activity is not clear, and the authors have attributed it to nanoscale roughness which can rupture the virus envelope. Also, the size of the viruses is very small and thus can get trapped in the structure which may be detrimental to their viability [53, 54] . From the above literature of biocidal activity on patterned surfaces, we can conclude that the biocidal mechanism on the patterned surface is due to the cell rupturing upon penetration of surface followed by the death of microbes as shown in Fig. 4 . However, this mode of action may not be equally effective against the microbes having thicker cell walls or additional envelopes. Another important reason for the antimicrobial activity is the trapping of cells in the structure leading to cell death as shown in Fig. 5 . The summary of the literature on patterned antimicrobial surfaces is included in Table 2 . Functionalized surfaces can be developed by modifying a surface with a material that can actively kill or inhibit the microbes. The mode of action for functionalized surfaces can be through contact killing of the microbe by functionalized surface due to the materials chemical groups [59] , or the functionalized surface generates heat, reactive species on exposure to external stimuli to disrupt the activity of microbes [60] . The chemically active functionalized surfaces involve the use of non-leachable materials such as polycations, which provide effective biocidal activity through direct contact with microbes. Such a surface with polycation functionalization enhances the adsorption of negative surface charged microbes by electrostatic interaction between the cell membrane and material surfaces [61] . As a consequence, the genetic material of the microbes undergoes leakage and loss its effectivity. This is one of the most promising approaches for a wide range of microbes. Lin et al. have reported the role of polyethyleneimine and its molecular weight on bactericidal (S. epidermidis, S. aureus, P. aeruginosa, and E. coli) and fungicidal (S. cerevisiae, C. albicans) activity. They observed that higher molecular weight polymer showed higher microbicidal activity. This is because the polymer chain length higher than or equal to the size of bacteria facilitates easy penetration into the bacterial cell and destroys the cell membrane [62] . Wong et al. have reported layer-by-layer film of N,N-dodecyl,methyl-polyethylenimine with a polyanion, such as poly(acrylic acid) to develop an effective microbicidal surface against S. aureus, E. coli, and influenza (H1N1) virus. Both high positive charge density and length of alkyl chains are important parameters. Also, a higher number of bilayer deposition was necessary for higher virucidal activity as compared to that of bacteria, as the size of a virus (*100 nm) is approximately 1/10th size of bacterium (*1 lm). The lower antiviral activity observed for a lesser number of bilayers is due to the fact that the voids which are present on the surface are too big to fit a virus. The possible mechanism for antimicrobial activity is through contact killing of microbes by polycationic chains [59] . Silva et al. have developed Silica NPs modified surface using positively charged amine group which resulted in a 50% reduction of vesicular stomatitis virus G (VSV-G) transduction. This is due to the strong interaction of modified silica particles with the virus, which resulted in the blockage of direct contact among cells and viral particles [63] . Likewise, Meder et al. have developed colloidal alumina particles functionalized with an amine group and different functional groups to investigate their controlled interaction with viruses [64] . Polyglycerol sulfate allows electrostatic interaction with the virus, while alkyl chains provide enhanced antiviral action through hydrophobic interactions [65] . It has been observed that some surfaces are effective against only enveloped viruses but not against non-enveloped viruses. Tuladhar et al. have reported the role of hyperbranched quaternary ammonium coating against influenza virus (enveloped) and poliovirus (non-enveloped) virus. Virucidal activity of hyperbranched quaternary ammonium was found to be effective for influenza virus alone and the mode of action is through the disruption or detachment of viral envelope by the long-chain lipophilic tails and the high-density end groups [66] . Thus, functionalized surfaces are active against a vast spectrum of gram-positive and gram-negative bacteria, viruses, and fungi. The factors responsible for biocidal activity are the chain length of the alkyl polycation, molecular weight of the polymer, charge density, and so on. For instance, the highest activity against gram-positive bacteria and yeast was shown by a QAC containing chain length of 12-14 carbons, whereas chain length with 14-16 carbons shows the highest activity against gram-negative bacteria [67, 68] . Other than chemically active functionalization, antimicrobial surfaces can be activated by physical methods. One such example is photothermal therapy. The mode of action is based on the generation of local heat by a photothermal agent upon exposure to light of a suitable wavelength. This results in hyperthermia and leads to microbial death by protein denaturation, rupturing of the cell membrane, cellular fluid evaporation, etc. [69, 70] . Yang et al. reported excellent photothermal activity of Silver functionalized SnS 2 surface. Antibacterial activity of 100% growth inhibition against E. coli and S. aureus was achieved with an Ag-SnS 2 concentration of 0.5 mg/mL after exposure to near-infrared (NIR) for 5 min. The sample was also tested for in vivo antibacterial activity against S. aureus on mice as shown in Fig. 6 [71] . Similarly, photothermal antifungal and antibacterial activity was reported by Lei et al. Polydopamine nanocoating achieved 84%, 96%, and 93% killing efficiency for E. coli, S. aureus, and C. albicans, respectively, upon exposure to NIR [72] . In addition, UV is also used to activate metal oxides such as ZnO and TiO 2 that leads to the degradation of microorganisms via photocatalysis. Semiconducting oxide generates high-energy electron-hole pair when exposed to radiation (typically UV-visible light) with energy more than the bandgap. These high electronhole pairs bring about redox reactions at the surface of the oxide particles, resulting in the generation of various reactive oxygen species and free radicals which induce oxidative stress in the microbial cell and lead to their death [73, 74] . Kim et al. have reported photocatalytic viral inactivation against MS2 bacteriophage, influenza virus, and murine norovirus using TiO 2 nanoparticles prepared with different calcination temperatures. The sample calcined at 700°C showed enhanced virucidal activity due to the presence of a mixed anatase-rutile phase [75] . The summary of various studies reported on antimicrobial activity of the functionalized surfaces is listed in Table 3 . The development of biofilm on an external surface occurs by the attachment of microbes to the surface. The crucial step to control biofilm formation is by developing antiadhesion surfaces that can restrict the contact between microbes and the surfaces. Such surfaces can be developed by enhancing the wettability of a material. This can be done by combining roughness with chemical treatment onto the surface [84] . A surface is said to be superhydrophobic when it exhibits a water contact angle (WCA) above 150°. On such surfaces, water droplet rolls off even at a small tilting angle [85] , whereas, superhydrophilic surface shows a WCA equal to or less than 5°and water droplet completely spread on the surface [9] . Superhydrophobic surfaces are efficacious for a wide array of microbes inclusive of various species of bacteria, fungus, and viruses. The superhydrophobicity imparts antimicrobial character by minimizing the attachment of microbes on the surface [14] . In a study where a lotus leaf-like surface made of Ti was reported, it was observed that the bacteria were unable to adhere to the superhydrophobic surface due to the presence of trapped air nanobubbles and microbubbles in the hierarchical nanostructure. The bacteria start to assemble at the tri-phase interface, as illustrated in Fig. 7 . The nanostructure and the trapped air (hydrodynamic force) minimize the area of contact among bacteria and the surfaces imparting an antifouling activity [86] . Ellinas et al. have reported antibacterial superhydrophobic micro-nanotextured surface of poly (methyl methacrylate) (PMMA) having WCA greater than 155°. The surface exhibited high bacterial repulsion against cyanobacteria Synechococcus sp. for a long period of 72 h and low bacterial adhesion upon immersion in bacterial cell solution till the fourth day. It had been previously reported that the superhydrophobicity of a surface vanishes, when kept immersed, with time due to the eventual depletion of the air layer and leads to bacterial adhesion. This paper shows that the surface design and feature sizes are important parameters to avoid this depletion of the air layer and to maintain a superhydrophobic character [73] . Privett et al. prepared an antifouling superhydrophobic surface using silica-colloid-doped fluorinated substrates. According to their findings, the presence of both roughness and chemical modification using low-energy materials are responsible for minimizing bacterial adhesion [87] . Yeongae Kim et al. have reported antifungal activity of superhydrophobic aluminum surface against Penicillium, Cladosporium, and Aspergillus which exhibited a water contact angle of 169°. They have observed that only superhydrophobic surface was not contaminated, while the superhydrophilic and hydrophobic surfaces were contaminated on the direct inoculation of fungal spores [88] . Like bacteria and fungus, superhydrophobic surfaces also show a reduction in viral adhesion. Katoh et al. have reported that the fabric of personal protective equipment made of non-woven polypropylene with enhanced WCA can potentially reduce the risk of virus carryover by repelling the infectious body fluids [89, 90] . The adhesion of infected fluid on PPE can lead to a higher risk of spreading infection. Hence, superhydrophobic surfaces can lower the risk of viral infection. The antimicrobial mechanism of superhydrophobic surfaces is mostly restricted to avoiding microbial adhesion rather than actively killing them. One of the ways to enhance the antimicrobial activity of superhydrophobic surfaces is by combining the superhydrophobicity with antimicrobial metal or metal oxide nanoparticles. This alters the roughness and also actively kills the microbes present on the surface [91, 92] . Berendjchi et al. have reported superhydrophobic and antibacterial cotton surfaces using copper doped silica nanoparticles followed by hydrophobic modification with hexadecyltrimethoxysilane. The addition of copper (Cu) on the silica sol provides roughness to the silica surface and enhances the antibacterial property [92] . Likewise, Singh et al. have reported antimicrobial white cement composite embedded with different amounts of zinc oxide (ZnO) nanoneedles. The surface showed biocidal action for E. coli (gram-positive bacteria), Bacillus subtilis (gram-negative bacteria), and Aspergillus niger (fungus) [74] . Dimitrakellis et al. have reported bacteria repelling and bactericidal surface against E. coli developed by superhydrophobic micro-nanotextured PMMA surface modified with Cu (shown in Fig. 8 ). The hierarchical rough surface was developed by plasma etching of PMMA, and the biocidal activity was speculated to be due to its surface structure that allowing the mechanical killing of bacteria. The presence of superhydrophobic surface deposited by fluorocarbon (CFx) acted as a bacteria repealing surface by restricting the contact between bacteria and the surface. Finally, the Cu acted as a biocidal material and instantaneously killing any bacteria adhering to the surface [93] . Similarly, other metals and metal oxides such as silver and titanium dioxide (TiO 2 ) are also effective antimicrobial agents [94, 95] . The uptake of silver and copper ions or nanoparticles by microbes leads to their death by disrupting various cellular processes such as enzyme activity, DNA replication. The water droplets on superhydrophilic surfaces spread promptly to completely wet the exposed area and make a compact water layer. This works as a barrier that restricts contamination on the exposed area and enables self-cleaning [85] . Based on this property, superhydrophilic surfaces exhibit antifouling activity [96] . Qian et al. developed a superhydrophilic surface with antibacterial and antifungal properties on stainless steel surfaces by depositing polydopamine (PDA) and silver nanoparticles (AgNPs). This work was inspired by mussels and was further modified with a hydrophilic material methoxy-polyethyleneglycol thiol (mPEG-SH). The water contact angle achieved was close to 0°, and this superhydrophillic surface showed antibacterial and antifungal activity against S. aureus, E. coli, and Penicillium F2-1. The antibacterial and antifungal activities were due to the combined effects of the antiadhesion created by water layers, the bacteria-killing ability of AgNPs, and the stereo hindrance caused by mPEG-SH molecular chains as shown in Fig. 9 . Upon immersion of the surface on the inoculated medium, a huge quantity of Ag ions discharged on the medium which leads to the killing of attached and neighboring bacteria. However, the antifungal properties of superhydrophobic surfaces were tested in presence of a humid atmosphere where the bound water layer was in direct contact with the humid air. Here the released Ag ions were present in the bound water layer which leads to antifungal activity. This water layer remains stable due to the strong attraction of superhydrophilic surface to the water, which helps in sustaining the antifungal property of the surface. Further, the incorporation of antimicrobial metal or metal oxides enhances the antimicrobial properties of superhydrophilic surfaces by allowing higher wetting [97, 98] . Other than metal or metal oxides, hydrophilic polymers are utilized for antimicrobial surfaces [99] [100] [101] . Polyethylene glycol (PEG) is one such polymer that shows enhanced hydrophilicity with stereo hindrance effect due to long-chain PEG [101, 102] . The summary of studies reported on the antimicrobial activity of various superhydrophobic and superhydrophilic surfaces is shown in Table 4 which indicates that like superhydrophobic surfaces, these surfaces also inhibit the adhesion of microbes mainly bacteria and fungus rather than killing them directly. The affinity of water with hydrophilic surface is more in the case of high surface energy material than that of organic molecules due to which water molecules bound tightly with the superhydrophilic surface and prohibited the cell surface interaction [103] . Various types of antimicrobial surfaces have been developed in the past few decades based on the strategies discussed above. However, surfaces with single functionality have some serious drawbacks such as antifouling surfaces cannot maintain their non-adhesive property for the long term and eventually leads to biofilm formation. Also, biocidal surfaces can effectively kill the microbes, but after a certain period of time, a higher amount of debris or dead microbes starts accumulating on the surface, and as a result, it affects the functionality of the surface [108] . Therefore, combinational surfaces with kill and release ability can help in developing effective antimicrobial surfaces for the long term. The mode of action for developing smart surfaces is based on killing the microbes attached to the surface using various antimicrobial agents, and the dead microbes are released using stimuli-responsive polymers [11, 12, 108] . One such example is of thermoresponsive polymers such as poly(N-isopropylacrylamide) (PNI-PAAm) which has its lower critical solution temperature (LCST) at 32°C. Lopez et al. have reported dual functional antibacterial and antifouling surfaces against E. coli and S. epidermidis as shown in Fig. 10 . In their works, lysozyme and quaternary ammonium salt (QAS) was used as a biocide and PNIPAAm was used as an antifouling material. The switchable surface was obtained by changing the temperature across the LCST of PNIPAAm. E. coli attach to the surface at 37°C. An increase in the temperature above the LCST (37°C) results in the collapse of the secondary chain structure of PNI-PAAm. This exposes the underlying biocide which kills the bacteria. Also, as the temperature changed below the LCST of PNIPAAm (25°C), the dead bacteria are released from the surface due to the conformational change of PNIPAAm [109, 110] . Similarly, Yan et al. have reported switchable surfaces consisting of an inner antimicrobial peptide (AMP) layer surrounded by a pH-responsive poly (methacrylic acid) (PMAA) layer as shown in Fig. 11 . PMAA initially restricts bacterial adhesion due to its hierarchical surface. Once the bacteria colony formation starts, it increases the acidification of the surface which results in the collapsing of PMAA layer. This leads to the exposure of the AMP layer that kills the bacteria. In addition, the dead bacteria release from the surface as the hydrophilicity of the polymer resume due to an increase in the environment pH [111] . In addition, Jiang et al. have reported smart surfaces with switchable antimicrobial and antifouling properties. They have developed a surface modified with poly(N,N-dimethyl-N-(ethoxycarbonylmethyl)-N-[2'-(methacryloyloxy)ethyl]-ammonium bromide) which showed bactericidal activity against E. coli by killing more than 99.9% E. coli in one hour. However, these surfaces act as an antifouling surface upon hydrolysis to form a zwitterionic polymer and released 98% dead bacteria [112] . Developing a biocide free surface for killing bacteria is of great interest as it does not have toxicity issues or a chance of evolving multidrug resistant bacteria. Photothermal agents (PTA) are one such example of biocide free materials where biocidal activity is due to the heat generated by the PTA upon exposure to light which results in bacterial cell damage [113] . Qian et al. have developed switchable surfaces with bacteria killing and releasing ability using tannic acid/Fe 3? (TA/Fe) ion complex as a photothermal bactericidal agent and PNIPAAm as an antifouling material. TA/Fe showed biocidal activity against E. coli and methicillin resistant Staphylococcus aureus (MRSA) on exposure to NIR and all the dead bacteria were released from the surface at a temperature lower than the LCST of PNIPAAm [114] . Similarly, another switchable surface with photothermal bactericidal activity has been developed by Qian et al. In this work, instead of using stimuli Figure 9 The schematic diagram of a the antibacterial and b antifungal mechanism of superhydrophilic surface. Adapted with permission from Ref. [98] Copyright (2019) Elsevier. [102] responsive polymer for switching the activities they have used gold nanoparticle layer and phase-transitioned lysozyme film (GNPL-PTNF). GNPL upon exposure to NIR showed bactericidal activity against E. coli and S. aureus with killing efficiency of [ 99% and [ 96% respectively, whereas PTNF on contact with vitamin C solution degraded and removed all the dead bacteria present on the surface [115] . However, these types of surfaces are mostly reported for bacteria. Some of the reports on smart surfaces with dual functionality are listed in Table 5 . Depending on their genotype and physiology, microorganisms can have a significant and often negative impact on materials, food sources, and the health of humans and livestock. Consequently, developing an antimicrobial surface is of significant importance in many areas. Some of the areas of applications of antimicrobial surface are as follows Infections during surgeries can lead to devastating consequences including septic shock which leads to multiorgan failure and death. In particular, microbial contamination of implant during surgery and subsequent colonization of microbes on the implant-tissue interface may lead to post-surgery complications [7, 68] . So, developing an implant with a microstructure which either does not allow the growth of microbes or kills them directly is essential. Micro-or nanopatterned implant surfaces can be one possible way to develop antimicrobial surface which does not allow the accumulation of microbes by killing them and can reduce the risk of post-surgery complications and implant rejection [49, 128] . The viruses can spread by liquid droplets from the coughing and sneezing of an infected person. These liquid droplets can get deposited on the exposed surfaces and lead to the spreading of infection through surface contact. Rapid spread infection can lead to a pandemic similar to COVID-19. In this case, since the viruses can spread predominantly by contact with a contaminated surface, the superhydrophobic surface can play a significant role in preventing viral contamination by not allowing viral droplets to adhere to the surface. Healthcare workers have the potential risk of carrying the viruses in personal protective suits, making superhydrophobic PPE suits can reduce the risk. Also, functionalizing a surface with long alkyl chain length polycations can effectively kill the microbes, so developing a mask with an outer layer functionalized with polycations cannot allow the entry of microbes by killing them during the contact period. Food spoilage leads to an excessive amount of food waste every day. One of the major reasons for spoilage is due to bacterial and fungal infections. A major amount of fruit and vegetable losses in the course of post-harvest is mainly due to diseases occurred by fungi and bacteria. Bacteria such as Geobacillus spp. and Bacillus are responsible for spoilage of canned foods and they cause ropiness in bread kept at high ambient temperatures. Also, Bacillus is responsible for producing gas and foul smells in chilled, vacuumpacked foods and milk. Fungi such as Candida are responsible for the spoilage of fruits, vegetables, and milk products [129, 130] . Developing antimicrobial food packaging can help in increasing the shelf-life of food articles. Biofouling is one of the major problems associated with the surfaces such as marine hulls [131] . These problems can be reduced by developing superhydrophilic surfaces. In the case of ship hulls, the surface is in continuous contact with water. Due to the strong affinity between superhydrophilic surfaces and water molecules, the water layer will act as a barrier to restrict the interaction between the fouling agent and the surface and thus prevent fouling. Also one of the reasons for pitting corrosion in metals is due to the presence of bacteria. Aluminum alloy degradation is caused by hydrocarbon-degrading bacteria Serratia marcescens and Bacillus cereus [132] . This can be prevented by developing surfaces with superhydrophobic or superhydrophilic materials which will not allow microbial adhesion or combination surface with more than one strategy to restrict the growth by killing the microbes. Exposed surfaces such as doorknobs, lift buttons, seating, and furniture in public places, touch surfaces in public transports have a higher risk of microbial contamination, and disinfecting the surfaces frequently is not so easy [133] . Hence, these surfaces can be coated with antimicrobial materials that can help in reducing the risk of spreading the infections.@story_separate@A brief summary of reports on producing antimicrobial surfaces is presented in this review. The approaches discussed involve altering the interaction between a microbe and a surface by modulating surface chemistry, wettability, and topography. The tuning of these aspects leads to 4 broad classes of antimicrobial surfaces based on a physical modification of the surface (patterned surfaces), chemical modification (functionalized surfaces), a combination of both physical and chemical modification (superwettable surfaces), and a smart surface with switchable ability to kill and release the microbes. Many of these strategies inhibit microbial adhesion while some of these kill them or both. It has been shown that microbial adhesion decreases on increasing hydrophobicity due to the decrease in interaction and contact area, whereas, on superhydrophilic surfaces, water wets the surface completely due to the strong affinity which acts as a barrier between the microbes and the surface. These approaches based on wettability are passive and a combination with biocidal material such as metal, metal oxide, or polymers can enhance the antimicrobial properties and function as both antifouling and biocidal surfaces. Often the production of superwetting surfaces involves topographical tuning and patterning of surface and coating. However, several reports have proven the achievement of antimicrobial activity by patterned surfaces, irrespective of materials. This activity is attributed to the physical confinement of microbes and/or penetration of surface features into the microbes. Further, chemically functionalized surfaces are also capable of biocidal activity due to electrostatic interaction with the microbes leading to disruption of the cell membrane or generation of local heat by absorbing light of a respective wavelength which leads to cell death. Also, smart surfaces with both kill and release ability have been reported in the literature. This type of switchable surface shows enhanced efficiency with long-term activity. While a vast amount of literature is available on the fabrication of antimicrobial surfaces, there are several important observations worth highlighting. a) Most of the reports provide an account of antimicrobial activity on very few types of microbes and species. It is known that the interaction of a microbe with a surface varies significantly. For example, a surface effective against gram-negative bacteria may not be able to eliminate a gram-positive bacteria or vice versa. Even within the same type of bacteria, the surface may not be equally affected by the various species. Similarly, an enveloped virus may be easily killed as compared to a nonenveloped virus. b) The fundamental science of the mechanism of activity remains not completely understood. This called for better multidisciplinary collaborative work. Only a comprehensive understanding can help produce broad-spectrum antimicrobial surfaces. c) The other important consideration is on the durability of the surfaces, particularly for applications demanding long-term usage and receiving a high microbial load. Very few reports address the application-specific design of antimicrobial surfaces and present suitable results with respect to antimicrobial activity with time during service. d) The scalability and economic feasibility of some of these approaches also need to be thoroughly examined. While competition and scale-up favor the economy, it is crucial that the strategies are technically scalable without compromising on quality. Quality control protocols have to be designed before the product is deployed and during service. e) Another very important consideration is the effect of these antimicrobial surfaces on commensal microbes which are part of the human microbiome. This microbiome is responsible for maintaining hygiene and providing immunity along with other specific functions such as digestion. It must be emphasized to perform tests on these organisms to ensure the normal population of these. The development of new strategies and better materials, along with an emphasis on identifying promising technology for the market, are necessary. One of the ways to increase the antimicrobial effect is by developing a combinational surface of more than one strategy to kill a wide range of microbes. In addition, most of the strategies depend on nano-/ microstructural surface features which can get destroyed over time due to wear and environmental factor, thus reducing the efficacy of antimicrobial surfaces. Thus, new approaches toward the development of robust and long-lasting surfaces are necessary for sustainable and commercially viable technology development.","The rapid spread of microorganisms such as bacteria, fungi, and viruses can be extremely detrimental and can lead to seasonal epidemics or even pandemic situations. In addition, these microorganisms may bring about fouling of food and essential materials resulting in substantial economic losses. Typically, the microorganisms get transmitted by their attachment and growth on various household and high contact surfaces such as doors, switches, currency. To prevent the rapid spread of microorganisms, it is essential to understand the interaction between various microbes and surfaces which result in their attachment and growth. Such understanding is crucial in the development of antimicrobial surfaces. Here, we have reviewed different approaches to make antimicrobial surfaces and correlated surface properties with antimicrobial activities. This review concentrates on physical and chemical modification of the surfaces to modulate wettability, surface topography, and surface charge to inhibit microbial adhesion, growth, and proliferation. Based on these aspects, antimicrobial surfaces are classified into patterned surfaces, functionalized surfaces, superwettable surfaces, and smart surfaces. We have critically discussed the important findings from systems of developing antimicrobial surfaces along with the limitations of the current research and the gap that needs to be bridged before these approaches are put into practice. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s10853-021-06404-0."
"Management in early life is a fundamental factor enabling dairy calves to reach key performance targets that impact on lifetime productivity [1] . For example, previous research has highlighted the influence of average daily gain (ADG) in the pre-weaned period on the onset of puberty, age at first calving and performance in subsequent lactations [2] . Ensuring that calves attain performance targets requires a multifaceted approach. This not only includes providing adequate nutrition, but also proactive management of their environment and healthcare to minimise the risk of reduced feed efficiency, survivability and compromised welfare. Physical aspects of the rearing environment, such as ambient and surface temperature, relative humidity, air speed and surface moisture, have been shown to influence calves' physiological state [3] . For example, combining cold temperature with damp surfaces and raised air speed will exacerbate cold stress in young calves leading to a diversion of energy toward maintenance of thermoregulation. Grouping strategies are also important, and evidence suggests that grouping calves at a young age can improve solid feed intake and calf welfare [4, 5] . This strategy also has the potential to greatly reduce labour; however, the increased risk of disease transmission between calves may concern producers [6, 7] . Hygiene management is also an important aspect of rearing calves as their immune systems are naïve and infectious pathogen levels can rapidly increase in the calf environment [8] . High levels of bacteria in the pen bedding, air, feeding equipment and the feed are associated with increased calf enteric and respiratory disease and should therefore be avoided [9] . Housing and management during rearing should therefore aim to minimise calf exposure to stress and pathogens, particularly those associated with enteric and respiratory diseases, within the first months of life. There is evidence of shortfalls in commercial dairy calf rearing practices throughout the UK with previous studies highlighting that health and performance of pre-weaned calves on dairy farms may be compromised. Brickell et al. [10] reported that 15% percent of dairy heifers, born alive, did not reach their first lactation, with a mortality rate of 6.8% within the first 6 months of life. As the dairy heifer accumulates cost throughout the rearing period until first calving, this represents a significant economic loss to farmers. More recently, Hyde et al. [11] found mortality in dairy calves in the UK to be 6% in the first 3 months of life and that changes in mortality were minimal when compared with the 1990s [12] . Improvements to calf health and performance with developments in nutrition and healthcare may be limited by a poor environment, and similarly poor nutrition and healthcare may negate the effects of a good environment. Therefore, all these aspects should be evaluated when assessing calf rearing systems. Although environmental factors such as ambient temperature have been shown to play a fundamental role in calf mortality [11] , many studies only provide limited environmental descriptors when reviewing calf rearing systems. Additionally, systems vary across different regions. Numerous surveys have been conducted in US regions where temperatures regularly fall below 0 • C and barn design is typically consistent across farms [13] [14] [15] [16] . Klein-Jöbstl et al. [17] noted similar consistency in areas such as colostrum management, housing and feeding procedures within Austrian dairy farms. In the UK however, there is a dearth of information linking calf performance and health with routine calf rearing practice and housing design. Moreover, there is no information surrounding calf rearing systems in Northern Ireland (NI). Information on current calf management practices is required to determine the major pitfalls in the rearing phase on commercial farms, providing direction for future research. Understanding the motivation behind particular farming practices will provide insight to solutions required to improve uptake of updated recommendations by producers. In order to address the knowledge gap, a survey of calf rearing was completed on dairy farms within NI. The key objectives were to describe common approaches to housing and management and identify potential areas of weakness that may lead to poor calf health and performance. In particular, the specifications and condition of housing used, and management of nutrition and disease were evaluated.@story_separate@Seventy-five farms were chosen for potential participation in the survey, this equating to approximately 3% of total dairy farm businesses in NI [18] . In order to be eligible for inclusion, farms had to be recording for NI national benchmark figures and have a herd size of more than 60 dairy cows. This was to ensure that adequate farm financial and physical data were available and that the rearing operation was large enough to require comparable calf rearing facilities, respectively. Within each of the six counties in NI, farms were stratified by size and a random sample was taken from each stratum to ensure appropriate herd size and geographical distribution. Farmers were first contacted by letter to indicate that farm recruitment would be occurring and following this they were contacted by phone to confirm their willingness to participate and to schedule the first farm visit. Of the 75 farms selected, 66 agreed to participate. Each participant was visited 3 times, over a 3-week period; the entire sampling period was carried out between the 18 January 2019 and 2 May 2019. Data collection took place through a combination of a questionnaire with the farm manager or main calf rearer, a review of farm records and on-farm housing and hygiene measurements collected by trained assessors. Prior to the beginning of data collection, assessors were trained in each element of data collection by the authors on a non-participating farm (CAFRE Greenmount dairy farm, Northern Ireland) prior to the study, to ensure consistency in of data collection techniques. The face-to-face questionnaire (available as Supplementary Material) was completed on the first farm visit and covered nutrition, healthcare, and housing practice from birth until weaning, incidences of calf morbidity and mortality over the current calving season, and farm demographic information. Number of calf births, calf mortality between 0 and 2 days of age, and from 2 days of age to weaning, as well as the number of calves treated for scour and pneumonia were ascertained from farm records and grouped by month. Other disease information was recorded in the survey questionnaire. Housing and hygiene measurements were carried out on all of the visits. Each second visit was arranged for the earliest convenient date after the first (average of 8 days after), and the third visit took place two weeks after the second visit. The nutritional management of pre-weaned calves until the age of 13 weeks was recorded. The level, concentration and nutritional composition of liquid and solid feed offered to calves were recorded to calculate dietary metabolisable energy content (ME, MJ/kg DM) [19] . Protein and fat content of cow's milk fed to calves were ascertained using milk recording data collected within the month of the first farm visit. ME (MJ/kg DM) of cow's milk was calculated using methods described in the [19] where lactose was taken as 4.85% [19] . Farmers were asked to demonstrate routine milk replacer mixing by preparing a perceived allocation for a calf or given number of calves. This was then weighed by technicians to determine the accuracy of the actual quantity of milk powder against the target amount. Age of access to water and forage as well as method of presentation for the pre-weaned period were also recorded. Hygiene in the calving pens was categorised as good, intermediate or poor using the following classifications. Hygiene was considered good when well strawed, no damp areas and with no sick cows. Poor hygiene was recorded when there was a clear build-up of dirt at animal height, engrained dirt in walls and floors, and presence of use for sick cows. Pens were classed as moderate where there was a low level of dirt build up, although mostly well strawed and no sick cows. Calving pens with porous surfaces at calf height (e.g., non-rendered stone walls), significantly cracked concrete floors, and poor access for machinery were described as not easy to clean. Pens were described as easy to clean where no porous surfaces were present, floors were intact and there was easy access for machinery. The area and volume of calf buildings and all calf pens, and the eaves and ridge height of calf buildings were recorded (Draper Laser Distance Measurer; LDM-40M, Draper Tools Ltd., Eastleigh, UK). The materials of calf house roofs, walls, and pen walls and floors were recorded. The slope of representative locations in pen floors were measured in up to three pens using a digital spirit level (Gain Express G0182105-JY4, Gain Express Holdings Ltd., Hong Kong) orientated to find the maximum slope. Ventilation was classified as natural or mechanical. The length and width of outlets above eaves height were measured directly or estimated from photographs, and the area of inlet in each sidewall was calculated from measured surfaces and known porosity of standard cladding materials [20] . The ventilation requirement (K) of each building was calculated for the maximum number of calves placed in the building at one time as stated by the farmer, using the methodology described by Bruce [21] . The minimum outlet and inlet areas derived from K were compared with the actual inlet and outlet areas (KO) to provide an assessment of natural ventilation capacity. Manufactured calf hutches (n = 5) were excluded from natural ventilation calculations, meaning calculations were completed for 61 calf houses. The ratio K:KO was used to describe the competence of inlet and outlet areas to provide natural ventilation for the housed animals. Ratios of ≥1:1 were classed as good, 1:1 to 1:0.75 was classed as average and <1:0.75 was classed as poor. The distribution of inlets around each building was described as good, average or poor based on the ratio of the derived minimum inlet area between the two main sidewalls (good = ratio of 40:60 or closer, average = 30:70 to 40:60, poor = 30:70 or wider). Spot measurements of natural and artificial light were taken at calf head height in 3 pens on each farm (Extech 45170 Multimeter, FLIR Commercial Systems Inc., Nashua, NH, USA) between 10:00 h and 16:00 h. The area of natural lighting in the roof was recorded as a percentage of the total roof area of the calf building. This was determined by dividing the number of transparent roof sheets by the total number of roof sheets. Data were entered into Genstat ® (version 21, VSN International Ltd., Hemel Hempstead, UK). For each continuous variable summary statistics were calculated to include the number of observations recorded, the mean and the standard deviation. For the categorical variables a table of counts showing the number of observations for each level of the variable in question and also a table showing the percentage of each level over the total in each case was produced. The median herd size of farms was 124 cows (ranging from 66 to 400). Holstein was the predominant breed of cow (78.8%) within the herds (Table 1) . Average 305-day yield production of the herds ranged from 5000 L to 11,000 L with a mean of 8000 L. Calving patterns differed greatly across farms, with the majority calving from approximately September to March (47%) or calving all year round (40.9%). The spread of the calving period ranged from 77 to 365 days with an average of 272.2 days. Within this dataset, the range of herd calving patterns and herd sizes meant that the requirements in relation to each farm's calf accommodation and management would also vary accordingly. The average age of the main person responsible for rearing of calves (e.g., farm manager, owner or main calf rearer) was 49 years, ranging from 23 to 82 years, and 86.4% were male. The average amount of time that this person spent per day feeding calves and completing other calf related tasks was 125.4 min. A second person that assisted in calf rearing was available on 42.4% of the farms. Within the farms surveyed, only 3% of farms left calves to suckle the dam for their first feed. This suggests that many farms were taking a direct approach to ensuring calves receive adequate colostrum by manually feeding it to ensure successful passive transfer of immunity [22] . Although the majority of farms (77.2%) fed 3 litres or more of colostrum in the first feed, only a small number (13.6%) regularly tested it to determine the quality, meaning that there is still a risk of calves having failure of passive transfer (FPT) if quality is low [23] . Of the 66 farms surveyed, 81.8% fed calves milk replacer throughout the pre-weaned period whereas 18.2% only fed cow's milk. The main method of feeding calves was through single teat feeders, likely due to the fact that 62.1% of the farms were not grouping calves until at least 1 week of age. Automatic feeders (AMF) were used on 21.2% of the farms which is similar to other regions [24] . The majority (57.1%) of farms using AMF calibrated them annually, whereas 7.1% did not routinely calibrate them. A variety of milk replacer products (n = 25) were used on farms, with protein and fat levels ranging from 20% to 26%, and 16% to 22%, respectively ( Table 2 ). Protein content of cow's milk fed ranged from 3.14% to 3.60%, with butterfat content ranging from 3.71% to 4.59%. The mean metabolisable energy (ME) (MJ/kg DM) of milk replacers fed was 19.16 MJ/kg DM whereas the mean ME of cow's milk fed was 24.85 MJ/kg DM. Milk replacer concentration was an average of 152 g/L, and the majority of samples (84.6% of 26 farms) weighed by technicians were within 90% to 110% of the targeted amount of milk replacer powder. Across the farms in this study, median peak milk volume allowance was 6 L/day (ranging from 4 L/day to 8 L/day), where there was a median of 7 days to peak milk volume. Calf starter feed was offered to calves from as early as birth and the mean first age of being offered this feed was 5 days. The majority of farms (78.8%) did not measure starter feed offered to pre-weaned calves. On the remaining farms, the mean quantity offered per day at the point of weaning was 2.1 kg, ranging from 1.5 to 3.5 kg. Protein content ranged from 16% to 21% and the average ME (MJ/kg) of starter was 9.83 MJ/kg. Water was offered at birth on 68.2% of farms, and the mean first age of offering was 4 days. The method of offering water varied across the pre-weaning period between and within farms, with bucket being the predominant (80%) method in the first week of life and free flow drinkers by 5 weeks of life (57.6%). Forage was first offered to calves at an average of 4 days of age. Straw (excluding bedding) was the main forage type (65.4% of farms) offered to pre-weaned calves. Alternative forages offered were hay (25%), haylage (1.9%), lucerne (1.9%) and straw within a mixed ration (1.9%). Forage was fed through racks or troughs, which varied within farms depending on the age of the animal; however, 20% of farms perceived bedding to be a suitable source of forage for calves and did not offer additional forage.  The average age at the start of weaning was 57.7 days and a variety of weaning criteria were used (Table 3) , the most common being age (33.3%). Within this dataset, 13.6% of farms weaned calves by concentrate intake alone, and farms that used this as a factor tended to quote intakes of 2 kg per day as the minimum threshold for weaning. Farms also weaned calves depending on visual appearance alone (7.6%) or on a combination of two or more factors (36.4%). The most common weaning method was abrupt weaning (31.8%), this followed by a 3-step reduction in milk volume (28.8) and then a 2-step reduction in milk (27.3%) volume. As a means of improving calf thermoregulation, calf jackets were used on farms routinely (28.8%), during situations of cold weather (19.7%) or where calves were small or sick (21.2) . Of the farms that used calf jackets, 34.7% did not wash them between calves, presenting a risk of pathogen transmission to younger calves. Heat lamps were used on a lower proportion of farms (50%), with only 4.6% using them routinely. Single calving pens were in use on 73% of farms. Bedding was added daily or after every calving on 61% of farms, with 21% of farms adding bedding 3 times per week or less. Only 15% of calving pens were cleaned out after every calving or every other calving, a further 33% weekly or fortnightly, and more than half (52%) had a cleaning interval of one month or more. Hygiene in the calving pens at the time of survey visit were scored as 32% good, 47% moderate and 21% poor, with 24% of pens considered not easy to clean. Furthermore, 25.8% of the farms reported using calving pens for sick cows. The median number of days between cleanings in single and group calf pens was 28 and 31.5, respectively. Cleaning frequency of feeding equipment (buckets and teat feeders) ranged from twice per day to every 84 days with a median of every 2 days. Feed buckets were cleaned when the respective calf was weaned on 18.2% of the farms. Automatic feeders were cleaned on average every 7.7 days, although 50% of farms using AMF did not routinely clean teats. The range of hygiene routines reflects the diversity of the farms sampled in this study. The majority of farms (83.3%) used disinfectant when cleaning however, 26.4% of these farms did not measure the concentration of disinfectant. Approximately 41% of the farms did not use or measure the concentration of disinfectant, this in turn suggests that there was a high risk of their pen hygiene management not being effective in completely removing pathogens. The majority of farms did not have designated areas to wash (65.2%) and dry (77.3%) feeding equipment (Table 4 ), making it difficult for them to consistently maintain effective hygiene practice throughout the calving period. Approximately 29% of farms had calves in the calf house throughout the year, so thoroughly cleaning housing would have been difficult.  A large diversity in calf housing was seen in farms across the dataset. Hutches were used as primary calf housing on 7.5% of farms. The majority of calf houses (92.4%) were classed as naturally ventilated as they did not have mechanical ventilation present; however, a large proportion had inadequate inlet and outlet areas (53%) to achieve natural ventilation (Table 5) . Additionally, 53% of the main calf houses were adjoined to other livestock buildings meaning that natural ventilation would be further compromised. Main building volume per calf ranged from 6.2 m 3 to 75.3 m 3 , with an average of 20.6 m 3 , lower than calf house volumes (mean of 36.4 m 3 ) surveyed in the US [14] . The most common time to group calves was between 1 and 4 weeks of age (39.4%; median age at grouping was 14 days) although there was a large variety across the dataset, ranging from birth to 64 days of age. Approximately one-quarter of farms surveyed (28.8%) did not give a response to reason for time of grouping; however, the most common responses given related to ease of management (22.7%), age of the calves (19.7%) and availability of space (15.2%). Space allowance (S) within calf pens ranged from 0.9 m 2 /calf to 4.9 m 2 /calf with an average of 2.3 m 2 /calf, more specifically average SA in single pens and group pens was 1.5 m 2 /calf and 3.4 m 2 /calf, respectively. Light intensity at calf level within calf housing ranged from 8.0 lux to 5156.7 lux (where lights were turned on) with a median of 256.7 lux. Average light intensity was below 100 lux on 19.7% of farms. Straw was the predominant substrate used to bed calves on farms (92.4%), likely due to its established ability to insulate and provide nesting for young calves [25] . In this study, an attempt was made to measure the slope of calf pen floors however the presence of deep bedding and calves in the pen made this challenging. Although 57.6% of farms had solid concrete floors under calves, the ability of these to sufficiently drain urine, spilled milk and water is determined by the slope and could vary significantly. Of the 99 successful measurements taken on 52 farms, the median slope recorded was 1.1 • , ranging from 0 • to 5.2 • .  Separation of sick calves is a well-established practice to minimise the risk of disease spread to susceptible calves [26] . Although 42.4% of the farms isolated calves at first signs of sickness, 24.4% of farms did not separate calves until severe signs of sickness and 18.2% did not isolate calves at all, potentially enabling transmission of pathogens between calves. According to farmers' records, infectious bovine rhinotracheitis (IBR) was the most common (21.2%) on farms within the last three years (Table 6) . Bovine Viral Diarrhoea (BVD) and Leptospirosis were the diseases most commonly vaccinated against on farms (75.6% and 72.7%, respectively) ( Table 7 ). In terms of biosecurity, 42.4% of the farms had visitor foot dips present at the entrance. However, 53.5% of these did not routinely replenish them, suggesting that these foot dips would not likely be effective as a biosecurity measure [27] . Of the other farms that had foot dips present, 28.5% replenished them routinely within two weeks whereas 14.4% routinely changed them less frequently than every two weeks. A much smaller percentage of farms (9.1%) used foot dips as a measure of biosecurity within farm and only 33% of these regularly replaced them within two weeks. As an alternative biosecurity measure, 21.2% of farms operated a closed herd policy where no cattle were purchased into the farm.  The herd sizes observed in this study were typically smaller than those reported in the US and other parts of the UK [11, 15] but larger than those surveyed in Europe [17, 28] . Several studies have associated larger herd sizes with increased calf mortality [28, 29] ; however, Murray et al. [30] found conflicting evidence. On average, farms were calving for over three quarters of the year. Only a small percentage of herds (6.1%) were spring calving with the aim of a short calving period, in contrast with that typically seen in the Republic of Ireland [31, 32] . Increasing the spread of the calving season has been associated with an increase in calf mortality [30] and calves born in winter or spring months are at greater risk of morbidity and mortality [33, 34] . The feeding of cow's milk to calves, as reported on 18.2% of farms surveyed, allows calves to benefit from the increased fat content and ME content compared to milk replacer [19] , but there is a risk of disease transmission and inconsistency in milk components [35] . Much research has been completed in the area of milk volume fed to calves and increased peak volume of milk is associated with improved calf performance, health and welfare [36, 37] . Peak milk volume ranged from 4 L to 8 L per day across farms and the difference in energy intake may have significantly impacted performance of calves. However, the median time to peak milk allowance of 7 days was comparatively less than in calf diets reported by Jorgensen et al. [37] , where the mean was 18 ± 11.4 days and increasing the number of days to peak allowance was associated with poorer calf health. One in five farms did not offer calves forage. Forage provision in the pre-weaned period increases starter intake in the same time period, with a subsequent increase in calf performance [38] . Furthermore, the provision of alternative forages may reduce the intake of straw bedding, which presents one of the largest sources of pathogens in the environment of calves and is therefore a risk factor for disease [26, 39] . The simple addition of forage to pre-weaned calves would not only reduce the risk of calves consuming contaminated bedding, but provide an opportunity for novel behaviours that reduce cross-suckling [40] and increase the consumption of starter intake which is important for effective and nondisruptive weaning [41] . The adequate consumption of starter at the beginning of weaning has been shown to be more important than the weaning method employed [42] . Age is the simplest unit of measurement for weaning and was used as the sole reason for weaning on a third of farms. However, controlled research suggest that calves should be eating approximately 1.5-2 kg of starter feed before being fully weaned [43] . Weaning before calves are consuming this quantity of concentrate has been indicated to result in an increase in stress and poor performance as they are not fully adapted to a post-weaned diet [42, 44, 45] . Therefore, weaning by age alone may present a risk to calf performance and health in weaned calves. Industry has pointed to a target of doubling birthweight by the point of weaning [46] , which has put some emphasis on live weight as criteria for weaning. However, only 9.1% of farms in this study weaned by weight alone. This may reflect a lack of accurate methods for accurate weighing of calves on farms which is supported by the fact that very few of the farms had an appropriate equipment for weighing calves (weigh scales or chest girth band) [47] . Of farms surveyed, 31.8% chose to wean calves abruptly with no prior reduction in milk volume. Abrupt weaning has previously been linked with increased signs of hunger and cross suckling due to sudden reduction in energy intake [48, 49] . Where there has been failure to establish calves on sufficient concentrate feed prior to weaning, stalls or reduction in growth performance have also been observed [50, 51] . The variety of weaning criteria and methods observed across farms suggest lack of clear guidance and understanding surrounding the requirements of the weaned calf, or the necessary tools required to measure calves' suitability for weaning. Poor calving pen hygiene has been established as a factor in the transmission of cryptosporidium to calves [52] as well as playing a small role in early lactation uterine infections in the dam such as endometritis [53] . Increased exposure to manure of adult cattle as well as exposure to adult cows other than the dam has been shown to increase the risk of Johne's disease transmission to newborn calves [54] . The use of multiple cow calving pens may also increase the risk of Johne's disease transmission, although other factors such as the pooling and contamination of colostrum are also known to play a role [55] . Within the present study, 25% of farms surveyed housed sick cows in calving pens. As indicated above, this suggests there is significant potential for accumulation of pathogens within calving pens and therefore an increased risk of disease transmission to newborn calves. Increasing awareness to the potential disease risks originating from the calving pen may increase uptake of more bio-secure and hygienic practices. However, considering the role of available space and time in other management decisions within this study, such as grouping and weaning, the design of facilities may be a limiting factor in the hygiene management of calving pens. One of the major challenges for calf houses is the duration of use throughout the year, for example 89.4% of farms calved cows over 9 months of the year or more. The sustained arrival of new calves over an extended period such as this makes it difficult for farmers to find opportunity to clean. Additionally, the continuous flow of calves through calf housing without breaks presents the challenge of young calves being exposed to older calves [56] . The use of modular housing (where the management of one pen can be independent of another) has the potential to allow farms to maintain hygiene practices where there is constant demand for calf accommodation. Four out of five of the farms using hutches availed of this benefit, with cleaning occurring after each use. The proportion of farms using hutches (7.6%) was much lower than in countries such as the US where hutches are commonplace [57] . On many farms, the lack of routine cleaning regimes may be a factor in poor calf health and performance. Across the dataset, ventilation of calf houses was poor; a small number of farms used mechanical ventilation (7.6%) and a large proportion had limited natural ventilation design present in calf housing (53%). Adjoining calf houses to other buildings not only risks impacting ventilation [14] , but in many cases would also risk airborne pathogens being transferred from older animals to calves that would be detrimental to calf health [56] . Pre-weaned calves were forced to share airspace with older, post-weaned animals in 51.5% of the calf houses. Calf houses were adjoined to other buildings on 53% of farms. Even where design of housing allows natural ventilation, it is suggested calves do not produce sufficient heat to drive thermal buoyancy meaning that during times of low wind speed, ventilation will be inadequate [56] . Very little recent research has been carried out in the natural ventilation of cattle housing within the UK climate. Additionally, anecdotal evidence would suggest that mechanical ventilation is not commonly used in calf housing in the UK. More research should be conducted to assess the ability of naturally ventilated housing to meet the ventilation requirements of pre-weaned calves as this will vary with geographical area, topography, building orientation, situation and cladding design [58, 59] . Although the average calf house volume (20.6 m 3 ) was lower than those surveyed in the US by Lago et al. [14] , there was a similar range whereby the volume of many buildings was excessively higher than the minimal recommendations of 6 m 3 /calf [60] . The range of 69.1 m 3 between the smallest and largest volumes per calf may have an impact on ventilation rates [14] . The average SA in calf pens was higher for group pens (3.4 m 2 /calf) than single pens (1.5 m 2 /calf). Research assessing calf barn design on airborne bacterial density has concluded that SA has a large impact on calf house air quality and has recommended a SA of 3.0 m 2 per calf [14] , which has also been suggested to improve calf welfare [61] . This highlights that group pens within this study that are typically constructed within the housing are of more adequate space than the single pens being used. As many of the single pens used on farms surveyed were commercially available products, achieving an overall increase in SA would be heavily dependent on the adoption of larger pen designs by manufacturers. Although little research has been conducted on the impact of light intensity on calf performance and welfare, levels below 100 lux have been associated with reduced feeding and social behaviour and increased lying behaviour [62] . As the median light level was 256.7 lux, it could be considered that light was not a limiting factor on farms surveyed. The provision of natural light is often thought to be productive in the elimination of pathogens although this has been seen to vary across micro-organisms and the intensity of sunlight [63] . Ease of management was the primary reason for the age of grouping calves on 22.7% of farms surveyed. Labour associated with calf rearing is a challenge for many producers and grouping calves earlier is widely recognised to ease management, regardless of herd size [64] . The time gained by easier management may be of benefit to farmers and allow other tasks such as cleaning to be carried out more thoroughly. The finding that 15.2% of farms grouped calves based on the availability of space suggests that the suitability of housing may have been a limiting factor in calf rearing. Further research could investigate relationships between housing system and grouping management. Jackets and heat lamps are typically used on farms to help calves maintain an appropriate body temperature [65] . A majority of farms (74.2%) used calf jackets in some capacity, despite the fact that research has shown no clear benefit to improving performance or health [65, 66] . As there may be variation in what temperatures are classed as cold by individual farmers, the use of thermometers in the calf house would provide a more accurate method of monitoring temperature and placing a minimum threshold under which calves are provided with jackets. Heat lamps were predominately used for sick or small calves but the majority (54.4%) of farms did not use them at all. Young calves prefer heat supplemented areas during cold periods so the provision of heat lamps to pre-weaned calves could be a benefit to their welfare and health [67] . Straw plays a large role in the rearing of dairy calves in Northern Ireland, between its uses mainly as bedding, but also as a source of forage for early rumen development. As arable and horticultural crops make up just 4.3% of the farmed land area [18] as opposed to 26.9% across the UK [68] , livestock farms in Northern Ireland are heavily reliant upon imported straw, the cost of which is volatile. This presents the risk of high cost of bedding and anecdotal evidence would suggest that this may cause reluctance to substantially bed calves to maintain a high level of hygiene and provide suitable nesting and thermal protection. The ability of substrate to maintain a dry bed is influenced by the specification of the floor beneath it [56] . The use of perforated (slats) or permeable floor materials (stones or woodchip) on farms may increase drainage efficiency, but there may be implications on ease of thorough cleaning of the surface between calves, although no research has evaluated previously this. Considering the potential influence drainage may have in the calf environment [69] , further research into this area is required. In this study, the majority of farmers vaccinating for BVD and Leptospirosis was similar to that of a previous survey of UK cattle health management, as was the percentage of farms operating a closed herd policy [70] . Although the number of farms that reported vaccinating for Rotavirus (43.9%), Coronavirus (34.8%) and E. coli K99 (22.7%) differed, there is evidence that the only commercially available calf enteritis vaccines cover all the aforementioned diseases. This discrepancy may suggest that some farmers are not fully aware of the targeted diseases of the vaccines they purchase. This may highlight deficiencies in knowledge transfer from the farm's veterinary service. For future studies, it may be beneficial to discuss questions pertaining to the health and vaccination management with the farm veterinarian. Results of the present study have highlighted areas of increased risk within numerous aspects of calf rearing management. The adage 'measure to manage' is often used in agriculture but in this survey, lack of measurement has been demonstrated within the nutrition, hygiene and physical management of calves. The majority of farms did not measure colostrum quality and starter intake in calves. Adequate intake of high-quality colostrum is paramount to the health of neonatal calves and measuring colostrum quality is an inexpensive and relatively quick method of ensuring calves are ensuring more than 10 mg/mL of immunoglobulins [23] . Similarly, sufficient intake of solid starter feed is necessary to maintain energy intake and minimize stress in the post-weaning period so that calves continue to thrive and are not at greater risk of diseases like pneumonia [42, 71] . Additionally, only a small percentage of farms weighed calves, which is a vital monitoring requirement to ensure they are reaching key performance targets. Failure to meet performance targets throughout the rear period may lead to a greater age at conception and first calving. As the cost of rearing is approximately GBP 1819/heifer [72] , it is expensive to wait until calving before identifying issues stemming from the rearing phase [73] . The ability to categorise performance in the pre-and post-weaned periods allows producers to evaluate the effectiveness of their nutrition, health and rearing environment in enabling optimum growth in calves. Regarding hygiene, a large proportion of the farms did not use or measure disinfectant when cleaning calf pens, meaning there is a likelihood that their hygiene management was not effective. If the level of cleaning between groups of calves is insufficient there is a risk of pathogens being transmitted from calves to others subsequently housed in the same pens [74] . Furthermore, there as a large proportion of farms were rearing calves for 9 or more months of the year, there would likely be a gradual increase in the level of microbes in the calf house potentially increasing the risk of calf enteritis [17] .@story_separate@A large degree of variation was observed in nutrition, hygiene and housing management across farms in this study. Observations within the study that may negatively impact calf performance and health included small space allowance for calves, suboptimal ventilation, and inadequate drainage and it is suggested that the housing management of pre-weaned calf housing in Northern Ireland may be a limiting factor for calf performance and health. Additionally, the specification and condition of physical housing on farms may limit the ability of farmers to adopt improved housing and hygiene practice. Increased availability of products and materials that facilitate simpler management of calves and ease of cleaning may improve uptake of best practice. Sub-optimal nutritional and rearing environment can be identified through measuring of calf growth performance; however, this was not carried out routinely on the majority of farms. Increasing the monitoring of calf liveweight would provide a cost effective indication of performance efficiency in pre-weaned period, which allows changes to be made long before the animal reaches first calving. Similarly, regular measuring of inputs would increase the accuracy of nutritional and hygiene factors that impact rearing success. By measuring these inputs and outputs, farmers would be in a better position to ensure their system is enabling the rearing of healthy, productive calves. This paper identifies several weaknesses as well as strengths in dairy calf management in NI. Describing these presents the opportunity for research and knowledge transfer to target key areas within calf rearing to the benefit of calf health, welfare and performance, and subsequently dairy farm resilience and profitability.","SIMPLE SUMMARY: Calf health, welfare and performance in the first few months of life are important for the lifetime productivity of dairy cows. Therefore, all aspects of management, including nutrition, hygiene, housing and health, should be optimal to maintain productive calves. As there is little information surrounding these areas on Northern Irish dairy farms, this study investigated 66 farms with the objective of describing common trends and identifying factors which have the potential to compromise calf health, welfare and performance. A large degree of variation was seen in calf housing and other management practices across the farms. The majority of housing, in terms of building design and functionality, was identified as sub-optimal and is likely a contributing factor for selection of particular management practices. Regular monitoring of management practices such as measuring quantity of feedstuffs provided and animal live weight allows farmers to assess the effectiveness of their rearing system. However, this study highlighted a lack of appropriate monitoring and measuring in several key areas that present a risk for calf health and performance. ABSTRACT: The first few months of life are of great importance to the longevity and lifetime performance of dairy cows. The nutrition, environment and healthcare management of heifer calves must be sufficient to minimise exposure to stress and disease and enable them to perform to their genetic potential. Lack of reporting of farm management practices in Northern Ireland (NI) makes it difficult to understand where issues impacting health, welfare and performance may occur in the rearing process. The objective of this study was to investigate housing design and management practices of calves on 66 dairy farms across NI over a 3-month period and also identify areas that may cause high risk of poor health and performance in dairy calves. An initial survey was used to detail housing and management practices, with two subsequent visits to each farm used to collect animal and housing-based measurements linked to hygiene management, animal health and performance. Large variations in key elements such as weaning criteria and method, calf grouping method used, nutritional feed plane, and routine hygiene management were identified. The specification of housing, in particular ventilation and stocking density, was highlighted as a potential limiting factor for calf health and performance. Lack of measurement of nutritional inputs, hygiene management practices and calf performance was observed. This poses a risk to farmers’ ability to ensure the effectiveness of key management strategies and recognise poor calf performance and health."
"A growing body of evidence suggests that respiratory rate, also known as respiratory frequency (f R ), is a fundamental variable to be monitored in different fields. In healthcare, f R is a vital sign which provides information on clinical deterioration, predicts cardiac arrest, and supports the diagnosis of severe pneumonia [1] [2] [3] [4] [5] . Furthermore, f R responds to a variety of stressors, including emotional stress, cognitive load, cold, and hyperthermia [6] [7] [8] [9] . During exercise, f R is a good marker of physical effort and fatigue [10] [11] [12] [13] [14] [15] [16] [17] and is associated with exercise tolerance in different populations [14, 18] . Recent advances in the understanding of the control of ventilation corroborate the importance of monitoring f R and explain why f R but not tidal volume (V T ) (the other component of minute ventilation) responds to a variety of non-metabolic stressors [7, 11, 12, 17, [19] [20] [21] [22] . Likewise, technological development in the field of sensors and techniques for measuring f R is growing exponentially, and a series of measurement solutions are currently available [23] [24] [25] [26] . The ever-increasing interest in technological solutions for respiratory monitoring is manifested by the number of recent reviews published on this topic [23] [24] [25] [26] [27] [28] . These reviews describe the advanced state of the art of the development 2 of 46 of measurement systems for monitoring f R and other ventilatory variables [23] [24] [25] [26] [27] [28] . Nevertheless, one of the main challenges commonly highlighted is the limited use of respiratory systems in everyday-life monitoring. This issue is especially evident from the findings of a recent systematic review by Vanegas et al. [26] . Indeed, advances in respiratory physiology, applied sciences, and technology are not yet accompanied by a large diffusion of effective respiratory monitoring services in different fields. For instance, f R monitoring is not performed routinely in healthcare or in the field of sport and exercise [14, 24, 26] . A major factor determining this limitation is the inadequate establishment of synergies between the different disciplines related to respiratory monitoring. This review proposes the adoption of a multidisciplinary approach to respiratory monitoring as a solution to improve the development and efficacy of f R monitoring services. We present a solid physiological rationale explaining why f R is particularly sensitive to different non-metabolic stressors, thus corroborating the importance of f R monitoring for different applications. Furthermore, we show how the understanding of the f R response to different stressors facilitates the identification of suitable sensors and techniques for f R monitoring in different measurement scenarios. Related implications for the development of measurement systems, algorithms, validation procedures, and respiratory monitoring services are discussed in this review. Briefly, Section 2 describes this approach in detail for different fields of use and applications, while Section 3 builds on such a multidisciplinary approach to propose an original framework for the development of respiratory monitoring services. Current challenges and directions for future research in the field of f R monitoring are discussed in Sections 4 and 5.@story_separate@This section presents a series of monitoring goals where the measurement of f R is invaluable, but with no attempt to cover all the potentially relevant applications. These goals are organized in different subsections, each of which is composed of two parts: (1) Current evidence; and (2) Measurement and Computing. The ""Current evidence"" sections present the importance of f R monitoring for the specific goal identified, while the ""Measurement and computing"" sections describe suitable sensors and techniques to monitor f R in specific measurement scenarios, which are taken as examples (see Figure 1 for a schematic representation). With this structure, we show how the choice of the f R measurement technique depends on specific monitoring goals and measurement scenarios and is facilitated by the understanding of how f R responds to different stressors. Accordingly, we provide the reader with specific examples on how to use available technologies for different applications and fields of use. When relevant, we also comment on the need to complement f R monitoring with the measure of other ventilatory variables (e.g., V T ), and on the physiological rationale underlying this need. Breathing is a vital physiological function of the human body. It guarantees gas exchange, acid-base balance regulation, and other homeostatic functions even under stressful conditions. As such, f R is one of the most fundamental vital signs [1, 29] . Normal f R values (eupnea) range from 12 breaths/min to 20 breaths/min in adults [30] , while the normal values for children vary according to age [31] . Different stressors acting on the human body determine variations in f R outside the eupnea range, and this topic is covered in detail in the following subsections of Section 2. Differently, here we discuss the importance of detecting the presence of breathing per se, which has relevant implications for different fields of use. For instance, it is valuable for survivor identification in civil and military rescue scenarios [32] and for children below one year of age that are at risk of sudden infant death syndrome [33, 34] . Furthermore, the assessment of breathing is fundamental in cardiorespiratory resuscitation. This evaluation is usually performed by manual counting, although even trained medical students and healthcare professionals may find this task challenging [35, 36] . Hence, the objective measurement of f R in cardiopulmonary  Breathing is a vital physiological function of the human body. It guarantees gas exchange, acidbase balance regulation, and other homeostatic functions even under stressful conditions. As such, fR is one of the most fundamental vital signs [1, 29] . Normal fR values (eupnea) range from 12 breaths/min to 20 breaths/min in adults [30] , while the normal values for children vary according to age [31] . Different stressors acting on the human body determine variations in fR outside the eupnea range, and this topic is covered in detail in the following subsections of Section 2. Differently, here we discuss the importance of detecting the presence of breathing per se, which has relevant implications for different fields of use. For instance, it is valuable for survivor identification in civil and military rescue scenarios [32] and for children below one year of age that are at risk of sudden infant death syndrome [33, 34] . Furthermore, the assessment of breathing is fundamental in cardiorespiratory resuscitation. This evaluation is usually performed by manual counting, although even trained medical students and healthcare professionals may find this task challenging [35, 36] . Hence, the objective measurement of fR in cardiopulmonary resuscitation procedures might help in emergency management. While the accurate and objective monitoring of fR would also prove of great value for a variety of other applications, fR is often the least recorded vital sign [14, 29, 37, 38 ]. The first requirement for any effective respiratory monitoring service is the need to obtain a good respiratory signal (respiratory waveform). This is particularly relevant when the aim is to detect the presence of breathing per se, as portions of low-quality signal may impair the possibility to unambiguously distinguish whether the user is breathing or not. However, this goal is  The first requirement for any effective respiratory monitoring service is the need to obtain a good respiratory signal (respiratory waveform). This is particularly relevant when the aim is to detect the presence of breathing per se, as portions of low-quality signal may impair the possibility to unambiguously distinguish whether the user is breathing or not. However, this goal is complicated by the fact that the quality of the respiratory signal is influenced by numerous factors, including the type of sensors, the front-end/back-end electronics, the sensor(s) placement, undesired human movements, and environmental factors. A possible solution to address this issue is the assessment of the quality of the respiratory waveform before f R values are obtained, as even a suitable and validated sensor may provide a low-quality signal under specific circumstances (e.g., misplacement of the sensors). While this approach is not yet common in respiratory monitoring, a quantitative assessment of the signal-to-noise ratio has been proposed by some researchers, with promising results. Given the indirect nature of f R measurement from the electrocardiographic (ECG) and photoplethysmographic (PPG) signals, it is not surprising that such an approach of respiratory signal quality assessment has been used to a great extent when f R is extracted from these signals. For instance, signal quality indices (SQI) are well-established indicators used to identify the presence of artifacts in the ECG and PPG signals and to improve the robustness of f R estimation algorithms [39] . It has further been suggested that ad-hoc respiratory quality index algorithms based on Fast-Fourier Transform, Autoregression, Autocorrelation, and the Hjorth Parameter Complexity perform better than classical SQI in identifying Sensors 2020, 20, 6396 4 of 46 poor-quality respiratory waveforms extracted from raw PPG and ECG signals and in estimating f R [40] . In fact, the assessment of respiratory waveform signal quality can be applied to signals collected with a variety of respiratory sensors and is particularly useful for signal selection when different sensors are used simultaneously. An example is the evaluation of the quality of signals collected with different strain sensors attached to the chest and abdomen, which is particularly relevant when the respiratory signal is affected by motion artifacts during physical activity [41, 42] . A similar approach was used by Siqueira et al. [43] , who simultaneously recorded the respiratory waveform with multiple tri-axial accelerometers positioned on the chest and the abdomen. They found that a method based on independent component analysis was suitable to extract the respiratory waveform blindly, and that the quality of the respiratory signal was influenced by the sensor location [43] . The use of a SQI was also proposed for the quantification of the signal-to-noise ratio of respiratory signals recorded with a thermal camera [44] . The authors developed a SQI ranging from 0 to 1, which is based on four features that take both high-frequency and low-frequency noise into account [44] . The rescue of trapped victims is a typical example of a measurement scenario where the detection of the presence of breathing is of great value [32] . Contactless techniques can be used for victim identification, as the ultra-wideband (UWB) through-wall radar provides an estimation of f R , while calculating at the same time the distance between the radar and the human subject [45] . This feature of UWB radars is essential for survivor identification and location. However, a low signal-to-noise ratio can be found in complex environments and may result in significant errors in the estimation of f R and distance. This problem can be counteracted with the development of robust algorithms as proposed by Shikhsarmast et al. [45] , who implemented a random-noise denoising and clutter elimination algorithm using wavelet transform. Other approaches are based on complex signal demodulation techniques and frequency accumulation methods to suppress mixed products of the heartbeat and respiration signals and spurious respiration signal harmonics [46, 47] . When the presence of breathing needs to be assessed, it is preferable to measure f R on a breath-by-breath basis (see Table 1 for a summary). Substantial evidence suggests that an elevated resting f R is associated with cardiac arrest [1, 2, [48] [49] [50] [51] . Indeed, f R was found to be the most accurate vital sign to predict this adverse cardiac event [2, 48, 50, 51] , and this is why f R has the highest weight in the cardiac arrest prediction model developed by Churpek et al. [2] . In this model, progressively higher scores are attributed to f R values > 20 breaths/min, with the highest score assigned to values > 29 breaths/min [2] . Likewise, Fieselmann et al. [48] found that an f R > 27 breaths/min was a better predictor of cardiopulmonary arrest compared to the heart rate and blood pressure in internal medicine inpatients, and other f R thresholds were also predictive of cardiopulmonary arrest. The rise in resting f R is observed hours before the occurrence of cardiac arrest [48, 50, 51] , thus suggesting that f R monitoring may help in the early detection and management of adverse cardiac events [1] . The prognostic power of f R was also documented in patients with acute myocardial infarction, where f R was found to be an independent predictor of the post-treatment outcome, with a doubling of mortality for every four-breath increment in f R [52] . Furthermore, a study involving more than 900 patients with acute myocardial infarction found that nocturnal f R (cut-off value > 18.6 breaths/min) was a good predictor of non-sudden cardiac death [53, 54] . Likewise, a nocturnal f R ≥ 16 breaths/min was found to be an independent predictor of long-term cardiovascular mortality in older adults [55] . The importance of these findings is not confined to healthcare settings but extends to in-home monitoring of patients at risk. Indeed, out-of-hospital cardiac arrest is a leading cause of cardiac death worldwide [56] , and respiratory monitoring may aid the prediction or early management of such an event [57] . However, f R is still poorly recorded in healthcare [29, 38, [58] [59] [60] , despite substantial evidence of its clinical relevance. This contrasts with the ever-growing increase in technological development observed in the field of respiratory monitoring in the last years [23] [24] [25] [26] . Therefore, we urge the improvement of respiratory monitoring services to help reduce the incidence of cardiac arrest and to lower the associated morbidity and mortality. Prevention of out-of-hospital cardiac arrest is a vital monitoring goal for patients at risk. These patients may require continuous monitoring during everyday life and would benefit from vital sign measurement through wearable devices. Here, we present some techniques suitable to monitor f R in a real-life scenario. Several technological solutions are currently available for the continuous monitoring of the ECG signal, including standard Holter devices, and sensors integrated into patches or garments [61] . When cardiopathic patients wear a device measuring ECG, it is tempting to extract f R from this signal or to use ECG electrodes to measure f R via impedance plethysmography. These two solutions have been commonly employed for respiratory monitoring, leveraging on the fact that no extra device is needed. The morphology of ECG is affected by breathing, which determines the amplitude, frequency, and baseline modulations of this signal [62] . The estimation of f R from the ECG has proven to be successful in specific measurement scenarios, especially during nocturnal recording [53, 54] . f R estimated from the ECG of a Holter device was found to be a good predictor of non-sudden cardiac death, and this association was not substantially influenced by the number of ECG leads [53, 54] . The same study showed good agreement between f R derived from ECG and that measured with a piezoelectric sensor, but only when calculating the local maxima of different ECG-derived respiratory time series and not when using spectral analysis [54] . This suggests that the choice of the algorithm to process the ECG signal is critical. The nocturnal measurement of f R from ECG was found to be suitable also in patients with sleep apnea [63] . Sleep monitoring for cardiopathic patients may also benefit from the recording of breathing sounds to assess the presence of agonal breathing, which is a frequent but under-appreciated diagnostic sign of cardiac arrest [57] . Machine learning algorithms have been developed to classify agonal breathing instances in real-time Sensors 2020, 20, 6396 7 of 46 within a bedroom environment, with simulations showing a sensitivity of 97.24% and a specificity of 99.51% [57] . The estimation of f R from ECG may present some problems during everyday-life activities. Indeed, the error in f R estimation was found to be higher during a driving task compared to sleep, and increased for f R values outside of the 0.1 Hz-0.4 Hz range [63] . An alternative approach is impedance plethysmography, where the ECG electrodes are used to detect respiratory-induced changes in thoracic impedance [24] . However, impedance plethysmography usually underperforms compared to techniques measuring respiration-related chest wall movements with strain sensors. This has been shown in different conditions, including exercise, ambulatory monitoring, and drug-induced respiratory depression [24, 64, 65] . Strain sensors (e.g., resistive, capacitive, and inductive sensors) may be suitable solutions to register the respiration-induced movements of the thorax or the abdomen and measure f R continuously [24] . These techniques can provide real-time streaming of data for remote processing and visualization thanks to small electronics and connectivity capabilities [66] . Breath-by-breath f R monitoring may not be strictly required for cardiopathic patients performing activities of daily life, and average f R values over 60 s are sufficient in most cases. Conversely, the detection of agonal breathing requires the processing of the raw respiratory signal with machine learning algorithms [57] . Sleep apnea is a serious breathing disorder associated with major neurocognitive and cardiovascular sequelae [67] . A causal relationship has been found between sleep apnea and the incidence and morbidity of hypertension, coronary heart disease, arrhythmia, stroke, and heart failure [68] . Furthermore, sleep apnea is associated with poor sleep quality, daytime fatigue, sleepiness, neuropsychiatric disorders (e.g., cognitive impairment and depression), and impairments in the quality of life [69, 70] . Obstructive sleep apnea (OSA) is the most common form of apnea. It affects almost 1 billion people worldwide and its prevalence exceeds 50% in some countries [67] . Obesity is the major risk factor for OSA, but 20% to 40% of OSA patients are not obese [68] . Apnea events are differentiated from hypopnea events but both types concur to the computation of the Apnea-Hypopnea Index (AHI), which describes the severity of the disease [71] . An apnea event occurs when the airflow is absent or nearly absent (drop by ≥90% of pre-event baseline respiration) for at least 10 s, while hypopnea consists in a respiratory drop by at least 30% of pre-event baseline respiration for at least 10 s [71] . Hence, hypopnea detection requires the measurement or estimation of airflow (both f R and tidal volume) [71] . The concomitant use of different sensors is needed for the differential diagnosis of OSA, central sleep apnea (CSA), or mixed sleep apnea, and different guidelines have been provided for children and adults [71] . However, most cases of obstructive sleep apnea remain undiagnosed and untreated, even in developed countries [67] . This is partially due to the laborious procedures required for the diagnostic testing of sleep apnea, which is usually performed overnight in sleep laboratories, involves high costs, and is uncomfortable for patients [72] . Hence, there is a growing interest in the development of cost-effective, noninvasive, and user-friendly solutions for the preliminary identification of sleep disorders or the home-monitoring of patients with sleep apnea [72, 73] . Indeed, the timely diagnosis of sleep apnea and recognition of exacerbations can decrease morbidity, mortality, and the economic burden for healthcare systems. f R monitoring plays an important role in achieving these goals. The choice of measurement techniques for sleep apnea detection depends on specific monitoring goals and scenarios. Here, we describe some of the techniques used for: (1) patients suspected of having sleep apnea (polysomnography) is usually conducted overnight in sleep laboratories. The differentiation between OSA and CSA requires the simultaneous use of different sensors because the recording of chest and abdomen movements is required along with apnea identification. When these movements are present (i.e., the so-called ""respiratory effort"" is observed), the patient is diagnosed with OSA; otherwise, with CSA. Specific guidelines describe the measurement techniques needed as diagnostic tools for sleep apnea identification [71] . Apnea and hypopnea events are identified with the concomitant use of nasal pressure sensors and oronasal temperature sensors. Nasal pressure sensors provide a signal proportional to the square wave of the airflow and are sensitive to even subtle changes in airflow [71] , although their sensitivity is higher at high flow rates compared to low flow rates. However, they may fail to detect or estimate oral airflow. This limitation is overcome with the simultaneous use of oronasal temperature sensors. These sensors (i.e., thermistor, thermocouples, pyroelectric and fiber optic sensors) show low obtrusiveness (a few millimeters in diameter), good response time (from some ms up to some seconds), and a high sensitivity to airflow in the temperature range of interest for respiratory monitoring [24] . On the other hand, the signal from temperature sensors is not proportional to the airflow, which determines an overestimation of low flow rates and an underdetection of hypopnea events [71, 74] . While not considered by current guidelines, humidity sensors may provide a valid alternative to temperature sensors. Indeed, miniaturized relative humidity sensors (typically embedding nanocrystals and nanoparticles) exploit the water vapor differences between inhaled and exhaled air and are characterized by excellent response time (order of 40 ms) [24] . Besides, unobtrusive solutions based on hot-wire anemometers for direct oral/nasal airflow detection are promising and deserve consideration [75] . While apnea is usually detected with nasal pressure sensors and oronasal temperature sensors, the use of respiratory inductive plethysmography (RIP) (consisting of two belts positioned at the thorax and abdomen levels) or polyvinylidene fluoride sensors is recommended for ""respiratory effort"" detection [71] . However, other technologies based on conductive sensors (i.e., piezoresistive, piezoconductive, and capacitive sensors) are suitable for ""respiratory effort"" detection and should be considered in future guidelines. These sensors have been extensively reviewed by Massaroni et al. [24] and can be integrated into garments, belts, straps, and patches. One of the open challenges in the diagnostic testing of sleep apnea is the identification of hypopnea events, as the use of different criteria and sensors may result in marked differences in AHI values [76, 77] , with important implications for disease identification, severity grading, and clinical decision making. A hot topic in sleep apnea research is the development of home sleep apnea testing procedures for the out-of-lab diagnosis, which requires the identification and use of less obtrusive solutions. Among the proposed technologies, tracheal sound measurement is a sensitive, reliable, and noninvasive technique [78] [79] [80] . When a microphone is placed at the suprasternal notch, tracheal sounds effectively detect sleep apnea events, even those missed by nasal pressure sensors due to mouth breathing or nose obstruction [79] . Hence, tracheal sound sensors meet the oronasal flow evaluation criteria for apnea detection required by the American Academy of Sleep Medicine, and can thus be used as alternatives to temperature sensors [79, 80] . Furthermore, these sensors can provide additional useful information on snoring sounds and sleep/wake status discrimination [78, 81] . Acoustic sensors can also be used in home settings when the aim is not to perform a diagnostic test for sleep apnea identification but to monitor the patient on a routine basis. To this end, sleep apnea can be detected with a mobile phone built-in microphone [81, 82] . Other available techniques for apnea monitoring include the use of camera sensors for the recording of surveillance videos that can be post-processed to retrieve apnea episodes [83, 84] . Besides, techniques based on instrumented items (e.g., sleep mats) have also been designed and tested, but further research is needed to improve their sensitivity to sleep apnea detection [85] . In patients with cardiac implants, Defaye et al. [86] provided a valid solution for night-to-night apnea monitoring using an implantable transthoracic impedance sensor. They observed a sensitivity of 100% and a specificity of 80% for sleep apnea and hypopnea detection, with important implications for the clinical management of this patient population [86] . Apnea detection requires the acquisition and storage of raw respiratory data because manual scoring is often performed [87] . On the other hand, several computing techniques have been used for the automatic detection of apnea, hypopnea, and related scores, including amplitude and adaptive thresholding, linear and kernel methods, tree based models, artificial neural networks, deep learning, and fuzzy logic systems and networks [87] . Pneumonia is a leading cause of post-neonatal death in children under-five years [3, 88] . The World Health Organization guidelines suggest that f R should be integral to the pneumonia diagnostic pathway [3, 88] , especially in low-and middle-income countries, where timely pneumonia diagnosis is a much greater challenge because of limited resources [88] . This issue is of great relevance, considering that childhood pneumonia deaths could be prevented with simple interventions and appropriate treatments [89] . f R cut-off values for severe pneumonia correspond to ≥60 breaths/min, ≥50 breaths/min, and ≥40 breaths/min for children who are <2 months of age, between 2 months and 11 months, and between 12 and 59 months of age, respectively [3] . Pneumonia is a serious infectious disease for other populations as well, including older adults [90, 91] and patients with chronic obstructive pulmonary disease [92] . Furthermore, pneumonia outbreaks, as the pandemic caused by the SARS-CoV-2 virus (COVID-19 disease), constitute major medical, social, and economic challenges worldwide [93] . f R monitoring may prove to be of great value in these circumstances, given the clinical relevance of f R in the diagnosis, prognosis, and clinical management of COVID-19 [5] . Given the fact that f R is altered substantially by pneumonia, f R is among the variables used to define criteria for the diagnosis of severe pneumonia (≥30 breaths/min) and for the achievement of clinical stability (≤24 breaths/min) [94, 95] . A large body of evidence suggests that f R is an important prognostic marker and a predictor of mortality in patients with pneumonia [95] [96] [97] [98] , but not all the studies entirely support this notion [90, 99] . Different findings between studies may be partially due to the fact that f R is too often not accurately measured in the context of pneumonia [5, 88, 89, 100] . Given the clinical relevance of f R for the management of this disease, it is essential to use accurate systems for f R measurement. The COVID-19 pandemic has rapidly increased awareness of the importance of effective respiratory monitoring [5, 101] , which is an unprecedented opportunity to solve long-standing issues related to f R monitoring in the context of pneumonia. Here, we focus on the measurement techniques suitable for pneumonia monitoring in children, a condition presenting some peculiar challenges, including high resting f R values (especially in newborn babies) and the possible presence of artifacts in the respiratory signal due to movement and crying. A particularly relevant measurement scenario is that of pre-hospital settings in low-income countries, where the affordability of measurement systems and their simplicity of use are additional factors to take into account [88] . Methodological inconsistencies across studies have so far resulted in difficulties in the identification of suitable techniques to measure f R in such a scenario [89, 100] . Despite the important limitations of manual counting [88] , this is still a commonly used method to measure f R and is even selected as a reference method for validation studies [89] . Indeed, the choice of the reference system is a critical problem, as highlighted by a recent systematic review on the technological solutions available to measure f R for pneumonia identification in children [89] . The authors reported great heterogeneity in the selection of reference systems, which may impact on the quality of some of the reviewed studies and limit the possibility to compare the performances of techniques tested in different studies [89] . Nevertheless, some contactless solutions appear promising [89] . Some of these technologies measure f R from the detection of respiration-induced body movements, including depth sensors, radiofrequency sensors, and RGB (red, green, blue) camera sensors [23, [102] [103] [104] . When the respiratory waveform is obtained from video image recordings, magnification algorithms can be used to improve the signal-to-noise ratio, especially when small movements of the chest wall are observed [102] . Alternatively, solutions based on the use of pressure or strain sensors embedded in mattresses or other bed components can be used to obtain accurate f R values [105] . All these techniques are relatively cheap and can prove useful in non-collaborative subjects like newborns and children, with no need to attach sensors on the patient's body. Thermal cameras and laser vibrometry sensors are other interesting solutions for the contactless monitoring of newborns in clinical scenarios [44, 106] , but their cost is relatively high [23] . On the other hand, contact-based solutions such as nasal pressure sensors, oronasal thermistors, and impedance plethysmography are currently used as diagnostic tools for sleep apnea in children [107] . These are suitable techniques for continuous f R monitoring but are not practical for routine vital sign monitoring of patients suspected with pneumonia, especially in low-income countries. Breath-by-breath f R monitoring is not strictly needed in this context, and current UNICEF guidelines on diagnostic aids for acute respiratory infection require accuracy of ±2 breath/min over a recording period of 60 s [89] . While a series of contact-based and contactless techniques fulfill this requirement [23, 24] , so far their development and use have been limited by inadequate consideration of the specific needs of children living in low-income countries [89] . Evidence suggests that f R is an important marker of clinical deterioration for a variety of pathological conditions in both adults [4, [108] [109] [110] and children [111] . Indeed, f R is a fundamental variable included in the majority of prognostic scores developed for the prediction of different outcomes, including intensive care unit (ICU) admission and mortality [4, 109, 110] . As such, f R contributes to the computation of the most accurate prognostic scores developed so far, such as the National Early Warning Score (NEWS) and the Modified Early Warning Score (MEWS) [4] . The NEWS assigns a score to f R values outside of the 12-20 breaths/min range, with the highest score attributed to f R values ≤8 and ≥25 breaths/min [112] , while the highest score for MEWS is attributed to f R values ≥30 breaths/min [113] . A modified version of NEWS (i.e., NEWS2) has shown a good predictive capacity for the identification of in-hospital early mortality (all-cause) even when vital signs were collected at pre-hospital level, with f R showing lower values in survivors compared to non-survivors [114] . f R is also among the signs used for sepsis identification [115] [116] [117] . Furthermore, a nocturnal f R ≥ 16 breaths/min is an independent predictor of long-term all-cause mortality [55] . A further increase in the accuracy of early warning scores is expected with measures performed at different time points as opposed to single measures [116, 118] , thus requiring devices to collect vital signs on a periodic or even continuous basis. This is important for timely critical care assistance because f R may increase several hours before the occurrence of an adverse event [4, 118, 119] , and such f R changes should be promptly identified. However, despite the clinical relevance of f R , this vital sign is often under-recorded [29, 37, 120, 121] or not measured accurately [38, 59, 60, [122] [123] [124] . This may impair the efficacy of early warning scores [118, 120, 121] , which also suffer from other methodological issues [109, 110] . Therefore, it is imperative to improve the accuracy and frequency of f R monitoring throughout the healthcare chain (pre-hospital, hospital, and post-hospital). Vital signs are commonly measured during hospital admission at triage. However, f R is measured by manual counting or is still too often not recorded at all [37, 59, 60, 121] . The important limitations of this current practice have been discussed elsewhere [29, [58] [59] [60] [122] [123] [124] [125] . This section presents some of the suitable techniques to measure f R at hospital admission, with special attention to those allowing for periodic or even continuous monitoring of the patients needing hospital recovery. The extraction of f R from the PPG signal is a practical solution as this signal is obtained from the pulse oximeter, which is routinely used in clinical settings to measure peripheral arterial blood oxygen saturation and heart rate. The pulse oximeter is usually applied at the finger (but also other locations can be used), is non-invasive, easy to use, and is suitable for the continuous monitoring of patients requiring special care. f R can be extracted from the PPG signal because breathing affects this signal by determining the phenomena of baseline wander, amplitude modulation, and frequency modulation [62] . However, the occurrence of these phenomena depends on different factors, including breathing patterns, finger perfusion, health conditions, and body position [62] . This makes f R estimation challenging and explains why a great body of research in this area is focused on the identification of computing solutions to improve the estimation of f R . A plethora of algorithms have been developed for the extraction and fusion of respiratory signals, for f R estimation, for the fusion of f R values obtained from different signals, and for quality assessment [62, 126] . Given the indirect nature of f R estimation from PPG, signal quality assessment is an important process requiring the assessment of both PPG signal quality and respiratory quality indices [62] . Indeed, the accuracy of f R measurement is not only dependent on the quality of the PPG signal but also on the extent of breathing modulation. Despite extensive research in this area, the implementation of algorithms estimating f R from PPG is still not common in commercial devices. One of the exceptions is the Nellcor TM Respiratory Rate Software application (Medtronic, Dublin, Ireland), which showed a good performance when tested in hospitalized patients against the capnography reference method (Mean of difference, MOD ± Limits of agreement, LOAs, 0.07 ± 3.90 breaths/min) [127] . Conversely, lower performances were found in the challenging measurement scenario of patients undergoing sedation and analgesia for endoscopy procedures, with a substantial difference observed between the f R estimated from PPG with the Nellcor TM 2.0 monitoring system (Covidien, Mansfield, MA, USA) and that obtained from capnography (MOD ± LOAs, 2.25 ± 10.60 breaths/min) [128] . Cardiac arrhythmias may also affect the physiological mechanisms responsible for the respiratory modulation of the PPG signal, and thus the quality of f R measurement [62] . Nonetheless, the implementation of algorithms extracting f R into commercial devices opens important avenues for f R monitoring in clinical settings. The current limitations of f R measurement from PPG suggest that other techniques may complement the use of PPG devices at hospital triage. Contactless techniques have some practical advantages over contact-based techniques in this scenario, where the vital signs of several patients need to be recorded over a short period of time. Contactless techniques avoid the problem of sanitizing the measurement device after each use and generally make the patient less aware of the measurement, which matters because measurement awareness affects f R values at rest [60] . Different sensors registering respiration-induced body movements can be suitable for this purpose, including depth sensors, camera-based sensors and radiofrequency sensors [23] . Depth sensors (e.g., Time-of-Flight sensors) are commercially available (e.g., Microsoft Kinect v2, Microsoft Corp., Redmond, WA, USA), provide an accurate measure of f R when the patient is seated [129] , and are less influenced by environmental factors (e.g., ambient light) compared to other contactless techniques [23] . Camera-based sensors and radiofrequency sensors (radar sensors and WiFi sensors) also show relatively good performances when measuring f R in resting patients [130] [131] [132] , and can be used to monitor different patients simultaneously [23] . However, further research is needed to assess the suitability of contactless sensors for f R monitoring in hospital settings. For patients needing hospital recovery, contact-based solutions allowing for continuous monitoring during a hospital stay may prove suitable, and some commercial devices have been developed for this purpose. Subbe and Kinsella [133] have assessed the validity of a wearable commercial device (RespiraSense™, PMD Solutions, Cork, Ireland) in patients admitted to the hospital as medical emergencies. This device measures respiration-related movements through a piezoelectric array located at the lower thorax level. On-board accelerometers and algorithms allow for the detection and partial removal of artifacts such as cough, speech, and motion artifact [133] . RespiraSense™ showed good accuracy when f R (recorded over 15 min periods) was compared to capnography derived f R [133] . This system can be worn for some hours and may increase the robustness of f R measurement by selecting suitable (e.g., without motion artifacts) and multiple portions of the registered signal [133] . Two other FDA-approved wearable devices have been tested for validity, feasibility, and usability in patients admitted to the hospital and transferred to the general ward [118, 134] . The ViSi Mobile system (Sotera Wireless, San Diego, CA, USA) measures f R with impedance sensors attached on the chest [118, 134] , while the HealthPatch (Vital Connect, Campbell, CA, USA) is a disposable adhesive patch with reusable sensors, and extracts f R from the ECG signal and the accelerometer signal [118, 134] . Both devices were successfully used for the continuous monitoring of patients over 2-3 days of hospitalization, but the accuracy of f R measurement was only tested against manual counting performed by nurses [118, 134] . The discrepancy found between the f R values measured with the ViSi Mobile and the HealthPatch and those collected by nurses impacted the computation of the MEWS [118, 134] , thus requiring further validation of the devices against an objective reference system. Use in real clinical settings also highlighted problems with connectivity, data loss, and artifacts affecting the signal [118, 134] , which requires consideration of the improvement and development of respiratory systems for patient monitoring in hospitals. The advantage of these techniques is the possibility to monitor the patient continuously throughout the healthcare chain, which greatly outperforms the current approach of manual counting over 60 s or even shorter periods of time [60] . However, more research is needed to improve the accuracy and suitability of respiratory devices for the assessment of clinical deterioration. Among the factors accounting for f R being a marker of clinical deterioration, the association between f R and dyspnea deserves consideration. Dyspnea is a major symptom in patients with chronic obstructive pulmonary disease (COPD) and other cardiorespiratory diseases [135, 136] , in obese individuals [137] , and in older adults [138] . Furthermore, it is a major determinant of exercise intolerance and sedentary behavior in these populations, with consequent impairments in function and quality of life [135, [137] [138] [139] . While dyspnea is a sensation of breathlessness (i.e., a symptom), an increase in resting f R is its major physiological sign [140] . An association between f R and dyspnea is observed both at rest and during physical exercise. At hospital admission, the resting f R of patients admitted with dyspnea contributes to predicting the occurrence of different clinical outcomes, i.e., the use of non-invasive ventilation, ICU admission, and mortality [121] . The sensitivity of resting f R as a predictor of COPD exacerbations is corroborated by findings from several studies [136, [141] [142] [143] , and is of paramount importance for the early detection and treatment of these adverse events. During exercise, a close association between f R and dyspnea is observed in patients with different respiratory diseases, as similar responses are observed in patients with COPD and in those with interstitial lung disease [18] . Importantly, a neurophysiological link between dyspnea and f R is evident because they are both regulated, at least to some extent, by the activity of areas of the brain relating to motor control, volition, cognition, and emotion processing [11, 21, 22, [144] [145] [146] . On the other hand, dyspnea is a multidimensional sensation composed of three respiratory sensations with somewhat different underlying mechanisms and signs, i.e., respiratory effort, air anger, and chest tightness [145] . For instance, air anger is at least partially regulated by the magnitude of chemoreceptor afferent activity [147] , and may thus be associated with a predominant increase in V T [19, 22, 147] . An emblematic example is the air hunger associated with the deep and regular breathing observed in patients with metabolic acidosis, which is known as the ""Kussmaul's sign"" [148] . Conversely, respiratory effort is at least partially regulated by the central motor drive to the locomotor and respiratory muscles (i.e., central command) [145, 147] , and may thus determine a predominant increase in f R [13, 22] . Given that patients present with various combinations of the afore-mentioned respiratory sensations [147] , the monitoring of f R and V T may help shed some light on the pathophysiological mechanisms underlying dyspnea. As such, respiratory monitoring plays a fundamental role in the detection and management of dyspnea. The assessment of the signs of dyspnea (e.g., an increase in f R ) is particularly relevant during daily life activities (e.g., walking and stair climbing) where this symptom is exacerbated [149] . Here, we present some suitable measurement techniques for respiratory monitoring in this scenario. The need to monitor f R during daily life requires the simultaneous identification of the activities performed by the patient [27] . Indeed, the severity of dyspnea is better described if the levels of f R are interpreted along with the intensity and type of the physical tasks performed [149] . This information can be obtained from inertial measurement unit (IMU) sensors [27] . When located in specific parts of the trunk, IMU sensors may also be used to estimate both f R and the respiratory amplitude [150] [151] [152] . By positioning accelerometers on the thorax and the abdomen, Fekr et al. [151] found that the use of a robust classification algorithm was suitable for the identification of eight different pathological breathing patterns, including the Kussmaul's sign. However, the quality of the respiratory signal obtained from IMU sensors is largely affected by motion artifacts during physical activities [24] . On the other hand, IMU sensors can be used to improve the quality of the respiratory signal obtained with other sensors (e.g., strain sensors), through motion artifact identification and removal [24] . Therefore, it is preferable to complement the use of IMU sensors with other techniques for respiratory monitoring [24] . Strain sensors embedded into garments may prove particularly useful to measure f R in patients with dyspnea, with a preference for those allowing for the estimation of V T (or the respiratory amplitude as a surrogate) [24, 153] . A smart garment designed for measuring physiological signs of dyspnea would benefit from the integration of strain sensors situated in specific locations of the trunk. Indeed, sensor redundancy improves the accuracy of f R and V T measurements [24, 153, 154] , and may help detect other signs observed in patients with dyspnea such as the temporal thoracoabdominal asynchrony between the movements of the thoracic and abdominal compartments [155, 156] . Thoracoabdominal asynchrony is often computed by means of the phase angle analysis, is higher during exercise compared to rest, and increases with exercise intensity [155] . Respiratory inductive plethysmography is a classical technique used to compute thoracoabdominal asynchrony with wearable sensors, and consists of two elastic cloth bands containing insulated wires encircling the rib cage and the abdomen [24] . Similar performances were found when comparing thoracoabdominal asynchrony measured with RIP and optoelectronic plethysmography (the reference system for measuring compartmental volumes [156] ) in healthy individuals and patients with COPD and interstitial lung disease [155] . However, the agreement between the two techniques was higher at rest and during moderate exercise compared to heavy exercise, where a wide variability in the phase angle was observed [155] . Capacitive and resistive sensors also have metrological characteristics that are suitable for monitoring patients with dyspnea [24] . Naranjo-Hernández et al. [157] tested the feasibility of a remote respiratory service for monitoring the f R of COPD patients during the recovery from home-based exercises. The measuring system was a smart vest embedding capacitive sensors, which showed superior performances (MOD ± LOAs, −0.14 ± 0.54 breaths/min) compared to those of some other measuring systems validated in the literature [157] . However, the authors did not assess the performances of the system during exercise, which is an important requirement for f R monitoring in COPD patients and other patients presenting with dyspnea. Chu et al. [153] reported the good performance of small wearable piezo-resistive strain sensors situated at the level of the ribcage and the abdomen when f R and V T were compared with the same variables obtained with a spirometer. The wearable system was tested at rest and during ambulatory conditions, with interesting implications for the remote monitoring of patients with dyspnea [153] . However, the system was only tested on healthy individuals, and the respiratory signals were affected by motion artifacts (e.g., torsion of the trunk) during walking. Further research should focus on the development of wearable systems specifically designed for patients with dyspnea performing daily-life activities. High-quality respiratory waveforms are needed to compute thoracoabdominal asynchrony and compartmental volumes. As such, it is preferable that respiratory systems measuring f R in patients with dyspnea are validated on a breath-by-breath basis. Pain is a leading cause of morbidity worldwide [158] . For instance, pain is a major healthcare issue in postoperative patients [159] and a common problem in patients requiring emergency medical service assistance [160, 161] . It is well-established that pain influences breathing and generally determines an increase in minute ventilation [7, 162] . This effect is mediated by an increase in f R , V T, or both, depending on the nature of the painful stimulus [7, 162] . The hormonal stress response which accompanies acute pain induces a predominant increase in V T [163] , while the psycho-behavioral changes induced by pain (e.g., discomfort, fear, and displeasure) affect f R more. An example is the increase in f R that occurs with the anticipation of pain before the advent of the nociceptive stimulus [164] . The stimulation of nociceptive afferents leads to a predominant increase in f R , which is documented by the elevated f R observed in surgical patients under anesthesia [7] . In a cohort of over 50.000 patients with acute pain, Bendall et al. [165] found that an f R > 25 breaths/min was the most important predictor of pain severity compared to other vital signs such as heart rate and blood pressure. Likewise, among different vital signs, f R showed the strongest association with the severity of pain in over 18.000 patients requiring prehospital emergency medical service assistance due to pain [160] . It is also of note that f R decreases with the administration of commonly used pain drugs (i.e., opioids), which makes f R monitoring important to alert when the patient is at risk of respiratory depression [166] , more so than arterial oxygen saturation measured by pulse oximetry [167] . This matters because opioid-related death is among the major causes of accidental mortality in adults [161] , and brain damage may also occur [166] . Respiratory monitoring is also useful for the evaluation of pain in nonverbal critically ill patients or infants [162, 168, 169] . On the other hand, breathing may affect pain; several clinical and laboratory studies have reported a beneficial effect of slow deep breathing on pain [162] . Slow deep breathing may decrease pain perception through respiratory-induced cardiovascular/autonomic changes (e.g., respiratory sinus arrhythmia and variations in baroreflex activity), the modulation of cortical activity, and psycho-behavioral factors [162, 170, 171] . The effect of slow breathing on pain may improve with the use of respiratory biofeedback strategies [162, 172] . Collectively, these findings suggest that respiratory monitoring is of great importance for pain detection and management. A typical scenario where f R can be used as a marker of pain is in postoperative patients. In this context, the main measuring challenge is the detection of respiratory depression, which may occur as a side effect of the administration of pain drugs (i.e., opioids), especially within 24 h of surgery [65, 166] . Ermer et al. [65] conducted an interesting study specifically targeting the identification of suitable sensors capable of detecting f R values below 10 breaths/min in sedated volunteers. Some methodological limitations of the study require caution in the interpretation of their findings, but useful information for further research have been provided [65] . The authors found that an abdominal accelerometer and a capnometer showed better performances compared to a nasal pressure transducer, an oronasal thermistor, a peritracheal microphone, transthoracic impedance sensors, and photoplethysmography [65] . The last two techniques listed showed the worst performances [65] . However, the sensors were validated against RIP, which may not be an ideal reference technique. This may partly explain the superior performances of the abdominal accelerometer, which was positioned in the same location of the abdominal RIP belt. Besides, a microphone may estimate f R more effectively when located on the suprasternal notch [79] compared to a peritracheal location [65] , and thermistors may underperform compared to other temperature sensors (e.g., pyroelectric sensors) [24] . Another possible limitation of the study is the use of the same algorithm to compare the performances of the different waveforms acquired with the various sensors [65] . In another study, the authors used the same data set to test the efficacy of a machine-learning algorithm in the identification of ataxic breathing severity, using breath-by-breath data of f R and V T collected with the RIP sensors and the nasal pressure sensor [173] . Given that alterations in ventilatory variability are commonly observed under the effect of opioids [173, 174] , the good performances of the support vector machine classifier tested by Elmer at al. [173] provide interesting perspectives on the identification of drug-induced irregular breathing. However, these findings [65, 173] may not directly translate to everyday pain assessment as volunteers were asked not to talk or move and were monitored for relatively short periods of time, while postoperative patients require continuous monitoring [166, 175] . Nonetheless, the study by Elmer et al. [65] highlights the importance of validating different sensors in a situation that resembles some of the characteristics of the measurement scenario of interest (i.e., opioid-induced respiratory depression). While all the sensors tested by Ermer et al. [65] require direct contact with the patient's body, less obtrusive techniques may also prove useful for the continuous monitoring of the f R of patients suffering from pain. Isono et al. [176] tested an interesting solution for estimating f R with four load cells placed under a medical bed. f R was estimated by measuring the centroid shift in the cranio-caudal direction caused by the respiratory-related movements of the visceral organs. Accurate values of f R were obtained in the range of 4 breaths/min to 40 breaths/min in different body positions, while f R was underestimated above 40 breaths/min [176] . A similar solution with load cells under the bed proved valid for the estimation of apnea (100% sensitivity and 97% specificity) and hypopnea events [177] , which makes this application suitable for respiratory depression detection. While non-respiratory movements may negatively affect the estimation of f R , the use of load cells facilitates the identification of movement artifacts. Given the importance of detecting respiratory depression and irregular breathing induced by opioids, breath-by-breath monitoring of f R and V T is advised, although rarely performed, in the current clinical practice. Breath-by-breath monitoring and validation are also important requirements when measurement systems are used to alleviate pain through respiratory biofeedback. On the other hand, average f R values over 60 s may provide sufficient information for the assessment of the pain-induced increase in f R . Along this line, the American Society of Pain Management Nursing Guidelines require that ""respirations should be counted for a full minute and qualified according to rhythm and depth of chest excursion while the patient is in a restful/sleep state in a quiet unstimulated environment"" [178] . It is well established that emotions affect ventilation, with a preferential influence exerted on f R rather than V T [8] . This is not surprising considering that f R has been defined as the behavioral component of minute ventilation [19, 20, 22] . f R increases with experimentally-induced anticipatory anxiety, unlike V T , oxygen uptake or carbon dioxide output [179] . This increase in f R is positively related to individual trait anxiety scores [179] . Besides, f R is sensitive to changes in affective valence and arousal [180] . This makes f R a good candidate to identify emotional states in a variety of conditions and populations. For instance, f R increases during panic attacks [7, 181] and may discriminate between different pathological conditions; it is higher in patients with panic disorder compared to those with social phobia [182] . The fact that f R is a good marker of emotional stress can be attributed to the fact that f R is partially regulated by the activity of areas of the brain involved in emotional processing [8, 183] . Indeed, direct stimulation of the amygdala produces a rapid increase in f R [8] . On the other hand, the pattern of breathing influences emotions since voluntary breathing techniques (e.g., slow deep breathing) may attenuate negative emotional states [184] . Hence, the understanding of the interrelationship between breathing and emotions is fundamental to provide insight on how to treat anxiety, stress, depression, and emotional disorders [184] . When respiratory monitoring is purported to detect emotional stress, unobtrusiveness is an important requirement for the choice of the technique, as measurement awareness and obtrusive technologies may affect the individual emotional state and ventilatory responses [24, 60] . Here, we present two measurement scenarios: (1) emotion recognition in the laboratory; (2) emotional stress detection in everyday life. In research laboratories, f R is among the signs that may help recognize and classify emotions, along with heart rate, heart rate variability, galvanic skin response, body temperature, body posture, and facial expressions [180, 185, 186] . Contactless techniques are suitable for monitoring f R in this scenario, and the use of techniques that can simultaneously record other relevant signals is particularly valuable. For instance, a thermal camera can be used to retrieve f R and detect facial expressions at the same time from thermal video frames [187, 188] . With this technique, f R estimation is performed by analyzing respiration-induced changes in pixel intensity in a specific region of interest (at the level of the nose or mouth) [187] . However, the post-processing of video images is generally time consuming when compared to the majority of contact-based techniques, and infrared video images are usually analyzed after data collection. Other contactless sensors that can simultaneously register f R , face expressions and cardiovascular variables are RGB camera sensors and depth sensors [23, 189, 190] . When the area of the upper chest is filmed, RGB camera sensors can be used to retrieve respiration-induced body movements from the post-processing of video images [132] . Alternatively, if the face of the user is recorded with a camera, RGB camera sensors can be used to extract f R from the modulation of the video PPG signal [189] . The understanding of the interrelationship between breathing and emotions depends on the accurate characterization of a number of respiratory features that can be extracted from the respiratory waveform [191, 192] . Noto et al. [192] developed an open-source tool box (BreathMetrics) that automatically extracts a number of meaningful features embedded in human nasal airflow recordings. These include f R , V T , inspiratory and expiratory time, and inspiratory and expiratory pauses [192] . The use of the nasal flow measure was dictated by the close link between nasal flow and the activity of olfactory and limbic areas of the brain, but the authors are also trying to extend BreathMetrics functionality to respiratory waveforms obtained from sensors measuring the movements of the chest wall [192] . This would favor the recording of some important respiratory features in real-life scenarios. For instance, the possibility to record sigh events and ventilatory variability may further our understanding of the ventilatory response to emotional stressors [191] . Indeed, sighs and ventilatory variability are important elements in the regulation of breathing and emotions, with implications for the management of emotional stress and the prescription of therapeutic interventions in different diseases [191] . The respiratory waveform can also be analyzed with deep learning emotion recognition models, as good accuracy in the estimation of affective valence and arousal was found by Zhang et al. [180] . These findings open interesting perspectives for the real-life monitoring of emotional states. Considering the aforementioned requirements, strain sensors recording the movements of the chest wall appear to be particularly suitable solutions to monitor emotion-related changes in f R during everyday life, with a preference for resistive, capacitive, and inductive sensors [24] . The metrological characteristics of these sensors are detailed in a previous review by Massaroni et al. [24] . Strain sensors can be embedded into straps, bands, and t-shirts, and the electronics can provide real-time analysis and data streaming. Since the quality of the respiratory waveform affects the possibility of obtaining important respiratory features [192] , it is preferable that the measurement systems used to detect the ventilatory response to emotional stress are validated on a breath-by-breath basis. This requirement is also needed for systems intended to provide ventilatory variability indices and respiratory biofeedback support for emotion management (see also the ""2.13. Respiratory biofeedback"" section). It is well documented that f R , unlike V T , is sensitive to a variety of cognitive tasks and increases in proportion to the difficulty of the task [9] . It is, therefore, evident that f R is the ventilatory variable that preferentially reflects cognitive load [9] . At rest, tasks like mental arithmetic, inhibition tasks, and working memory determine an increase in f R , with either no changes or a decrease in V T [6, 9, [193] [194] [195] . Hence, f R monitoring may help the detection of cognitive load in a variety of scenarios. This is particularly relevant for workers exposed to mentally demanding tasks and weighty responsibilities, including surgeons, soldiers, and pilots [9, 195] . The variability of breath-by-breath f R may provide additional insight into how f R responds to cognitive load, but experimental evidence is scant and further studies are required to elucidate this issue [9] . The fact that f R is sensitive to cognitive load is preserved during exercise; a cognitive task superimposed to physical exercise increases f R compared to the sole physical task condition [196, 197] . This has important implications for monitoring the extra load imposed by cognitive tasks during a variety of working and sporting activities that are characterized by different levels of psychophysical stress. The fact that f R substantially responds to cognitive load suggests that f R may at least partially be regulated by the activity of brain areas involved in cognitive processing. This input to ventilation has been defined as the ""wakefulness drive to breathe"", i.e., an increase in central neural activity or arousal, similar to alertness or awareness [6] . While it has also been suggested that the increase in f R may reflect the metabolic demand of the cognitive task [9] , this interpretation is unlikely in light of the notion that metabolic inputs do not play a substantial role in the regulation of f R [11, [19] [20] [21] [22] . Hence, f R is a sensitive marker of the cognitive effort exerted in a task, with important implications for the health and performance of a variety of workers [198] [199] [200] . The quantification of cognitive load is of great relevance for numerous working activities. Here, we present measurement techniques that can be used to continuously monitor f R during both static and dynamic working activities. Typical examples of workers reporting cognitive load under static activities are pilots, drivers, and computer workers [195, 201, 202] . As reported in a recent review of vital sign monitoring in automotive environments, a variety of techniques can be used for measuring f R [203] . Indeed the car can be equipped with different sensors located in the seat, the backrest, the safety belt, the steering wheel, or the cockpit [203] . Interesting solutions include the use of strain/pressure sensors, camera-based sensors, and radar sensors [203] . Strain/pressure sensors have been used more often than the other solutions [203] , and relatively good performances were reported in some studies [204] . Camera-based techniques are promising for obtaining accurate f R values in this measurement scenario, but these solutions have received limited attention so far [203] . Several factors may explain the limited use of camera-based sensors in automotive environments [203] , including privacy issues (especially when the face of the user is captured), the computational processing load of video images, and variable light conditions [203] . Another open challenge common to the afore-mentioned techniques is the susceptibility to motion artifacts (e.g., vibrations of the car). As such, Leonhardt et al. [203] suggest the simultaneous use of different respiratory sensors and the development of sensor fusion algorithms to provide a more robust measure of f R . Optical sensors, radiofrequency sensors, and strain/pressure sensors embedded in instrumented chairs are also suitable for monitoring computer workers [23, [205] [206] [207] [208] . Breath-by-breath f R estimated from video recordings is generally more accurate compared to other contactless techniques, with errors below 4 breaths/min in the 10-40 breaths/min f R range [206] . Cognitive load is also common in a variety of workers performing dynamic tasks, including soldiers [200] and healthcare professionals (e.g., nurses) [198] . Contact-based techniques are the best candidates to monitor f R in these workers [24] . The sensors measuring chest wall movements appear more suitable than others, especially strain sensors. These sensors register changes in strain determined by respiration-related movements of the chest wall, and can be easily integrated into smart garments in the case of resistive, capacitive, and inductive sensors [24] . The accuracy of strain sensors is generally higher compared to that of contactless techniques (errors even lower than 1 breaths/min) [24] . Besides, the use of strain sensors is more suitable for breath-by-breath f R monitoring compared to contactless techniques, as they require less computational resources compared to optical sensors, where a high quantity of information is processed to extract the respiratory waveform. Furthermore, strain sensors can be combined with other movement sensors (e.g., IMU) to reduce the influence of motion artifacts and improve the robustness of breath-by-breath f R monitoring, even in real-time [24] . Breath-by-breath monitoring of f R is required when attempting to gain insight into cognitive load by means of ventilatory variability analyses. Consequently, breath-by-breath validation is advised. Conversely, when monitoring cognitive load by means of f R changes over time, average values over 60 s provide sufficient information. Evidence shows that f R is very sensitive to different environmental stressors, including heat, cold, and hypoxia. Numerous studies suggest that f R is the primary component of minute ventilation that responds to the heat stimulus [7, 209] . A predominant increase in f R with heat is observed both at rest and during exercise [7, 209, 210] , where a good association between f R and body temperature is generally found [210] . This association has important implications for the identification of workers at risk of heat strain [211, 212] , including those wearing protective garments (e.g., firefighters), those working in tropical climates, soldiers, agricultural workers, and individuals participating in major events organized in hot environments (e.g., sporting competitions). While the quantification of environmental factors (e.g., temperature and humidity) is useful to predict the risk of thermal strain, f R monitoring is essential to understand the individual response to hot-environment exposure. Indeed, the attainment of critical levels of body temperature may derive from the combined effects of environmental-induced stress, equipment used and physical activity (a major source of body temperature increase) [212] [213] [214] . f R is also sensitive to cold-induced stress, especially when sudden cold occurs. An emblematic and dangerous condition is the response to cold water shock, where f R increases very rapidly and reaches values even higher than 60 breaths/min [7, 215] . Conversely, a preferential increase in V T is observed under prolonged cold as a result of the metabolic demands of shivering [7, 19] . On the other hand, f R reflects a cold-induced reduction in exercise capacity, as it increases with cold water immersion [216] and prolonged rain [217] compared to control conditions. These findings have implications for the monitoring of workers operating in cold conditions, including soldiers and maritime workers [218] . f R is also sensitive to hypoxia both at rest and during exercise [7, 219] , with important implications for individuals working in low oxygen environments [220, 221] . Therefore, f R monitoring is fundamental for workers exposed to a variety of environment-induced stressors, both in terms of health safety and work productivity. The need to face environment-induced heat strain is a typical requirement for individuals working in challenging environments. Here, we present the main measurement techniques suitable for f R monitoring in hot environments. Some of the workers facing heat challenges wear masks as personal protective equipment. Examples are self-contained breathing apparatus used by firefighters or soldiers [222] and face masks used by healthcare professionals facing outbreak challenges (e.g., the 2013 Ebola virus West Africa outbreak) [214] . A variety of sensors can be integrated within a mask for f R monitoring. These include airflow sensors (e.g., miniaturized pressure sensors and hot-wire anemometers), temperature sensors (e.g., thermistors, thermocouples, and pyroelectric sensors), humidity sensors, and acoustic sensors [24] . The performances of recently developed humidity sensors deserve special consideration in this context. He et al. [223] reported that graphene nanochannels confined poly-dopamine humidity sensors embedded in a mask show high sensitivity, ultrafast response (20 ms), and little humidity hysteresis. These performances were not substantially affected by high relative humidity (~75%), wind (up to 10 m/s) or physical activity [223] . Furthermore, the same sensors may even be capable of voiceprint recognition [223] , thus making it possible to recognize when the respiratory signal is affected by speech without the need for additional acoustic sensors. This is an important feature for the continuous monitoring of f R in real-life working scenarios. For those individuals not wearing protective masks, f R can be monitored with sensors embedded in belts or garments. Different commercial devices have been developed for vital sign monitoring in occupational settings. These include Zephyr TM BioHarness TM (Zephyr Technology Corporation, Annapolis, MD, USA) (i.e., a belt embedding capacitive sensors) [224] , Equivital TM EQ02 LifeMonitor (Hidalgo, Cambridge, UK) (i.e., a belt embedding inductive sensors) [225] , LifeShirt TM (Vivometrics, Ventura, CA, USA), and Hexoskin ® (Carre´Technologies Inc., Montreal, Canada) (i.e., shirts embedding inductive sensors) [226, 227] . These devices generally show good accuracy for f R measurement even during exercise [14] . The performances of the Zephyr TM BioHarness TM were also tested during 40 min of submaximal exercise in a hot environment and found to be relatively good (MOD ± LOAs, 0.2 ± 8.3 breaths/min), but not as good as those observed during exercise in temperate conditions (MOD ± LOAs, −0.6 ± 5.0 breaths/min) [224] . This difference is possibly due to the fact that moisture affects the properties of the capacitive sensors [224] . Besides, the comfort of some of these devices could be improved, and sensors directly integrated into smart textiles are attractive alternative solutions. Several factors should be considered when developing smart clothing for hot environments and extreme environments in general. Not only may different sensors change their properties with environmental factors (e.g., temperature and humidity), but conductive wires may also be affected, depending on the fabric of the smart textile [228] . When dealing with the choice of suitable sensors, the use of fiber optic sensors is encouraged, as their performance is not affected by changes in relative humidity [229] . However, despite recent advances in the field of respiratory monitoring with fiber optic sensors [28, 229] , further research and development are needed to use this technology during real-life working activities [28, 229] . In an attempt to characterize the performances of smart textiles in challenging environments, Torreblanca González et al. [228] have developed a methodology for testing the effect of environmental factors on specific components of a smart textile. This methodology or similar approaches should be used to guarantee the correct functioning of smart garments designed for f R monitoring in challenging environments. In most of the cases, breath-by-breath f R monitoring is not necessarily required for the detection of environment-induced stress, and data averaged over 60 s provide sufficient information. As such, most of the commercial devices used in occupational settings have been validated over 60-s long time windows [224, 226, 227] . More detailed information (e.g., 10-s average f R values) may be required for specific needs, like for a proper description of the cold shock response [215] . As recently reviewed by Nicolò et al. [14] , f R is one of the most important variables to be monitored during sport and exercise. It is closely associated with perceived exertion during exercise protocols with different durations, formats (e.g., continuous and intermittent) and modalities (e.g., cycling and running) [10, 12, 13, 15, 16, 230] , at least during high-intensity exercise [11] . Furthermore, it is associated with exercise tolerance under a variety of experimental conditions, including hyperthermia, cold, hypoxia, muscle damage, muscle fatigue, dietary-induced glycogen depletion, respiratory muscle fatigue, and prior exercise [14, 20, 21] . Conversely, other physiological variables such as oxygen uptake, blood lactate, and heart rate may not be associated with perceived exertion and exercise tolerance in at least some of the aforementioned conditions [10, [12] [13] [14] 16, 21] . Furthermore, unlike other physiological variables, f R shows a rapid response at exercise onset and offset [12, 14, 15, 231] , with important implications for monitoring intermittent-based activities like soccer and other team sports [15] . As such, f R provides invaluable insight into physical effort, and its time course reflects exercise-induced fatigue in different populations [10, [12] [13] [14] [15] [16] 18, 21] . The fact that f R is a valid marker of physical effort is corroborated by our current understanding of the control of ventilation [22] . During high-intensity exercise, the central motor drive relating to voluntary muscle contraction (i.e., central command) is a major regulator of f R [11] [12] [13] 17, 21, 22] . This is interesting considering that central command is also the primary regulator of perceived exertion [22, 232, 233] , thus explaining the close association between these two variables [11] [12] [13] [14] 22] . In fact, f R has several advantages over perceived exertion monitoring as it is an objective physiological variable that can be monitored continuously and in real-time, and provides detailed information on how physical effort is distributed over a given training session or more [14, 15] . When maximal effort is exerted, f R reaches peak values of about 50 breaths/min in the general population [234] and of about 60 breaths/min in athletes [12, 13] ranging from 20-29 years old, but higher f R peak values can also be observed [15, 235] . The f R peak shows a 5% decrease per subsequent decade and slightly lower values in females than males (the difference is 2 breaths/min on average), while it is not affected by stature [234] . However, inter-individual differences in f R values [14, 15, 235] imply that f R monitoring should be tailored on an individual basis for training optimization and performance assessment. This goal can be achieved with the routine use of accurate respiratory wearables specifically designed for exercise monitoring. The importance of f R monitoring in sport and exercise is not currently followed up by widespread use of respiratory devices in training and competition settings. This is partially due to the fact that f R has only recently emerged as a fundamental variable to be monitored in the field of sport [14] . Indeed, it has even been defined as ""the neglected physiological measure"" during exercise [14] . However, there is also a paucity of wearable solutions specifically designed for exercise monitoring [14] , which poses several measurement challenges. Indeed, sport-specific movements, changes in body posture, and physical contact with team members and opponents (e.g., in team sports) determine a variety of motion artifacts that may impair the quality of the respiratory signal [236] . Furthermore, exercise presents some thermoregulatory challenges (e.g., increases in body temperature and consequent sweating) that need to be considered in the choice of sensors, textiles, and components of the measurement system. Outdoor exercise monitoring is even more complex as environmental factors, including rain, snow, wind, humidity, temperature, and noise may constitute further obstacles for using some measurement techniques. This may be the case of the contactless methods [23] , and of the contact-based methods based on air temperature, air humidity and acoustic sensing [24] . On the other hand, the abundance of technological solutions for measuring f R makes exercise monitoring entirely feasible if the sports industry sector devotes efforts in this direction. We present here some suitable techniques to monitor f R in the challenging measurement scenario of outdoor exercise. With the aforementioned considerations in mind, the contact-based techniques measuring the movements of the chest wall appear to be good candidates [24] . As such, it is not surprising that most of the commercially available solutions tested during exercise use these techniques [14] , with a preference for strain-sensitive conductive sensors. Among these, the most frequently used sensors are resistive, inductive, and capacitive sensors [14] . Different commercial devices have been validated during exercise, and good performances were generally reported [14, 224, 227] . However, in most of the cases, these devices were only validated in the laboratory, and less is known on the feasibility and suitability of their routine use in applied scenarios, like during outdoor training [237] . This is partially due to the fact that these devices were not specifically designed for sporting activities and that their wearability needs to be improved in some instances [14] . Besides, validation during exercise is rarely performed on a breath-by-breath basis, even though this is an essential requirement for real-time respiratory monitoring [238] . Furthermore, detailed information on f R (i.e., average values over 3-5 s) describes the rapid f R changes that occur during intermittent-based activities and provides insight on how effort is distributed during exercise [14, 15] . Therefore, breath-by-breath f R validation is strongly advised for sports respiratory wearables [238] . Another critical factor when designing sports wearables is the choice of the sensor position. Suitable body locations may partially change with exercise modality due to sport-specific postures and movements, with clear differences observed when comparing cycling, running, rowing, and swimming. Unfortunately, this problem has been overlooked, and only a few studies have attempted to address this issue so far [42, 154] . While the abdominal rib cage appears to be a good body site to locate respiratory strain sensors during both running and cycling [42, 154] , sensors located on the abdomen showed good performances in cycling [154] but not in running [42] . Differently, sensors located in the upper thorax showed lower performances compared to those positioned on the abdominal rib cage both in cycling and running [42, 154] . Besides, the posterior side of the trunk (both at the abdominal rib cage and abdomen levels) appears to be a suitable location that deserves consideration during cycling exercise [154] . However, these are only preliminary findings that need to be corroborated and expanded by future research. Studies testing the performances of multi-sensor measuring systems are also valuable to shed some light on the influence of the number of sensors on f R accuracy. Indeed, sensor redundancy is advised to improve the robustness of f R measurement, and this is suggested by several studies performed both at rest and during exercise [41, 42, 154] . Sensor fusion with other sensors (e.g., inertial sensors) may also be beneficial for motion artifact identification and removal [42] . This requires the development of ad-hoc and adaptable algorithms resilient to breathing-unrelated movements [41, 42] , which is an important area of computing research for exercise monitoring. On the other hand, sensor redundancy may determine an increase in battery consumption and, therefore, a trade-off needs to be found depending on the specific application. Some of the aforementioned challenges can be overcome with the advent of a new generation of sensors and electronic components. Stretchable and flexible sensors and electronics are particularly suitable for exercise monitoring as they ensure good adhesion with the body while exerting a minimal mechanical load on athletes [239] . Furthermore, the possibility to cover the stretchable system with moisture-resistant barrier layers and coatings help limit failure in functionality caused by sweat excretion and fluid exposure [239] . Strain stretchable sensors can be designed in various forms [240, 241] , which offers a myriad of solutions to satisfy specific measurement needs. These systems can be integrated into garments, patches, or can even be directly applied to the skin. For instance, Yang et al. [242] developed and tested an epidermal electronic system composed of metallic sensors connected by gold-on-polyethylene terephthalate serpentine ribbons, and promising results were observed for f R measurement. The development of these wearable devices has been favored by advances in microelectronics and the use of intrinsically stretchable and flexible materials [239] . Other interesting solutions proposed for f R measurement are textile-based sensors, conductive yarns, and highly-sensitive graphene strain sensors [240, 243, 244] . Facing the measurement challenges posed by sport and exercise has several advantages that go beyond this field of use and the sports industry sector. Indeed, accurate respiratory wearables suited for exercise monitoring can easily be scalable for everyday-life monitoring of patients, workers, and other users. A highly relevant field of research is the identification and compensation of respiratory artifacts to improve other biological measurements. The alternation of inspiration and expiration determines periodic movements of the organs situated in the abdominal and thoracic cavities, hence impairing the recording of biological signals coming from these body sites. Most of the imaging techniques used for medical diagnosis suffer from this problem, with specific challenges for different techniques. For instance, the time required to image the thorax is different for Positron Emission Tomography (PET) (6-9 min) and Computed Tomography (CT) (~15 s) [245] , which increases the difficulty of combining images acquired with the two different techniques [245] . Hence, it is not uncommon to observe between-image spatial misalignments, which may result in the mislocalization of a tumor lesion or the inaccurate quantification of indicators used as criteria for malignancy [245] . Magnetic Resonance Imaging (MRI) quality is also impaired by respiratory artifacts [246] . As such, research in the area of respiratory artifact compensation is growing exponentially, and a variety of technological solutions are currently available for the improvement of imaging for medical diagnosis [247, 248] . Likewise, the management of respiratory artifacts is essential for some therapeutic fields, and especially for radiotherapy [249] . Indeed, the side effects of radiotherapy are reduced if the patient's breathing pattern is taken into account, and a consequent improvement in therapy effectiveness can also be observed. An example is the use of the deep inspiration breath-hold technique in breast cancer patients, which reduces the radiation dose to healthy organs at risk, including the heart [249] . The deep inspiration moves the heart away from the radiotherapy beam, and the breath-hold maneuver minimizes respiratory movements. This and other breathing techniques used for radiotherapy (e.g., respiratory gating) are performed with the help of respiratory biofeedback [250] , thus making respiratory monitoring essential. Other biological signals are also affected by respiratory artifacts, including the ECG and the electroencephalographic signals, and different approaches have been proposed to address this issue [126, [251] [252] [253] . Hence, respiratory monitoring is fundamental for improving the quality of a variety of biological signals and the management of different diseases. The accurate and robust measurement of the respiratory waveform (not only of f R ) is fundamental to counteract the challenges posed by respiratory artifacts. Here, we present some suitable techniques to record the respiratory waveform during imaging acquisition for diagnostic purposes, and during radiotherapy delivery. A variety of contact-based and contactless techniques can be used to improve diagnostic imaging, but the materials composing the measurement systems need to be compatible with magnetic fields. Another requirement is the connection between the measurement system and the controller of imaging device scans (e.g., CT, PET, and MRI). The simultaneous recording of diagnostic images and of the respiratory waveform allows for respiratory artifact removal through motion compensation algorithms [245] . In most of the cases, the respiratory waveform is obtained with sensors capable of measuring respiratory-induced phenomena. Some examples are pressure sensors, airflow sensors, temperature sensors, and the Real-Time Position Management Respiratory Gating System (Varian Medical Systems, Palo Alto, CA, USA) [245] . Other approaches are based on recording the respiratory waveform directly from CT or MRI scans, without the need for additional respiratory sensors. For instance, Shahzadi et al. [246] tested the efficacy of three different methods of respiratory motion detection and compensation using data directly acquired from MRI under free-breathing conditions. The methods were based on the Golden-Angle Radial Sparse Parallel MRI technique that combines parallel imaging and golden-angle radial sampling. These methods showed good performance and the possibility to sort the data into different respiratory phases, with their suitability depending upon the specific clinical application [246] . The presence of respiratory artifacts is particularly challenging in combined PET/CT imaging because of the different acquisition time of PET and CT and of possible misalignments between images. Given the abundance of methods for correcting motion in CT and PET images, the interested reader is referred to a previous review [245] for detailed information on this topic. With the technological advances of imaging in resolution and quality, an ever-increasing demand for respiratory artifact compensation solutions is expected in the next future [247] . Numerous techniques and algorithms used to improve diagnostic images' quality are also suitable for respiratory artifact management during radiotherapy delivery. In this measurement scenario, it is even more important that the respiratory waveform is recorded in real time, and the use of respiratory biofeedback is common [250] . Different technologies based on direct contact with the body are available for this purpose. Among others, commercial devices based on airflow measurement are used in clinical practice to help patients perform apnea maneuvers during radiotherapy. An example is the Active Breathing Coordinator developed by Elekta (Elekta Oncology systems Ltd, Crawley, West Sussex, UK), an apparatus consisting of a turbine flow meter and a balloon valve capable of enforcing patient breath-holds at preselected respiratory volumes [254] . While methods based on airflow measurement are accurate, they are also obtrusive and sometimes not tolerated by patients. In some radiotherapy procedures, the recording of the chest wall position is of additional value to address the problem of inaccurate radiation dose delivery caused by respiratory movements. Some of the technologies used to track respiration-induced chest wall movements during radiotherapy have been reviewed by Glide-Hurst and Chetty [255] . Among these, pressure-sensitive belts, infrared tracking systems, and camera-based sensors are commonly employed in clinical practice [255] . An example of a pressure-sensitive belt solution is the Anzai Respiratory Gating System (Anzai Medical Co. Ltd, Tokyo, Japan) [256] ; the sensitive element is located at the right upper quadrant of a patient's abdomen and the system includes two pressure sensors (with different sensitivities for patients with shallow vs. deep respiration amplitudes) [256] . The CyberKnife (Accuray, Sunnyvale, CA, USA) is an example of an infrared tracking system used to record the trajectories of hemispherical photo-reflective markers taped on the skin of the patient undergoing stereotactic radiosurgery [257] . Respiratory gating techniques have also been implemented with the use of an infrared respiratory camera tracking the movements of a reflective marker box placed on the patient's abdominal surface [258] . Another alternative technique for respiratory motion compensation during radiotherapy is based on ultrasound motion tracking [259, 260] . The ultrasound system used by Ting et al. [259, 260] records respiratory movements at 30 Hz, with a total delay time of approximately 350 ms. Together with the use of respiratory motion algorithms, these performances may favor a reduction in the size of the planning target volume margin and an increase in the accuracy of the radiotherapy dose delivery [260] . The use of markers for tracking chest wall movements may suffer from the disadvantage of relative motion between the skin and the tracking markers [261] . This problem can be overcome with the use of video markerless approaches. Numerous optical systems that do not require markers are commercially available, including the AlignRT/GateCT (VisionRT Ltd, London, UK), the Sentinel (C-RAD AB, Uppsala, Sweden), and the Galaxy systems (LAP Laser, Luneburg, Germany) [261] . The AlignRT optical system was used by Schaerer et al. [261] to record the chest wall movements at three different phases of the breathing cycle (i.e., maximum inhale, maximum exhale, and an arbitrarily chosen intermediate position) and to develop an algorithm suitable for respiratory artifact removal. Depth sensors can also be used for respiratory motion tracking during radiotherapy. A good agreement was found between the Microsoft Kinect v2 and the more commonly used Anzai Respiratory Gating System and Varian's RPM system [262] . The characteristics and ease of use of depth sensors [262] provide interesting avenues for improving radiotherapy management. The voluntary modulation of breathing leads to a series of systemic effects inducing potential benefits in a range of disorders. For instance, slow breathing decreases blood pressure in patients with hypertension [263] , reduces stress, anxiety, and pain [264] [265] [266] [267] [268] [269] [270] , reduces the frequency and severity of migraine headaches [271] , and improves various aspects of health-related quality of life in heart failure patients [272, 273] . These effects are best achieved if the voluntary modulation of breathing is performed via respiratory biofeedback, which facilitates the maintenance of a given breathing pace or the execution of specific breathing exercises. f R is a fundamental variable for any respiratory biofeedback strategy, but other ventilatory variables may also prove useful, such as V T , inspiratory time, and expiratory time [274, 275] . The primary role of f R in respiratory biofeedback is given by the marked effect of its change on different physiological systems, including the modulation of heart rate variability, which is particularly effective at f R values around 6 breaths/min [276] . It has even been suggested that the effect of respiratory modulation on heart rate variability is maximized when f R is set at the so-called individual resonant frequency, but more research is needed to further test this hypothesis [276] . Respiratory biofeedback is usually delivered by sound output and/or visual feedback, including attractive forms like music and biofeedback games [275, 277, 278] . It is a very useful technique to learn respiratory skills and exercises (e.g., diaphragmatic breathing) [271] , especially for less compliant individuals like children and older adults. For instance, respiratory biofeedback is effectively used in tumor patients undergoing radiotherapy, where it favors therapy delivery while minimizing the therapy side effects [250] . Therefore, respiratory biofeedback has important implications in various settings, including clinical, rehabilitation, occupational, and leisure settings. Finally, research on respiratory biofeedback is very promising as it may help reconsider the role of respiratory devices, which may offer therapeutic solutions along with respiratory monitoring. The essential requirement for any respiratory biofeedback system is the real-time display of the respiratory signal, either in the form of raw data (respiratory waveform) or in the form of ventilatory variables (e.g., f R , V T , inspiratory time, and expiratory time). In most cases, the patient is required to match a predefined respiratory pattern template by altering breathing voluntarily. Visual, auditory, or other forms of feedback are provided to help the patient accomplish this task. The choice of the respiratory measurement technique depends on the specific goal to achieve and on related measurement requirements. Here we provide an example of two measurement scenarios, where the patient performs (1) breathing exercises as part of structured therapeutic plans; or (2) biofeedback-guided exercises for everyday-life stress management. Respiratory biofeedback is particularly relevant for patients performing breathing exercises as part of their therapeutic plans. This is the case for patients undergoing radiotherapy, who need to perform breathing familiarization sessions before real therapy sessions. Commercially available depth cameras (e.g., Microsoft Kinect) are good and relatively low-cost solutions to help patients practice with breathing exercises and procedures. When assessing the reproducibility of breathing maneuvers used for image-guided interventions, Heerink et al. [279] found that respiratory biofeedback delivered with the Microsoft Kinect v1 camera (Microsoft Corp., Redmond, WA, USA) was effective in reducing the respiratory motion variability observed when no biofeedback was used, with important implications for radiotherapy delivery. The patient undergoing radiotherapy is also advised to perform biofeedback-guided respiratory training outside of the clinical setting, although this practice is not performed regularly. In an attempt to partly overcome this limitation, Oh et al. [280] have developed a respiratory biofeedback system based on a micro-electro-mechanical-system magnetic sensor. This system showed good performances and relatively small errors in respiratory frequency and amplitude quantification [280] . However, the high magnetic fields generated by this system limit its applicability in some populations, including patients with pacemakers [280] . The use of other sensors registering respiration-induced chest wall movements (e.g., strain sensors) [24] can overcome this problem. Other patients that would benefit from respiratory biofeedback therapies are those with hypertension [263, 273, [281] [282] [283] or panic disorders [284, 285] . Substantial evidence shows a significant reduction in blood pressure with the administration of biofeedback-guided slow deep breathing exercises [273, 281, 282] . A number of these studies have used an FDA-approved commercially available biofeedback system called RESPeRATE ® (Intercure Inc, Fort Lee, NJ, USA) [273, [281] [282] [283] , which interactively reduces the patient's f R through auditory feedback with different tones for the inhalation and exhalation phases. This system is suitable for self-treatment at home and is made up of a sensor registering chest-wall movements embedded in an elastic belt, a computerized display, and headphones [282] . Differently, patients with panic disorders are usually treated with capnometry guided respiratory biofeedback because these patients hyperventilate and need to restore isocapnia [284, 285] . Capnometry guided respiratory biofeedback provides values of end-tidal carbon dioxide and f R to the user. Tolin et al. [285] have documented the feasibility of a remote monitoring service based on home-delivered capnometry-guided biofeedback therapy. The measuring system consisted of a CO 2 sensor, a nasal cannula, and an app installed on a smart device that provided real-time audiovisual feedback of the variables of interest. Data from each session were streamed on a secure service after each treatment, thus allowing the clinician to remotely evaluate clinical progress and therapeutic adherence [285] . Respiratory biofeedback is also gaining interest in stress management during everyday life [275] . For instance, technological solutions may help face acute events of stress, anxiety, or panic. This need requires the use of unobtrusive technologies, with smart garments being particularly suitable as the user can access respiratory biofeedback as needed. Several sensors may be integrated into shirts or bands, including inductive, resistive, capacitive, and impedance sensors [24] . Among these, sensors with good response time and an output proportional to airflow should be preferred, as respiratory biofeedback delivery is more effective when respiratory amplitude (surrogate measure of V T ) values are provided together with f R values. Most of the solutions proposed so far have used sensors embedded in belts [278, 286, 287] , but smart clothing may offer additional benefits in terms of wearability [288] . The previous section has highlighted the importance of respiratory monitoring for different goals and measurement scenarios, thus pointing to the need to approach respiratory monitoring from a multidisciplinary perspective. Indeed, current advances in the fields of respiratory physiology, applied sciences, and technological development have so far not been accompanied by a proportional increase in the development and diffusion of respiratory monitoring services. Here we show how fruitful synergies between different disciplines may provide avenues to address this issue (see Figure 2 for a schematic representation). Research in the field of respiratory physiology provides important insight into the rationale behind the choice of the ventilatory variables to monitor in different fields and applications. Indeed, the fact that f R is more sensitive than tidal volume to a variety of stressors (see Section 2 for details) is in line with our current understanding of the control of ventilation. Increasing evidence suggests that f R is substantially regulated by non-metabolic inputs [7, 11, 12, 17, [19] [20] [21] [22] , including brain areas relating to motor control, and those involved in emotion and cognitive processing [6, 8, 11, 12, 22] . This explains why f R often increases in proportion to the extent of emotional stress, cognitive load, dyspnea, heat stimuli, and physical effort [7] [8] [9] 12, 13, 18, 22, 210] . As such, f R is the behavioral component of minute ventilation [19, 20, 22] . Conversely, most of these factors/conditions do not determine a consistent change in V T , which may even show opposite responses compared to those of f R , as observed under the influence of emotional and cognitive stimuli [6, 8, 193] . Indeed, V T is the metabolic component of minute ventilation, and is adjusted on the basis of metabolic inputs and f R levels to match alveolar ventilation with metabolic requirements [11, 20, 22] . In turn, f R is influenced by V T , but to a minor extent compared to how V T is affected by f R [11, 22] . These are the essential features (see Figure 3 ) of a recently-developed model of ventilatory control [11, 12, 19, 22] , which provides a physiological rationale for choosing when to monitor V T alongside f R . The model shows that the measure of V T is essential when respiratory monitoring is performed to identify the human response to metabolic stimuli like hypercapnia and metabolic acidosis. While this model deliberately simplifies the complexity of breathing control [7, 19, 20, 22] , it offers valuable insight on how to choose the ventilatory variables needed in different monitoring services. [275] . For instance, technological solutions may help face acute events of stress, anxiety, or panic. This need requires the use of unobtrusive technologies, with smart garments being particularly suitable as the user can access respiratory biofeedback as needed. Several sensors may be integrated into shirts or bands, including inductive, resistive, capacitive, and impedance sensors [24] . Among these, sensors with good response time and an output proportional to airflow should be preferred, as respiratory biofeedback delivery is more effective when respiratory amplitude (surrogate measure of VT) values are provided together with fR values. Most of the solutions proposed so far have used sensors embedded in belts [278, 286, 287] , but smart clothing may offer additional benefits in terms of wearability [288] . The previous section has highlighted the importance of respiratory monitoring for different goals and measurement scenarios, thus pointing to the need to approach respiratory monitoring from a multidisciplinary perspective. Indeed, current advances in the fields of respiratory physiology, applied sciences, and technological development have so far not been accompanied by a proportional increase in the development and diffusion of respiratory monitoring services. Here we show how fruitful synergies between different disciplines may provide avenues to address this issue (see Figure 2 for a schematic representation). Research in the field of respiratory physiology provides important insight into the rationale behind the choice of the ventilatory variables to monitor in different fields and applications. Indeed, the fact that fR is more sensitive than tidal volume to a variety of stressors (see Section 2 for details) is in line with our current understanding of the control of ventilation. Increasing evidence suggests that fR is substantially regulated by non-metabolic inputs [7, 11, 12, 17, [19] [20] [21] [22] , including brain areas relating to motor control, and those involved in emotion and cognitive processing [6, 8, 11, 12, 22] . This explains why fR often increases in proportion to the extent of emotional stress, cognitive load, dyspnea, heat stimuli, and physical effort [7] [8] [9] 12, 13, 18, 22, 210] . As such, fR is the behavioral component of minute ventilation [19, 20, 22] . Conversely, most of these factors/conditions do not determine a consistent change in VT, which may even show opposite responses compared to those of fR, as observed under the influence of emotional and cognitive stimuli [6, 8, 193] . Indeed, VT is the metabolic component of minute ventilation, and is adjusted on the basis of metabolic inputs and fR levels to match alveolar ventilation with metabolic requirements [11, 20, 22] . In turn, fR is influenced by VT, but to a minor extent compared to how VT is affected by fR [11, 22] . These are the essential features (see Figure 3 ) of a recently-developed model of ventilatory control [11, 12, 19, 22] , which provides a physiological rationale for choosing when to monitor VT alongside fR. The model shows that the measure of VT is essential when respiratory monitoring is performed to identify the human response to metabolic stimuli like hypercapnia and metabolic acidosis. While this model deliberately simplifies the complexity of breathing control [7, 19, 20, 22] , it offers valuable insight on how to choose the ventilatory variables needed in different monitoring services. Advances in the field of respiratory physiology are, in turn, favored by technological development and experimental evidence provided by applied sciences. For instance, the vast amount of evidence supporting the clinical relevance of fR for a variety of diseases should guide basic research in the attempt to unravel the mechanisms underlying the commonly observed tachypneic breathing pattern. For instance, the well-documented importance of fR in the context of cardiac arrest and severe pneumonia should trigger further research aiming to understand why fR is particularly sensitive to these diseases. The importance of technological development for respiratory physiology and applied sciences is straightforward, as the widespread availability of accurate respiratory devices may speed up basic and applied research in different fields. This, in turn, would produce further knowledge to guide the development of respiratory measurement systems and monitoring services. [22] for further information). While respiratory rate (the behavioral component of minute ventilation) is substantially influenced by non-metabolic stressors, V T (the metabolic component of minute ventilation) satisfies the metabolic requirements of the human body. As such V T is fine-tuned according to the levels of respiratory rate and the magnitude of metabolic inputs, while f R is influenced by V T to a lesser extent. This model explains why f R is more sensitive than V T to a variety of non-metabolic stressors and corroborates the importance of f R monitoring in different fields of use. Advances in the field of respiratory physiology are, in turn, favored by technological development and experimental evidence provided by applied sciences. For instance, the vast amount of evidence supporting the clinical relevance of f R for a variety of diseases should guide basic research in the attempt to unravel the mechanisms underlying the commonly observed tachypneic breathing pattern. For instance, the well-documented importance of f R in the context of cardiac arrest and severe pneumonia should trigger further research aiming to understand why f R is particularly sensitive to these diseases. The importance of technological development for respiratory physiology and applied sciences is straightforward, as the widespread availability of accurate respiratory devices may speed up basic and applied research in different fields. This, in turn, would produce further knowledge to guide the development of respiratory measurement systems and monitoring services. The need for a multidisciplinary approach to respiratory monitoring is also manifested by the different levels of expertise required to structure respiratory monitoring services. As shown in Section 2, the development of new technologies should be guided by specific monitoring needs because measurement requirements depend on monitoring goals and measurement scenarios. Yet, respiratory devices are often not developed for specific purposes. An example is the limited diffusion of respiratory wearables specifically designed for monitoring sporting activities [14] , where different measuring challenges arise. Likewise, limitations are encountered in the development of respiratory devices to support the diagnosis of pneumonia, especially in low-resource settings, where specific requirements are needed [89] . To partially address these challenges, we have developed a conceptual framework that may help to guide the development of respiratory monitoring services (see Figure 4 ). The definition of monitoring goals is the first step in the development of a respiratory monitoring service. This is fundamental because the technological solutions identified may change extensively if the monitoring goal is, for instance, to detect apnea, identify emotional stress, or measure physical effort during exercise. It is also important to define the specific measurement scenario, which should orient the choice of measurement techniques. An example is provided in Figure 4 (panel B), which reports different measurement techniques for the remote respiratory monitoring of COVID-19 patients, depending on the need for periodic vital sign monitoring, or continuous monitoring [5] . Several other examples are provided in Section 2. When the monitoring goal and scenario are established, the development of a conceptual framework of the monitoring service will help identify relevant characteristics of the service, including users, resources, and facilities. An example is the framework proposed by Naranjo-Hernández et al. [157] (Figure 4 , panel C), where a smart garment and a communication platform enable the remote monitoring of COPD patients by healthcare professionals and caregivers in different scenarios (i.e., hospital, e-health center, home and outdoor monitoring). The next step is the identification of relevant variables, sensors and algorithms. The above considerations on when to monitor V T provide an example of how to choose relevant ventilatory variables. Similar considerations may lead to the identification of other physiological and mechanical variables in order to achieve the desired goals. The different variables selected may be computed from different signals (e.g., respiratory waveform, ECG, and accelerometer signal), each requiring the use of specific sensors and algorithms. The different signal outputs can be combined to obtain monitoring features and metrics. An example is the detection of apnea/hypopnea events, which is often performed with the simultaneous recording of signals coming from different sensors (see Figure 4 , panel D). For each signal selected, the elements composing the measurement chain need to be identified, i.e., measurand, sensor, electronics, and data acquisition, and signal analysis stages. As detailed by Massaroni et al. [24] , the elements of the measurement chain change according to the different sensors used, and an example is provided in Figure 4 (panel E) for resistive sensors used to measure f R . The output of all the signals should be compared with similar signals coming from reference systems, and each variable of interest should be validated. The validation output is affected by different factors, including the choice of the reference system, the algorithms used for signal processing, the validation indices selected, and the validation protocol and scenario. While it is outside of the scope of the present manuscript to discuss these points, the interested reader is referred to previous studies providing more details on this issue [26, 62, 65, 89] . As an example, commonly computed validation indices are Mean of Difference and Limits of Agreement, which provide information on both the accuracy and precision of the tested measurement system. These indices are often depicted graphically by means of the Bland-Altman plot [289] , as shown in the example provided in Figure 4 (panel F) [290] . Based on the characteristics of the measurement system and the respiratory service, a suitable communication architecture needs to be identified to enable the service. Three different layers (or tiers) can usually be identified in a communication architecture [291, 292] . The first layer pertains to the communication between the body sensors and the sink, and is therefore called sensor-based tier [291, 292] . The second layer is called gateway-based tier and pertains to the communication between the sink and one or multiple access points (e.g., smart device) [291, 292] . The third layer pertains the communication beyond the access points and is usually composed of a medical server, a patient database, and a medical environment (in healthcare monitoring systems) [291, 292] . Figure 4 (panel G) shows an example of a communication architecture used for the remote monitoring of COPD patients, where data from wearable sensors are acquired by a smart device through wireless communication and are streamed to health professionals and caregivers through cloud and telecom infrastructures [293] . Prior to service implementation in real contexts, a performance assessment of the service needs to be performed. Some of the factors that can be evaluated at this level are data transmission performance, data security, and user-friendliness [294] [295] [296] . Among different options, data transmission performance can be assessed with metrics like % data received and time delay [295] , as reported in the example provided in Figure 4 (panel H). Once the service is implemented in routine activities, the efficacy of the service needs to be evaluated. The assessment methodology may depend on the specific service developed. A recent study [296] has reported an interesting methodological approach to evaluate the efficacy of smartphone-based respiratory monitoring services. The authors have identified four categories determining the efficacy of respiratory services, i.e., smartphone performance metrics factors (e.g., data security and privacy level), patient status factors (e.g., level of patient satisfaction), cost-related factors (e.g., cost to the customers), and resource-related factors (e.g., internet connectivity level) [296] . For the healthcare services based on the use of early warning scores to predict clinical outcomes, the efficacy is commonly evaluated by computing metrics of specificity and sensitivity, and by producing receiver operating characteristic (ROC) curves [110] . An example is provided in Figure 4 (panel I), which reports ROC curves for repeated respiratory rate measurements collected within the first hours from admission to an emergency department [116] . In this example, the ROC curves show that repeated measurements of respiratory rate are better associated with patient deterioration compared to a single measurement at hospital admission [116] . However, recent systematic reviews have found little evidence of any clinical effectiveness of the early warning scores commonly used in the clinical setting [110, 297, 298] . This finding can be largely attributed to a series of methodological weaknesses related to the development and validation of early warning scores, including participant selection, the choice of outcome measures, and the analysis performed ( Figure 4 , panel J) [110] . To overcome this problem, Gerry et al. [110] have provided a series of recommendations on population description, sample size, missing data management, outcome measures, time horizons, statistical methods, validation methodology, and on the selection of metrics for testing model performance [110] . This is an emblematic example of how evidence-based approaches should guide the improvement of respiratory monitoring services. Given the ever-increasing growth of science and technology in the field of respiratory monitoring, an evidence-based approach should also be used to improve respiratory monitoring services at any step of the here proposed framework. Each of the ten steps is accompanied by a graphical example reported on the right-hand side of the figure. Panel (A) reports the thirteen monitoring goals described in this review. The graph in panel (B) is reproduced from Massaroni et al. [5] . The graph in panel (C) is reproduced from Naranjo-Hernández et al. [157] . Panel (D) provides an example of the output of some of the sensors used to detect apnea events in sleep laboratories. The graph in panel (E) is slightly modified from Massaroni et al. [24] . The graph in panel (F) is slightly modified from Lo Presti et al. [290] . The graph in panel (G) is reproduced from Tomasic et al. [293] . Panel (H) provides an example of data transmission performance evaluation. The graph in panel (I) is slightly modified from Quinten et al. [116] . The graph in panel (J) is slightly modified from Gerry et al. [110] . Despite the importance of fR in different fields of use, the fact that fR is sensitive to a variety of non-metabolic stressors [7] [8] [9] 11, 12, [19] [20] [21] may impair our understanding of the factors determining an increase in fR. As shown in Figure 5 , similar values of fR can be observed when the user is under cognitive load, emotional stress, pain, dyspnea, or is simply performing moderate exercise. This problem can be partly overcome with the simultaneous measurement of other physiological and mechanical variables. For instance, it is important to characterize the postures and activities of the user, as commonly performed with the use of inertial sensors [299] . This is particularly relevant for monitoring dyspnea and physical effort during everyday life activities and exercise [27] , but is also important for the identification of suitable portions of the respiratory signal (e.g., with no movements or artifacts) to compute resting fR [133] . The concomitant measure of other vital signs is also beneficial, as in the development of early warning scores for the prediction of clinical deterioration [4] . Besides, fR and its variability change across wakefulness and different sleep stages [300, 301] , which is relevant when interpreting nocturnal fR values. Sleep stages are usually identified with electroencephalography, but approaches based on breathing sound processing have also been proposed [81] . Another strategy to gain insight into the factors behind the changes in fR is the The framework is composed of ten steps that are numbered and listed on the left-hand side of the figure. Each of the ten steps is accompanied by a graphical example reported on the right-hand side of the figure. Panel (A) reports the thirteen monitoring goals described in this review. The graph in panel (B) is reproduced from Massaroni et al. [5] . The graph in panel (C) is reproduced from Naranjo-Hernández et al. [157] . Panel (D) provides an example of the output of some of the sensors used to detect apnea events in sleep laboratories. The graph in panel (E) is slightly modified from Massaroni et al. [24] . The graph in panel (F) is slightly modified from Lo Presti et al. [290] . The graph in panel (G) is reproduced from Tomasic et al. [293] . Panel (H) provides an example of data transmission performance evaluation. The graph in panel (I) is slightly modified from Quinten et al. [116] . The graph in panel (J) is slightly modified from Gerry et al. [110] . Despite the importance of f R in different fields of use, the fact that f R is sensitive to a variety of non-metabolic stressors [7] [8] [9] 11, 12, [19] [20] [21] may impair our understanding of the factors determining an increase in f R . As shown in Figure 5 , similar values of f R can be observed when the user is under cognitive load, emotional stress, pain, dyspnea, or is simply performing moderate exercise. This problem can be partly overcome with the simultaneous measurement of other physiological and mechanical variables. For instance, it is important to characterize the postures and activities of the user, as commonly performed with the use of inertial sensors [299] . This is particularly relevant for monitoring dyspnea and physical effort during everyday life activities and exercise [27] , but is also important for the identification of suitable portions of the respiratory signal (e.g., with no movements or artifacts) to compute resting f R [133] . The concomitant measure of other vital signs is also beneficial, as in the development of early warning scores for the prediction of clinical deterioration [4] . Besides, f R and its variability change across wakefulness and different sleep stages [300, 301] , which is relevant when interpreting nocturnal f R values. Sleep stages are usually identified with electroencephalography, but approaches based on breathing sound processing have also been proposed [81] . Another strategy to gain insight into the factors behind the changes in f R is the recording of concomitant symptoms, including pain, dyspnea, and emotional stress. These symptoms can be assessed with validated scales [302, 303] , but this approach can only be used on collaborative patients. recording of concomitant symptoms, including pain, dyspnea, and emotional stress. These symptoms can be assessed with validated scales [302, 303] , but this approach can only be used on collaborative patients. Computing solutions may also provide insight into the factors affecting fR. For instance, analyses of ventilatory variability have been used to identify cognitive load and emotional stress [9, 191] , but further research is needed to guide their implementation in respiratory monitoring services. Artificial intelligence approaches can also be applied to respiratory monitoring for the identification of the different stressors reported in Figure 5 . Some of these methods of analysis have been used in the field of apnea monitoring [87] , for the prediction of clinical deterioration in COVID-19 patients [304] , and for the identification of opioid-induced ataxic breathing [173] . Another common issue in the field of healthcare is the use of fixed (sometimes arbitrary) cut-off values of fR [100, 119] , which do not take into account inter-individual differences in fR resting values and in the responses to different stressors [3, 305] . Most of these challenges can be overcome with the development of accurate respiratory systems and the effective implementation of respiratory monitoring services in routine use. This would lead to the availability of a large amount of respiratory data obtained in different measurement scenarios, hence fostering the processes described in Figures 2 and 4 . The authors declare no conflict of interest. The manuscript contains the following abbreviations: AHI @story_separate@This review presents a multidisciplinary approach to respiratory rate monitoring, with the aim to improve the development of respiratory monitoring services in different fields of use. We have identified thirteen monitoring goals where the measure of respiratory rate is invaluable and Computing solutions may also provide insight into the factors affecting f R . For instance, analyses of ventilatory variability have been used to identify cognitive load and emotional stress [9, 191] , but further research is needed to guide their implementation in respiratory monitoring services. Artificial intelligence approaches can also be applied to respiratory monitoring for the identification of the different stressors reported in Figure 5 . Some of these methods of analysis have been used in the field of apnea monitoring [87] , for the prediction of clinical deterioration in COVID-19 patients [304] , and for the identification of opioid-induced ataxic breathing [173] . Another common issue in the field of healthcare is the use of fixed (sometimes arbitrary) cut-off values of f R [100, 119] , which do not take into account inter-individual differences in f R resting values and in the responses to different stressors [3, 305] . Most of these challenges can be overcome with the development of accurate respiratory systems and the effective implementation of respiratory monitoring services in routine use. This would lead to the availability of a large amount of respiratory data obtained in different measurement scenarios, hence fostering the processes described in Figures 2 and 4 . This review presents a multidisciplinary approach to respiratory rate monitoring, with the aim to improve the development of respiratory monitoring services in different fields of use. We have identified thirteen monitoring goals where the measure of respiratory rate is invaluable and presented suitable f R measurement techniques for specific measurement scenarios. The variety of monitoring goals presented has allowed us to show differences in the state of development of respiratory services across different fields of use. For instance, the field of apnea detection is regulated by detailed guidelines on the use of respiratory sensors, while the field of pneumonia suffers from a lack of consensus on how to accurately and objectively measure f R . Likewise, respiratory monitoring requires more consideration and development in the field of sport and exercise, where f R is emerging as a valid marker of physical effort and exercise-induced fatigue. We hope that the multidisciplinary approach presented may have contributed to corroborate the importance of measuring f R in different fields, and to provide solutions for the effective development of respiratory monitoring services.","Respiratory rate is a fundamental vital sign that is sensitive to different pathological conditions (e.g., adverse cardiac events, pneumonia, and clinical deterioration) and stressors, including emotional stress, cognitive load, heat, cold, physical effort, and exercise-induced fatigue. The sensitivity of respiratory rate to these conditions is superior compared to that of most of the other vital signs, and the abundance of suitable technological solutions measuring respiratory rate has important implications for healthcare, occupational settings, and sport. However, respiratory rate is still too often not routinely monitored in these fields of use. This review presents a multidisciplinary approach to respiratory monitoring, with the aim to improve the development and efficacy of respiratory monitoring services. We have identified thirteen monitoring goals where the use of the respiratory rate is invaluable, and for each of them we have described suitable sensors and techniques to monitor respiratory rate in specific measurement scenarios. We have also provided a physiological rationale corroborating the importance of respiratory rate monitoring and an original multidisciplinary framework for the development of respiratory monitoring services. This review is expected to advance the field of respiratory monitoring and favor synergies between different disciplines to accomplish this goal."
"As this paper is being written, the COVID-19 pandemic has spread across the world (Trackcorona 2020). To manage and control the pandemic, countries and regions are taking different approaches. Enacting partial to full lockdown (with an exception of countries like Sweden), mandating safe physical-distancing measures, face mask wearing for general public, measures for closing/reopening schools and universities, encouraging remote working, border control, using manual and digital contact tracing, and using hygiene measures are among the widely adopted strategies to the pandemic (Han et al. 2020) . Many countries started the the introduction of contact tracing apps to manage and control the spread of the virus (EDPB 2020a). Contact tracing apps should complement and support the manual contact tracing as such a technology may not be able to penetrate in some populations (e.g. children or elderly). Further, in some countries people may not have access to smartphones and mobile devices to install such apps. Accordingly, manual contact tracing remains the main method of contact tracing (EDPB 2020a; Ferretti et al. 2020) . To date, there are many contact tracing apps 1 and many of them hastily designed, developed, and produced (Covid-19 apps 2020). These apps while essentially beneficial -in terms of generating a memory of proximity identifiers and urgently alerting users if they come into contact with a COVID-19 positive case (Ferretti et al. 2020 ) -present substantial security and privacy challenges. In this article, we analyze the privacy and security aspects of COVID-19 contact tracing apps (a set of 28 apps available on Android platform) by applying a multilateral analysis method for Android apps introduced by Hatamian et al. (2019) . Through this method, we analyze the permissions declared in the Android manifest files. In particular, we focus on the dangerous permissions. We analyze the privacy policies of these apps. We monitor app behavior by logging in each resource access event during run-time; here we fundamentally focus on dangerous permissions even though the monitoring app documents other types of resource access events. Lastly, to complement the privacy analysis we perform a security analysis of each app's program code in order to detect possible vulnerabilities. The period of our analysis was May-June 2020, we collected data for accessible apps for this study, archived their documentation, installed them on test phones in our labs, and started analyzing their behavior. Research questions. Researchers have pointed out the privacy and security challenges of COVID-19 apps (Dar et al. 2020; Raskar et al. 2020) . However, in this case, our motivation is driven by several fundamental questions: What privacy sensitive data do the COVID-19 apps aim to extract from the users? Does the apps' behavior match what has been indicated in their privacy policies? Are there vulnerabilities that would undermine the information security and privacy protection goals? To which extent the studied apps are compliant with legal requirements (especially the EU General Data Protection Regulation (GDPR) as this study has been conducted in EU)? To address these questions, we develop a method that combines four metrics: (i) apps' data access potential from their permission requests, (ii) dynamic app-behavior analysis by monitoring their run-time resource accesses patterns, (iii) coverage of data protection principles by their corresponding privacy policy texts, and (iv) static app-code analysis to document existing security vulnerabilities. The collected data is used as a basis for a multi-perspective privacy and security analysis of COVID-19 contact tracing apps, enabling an understanding of privacy and security related quality indicators for rapidly deployed smartphone apps within the context of a health pandemic. It should be noted that the privacy and security analysis scope of this paper is focused on Android ecosystem and the reason is threefold: (1) the open-source nature of Android and its flexibility in modifying the core components of the operating system to monitor apps' behavior along with the existence of a wide range of static and dynamic analysis tools (2) compared to other mobile operating systems, Android is globally used by many users, thanks to its domination of the market share (Mobile operating system market share worldwide 2020; Statista 2020; IDC 2020). Furthermore, it is among the operating systems with the most detected vulnerabilities (Android is the most vulnerable operating system 2019; DigitalInformationWorld 2019), making it an attractive platform for adversaries to conduct malicious activities, and (3) during the data collection phase (see Section 3.1), we noticed that all studied contact tracing apps are available on Android platform (28 out of 28 apps), while some of them were not available on iOS platform (6 out of 28 apps). Structure of the paper. The rest of this paper is organized as follows: Section 2 provides background information regarding the available COVID-19 contact tracing apps including the technologies they rely on. It also provides some insights into compatability of these apps with privacy regulation. While Section 3 elaborates on the steps that were taken into consideration to design our study, Section 4 details our multi-perspective privacy and security analysis. Further, based on the results obtained from the multi-perspective analysis, Section 5 provides a holistic view through proposing and conducting an impact assessment to compare the privacy and security performance of contact tracing apps. Section 6 discusses the main key insights, examines the compliance of COVID-19 contact tracing apps with respect to fundamental privacy and security requirements, and provides several calls for actions to revamp the identified issues. This paper is then concluded in Section 7.@story_separate@Contact tracing is one of the methods used to contain a medical epidemic. By tracing humans exposed to an infected person, the spread of infections can be reduced if those potentially infected people can be isolated from the remaining population. In addition, contact tracing helps in tracking the areas that are exposed to an infection (EDPB 2020a) . In what follows, we briefly discuss the existing technologies used in contact tracing apps along with the compatibility of such apps with privacy regulation. During the emergence of the 2019-2020 pandemic disease COVID-19 caused by the SARS-CoV-2 virus which spread at high speed, governments turned towards digital tracing of their populations with the help of smartphone apps. The principal idea is that large parts of a population carry phones with them, thus phones could be used as sensors for both recording encounters between people as well as for registration of their whereabouts. Smartphone location tracking is not a new concept (Fritsch 2008a) . Mobile phones (including both smart and non-smart ones) send and receive low power radio signals. The signals are sent to and received from antennas that are attached to radio transmitters and receivers, usually referred to as base stations. The base stations are linked to the rest of the mobile and fixed phone cellular networks and pass the signal/call on into those networks. Therefore, as part of services, mobile phones are tracked as they move across different cellular networks (Al-Saffar et al. 2015) . However, the sensing capabilities of phones have exceeded that of phone networks both in precision and in the quality of information: GPS delivers more precise location data, while scanning each other's radio interfaces provides proximity information about close encounters to other phones. The latter is used in Bluetooth contact tracing, where the shortrange wireless Bluetooth Low Energy (BLE) technology is used to send and detect beacon signals in order to register phone encounters. Moreover, it is an established practice in marketing and customer intelligence gathering, and is widely used e.g. in digital media and radio distribution in order to identify stations, products or billboards (Lashgari 2018; Rocamora 2017) . Analyzing and processing of sensor-generated data (through sensors like accelerometer, gyroscope, etc.) and ultrasonic signals through embedded sensors (microphone) sent by other phones are other technique for digital contact tracing (eHealthNetwork 2020). In the case of ultrasonic signals processing, one can expect more reliable accuracy than BLE and GPS (Luo et al. 2018 ) as they both measure signals through walls and floors which could produce false positives indoors. However, this is not the case for ultrasonic signals as they will not travel through walls, floors, etc. Nevertheless, privacy concerns are not avoidable (ultra-privacy 2020) because of its reliance on the microphone (there is no guarantee that only ultrasonic signals are collected). In Table 1 the list of contact tracing apps analyzed and studied is available. This table also details the technologies used in each app. As can be seen, each GPS and BLE has accounted for 60% (17 apps) of all studied apps. This percentage is 10% (3 apps) for sensorbased technologies (e.g. gyroscope, microphone, etc.). Our analysis shows that the reviewed apps require a mixed variety of personal data to function, including precise location information, data generated by sensors, phone number, gender, age, devices' unique IDs, etc. We argue that depending on the technologies used, there might be a trade-off between privacy and functionality. According to existing guidelines (EDPB-letter 2020) , no usage of GPS and use of decentralized storage is well-aligned with best design practices as apps will not be able to track and monitor users' precise movement patterns and store these data on centralized storage. Additionally, the capability of devices to determine social distancing varies depending on technologies used. For instance, when it comes to indoors positioning, ultrasonic-based technologies are more reliable in terms of distance accuracy. On top of that, potential interference and spatial blockage between different devices is another challenge that needs further research and development (Omar Al Kalaa et al. 2016; Tshiluna et al. 2016 ). The fast spread of SARS-CoV-2 produced many proposals and implementations of contact tracing apps. Some countries imposed tracking apps on their populations that implemented a varying degree of government force on the bearers, ranging from self-reporting duties through biometric surveillance up to publishing infected citizen's movements on public web pages. For instance, the Indian authorities announced that the use of Aarogya Setu app is mandatory for federal government employees, food delivery workers, and some other service providers. Moreover, to access public transport and airports one needs to have it installed (IndiaMandatory 2020). In a similar scenario, Singapore contact tracing app (TraceTogether) became mandatory for migrant workers (Singapore 2020). Not quite irrelevant to this, a university in the US has shown an interest in mandating students to install a tracking app, otherwise they will face disciplinary proceedings and sanctions (US-University 2020). In the European Union (EU), compatibility with privacy regulation and proportionality of measures were quickly pointed out by the European Commission when contact tracing started being discussed (EDPB 2020b). For instance, on 8th April 2020 the European Commission adopted a recommendation (COVID-europe 2020) towards a common Union toolbox for the use of technology and data to combat and exit from the COVID-19 crisis, in particular concerning mobile apps and the use of anonymized mobility data to develop a common European approach for the use of apps at an EU level. Followed by this, the European Data Protection Board published a public letter (EDPB-letter 2020) where it was stated that ""contact tracing apps do not require location tracking of individual users. Their goal is not to follow the movements of individuals or to enforce prescriptions. The main function of such apps is to discover events (contacts with positive persons), which are only likely and for the majority of users may not even happen, especially in the de-escalation phase. Collecting an individual's movements in the context of contact tracing apps would violate the principle of data minimization. In addition, doing so would create major security and privacy risks"". Inspired by the contributions made by the European Data Protection Board (2020) and European eHealth Network (2020), on 16th April 2020 the European Commission published COVID-europe 2020) the guidance on apps supporting the fight against COVID-19 pandemic in relation to data protection to ensure a coherent approach across the EU and provided guidance to Member States and app developers regarding the features and requirements that contact tracing apps should meet to ensure compliance with the EU privacy and personal data protection legislation, in particular the General Data Protection Regulation (GDPR) 2016) and the ePrivacy Directive (2002) . In a very similar effort, on 4th May 2020 the UK Information Commissioner's Office published (2020) data protection expectations on contact tracing app development outlining nine data protection principles (transparency, data minimization, user control, data security, etc.) which are linked to the core principles and provisions of data protection law and are designed to support design and development decisions of app developers. The call for compliance with data protection acts and ethics is not only limited to the EU as it is globally demanded. On 28th May 2020, the World Health Organization (WHO) published an interim guideline covering the main ethical principles and requirements to achieve equitable and appropriate use of digital contact tracing technology. Again, most of these principles (transparency, data minimization, data retention, data security, etc.) are well-aligned with other globally endorsed data protection principles (WHO 2020). In this paper, we aim at inspecting the COVID-19 contact tracing apps from an information security and privacy perspective, to investigate if the developers and producers of these apps are mindful of the aforementioned legal requirements and if they have made their apps operational under privacy and security considerations aiming at respecting individuals' privacy. For this reason, we examine available COVID-19 contact tracing apps for cues about their privacy and security quality through several lenses as detailed in the next sections. Our multi-perspective analysis comprises multiple static and dynamic analysis techniques enabling a comprehensive understanding of privacy and security performance of existing COVID-19 contact tracing apps. A high-level overview of our study is shown in Fig. 1 . Our study consists of three main building blocks, namely Study Design (Section 3), Multi-Perspective Analysis (Section 4), and Impact Assessment (Section 5). The Study Design building block details the methods and design steps that were used as multiple inputs for the Multi-Perspective Analysis. As it is depicted in Fig. 1 , the first lot of Study Design is data collection (Section 3.1). The second lot (code's privileges analysis, Section 3.2) enabled an inspection of permission manifest (see Section 4.1). The third lot (privacy policy coverage analysis, Section 3.3) was subjected to inspection about policy coverage (see Section 4.2). Furthermore, the fourth lot (dynamic and static analysis, Section 3.4) was used to perform dynamic and static performance analyses (see Sections 4.3 and 4.4, respectively). It should be noted that the dynamic analysis phase yielded a secondary data set consisting of apps' run-time permission access logs, which was populated through a one week data collection campaign in Germany and Sweden (elaborated in Section 4.3). We archived all the data sets in a Git Repository 2 . The Multi-Perspective Analysis building block serves as an input for the Impact Assessment aiming at synthesising the results obtained from our multi-perspective analysis. Since strong security and strong privacy are preconditions for designing IT products (including smartphone apps) (Cavoukian 2010) , the methodology used in this paper relies on both privacy and security performance analysis of apps that avoids the pretense of false dichotomies such as privacy versus security. Hence, our work remains within the intersection of security and privacy aiming at investigating privacy and security quality indicators inspired by legal requirements such as data minimization, transparency, purpose limitation, and confidentiality. In addition, since our methodology comprises both static and dynamic analyses, it retains the main benefits of both techniques (Chaulagain et al. 2020; Choudhary and Kishore 2018) . In what follows, we explain the steps that were taken to collect our data. Afterwards, we elaborate on the main pillars of the study design -as shown in Fig. 1.  Our study is focused on Android contact tracing apps, however, Google Play Store does not provide any specific category for these apps. We identify them by searching for the strings like ""covid contact tracing"", ""covid"", and ""contact tracing"" on Google Play Store's search engine. However, such a search technique has two main limitations: (1) depending on the location where the queries are done (Germany and Sweden where the researchers of this paper were located), Google may eliminate some of the search results; and (2) such search strings sometimes return irrelevant results, e.g. COVID-19 symptom checker apps (that are not in the scope of this paper). Therefore, both these processes can produce false negatives and false positives, respectively. To overcome these limitations, for (1) we also repeated the same procedure on unofficial app stores (APKMirror 2020; APKPure 2020) that are location-independent that contain the APK files of apps regardless of where the search queries are done. For (2), we manually eliminate any app that does not introduce contact tracing (tracking and monitoring the spread of the virus) as one of its core functionalities. Furthermore, during our data collection process, we noticed that some of the contact tracing apps are not yet available in any app market other than their own websites (e.g. by regularly checking the list of produced COVID-related apps all over the globe (Covid-19 D y n a m ic A n a ly s is S ta ti c A n a ly s is Code's Privileges Analysis  Section 3.1 Section 5 Fig. 1 A high-level overview of multi-perspective privacy and security analysis of COVID-19 contact tracing apps apps 2020)). As such, we were able to find and collect the APK files for those apps as well. In total, we found 28 contact tracing apps as shown in Table. 1. Mobile operating systems follow certain mechanisms to control and limit the amount of personal information accessed by apps (Hamed and Ben Ayed 2016). As a particular example, in Android, apps can request to access the device's resources through permissions. Depending on the resource types, consent from users is required (Hatamian et al. 2017) . Every Android app has an AndroidManifest.xml file that contains information about that particular app (e.g., its name, author, icon, and description) and permissions that grant access to data such as call logs, contact lists or location tracks on smartphones . Approval from the user for granting a dangerous permission is required during the first use of the app. In such a permission managing scheme, it is difficult to perceive consequence for granting access and assess the risk, if not impossible (Zhauniarovich and Gadyatskaya 2016) . Moreover, information is hardly available about the usage of permissions that are allowed access to resources (Hatamian et al. 2017) . Hence, an app should only request and access those permissions that are relevant to its functionality. As such, this pillar of our analysis (see Section 4.1) collects and analyzes the code's privileges, i.e. access intentions from the Android apps' manifest to investigate the mapping, diversity, and critical aspects of COVID-19 contact tracing apps' permission requests with respect to users' privacy. The privacy policy of an app is a statement, or a legal document that gives information about the ways an app provider collects, uses, discloses, and manages users' data. By law, service providers (including app providers) are required to be transparent about their data Stop Covid X X X Based on collecting location and proximity data, sensor data (gyroscope, acceleration, magnetometer), activity data provided by the operating system, and user-ID, it logs interactions that span more than 15 minutes and take place within two meters. If a user tests positive, he can inform the app, and can provide information pertaining to contacts in the last five days. All other users which have been in contact with him in the last five days will be notified by the app. 9 Iceland Ranking C-19 X Tracks users' GPS data to compile a record of where they have been to look at whether those with a positive diagnosis are potentially spreading the disease. 10 India Mahakavach X Collects location history, name, email, phone number, age, gender, and medical records and history to trace the geographical spots a user has been to in the last 14-20 days, and checks how many other people may have come in contact with and thus, possibly transmitted the virus to. It collects these data for other purposes than contact tracing (e.g. to generate reports, heat maps and other statistical visualizations). India COVA Punjab X Collects personal data such as demographics, IMEI/IMSI number, device ID, and movement patterns to trace the geographical spots a user has been to, and checks how many other people may have come in contact with and thus, possibly transmitted the virus to. If the user delets the app, he will still continue to be a registered user of the app and receive promotions /newsletters/ notifications. Aarogya Setu X X Works based on access to proximity (via BLE) and GPS information to alert people when they come in contact with someone who has tested positive. Israel Hamagen X Works based on GPS information and correlates location history to alert people when they come in contact with someone who has tested positive, including the exact time and location. 14 Italy SM-Covid19 X X Uses BLE and GPS to monitor the number of contacts, duration of time with contacts, and distance between contacts. 20 Norway Smittestopp X X Collects proximity (via BLE) and location data to detect other nearby phones with the app installed. Anyone defined as a close contact in the days prior to the diagnosis will receive an SMS. collection, sharing, and processing practices and specify how they comply with legal principles (Hatamian 2020) . Moreover, privacy policies are the main sources that enable users to understand how their data is being handled by app developers/providers (Reidenberg et al. 2015) . Hence, this pillar of our analysis (see Section 4.2) provides insights into the extent to which the privacy policy texts of COVID-19 contact tracing apps cover fundamental privacy policy principles. Our study focuses on the fulfillment of fundamental legal principles proposed in Hatamian (2020) , the extent to which the privacy policy texts of COVID-19 contact tracing apps are correlated with what developers request (in manifest) and what they do in reality (actual permission usage), and ultimately the discrepancies/similarities in apps' privacy policies published by the EU and non-EU bodies in terms of covering fundamental privacy principles. Based on keyword-and semantic-based search techniques, for each privacy policy a group of three data protection experts with legal and technical background of data privacy and security went through the texts. The goal was to figure out if they can find any overlap between each and every section of privacy policy texts and the legal principles discussed in the following. As our research is conducted in EU countries, our analysis is solely based on the GDPR. In the following, we briefly discuss each of the privacy policy principles. The legal foundation is set in Art. 5 (1) and Art. 6 GDPR. While the former article states the general principles of processing personal data, the latter indicates when processing is lawful, including when consent is given, when it is necessary for the performance of a contract or compliance with a legal obligation, to protect vital interests of user or another natural person, and when processing is necessary for a task carried out in the public interest or for legitimate interests pursued by the controller or by a thirdparty. However, this applies if and only if such interests do not override the interests or fundamental rights and freedoms of users. Monetizing purposes, i.e., advertising, are not classified as necessary and therefore need to be based on another legal ground. Similarly, the processing of data to develop new features and services is not specific enough to comply with this section (ENISA 2017). Children Protection Information related to children must be treated with the utmost caution, as children ""may be less aware of the risks, consequences, and safeguards concerned and their rights in relation to the processing of personal data"" (Rec. 38 GDPR). This implies that services targeted at children are obliged to provide information in clear and plain language that children can understand easily (Rec. 58 GDPR). Art. 8 GDPR defines that the processing of children's data is only lawful where the child is at least 16 years old. The data processing of younger children is only legitimate if and to the extent, a parent or legal guardian has given consent. However, this article has an opening clause, allowing member states to set a lower age for those purposes, yet not below 13 years. Third-Party Sharing Third-party components (that might collect data as well) are often integrated into an app's development phase. The legal basis lies in Art. 13 (1e) GDPR, stating that the recipients or categories of recipients of personal data must be revealed to users. The GDPR dedicates its Chapter 5 to provisions on transfers of personal data to third-countries or international organizations. The transfer of data to other countries is only lawful, where a similar level of protection as provided by the GDPR is guaranteed. In fact, the protection of data travels with the data itself. Thus, if app providers share personal data with servers located outside the EU, they shall mention it in their privacy policy text how they deal with third-country data sharing practices. The GDPR in Art. 32 states that the data controller must implement appropriate technical and organizational measures to ensure appropriate security. This is of particular importance in smartphone ecosystems since they are typically linked to a huge amount of data transfer. The aspect of data protection is also closely correlated with privacy-by-design (Cavoukian 2010 ). The retention of data is a delicate issue, as app providers may want to retain data as long as possible to enable future transactions and purposes. However, this is often not in the interest of users, particularly not for sensitive data as available in smartphones (e.g., personal information from dating apps or health data from contact tracing apps). To protect users, the principle of data minimization and storage limitation in combination with transparency take effect. Accordingly, Art. 13 (2), 14 (2) of the GDPR state that the data controller must inform users for what period their data is retained. This is strictly required as users have ""the right to be forgotten"", which is set in Art. 17 of the GDPR. Chapter 3 of the GDPR is dedicated to the rights of users. The most important rights are the right to information and access to personal data; the right to rectification; the right to erasure (see the previous principle); the right to restriction of processing; the right to data portability; and the right to object and automated individual decision-making. By Art. 13 (2), 14 (2) of the GDPR, app providers are required to provide these rights to users to ensure fair and transparent data processing (principle of lawfulness, fairness and transparency Art. 5 (1a)). To further ensure lawful, fair, and transparent processing of data, app providers should inform users in a transparent and understandable way about privacy policy changes. This obligation is derived from Art. 12 of the GDPR. Privacy Breach Notification Besides Art. 12 GDPR, which lays the basis of informing users, this principle is based on Art. 34 GDPR where it is stated that if a data breach occurs that results in a high risk to the rights and freedoms of users, the data controller must inform users without undue delay. In this notification, the data protection officer must be named and likely consequences of the data breach as well as the measures taken to mitigate the effects are described. The same is applicable for the notification of the supervisory authority, which must be done not later than 72 hours after the detection of a personal data breach. This principle is subsumed under the principle of lawfulness, fairness, and transparency. Sometimes a privacy policy is not exclusively written for a specific app, but multiple services provided by the same app developer (data controller). For instance, Sunyaev et al. (2015) identified five reoccurring scopes of privacy policies, namely privacy policies for a single app, for multiple apps, for a back-end app, for a developer homepage or for all developer services. They also found that several privacy policies of apps did not have an app-related scope at all. Purpose Specification This principle is closely related to the data collection principle. While the focus of data collection is on what data is collected, this principle refers to the clear statement of data collection purposes. Besides the legal basis for data processing, app providers are required to specify data collection purposes according to Art. 13 (1c), 14 (1c) GDPR. This is not only important under the aspect of lawfulness, fairness, and transparency, but also the principle of purpose limitation to prevent exploitation of personal data for other use cases. Contact information is linked to the principle of lawfulness, fairness, and transparency. According to Art. 13 (1a), 14 (1a) GDPR, users have the right to be informed about the actual identity of data collectors, i.e., app providers. This includes the name of the app provider, if it is a legal entity, its legal representatives as well as its postal address. The latter must be provided to give users the possibility to file a formal complaint. Mobile app users trade their data for service usage in opaque ways. Accessibility to user data through permissions gives carte-blanche 3 access for an app without any constraints. Though the user has the option to revoke granted permissions, the absence of monitoring tools and unexpected consequences such as service exclusion (being unable to use a certain service) or malfunctions (e.g. UI malfunctioning) may cause hindrances in limiting access to permissions (Almuhimedi et al. 2015; Franzen and Aspinall 2016; Van Kleek et al. 2017 ). As such, it is not only important to assess the real data access patterns of apps, e.g. what personal data an app is accessing while the user is not using it, but also how the app performs in terms of limiting/minimizing potential vulnerabilities within its code. Therefore, this pillar of our analysis analyzes the contact tracing apps from two dimensions: 1. We dynamically measure apps' real permission access patterns (see Section 4.3) based on our previously implemented tools described in Hatamian et al. (2018) and Momen (2018) . Our approach is based on AppOps which is a privacy manager tool and introduced in Android 4.3. 4 In order to collect logs, a timer is sent to the PermissionUsageLogger service periodically. When it is received, the logger queries the AppOps service that is already running on the phone for a list of apps that have used any of the operations we are interested in tracking. We then check through that list and for any app that has used an operation more recently than we have already checked, we store the time at which that operation was used in our own internal log. These timestamps can then be counted to get a usage count. We argue that such an analysis can reveal apps' behavior and its impact on individual privacy, because of the fact that data collection can be identified as the first step that could lead to potential privacy violation (Daniel 2006) . 2. We statically analyze the contact tracing apps (see Section 4.4) and look for potential vulnerabilities in their program codes using Mobile Security Framework (MobSF) (Mobile security framework (mobsf) 2020). MobSF is a security analysis tool that is capable of performing both static and dynamic analysis, penetration testing and malware analysis of Android, iOS and Windows apps. Thus, this pillar of our analysis examines the existing security vulnerabilities of COVID-19 contact tracing apps. In this section, we expand each pillar of our analysis method. Section 4.1 details the permission manifest analysis. Section 4.2 elaborates on the privacy policy coverage through inspecting apps' privacy policy documents with respect to legally binding privacy principles. While Section 4.3 details the examinations regarding the real data access patterns of apps in a dynamic way, Section 4.4 digs into the steps that we took to uncover the potential vulnerabilities found in apps' codes. In Android, apps can request access to the device's resources through permissions. Depending on the resource types, consent from users is required. Android defines three types of permissions: 5 normal, dangerous, and signature. Normal level permissions allow access to resources that are considered low-risk, and they are granted during the installation of any package requesting them. The dangerous level permissions grant access to resources that are considered to be high-risk. In this case, the user is explicitly asked to grant permissions. So-called signature level permissions are granted at install time, but only when the app that attempts to use a permission is signed by the same certificate as the app that defines the permission. Every app has an AndroidManifest.xml file that contains information about that particular app (e.g., its name, author, icon, and description) and permissions that grant access to data such as call logs, contact lists or location tracks on smartphones. We collect and analyze app developers' data access intentions from the Android apps' manifest to investigate permission request patterns, suspicious permission requests, and discrepancies/similarities in apps' permission requests published by EU and non-EU bodies. Table 2 lists the detected permission requests by the apps within our data set together with their descriptions. In total, 31 permission requests were detected. Among these 31 permissions, 16 permissions (51.6%) belong to normal, 11 (35.4%) to dangerous, and 4 (12.9%) to signature categories, respectively. Figure 2 shows the details of permission requests per app. Overall, there are 335 permission request incidents. Almost one-third (31%) of these incidents pertain to the category dangerous having a direct impact on users' privacy, 66% normal permissions, and 3% signature permissions. Among the apps within our data set, Gerak and Mahakavach declare 8 (out of 18) and 7 (out of 14) of their permission requests from dangerous permission category, respectively. Followed by this, each of diAry, Covid Safe, and COVID Punjab apps  Allows read access to phone state, e.g. phone number and current cellular network information. Allows to record audio. requests 6 permissions from dangerous category. By contrast, we also found apps in our data set that either do not ask for any dangerous permission (1 app, Stopp Corona) or only ask for one dangerous permission (COVIDSafe). Figure 2 clearly indicates that, in general, COVID-19 contact tracing apps require a mixed variety of permissions to provide their desired services. In terms of median value, COVID-19 contact tracing apps request 12 permissions, 4 of them being labeled as dangerous. Although our main focus is on dangerous permission requests, non-dangerous permission requests can also pose serious risks on users' privacy. It is important to highlight that non-dangerous permissions can be easily misused to profile users. As a particular example, GET TASKS permissions which is requested by 5 apps can reveal sensitive information about which apps are being used by the user. As a result, the provider of the contact tracing apps is able to see ActivityManager.RecentTaskInfo 6 that can retrieve information about tasks that the user has most recently started or visited. Similarly, SYSTEM ALERT WINDOW is another example of non-dangerous permissions that can reveal highly sensitive information through creating overlays that can trick users by covering certain areas of the screen while making the overlaid area responsive Patsakis 2017, 2019) . It is also important to highlight that the combination of such permission requests may reveal interesting information about users Momen and Fritsch 2020) . For example, ACCESS WIFI STATE (requested by 8 apps), CHANGE NETWORK STATE (requested by 3 apps), and CHANGE WIFI STATE (requested by 4 apps) -these permissions are automatically granted to the apps as they are labeled as normal. The combination of accessing them allows an app to connect and disconnect from a given WiFi network. Such a combination of permission requests allows an app to retrieve information of the connected WiFi networks which can reveal highly sensitive information such as how long certain users stayed in a similar proximity, how often, when exactly, etc. Achara et al. (2014) and Alepis and Patsakis (2019). As it can be observed from our data set discussed in Section 3.1, the studied contact tracing apps have been developed and published across different countries. Therefore, this aspect of our analysis deals with the question of how contact tracing apps may behave differently or similarly in terms of dangerous permission requests when it comes their geographic area in which they got produced and published. Since our research is conducted in Europe and the legal requirements enforced by the EU General Data Protection Regulation (GDPR) (2016) and relevant European authorities serve as a benchmark for our multi-perspective privacy and security analysis, we generally categorize all the apps within our data set into two main categories, namely EU (9 apps) that mainly covers those Member States where the GDPR is enforced and non-EU apps (19 apps). Such a comparison enables us to not only compare the behavior of certain apps within a category, but also the discrepancies or similarities between each individual category in terms of requesting dangerous permissions. Figure 4 shows the comparative analysis in terms of dangerous permission requests per category (percentage is calculated by dividing the number of apps in a certain group, e.g. non-EU apps, which request a certain dangerous permission by the total number of apps in that group). Overall, Fig. 4 Comparison of dangerous permissions found in the manifest analysis: EU vs. non-EU contact tracing apps (the lower, the better) the results clearly indicate that the developers/providers of apps published within the EU category request less sensitive permissions than others. One potential interpretation for such a behavior might be the strong enforcement of the EU GDPR along with other strict European guidelines concerning designing and developing contact tracing apps (COVID-europe 2020 (COVID-europe , 2020 (COVID-europe , 2020 . Except for phone-related information (e.g. IMEI and phone number), the EU apps request less sensitive permissions than non-EU apps in all respects. For instance, when it comes to location-related permission requests, non-EU apps request slightly more than double in comparison to the EU ones. The reason for such behavior may be threefold. Firstly, the GDPR has a clear and strict vision on data minimization and purpose specification as it states in its Art. 5 (1b) that personal data shall be ""collected for specified, explicit and legitimate purposes and not further processed in a manner that is incompatible with those purposes; further processing for archiving purposes in the public interest, scientific or historical research purposes or statistical purposes shall, in accordance with Art. 89 (1), not be considered to be incompatible with the initial purposes"". Further, it states (in Art. 6 (4)) that ""data processing for incompatible purposes should be avoided unless it is on the basis of a specific set of criteria in the GDPR"". Secondly, in the EU, collecting location data forever and obtaining single user consent (for multiple purposes) was unacceptable under the previous Data Protection Directive (Directive 95/46/EC) (1995). Nevertheless, thanks to the GDPR, it is now even more stricter as the GDPR provides more detailed information on data usage and retention and consent becomes even more specific. Thirdly, ePrivacy Directive (2002) more in details elaborate on the issue of location data collection and clearly states that such a data collection may result in high privacy risks, particularly when individuals' movement patterns are tracked. Further, it states that ""such data may only be processed when they are made anonymous, or with the consent of the users or subscribers to the extent and for the duration necessary for the provision of a value added service. The service provider must inform the users or subscribers, prior to obtaining their consent, of the type of location data other than traffic data which will be processed, of the purposes and duration of the processing and whether the data will be transmitted to a third party for the purpose of providing the value added service. Users or subscribers shall be given the possibility to withdraw their consent for the processing of location data other than traffic data at any time"". Nevertheless, we should highlight that although the EU apps have better performance (compared to non-EU apps) in asking location-related permissions, their behavior is still not compliant with the requirements and recommendations published by the European Commission and other EU data protection authorities as many of them are asking to access such data. In our analysis, we pay attention to privacy policy analysis of contact tracing apps and fulfillment of 12 fundamental legal principles proposed in Hatamian (2020) , the extent to which the privacy policy texts of COVID-19 contact tracing apps match what developers request (in manifest) and what they do in reality (actual permission usage), and ultimately the discrepancies/similarities in apps' privacy policies published by EU and non-EU bodies in terms of covering fundamental privacy principles. However, one should bear in mind that since these principles are extracted from the GDPR, they may not be necessarily applicable to all non-EU apps, especially those that do not offer a strong data protection regulation as offered by the GDPR. Figure 5 shows the results of privacy policy analysis of the apps within our data set. The result indicates to which extent an app has been successful to fulfill the privacy policy principles. The results show that the Gerak app fulfills the maximum number of principles (9 principles), followed by Stopp Corona (8 principles) and VirusRadar (8 principles) . Surprisingly, our findings also revealed that some of these apps (five apps) do not fulfill any privacy policy principle either because they do not have any privacy policy text accessible on the Internet that is the case for two of them (e.g. Coronavirus-SUS and Aarogya Setu) or because they have very generic text that does not discuss the data collection and sharing practices of apps, rather irrelevant information that is the case for three of them (MyTrace, Covid Safe, and PrivateKit). Figure 6 provides a detailed overview regarding the total number of privacy policy principles covered by each contact tracing app. As it can be seen, the maximum number of principles covered is 9 (out of 12). Moreover, only 10 (35.7%) apps covered more than half of the principles, and the rest either did not cover any principles (17.8%, 5 apps) or covered less than half of the principles (46.5%, 13 apps). Figure 7 shows the coverage of privacy policy principles by the apps within our data set. Our inspection shows that data collection is the most covered principle (17 apps). Surprisingly, we found out that no apps fulfilled the privacy breach notice principle. This is highly critical as service providers in EU are enforced by law to adopt appropriate remedies in case of a data breach. This also shows that none of these apps are well-prepared in case users' personal data fall into the wrong hands due to a privacy breach. The same also holds for children protection and privacy policy changes where only a few apps (1 app for children protection and 3 apps for privacy policy changes) clarified how their data collection, sharing, and processing practices fulfill these essential principles.  Transparency is a basic data protection principle endorsed by privacy-by-design (Cavoukian 2010 ) and the GDPR. Importantly, it is one of the fundamental principles strictly endorsed by the WHO (2020) to be followed by the developers of contact tracing apps. Therefore, it is of particular importance to examine the extent to which contact tracing apps fulfill such a requirement. We developed a set of relevant keywords (e.g. location, proximity, precise, approximate, track, movement, gps, and so on) corresponding to each dangerous permission (e.g. ACCESS FINE LOCATION) defined by Android 7 to conduct a manual observation for this task. As shown in Fig. 8 , we found that only 14.2% (4 apps) of contact tracing apps fully justify the need for requesting dangerous permission requests. Further, 28.5% (8 apps) of them only partially clarify why they need to access certain dangerous permissions. This indicates that more than half of the contact tracing apps (57.2%, 16 apps) failed to specify the need for requesting dangerous permissions. We also conducted a comparative analysis similar to the analysis that we carried out regarding the permission requests of apps across the EU and rest of the world to compare the Figure 9 shows the comparative analysis in terms of privacy policy performance per category. Overall, similar to what we observed concerning the permission manifest analysis, the EU apps are the most legal compliant ones. Except for privacy policy changes, the EU apps perform better in all respects. For instance, when it comes to user's controls, only 42.1% of non-EU apps provide some specifications on how they allow users to exercise their rights (the percentage is calculated by dividing the number of apps in a certain group, e.g. non-EU apps, which fulfill a certain legal principle by the total number of apps in that group). While this percentage is 88.8% for European apps. As for contact information, only one-fifth of non-EU apps provided precise contact information in order to enable users (data subjects) to contact them. However, this amount is almost 90% for EU apps. The results clearly show that EU apps comply with the GDPR better than non-EU apps.  Run-time analysis is another pillar of our multi-perspective analysis that provides us with information regarding the permission access patterns of contact tracing apps at run-rime. This is of particular importance as once a permission is granted to an app through the Android permission manager system that was introduced in API 23 in Android 6.0 (Marshmallow) 8 to offer granular control to users, its risks are not fully mitigated Hatamian et al. 2017; Momen et al. 2017 ) as granted privileges remain available to resource-hungry apps and advertising libraries that could lead to privacy implications (Momen et al. , 2020 . This is why this section focuses on the question about how the apps exercise their granted privileges to access permissions at run-time. Permission usage monitoring was conducted using our previously proposed tools (Hatamian et al. 2018; Momen 2018 ) through logging, collecting, and analyzing permission access patterns of contact tracing apps (e.g. access to sensitive resources like GPS). To analyze the behavior of contact tracing apps listed in Table 1 , we installed our monitoring tool together with these 28 apps on laboratory devices. Next, while our monitoring tool was running in the background the whole time (i.e., it was monitoring the permission access frequency of apps), we started to open each and every contact tracing app once to trigger and activate their desired functionality. To make sure that we did not miss any certain functionality, we deliberately granted permissions whenever asked by the apps (either through the GUI or the permission manager system of Android). Afterwards, we let the apps to execute in the background (without any further interactions, but were connected to WiFi network and a power source). This was by intention as our goal was to figure out if apps access any sensitive resources while there is no legitimate reason for that. After a one week ongoing experiment, the data generated by the monitoring tool was collected and analyzed. Since the researchers of this study were located in two EU countries (Sweden and Germany), the log collection campaigns were carried out simultaneously in these two countries to investigate if contact tracing apps behave differently when being used in different geographic locations. Our objective was to find out what is being accessed by apps, at what time, and at which frequency. For instance, Fig. 10 shows an example of collected logs, where eRouska accesses a location-related permission at a certain time. Our motivation behind running parallel data collection campaigns in Germany and in Sweden about apps' run-time behavior was to identify geographical influence and to achieve clarity about app behavior. However, we observed similar, if not identical, permission access patterns during run-time. Hence, Fig. 11 presents a cumulative result from both data collection campaigns during a one week period. Majority of the apps (17 out of 28) are found to be accessing three variations of LOCATION permission. Hence, they create risks to location privacy that might lead to citizen tracking (Fritsch 2008b ). Among them, 12 apps are from non-EU countries and 5 apps are from EU countries. Non-EU apps seem to be exercising location privileges more often. However, an EU app -SM-Covid19 (Italy) showed the highest number of permission access within one week of data collection period (36.7K), which cancels out the good behavior (fewer permission access counts) of other apps. Variants of STORAGE permission (cumulatively presented as READ EXTERNAL STORAGE) were accessed by 9 non-EU apps and by 2 EU apps. Other than these, 2 non-EU apps accessed CAMERA (Covi-ID) and RECORD AUDIO (Novid). In Figs. 12 and 13, we present a timeline of the most and the least frequent permission access patterns observed in the data set from Sweden (top five and bottom five, respectively). Figure 12 presents the timeline of permission access patterns of the top five permission-hungry apps: (i) Contact tracer, (ii) Stop Covid, (iii) SM-Covid19, (iv) Private Kit, and (v) CoronApp. All of them are found to be accessing the location-related permissions frequently such as ACCESS FINE LOCATION. Figure 13 shows a visual timeline of the privilege access patterns of the top five privacy-preserving apps from our data set: (i) VirusRadar, (ii) Stopp Corona, (iii) Gerak Malaysia, (iv) Novid and (v) Coalition. Their permission access patterns demonstrate significantly fewer usage than the rest. However, Coalition showed permission access event for LOCATION permission, whereas, Novid showed permission access event for RECORD AUDIO permission in the timeline (although this might be justifiable to a certain extent due to the app being relied on ultrasonic technology, we wonder if these accesses are legitimate due to the app not being used actively). It should be noted that our analysis suffers from the limitation of false positive in the indication of good behavior. Though permission access frequency is rather low for these five apps in Fig. 13 , they could behave differently during real-life usage (e.g. with user interaction). On the other hand, the app could have over-privilege-issue, should it not require the corresponding permission. For example, good behavior is documented for both VirusRadar and Gerak in Fig. 13 , and at the same time, Fig. 2 shows that they have the LOCATION permission listed in its manifest file. Similarly, Novid and Coalition have the potential to behave in a privacy invasive manner What needs to be highlighted here is the fact that many apps access location-related permissions. We argue that this is extremely problematic from a privacy perspective due to (1) according to regulatory guideline documents (COVID-europe 2020; EDPB-letter 2020) contact tracing apps should not request and access location data (as this is not relevant to their proper functionality), (2) even though in some cases apps state (in their privacy policies) that they request location data, we believe this is still problematic and not aligned with best privacy and security practices, (3) as detailed in Fig. 8 , many apps failed to state if they use a certain dangerous permission (including location-related ones) and many of them started to access it in a non-transparent manner (Coronavirus-SUS, Ito, Covid Safe, PrivateKit, etc.), and (4) we are aware that in Android, apps must declare location permission in order to be able to access Bluetooth 9 . However, this does not mean that apps that use Bluetooth can actively use location permission. Granting location permission is meant to activate the general system setting Location Services. We observed that some of these apps (e.g. Ito, Covid Safe, Coalition) claim to rely on BLE technology, but our analysis shows that they have been accessing location-related permissions. One of the essential and critical aspects highlighted by the eHealth Network (COVIDeurope 2020) is the security of COVID-19 contact tracing apps. With the governments' and developers' rushing to roll out contact tracing apps, concerns are emerging in relation to the fact that there could be a broad range of security vulnerabilities lurking within these apps, which could put an individual's sensitive information at risk of being exploited. Further, these apps access dangerous permissions (as highlighted in the previous sections), which suggests that they indicate potential security risks. Therefore, it is highly important that these COVID-19 apps are secured against malicious attacks as they access, store and transmit sensitive data. In order to lessen any potential harm that would emerge from exploiting a vulnerability, an analysis is required in detecting potential security weaknesses that could be exploited and leveraged by attackers within these apps. As such, we applied static analysis to the complete app set shown in Table 1 using MobSF (Mobile security framework (mobsf) 2020). While there are other tools capable of performing static code analysis, MobSF was selected based on its popularity, scalability, easy adoption and its effectiveness in identifying vulnerabilities within Android apps much quicker (Ibrar et al. 2017; Zhang et al. 2018) . Vulnerability assessment through static analysis relies on data contained in the app's APK file, which includes the manifest and the compiled code (Knorr et al. 2015) . In this case, however, we do not dwell on the permission analysis as it has been extensively discussed in the prior sections. We analyzed the results generated by the MobSF with the intention of identifying critical vulnerabilities within the apps and comparing the security performance of apps published by EU and non-EU bodies. Tables 3 and 4 provide an overview of the security issues identified from the analysis of the APK files using MobSF. The framework uses Common Vulnerability Scoring System Version 2.0 (CVSS V2), Common Weakness Enumeration (CWE) and Open Web Application Security Project (OWASP) Top 10 Mobile Risks to list and score some of the common vulnerabilities within the app set as seen in Table 3 , where the vulnerabilities identified (listed as issues) have been selected and matched against these standards. While each app has considerable amount of vulnerabilities as per the report generated by MobSF, we picked vulnerabilities that had a score ranging from medium to high as highlighted under the CVSS V2. CVSS V2 provides an open framework for communicating the characteristics and impacts of IT vulnerabilities (Mell et al. 2007) while CWE 10 provides a point of reference for common vulnerabilities within software and hardware, and baseline for vulnerability identification. On the other hand, OWASP Top 10 provides a list of top ten mobile risks which creates awareness about application and software security (Qian et al. 2018 ). In addition to these, the framework provides the app's average CVSS score and App Security Score. The outcome of the analysis revealed a number of vulnerabilities which could be considered as critical in this case. For example, 89.3% of the analyzed apps, apart from Stopp Corona (Austria), Novid (US) and Mahakavach (India), contain potentially hard-coded sensitive information like usernames, passwords, keys etc, which is considered to be of high severity as mapped under the CVSS V2 (with a 7.4 score); however, while the vulnerability does not match to a CWE indicated in Table 4 , the use of hard-coded sensitive information (CWE-798) is of high risk as it could allow an attacker to circumvent authentication set by a software administrator. 11 This information can be accessed through reverse engineering the contact tracing apps' source code. As a result, armed with this information, an attacker is able to gain access to even more sensitive information, including the health status and location data. Further analysis of the results indicate that 75% of the apps use SQLite Database and execute raw SQL query, apart from Contact Tracer, StopKorona!, COVIDSafe, CoronApp, eRouska, VirusRadar, and StopCovid. Untrusted user input in raw SQL queries can potentially lead to a local SQL Injection in the contact tracing app. In addition to this, the apps tend to use an unencrypted SQLite Database. This leaves the sensitive information lying open to attackers (Jain and Shanbhag 2012) with physical access to the mobile device or a malicious app with root access to the device. Besides this, lack of encryption could lead to privacy infringement and non-compliance to data protection laws and regulation. Additionally, one app (Contact Tracer, Russia) uses an insecure implementation of SSL which leads to insecure communication. As indicated, the app could be vulnerable to MITM attacks, which undermines the information security goal of confidentiality and integrity (Jain and Shanbhag 2012) . These vulnerabilities identified within the aforementioned COVID-19 contact tracing apps result in not a robust security performance. While it was difficult to compare the security analysis of COVID-19 contact tracing apps to other generic apps empirically, we compared previous work that analysed the security of android apps using MobSF to determine whether the magnitude of the identified vulnerabilities could be similar to generic apps. Comparing the vulnerabilities detected in work done by (Papageorgiou et al. 2018) , it can be noted that COVID-19 contact tracing apps have significant and a high number of vulnerabilities. This can be supported by recent security analysis performed on COVID-19 contact tracing using MobSF by Sun et al. (Sun et al. 2020) , which shows the same magnitude of vulnerabilities we identified in our app set. Hence, it can be argued that while other android apps, e.g., m-Health apps contain vulnerabilities, the quick development and release of COVID-19 contact tracing apps without sufficient security analysis tends to result in apps that have a poor security posture. As mentioned, our app set contains a variety of COVID-19 contact tracing apps that come from different countries across the globe. Therefore, we aim at interpreting and answering the question of how these apps differ or are similar to each other in terms of their security fulfillment across the EU and rest of the world. Figure 14 shows a comparison graph of COVID-19 apps security score between EU and non-EU apps. The app security -which is as a result of the MobSF measurement -is given a score of 0-100, as manifested in the generated reports; where 0-15 indicates critical risk, 16-40 indicates high risk, 41-70 indicates medium risk, and 71-100 indicates low risk. Overall, it can be noted that majority of the apps have a very poor app security score. However, by taking each area in isolation, several differences can be identified. The EU contact tracing apps show major differences in app security score. While some apps show a medium app security score, for example, eRouska (Czech Republic) with an app security score of 60 and ito (Germany) with an app security score of 60, which is majorly attributed to the fact that they have few vulnerabilities, with even less that have high severity score, four of them show critical security scores. Interestingly, one of these apps with a critical app security score (SM-Covid19, Italy) has also been highlighted as one of the five apps that hungrily accesses dangerous permissions (see Section 4.3.2). These type of permissions go for more sensitive information from the user. Hence, if the vulnerabilities within these apps are exploited, an attacker would be able to access highly sensitive personal data -which would range from location to health status among other personal data. Despite the fact that these countries are GDPR enforced and some apps tend to access less sensitive resource events, it can be interpreted that the reason for such an app security score is due to the considerable number of vulnerabilities inherent in these apps. This of course leads to several security issues. For example, the static analysis conducted on diAry (Italy) shows that the app contains hard-coded sensitive information and the data stored within the SQLite database is not encrypted; an attacker could gain access to hard-coded credentials thus giving them further access to sensitive data, or they could potentially read sensitive personal information from the unencrypted SQLite database. When it comes to non-EU contact tracing apps, a majority of them manifest critical app security score, with Private Kit (US) manifesting an app security score of 0. From the report, it is indicated that the Private Kit contains vulnerabilities that have a high severity, for example, the use of a weak hash (MD5), which is mapped in Table 4 under CWE-327, could lead to a breach in confidentiality or privilege escalation when exploited (Jain and Shanbhag 2012) . Further, the app uses dangerous permissions for the purposes of contact tracing, which put a user's security at risk. While this is the case, there are some non-EU apps that perform well in terms of security aspects. Of noteworthy is COVIDSafe (Australia), which manifested a low risk app security score of 75. From the privacy analysis conducted earlier, COVIDSafe shows that it only accesses one dangerous permission. On the other hand, the security analysis shows that it has few vulnerabilities which can be considered as trivial and low risk (apart from the existence of potential hardcoded sensitive information). This indicates that several factors were considered, including the necessary security measures needed for data protection (Cavoukian et al. 2009 ) prior to its roll out. After conducting a multi-perspective analysis of existing COVID-19 contact tracing apps, we carry out an assessment to quantify and assess privacy and security the impact of the studied contact tracing apps. Such an assessment provides a basis for risk evaluation based on our empirical data and multi-perspective analysis. We calculate the impact as an aggregated score composed of the metrics explained in the previous sections of this article, namely dangerous permission requests (Section 4.1), privacy policy analysis (Section 4.2), dangerous permission usage (Section 4.3), and security vulnerabilities (Section 4.4). We stress that our score is a privacy and security impact indicator, not an assessment of personal data subject impact normally provided with GDPR-demanded data protection impact assessments nor an indicator of how dangerous/friendly an app might be for individuals' privacy.  show the impact assessment criteria, namely dangerous permission requests, absent clarification of privacy principles in policy texts, idle usage of dangerous permissions, and vulnerability threats. We also introduce a set of u users by . . , f v i } be the set of features for each app v i . Accordingly, f v i consists of ordered quadruples {(w 1 , x 1 , y 1 , z 1 ), (w 2 , x 2 , y 2 , z 2 ) . . . , (w e , x f , y g , z h )}. We determine each feature as an informative element regarding each app. As a result, the set of features related to all apps is defined as {(F v 1 ), (F v 2 ), . . . , (F v a )}, where (F v 1 ) represents the feature F v 1 associated to app v 1 . We denote the Impact Value (I V ) regarding each individual feature F v i concerning app v i (1 ≤ i ≤ p) while the following condition is met: where if there is a hit in To achieve the Total Impact Score per app, our goal is to fuse the quadruple impact assessment criteria (see Sections 4.1 to 4.4) as follows: 1. Dangerous Permission Request Score (w j ): The first impact criteria deals with the impact of dangerous permission requests as discussed in Section 4.1. The Dangerous Permission Request Score has an upper limit of 8, since each app can request a maximum number of dangerous permissions as many as 8 permissions detected in our analysis. As it is evident, the higher the Dangerous Permission Request Score, the severe the impact for privacy. For the sake of simplicity we have grouped all locationand storage-related permission accesses into LOCATION and STORAGE categories. Further, we have also normalized all the values between 1-100, being 100 means the most severe case (requesting the maximum number of dangerous permissions, i.e. 8 permissions). The second impact criteria deals with the impact of the absence of essential privacy principles in the privacy policy texts of contact tracing apps as discussed in Section 4.2. The Absent Specification in Privacy Policy Text Score has an upper limit of 12, since each app needs to cover 12 privacy policy principles. As can be inferred, the higher the Absent Specification in Privacy Policy Text Score, the severe the impact for privacy. For the sake of simplicity we have also normalized all the values between 1-100, being 100 means the most severe case (the absence of all privacy principles in privacy policy texts, i.e. the absence of 12 privacy policy principles). 3. Idle Dangerous Permission Usage Score (y h ): The third impact criteria deals with the impact of idle usage of dangerous permissions at run-time by contact tracing apps as discussed in Section 4.3. Similar to the Dangerous Permission Request Score, the Idle Dangerous Permission Usage Score has an upper limit of 8, since each app can access a maximum number of dangerous permissions as many as 8 permissions detected in our analysis (the higher the Idle Dangerous Permission Usage Score, the severe the impact for privacy). For the sake of simplicity we have grouped all location-and storage-related permission accesses into LOCATION and STORAGE categories. Further, we have also normalized all the values between 1-100, being 100 means the most severe case (accessing the maximum number of dangerous permissions while being idle/running in the background without any user interaction, i.e. 8 permissions). 4. App's Security Score (z m ): The fourth (last) impact criteria deals with the impact of detected security vulnerabilities of contact tracing apps as discussed in Section 4.4. The tool used for our analysis generates security scores per app within our data set ranging from 1 to 100, being 100 means the best security performance, however, to make such a score unified and compatible to other scores, we use the inverse percentage, e.g. an app that has a security score of 80 -the more closer to 100 means better security performance -will be inverted to 20 means that the more closer to 0 means better security performance. Therefore, while calculating the Total Impact Score, we use the inverted value of App's Security Score, meaning that an app security score of 100 means the most severe case. We propose the following Total Impact Score as the averaged value of all individual I V v i ,F v i as described above as follows: It is worth mentioning that, we presume all the quadruple impact assessment criteria to be equally risky for privacy and security. In addition, we treat the different data sets as contributing equally to the Total Impact Score when fusing the results. The reason for giving equal weights to these four criteria is threefold: (1) privacy-by-design (Cavoukian 2010 ) as a global standard framework demands both strong security and strong privacy that avoids the pretense of dichotomies such as privacy versus security. Hence, our work remains within the intersection of security and privacy aiming at investigating privacy and security quality indicators inspired by legal requirements. That is why our impact assessment scheme is based on the assumption that apps should offer both strong privacy and strong security, and therefore, all these four privacy and security criteria are equal, (2) these contact tracing apps have very clear functionality (breaking the chain of the virus) and for this functionality they must function with the minimum number of dangerous permission requests as their goal should be neither monetizing nor tracking and profiling (as opposed to other apps like social networking ones), and (3) there is a global consensus on how these apps should function under strict data protection goals (as discussed in Section 2.2). As such, we believe prioritizing one or some of these four criteria (which one is is less or more important) will go against the global demand for making these apps function under strict data protection requirements. We would like to mention that the main objective of such an impact assessment scheme is to classify apps according to their privacy and security impact and quantify the impact that each of them might have on individuals' privacy and security. In fact, the objective of such an impact assessment scheme is not to rank/score apps based on their privacy and security intrusiveness level and to say which one is more privacy-intrusive or more privacy-friendly. Rather, the goal is to see how each app performs when it comes to the impact that has for individuals' privacy. This is an important point as all these apps are homogeneous in terms of functionality (contact tracing apps) and expected (by law) to function without the need to access sensitive data (e.g. GPS data). As a result, the goal of such an aggregated impact assessment score (combination of four privacy and security criteria) was to see which of these apps show hungriness in requesting and accessing sensitive data (while they are not supposed to be data greedy by law), which ones are more transparent and open to users (while they are required by law to be fully transparent to users), and which ones are more advanced in terms of following security requirements within the app's code (while they are required to fulfill essential security requirements within the app's codes). Based on our analysis, the impact of an app on users' privacy can be deemed as more severe if it requests higher number of dangerous permissions, has less coverage of essential privacy principle in its privacy policy document, has higher number of dangerous permissions accesses at run-time, and has higher security vulnerabilities. Algorithm 1 details all the aforementioned steps corresponding to the measurement of both individual and total impact scores. We measured and assessed the scores related to the impact assessment criteria as shown in Fig. 15 . We show dangerous permission requests score with green, absent specification (1) dangerous permission requests score (green), (2) absent specification in privacy policy text score (red), (3) idle dangerous permission usage score (orange), (4) app security score (blue), and (5) the Total Impact Score (black). The more closer the scores to 100, the more severe the impact on individuals' privacy in privacy policy text with red, idle dangerous permission usage score with orange, app security score with blue, and the Total Impact Score with a thick black line. The more closer the scores to 100, the more severe the the impact on individual' privacy. As can be seen, the highest scores belong to Absent Specification in Privacy Policy Text Score and App's Security Score. This means that these apps have serious issues when it comes to having clear, concise, and regulation-friendly privacy policy texts. Similarly, most apps suffer from critical security issues (as detailed in Section 4.4) that resulted in very high scores (5 apps were scored as maximum, i.e. 100). Although there have been suspicious and abnormal run-time dangerous permission accesses, our results show that apps perform better when it comes to their Idle Dangerous Permission Usage Score (the highest and lowest scores are 37,5 and 0, respectively). As the results from four different sources are aggregated into a Total Impact Score as depicted in Fig. 16 , an overall comparison can be drawn from it by ordering from lowest to highest impact score. The bar charts are presented for each app. They also differentiate the EU apps from the non-EU ones. An app has the possibility to accumulate a Total Impact Score of 100. For instance, the results indicate that Covid Safe (US) has the highest Total Impact Score among others (66,2), meaning that it has more severe impact on users' privacy. The detailed assessment reveals that this app got scores 37,5, 100, 37,5, and 100 (each of them out of 100) for its w j (Dangerous Permission Request Score), x k (Absent Specification in Privacy Policy Text Score), y h (Idle Dangerous Permission Usage Score), and z m (App's Security Score), respectively. In terms of apps with less severe impact on users' privacy, COVIDSafe (Austrlia) has dominated others with a score as much as 22,9 (out of 100). Similar to the observations that we previously had with respect to the better privacy and security behavior of the contact tracing apps published by the EU institutes (where the GDPR is applied and enforced), the results of impact analysis also confirm that 6 (60%) EU apps (shown by blue in Fig. 16 ) are among the top 10 apps with minimum impact of users' privacy (only one EU app belongs to the top 10 apps with the most severe impact). Total Impact Scores per contact tracing app (averaged value of individual impact scores which is normalized between 0-100). Ordered from low to high -the more closer the scores to 100, the more severe the impact on individuals' privacy Our multi-perspective privacy and security analysis of COVID-19 contact tracing apps sheds light on a diverse number of data protection issues regarding the existing contact tracing apps. Firstly, we observed that COVID-19 contact tracing apps request a mixed variety of permissions, including dangerous and signature ones. A simple contact tracing app needs to only access a limited number of permissions. For instance, some of the existing frameworks are fully based on BLE technology 12 , and thus, requesting other permissions such as location, microphone, users' contact details, etc. become unnecessary and irrelevant to deliver the intended functionality -tracing the spread of virus. Surprisingly, we found a great number of apps within our data set that claim they do not require any sensitive permission to function (such as location), but when it comes to their run-time permission accesses, we observed they are actively accessing such permissions. Secondly, the privacy policy text analysis of contact tracing apps showed that these apps tend to be non-transparent regarding their data collection, processing, sharing and transfer practices. This raises serious concerns with respect to their compliance with existing privacy laws such as the GDPR (within the EU) and the corresponding data protection regulation of that country. Thirdly, the run-time analysis of the permission access pattern of these apps revealed that contact tracing apps are massively over-privileged and tend to be data-hungry even though the user was not actively using them. At the first glance, the problem of over-privileged apps is a design issue which is completely independent of national or international data protection legislation. In fact, an app that asks for as many permissions as possible (regardless of whether those permission requests are aligned with its proper functionality) is deviating from basic design principles such as Principle of Least Privilege (PoLP) (Saltzer and Schroeder 1975) that works by allowing only enough access to perform the required job which is also about monitoring and managing access for those who need access such as app developers. Apart from this, the problem of over-privileged apps is directly connected to fundamental legal principles such as data minimization (minimize access to personal data) and purpose limitation (limit access to personal data only for specified purposes) which are widely mandated and enforced by different data protection legislation such as the GDPR, the UK Data Protection Act 2018 (2018), Australia Privacy Act (2020), and Canada Personal Information Protection and Electronic Documents Act (2019). We also believe such a behavior (an app being over-privileged) is an obvious deviation from privacy-by-design (Cavoukian 2010) which is a basic design concept that asks software developers to integrate privacy-respecting measures into the design cycle of their software. Lastly, our study revealed that although there are some apps that behave in a privacy-respecting manner, they suffer from security issues. This is a clear deviation from the full functionality principle of privacy-by-design (Cavoukian 2010) , that seeks to provide the maximum level of functionality while satisfying all the legitimate objectives in a ""win-win"" manner by avoiding a false trade-off between privacy and security, indicating the possibility of having both strong privacy and strong security. Considering the critical situation caused by the COVID-19 pandemic, and the privacy and security risks revealed by our analysis regarding the available contact tracing apps, we would like to highlight that privacy and security considerations should be integrated into the design of contact tracing apps without undermining users' privacy. Thus, in what follows, we provide several calls for actions regarding existing privacy and security principles enforced by widely adopted data privacy laws such as the GDPR. However, what needs to be highlighted here is that since this study has been conducted in the EU, our work was inspired by the GDPR and other EU regulatory documents. Although there are overlaps between data protection requirements mandated by the GDPR and some other data protection acts enforced in non-EU countries (a particular example of these overlaps is children protection which is widely recognized and enforced under different data protection regimes such as US Children's Online Privacy Protection Rule (1998), Brazilian Data Protection Law (2018), South Korea Personal Information Protection Act (2020), India's Personal Data Protection Bill (2018), and many more), we would like to highlight that the discussion points discussed within this section may not be necessarily generalizable to all the studied apps (mainly non-EU apps). Our study revealed that COVID-19 contact tracing apps are actively accessing data (including sensitive ones) and sometimes contacting third parties ranging from advertising to analytic networks. For instance, using the tool in (Razaghpanah et al. 2015) , we found that Crashlytics (crash reporting and analytics), OneSignal (push notifications), and Facebook Graph (ads and analytics) are the most contacted third-parties by the contact tracing apps (17 apps). Contacting a third-party library by itself is not an obvious sign of a privacy breach, however, our results confirm the high presence of third-party components in contact tracing apps and some of them are associated with third-party advertisement and tracking services (which may to a certain extent justify why many of these apps request and access location-related permissions). We argue that this is not aligned with current EU and non-EU privacy guidelines for designing contact tracing apps. Furthermore, the GDPR states that personal data shall be ""collected for specified, explicit and legitimate purposes and not further processed in a manner that is incompatible with those purposes; further processing for archiving purposes in the public interest, scientific or historical research purposes or statistical purposes shall, in accordance with Art. 89 (1), not be considered to be incompatible with the initial purposes"", Art. 5 (1b). When data is collected from users in app ecosystems, such data has to be considered as personal data as in the meaning of the GDPR. The relevant data to the smartphone itself, such as the device's identifier is also categorized as personal data (ENISA 2017) . Therefore, to fully comply with the GDPR Art. 6 (4) (""data processing for incompatible purposes should be avoided unless it is on the basis of a specific set of criteria in the GDPR""), app providers must only process data when the contact tracing app has a specific lawful purpose for doing so. Furthermore, our study shows that there have been many cases where the apps accessed sensitive resources while there was no legitimate reason for that. For instance, those cases that apps are not claiming the need for requesting location data in their privacy policies, while they request and access such permissions in reality is quite problematic, or where they claim they are fully functioning based on BLE technology, but they access location-related permissions (e.g. Coalition). Similarly, even though some of the apps already stated in their privacy policies that they request location permissions (obviously because they are based on GPS technology), we still believe that this is against best data protection practices as our analysis of legal guidelines (see Section 2.2) confirms that active access to location-related permissions is an obvious violation of several data protection principles, including but not limited to data minimization and purpose limitation. More importantly, such an extreme tendency in requesting location-related permissions can result in apps being over-privileged (asking for as much data as they can, even those that are irrelevant to their functionality) and invade individuals' privacy, such as the claim by a Minnesota law enforcement official confirming that the state was employing contact tracing to identify connections among protesters who were detained during the ""Black Lives Matter demonstrations"" (2020). Moreover, we observed some of the contact tracing apps request and access signature-level permissions. It should be noted that since signature-level permissions usually give more control to an app through managing system-level functionalities, the developers of contact tracing apps should not request such permission requests unless they are aligned with purposes. We also observed that some of the contact tracing apps do not provide any privacy-by-default options to users to limit/isolate data sharing practices. Hence, data sharing must be isolated by default unless explicitly specified or otherwise chosen by the user. Third-country sharing When sharing data with third-parties, the legal standards of the country where the third-party resides plays an important role. Considering Art. 13 (1f), 14 (1f) of the GDPR, data transfer to other countries is only lawful, where a similar level of protection as provided by the GDPR is guaranteed. Whether this is the case needs to be assessed by the European Commission and can be done on the basis of a country, territory, specified sector, or international organization. Besides these restrictions on where to share personal data, Art. 13 (1f), 14 (1f) require data controllers to adopt appropriate safeguards and means followed by contractual arrangements with the recipient of the personal data approved by the European Commission. In our analysis, we found a number of data transmissions to different countries by contact tracing apps that have not been justified in privacy policy documents. The providers of contact tracing apps need to be fully aware that whenever third-party servers (outside the EU) are used as a back-end, the corresponding regulations applied to third-country data sharing practices are enforced. Our experiments showed that a significant number of contact tracing apps within our data set (71.4%, 20 apps), are not transparent regarding their data retention period. App providers/developers are required to be clear about the retention period and they should limit it only to the amount of time needed to provide the desired service. Thus, any personal data should be instantly deleted (including stored data on the remote servers) after the expiration of the retention period. Additionally, any confidential data, including location patterns, health-related data, etc. must be successfully deleted upon app deinstallation from the device and any other storage medium. Transparency Transparency is one of the key principles of the GDPR. Smartphone app providers/developers are required to be clear and explicit about their data access, collection, process, and transfer practices. Also, they are responsible for determining the internal rules once data collection purposes change. This also entails the direct communication of such changes to users before they come into effect. Furthermore, any incident regarding users' personal data shall be promptly communicated to users, e.g., in case personal data breach happens, users and the respective Data Protection Authority must be immediately informed (this also includes the potential occurred risks and possible countermeasures). However, our results confirmed that none of the examined contact tracing apps is transparent about how they react to a data breach, and how they handle such incidents in case users' personal data falls into the wrong hands. To further improve transparency, a clear, comprehensive, understandable, and legitimate privacy policy text should be accessible to users. We observed some of these apps either do not have any privacy policy document (e.g. Coronavírus -SUS, Brazil) or have very generic texts that do not focus on the app's data handling practices (e.g. Private Kit, US). Art. 5 (1f) of the GDPR states that, ""personal data shall be processed in a manner that ensures appropriate security, including protection against unauthorized or unlawful processing and against accidental loss, destruction or damage, using appropriate technical or organizational measures (integrity and confidentiality)"". Developers must make sure that the app's integrity is preserved intact by checking resources for potential modifications. One way is to restrict writing and modification permissions. However, we found that almost 40% of contact tracing apps tend to not restrict writing and modification capabilities. We observed that nearly 40% percent of the studied contact tracing apps use weak hash functions such as MD5 and SHA1. This is highly critical as it may cause reverse engineering attacks. Additionally, we noticed that 75% of contact tracing apps tend to store raw data on the device's storage without applying any encryption mechanisms. Enforced by the GDPR Art. 5 (1e), the implementation of appropriate technical and organizational measures to safeguard the rights and freedoms of users is highly important. Hence, app developers have to adopt and apply up-to-date protection and encryption mechanisms for data storing purposes. This is mainly because an insecure storage is not only a risk factor when the device is stolen, but also when another app accesses unencrypted raw data (?VASCO). Therefore, storing any sensitive data such as users' credentials, location information, etc. on the device's storage in an unencrypted form must be avoided. Accountability Accountability demands service/app providers to demonstrate how they comply with data protection regulations. This includes careful documentation of all decision-making procedures with respect to the ongoing data processing and conducting Data Protection Impact Assessments (DPIAs) to tackle data protection issues. When personal data is being processed, app developers are required to carefully document all decision making procedures with respect to the ongoing data processing such as maintaining certain documentation on what personal data is processed (how, for how long and for what purpose). A security report handling point (address) must be implemented and maintained aiming at enabling users to contact app developers/providers conveniently. However, our experimental results revealed that 57% of the studied contact tracing apps do not provide any contact information. In addition, all the required procedures must be anticipated and established in case a data breach happens (including a communication channel to react to reports on security and privacy issues). Following best privacy and security practices, all privacy-and security-relevant policies, processes, operations and testing procedures should be documented. This also includes the documentation of risk assessment and management procedures, compliance with regulations and requirements (e.g., the GDPR), a record of users' consent, objections, contracts with external service providers and third-parties from which the data is collected or transferred to. We believe the methodology used in this paper can not only provide a quick comparison of COVID-19 contact tracing Android apps' privacy and security behavior, but also a preliminary assessment of apps' privacy and security performance in general. This is due to the fact that, the static and dynamic app behavior analysis done in this paper is independent of the nature of the apps. Additionally, the compatibility with privacy regulation is not only meant for COVID-19 contact tracing apps. As such, the data protection principles discussed and covered in this paper are also obligatory for other apps with different functionalities and in different contexts. This also holds for the identified compliance issues.@story_separate@Our investigation shows that many of the early contact tracing apps were engineered quickly. They do not take privacy regulation fully into consideration. Their documentations and policies seem incomplete and incohesive. Program code seems poorly quality-assured. We performed various forms of analysis of COVID-19 contact tracing apps based on a group of assessment metrics. Our main findings are: -Privacy policies provided for many of the apps were found incomplete or non-existent at the time of our study. -Many apps showed a privacy-invasive permission access behavior, especially concerning location-related permissions. -Several apps began accessing location permissions even before personalizing and registering them. -From a regulatory perspective, the EU apps showed in general less privacy risk indicators than the non-EU apps. But still, most apps fail to comply with one or more of GDPR's privacy principles -Code vulnerability analysis showed several vulnerabilities in apps' codes. The above findings indicate, in our opinion, a very immature state of the infection tracing apps. Incomplete documentation such as privacy policies as well as a large number of vulnerabilities detected by code analysis are strong indicators of quick development. Though speculative, we think this may be the result of a very quick decision-making and of insufficient time for quality assurance. This might also explain that none of the apps covers the complete regulatory requirements set forth by the GDPR -not even the EU apps. Purposebinding and transparency issues as well as incomplete information for data subject consent have widely been detected in the sample. Limitations Several limitations arise when identifying security and privacy aspects of contract tracing apps. Firstly, our research is limited to the Android platform only. Therefore, it is difficult to speculate about iOS apps. However, we believe the results obtained from privacy policy analysis (see Section 4.2) could be easily generalized to iOS ecosystem as app providers normally publish the same privacy policies for both Android and iOS apps. In terms of resource access pattern analysis, one could think of device instrumentation to jailbreak an iOS device to enable a run-time analysis, but this could then face the incompleteness of analyzed data set (6 out of 28 apps in our data set were not available for iOS). Hence, we abstain from speculating about apps from other platforms. Secondly, we cannot claim that the presented results are reproducible with respect to variable contexts. This is an unavoidable limitation due to the ever-evolving nature of the apps, as well as of the Android platform itself. Apps get regularly updated along with their privacy policies, leaving static data snapshots outdated. Thus, it is a very difficult property to achieve because of the challenges associated with retrieving the older versions of apps, privacy policies, and various forks of the Android operating system to create a similar test bed, as well as identical data collection campaign. However, we have archived the corresponding data that was used to produce results and thus, the analysis could be run again. We also confirm that one should distinguish those apps that were considered to be mature at the time they were deployed and used, and others that were preliminary and not officially endorsed by public authorities, e.g. Ito (Germany). Thirdly, the result from permission analysis suffers from controlled environment that was controlled by avoiding interaction with the device, and therefore, the results of permission access patterns analysis may not be necessarily generalized to other app's states (e.g. an app being actively used). Lastly, although the results obtained from the vulnerability analysis tool shows a considerable number of security issues, this cannot be fully confirmed as the results may suffer from false-positives. This is due to the fact that, the tool applies regular expression search which might not be accurate enough to figure out more specific and correct security issues. Furthermore, we encountered several obstacles while preparing the test bed for the apps: (a) the official version of the app was restricted to geographic installation only, (b) apps were found incompatible to run on our rooted test devices, (c) we were compelled to exclude older test devices due to higher requirements (in terms of device and operating system) from the apps, (d) apps were found demanding registration with citizen data before running, and (e) apps' documentation was found incomplete, or in languages that are not automatically translatable using regular tools. So, we had to find work-around (e.g. acquiring .apk files from unofficial sources (Apkmirror 2020; Apkpure.com 2020) in order to prepare the test environment. Consequentially, the documented app behavior is expected to deviate from the privacy friendliness/intrusiveness of apps in a real-life device usage scenario. As another limitation of our work, since our study was conducted within the EU, our investigations mostly relied on compliance issues with regard to the GDPR requirements. This means although the results obtained from the legal compliance analysis can be easily generalized to all apps published within the EU, they may not be necessarily generalized to all non-EU countries as those countries may have less strict data protection requirements as offered by the GDPR, and therefore, they may not need to fully comply with all the studied requirements in this paper. In addition, our aggregated impact assessment scheme processes all metrics equally. We believe under certain circumstances some of these metrics may not be fully representative in certain countries where neither the GDPR nor other well-established privacy and security legal requirements are enforced under strict data protection regimes. Outlook A new generation of contact tracing apps is under launch. They are related to a shared code base that focuses on Bluetooth contact tracing with de-centralized local storage on the devices. Cryptography is used to generate pseudonyms, while data subjects are engineered to be in control over data release for tracing in many of the solutions. The apps are launched at the time of the completion of this article. We hope they will be more matured not only in their privacy architecture, but in addition in their documentation and their code quality once we inspect them. We wish their issuing authorities the best with their experimentation with digital contact tracing, which still is an unproven technology.","As this article is being drafted, the SARS-CoV-2/COVID-19 pandemic is causing harm and disruption across the world. Many countries aimed at supporting their contact tracers with the use of digital contact tracing apps in order to manage and control the spread of the virus. Their idea is the automatic registration of meetings between smartphone owners for the quicker processing of infection chains. To date, there are many contact tracing apps that have already been launched and used in 2020. There has been a lot of speculations about the privacy and security aspects of these apps and their potential violation of data protection principles. Therefore, the developers of these apps are constantly criticized because of undermining users’ privacy, neglecting essential privacy and security requirements, and developing apps under time pressure without considering privacy- and security-by-design. In this study, we analyze the privacy and security performance of 28 contact tracing apps available on Android platform from various perspectives, including their code’s privileges, promises made in their privacy policies, and static and dynamic performances. Our methodology is based on the collection of various types of data concerning these 28 apps, namely permission requests, privacy policy texts, run-time resource accesses, and existing security vulnerabilities. Based on the analysis of these data, we quantify and assess the impact of these apps on users’ privacy. We aimed at providing a quick and systematic inspection of the earliest contact tracing apps that have been deployed on multiple continents. Our findings have revealed that the developers of these apps need to take more cautionary steps to ensure code quality and to address security and privacy vulnerabilities. They should more consciously follow legal requirements with respect to apps’ permission declarations, privacy principles, and privacy policy contents."
"On the last day of 2019, pneumonia cases with unknown etiology from Wuhan City, Hubei Province was reported to China's World Health Organization's [WHO] Country Office. The number of the cases increased, and the source was thought to be related with the seafood market in Wuhan city. The causative agent was isolated on January 7, 2020 and identified as a new type coronavirus [1]. After one month had passed from the first cases of the 2019-nCoV outbreak, later named as SARS CoV2, WHO declared the outbreak as a 'Public Health Emergency of International Concern' and provided advice to the global community to control the outbreak on January 30. Almost more than one month later, it was declared as a controllable pandemic on March10 1 . First the new disease attracts attention of the scientists, medical world, then media and hall world. The number of the cases increased dramatically, and the speed of the curve became high. Still, many believed that it would be a limited outbreak 2 . Nobody seemed to expect this new viral pneumonia outbreak emerging in Wuhan to break the chains and turn out to be a pandemic. There were four important pandemics within the last century: Spanish Flu, Hong Kong Flu, Asian Flu and Swine Flu. Each left different story behind. Millions of people had infected, hundreds, thousands of people died 3 . In none of them, social media was the witness. Currently, hundreds and thousands of videos and photos have been shared in the social media, some of them are misleading and nothing to do with current outbreak. People have been searching more and more every day to reach true, updated data, as if people around the world were starving knowledge about the current COVID-19 outbreak. In the modern world, travel and communication opportunities seem to be limitless and fast, so is the spread of SARS-CoV-2. This time, the modern world had different tools to limit the SARS-CoV-2 outbreak. Modern laboratories were ready to investigate the new virus, produce new diagnostic tools, modern hospitals were ready to accept patients. National and international authorities were communicating with each other. Technology was on our side. So, the 2020 pandemic could be controlled easily more than ever. But is that what was happened? In the modern world, people have different preferences. They are less eager to obey the rules, they feel themselves more free or need more freedom, which overall can crush the quarantine procedures easily. Everybody can seek the top medicines, even if it is on the way in a trial. Patients can be more anxious. Besides the international organizations like WHO, UNESCO and UNICEF, the roles of local authorities, health ministries, other ministries, disease control centers, health protection agencies, research centers and universities are all very important in different operational levels to control and survive from the pandemic. This paper will review the 2 The Star, Star Media Group (2020). China celebrated Lunar New Year with more than 40 thousand families on the 18th of January 2020 [online]. Website https://www.thestar.com.my/news/regional/2020/02/06/wuhan-neighbourhood-sees-infections-after-40000families-gather-for-potluck [accessed 05 April 2020]. 3 Centers for Disease Control and Prevention (2020). Past pandemics [online] . Website https://www.cdc.gov/flu/pandemic-resources/ basics/past-pandemics.html [accessed 05 April 2020]. 4 World Health Organization (2020). Who we are [online] . Website https://www.who.int/about/who-we-are [accessed 13 April 2020]. 5 World Health Organization (2020). Novel Coronavirus (2019-nCoV) Situation Report -10 [online]. Website https://www.who.int/ docs/default-source/coronaviruse/situation-reports/20200130-sitrep-10-ncov.pdf?sfvrsn=d0b2e480_2 [accessed 30 January 2020]. 6 World Health Organization (2020). Coronavirus disease (COVID-19) outbreak [online] . Website https://www.who.int/westernpacific/ emergencies/covid-19 [accessed 13 April 2020]. 7 World Health Organization (2020). WHO Statement regarding cluster of pneumonia cases in Wuhan, China [online] . Website https:// www.who.int/china/news/detail/09-01-2020-who-statement-regarding-cluster-of-pneumonia-cases-in-wuhan-china [accessed 9 January 2020]. immediate response of different national and international institutions to COVID-19 pandemic.@story_separate@From 1948 till now WHO has been working on to promote health, keep the world safe, and serve vulnerable people. There are 150 WHO country offices. More than 7000 people from more than 150 countries are working under WHO. Beside many, one of their goal is also to improve monitoring, data and information. As WHO stated in its website, they analyses data, provide advice, coordinate with partners, help countries prepare, increase supplies and manage expert networks 4 .That is why WHO was the center of the news from the first time on. The different roles of WHO in COVID-19 outbreak are extremely important. Mainly the most important role at the very beginning of outbreak was the informative role of WHO, of course followed by preparedness and response acts. The interim name of the virus and the disease was also recommended by WHO as ""2019-nCoV acute respiratory disease"" which was named as COVID-19 (Co: corona, VI: virus, D: disease, which first cases appear in 2019) later on February 11 provided by International Classification of Diseases (ICD) 5 . They stated that further international spread of cases might appear in any country and every country should be prepared for containment. WHO declared the outbreak as a Public Health Emergency of International Concern and gave advice to the global community to control the outbreak on January 30 and more than one month later, on March 10, defined outbreak as a controllable pandemic 6 . WHO published the Situation report-1 on January 20 when the total confirmed case number had reached 282 and four countries, China, Thailand, Japan and Korea, had reported 2019-nCoV cases. Although closely monitored, the situation had been not expected to cause a pandemic on the early days of January and no restrictions for travel or trade were advised 7 . Later, when the case numbers had been dramatically increasing, WHO assessed the risk of this event to be very high in China, high at the regional level and high at the global level. On January 23, the situation was evaluated by WHO-International Health Regulations Emergency Committee and stated that it's was bit too early to consider this event as a Public Health Emergency of International Concern 8 . The situation reports of WHO have been followed by millions. The situation reports provide situation updates, surveillance reports, advices for preparedness and response, as well as country responses, subjects in focus and strategic objectives, all day by day 9 [2, 3] . In the modern world where millions of reports and news are releasing daily, these situation reports are very informative and brief to follow up. Besides situation reports, WHO also provide information under different topics: media resources, advice for public, travel advice, technical guidance, training and exercises. All are very important to reach correct and updated information during a global outbreak. Training and exercises section provides online COVID-19 training and simulation exercises plus simulation packages in several languages. From the treatment facility design to infection prevention and control measures, a broad range of online training are provided, which are extremely important, especially for those countries which lack enough professionals in the area 10 . WHO announced Donor Alert on the February 4, 2020 when the risk assessment was as very high for China, just high at the global level. The Global Resource Requirement was stated to be USD 675.5 million, of which USD 61.5 million were for the WHO, for urgent preparedness and response activities, primarily to help protecting states with weaker health systems. The Global Strategic Preparedness and Response Plan includes establishing international coordination and operational support, country readiness and response operations, accelerating priority research and innovation. As of April 9 2020, over USD 356 million was already received by WHO 11 . Many countries have active surveillance, protection and control systems for communicable or contagious diseases. So, either they have centers for diseases control and prevention or health protection agencies. CDC of USA is among the most popular one. The mission of CDC is to protect America from health, safety and security threats, both foreign and in the U.S. 12 . CDC's Traveler Health web site is also providing brief information on ongoing outbreaks and illnesses as well as protection advices for those who want to travel to any country. This web site is being updated continually and one can also find a clinic in a destination or access help from it. Information can be found both for physicians and others. In case of a pandemic the CDC web site is very informative especially for those countries lacking health professionals and advisory committees 13 . As an immediate response to COVID-19 pandemic CDC provided useful tools for both public and health professionals. How to protect yourself, what to do if you are sick are the immediate titles when you open their website, which are the most important topics for the public in an outbreak. To cope with the pandemic, keeping home environment safe and cleaning, keeping healthy life were all among the useful advices for the public. Extra precautions and a self-checker guide are also provided 14 . When the number of COVID-19 cases increased dramatically, CDC advised that the surgical masks should be kept for medical staff and they provided brochures and videos on how to make cloth face covering for the public 15 . The US Surgeon General Dr. Jerome Adams can be seen on a video to show how to prepare homemade masks. 16 . Still, there were also reactions from many US citizens criticizing lack of masks in an outbreak in a country which its economic is the number one in the world. For health care professionals CDC also provide brief and useful info under these titles 17 : · Evaluating and testing · Clinical care guidance · Infection control · Optimize personal protective equipment supply · Potential exposure at work · First responder guidance · Guidance for non-US facilities · Guidance for ambulatory care setting, · Guidance for pharmacies · Guidance for dental settings Of course, giving professional guidance for population and health care professionals are extremely important in an outbreak to prevent chaos. The panic level of the population is usually high in a pandemic so controlling the false and misguided information in social media is not easy. Having a trustworthy information source is very important. Outbreaks cause panic not only in the public but also among the professionals as well. The infection control teams and infectious disease [ID] physicians may be exhausted during the pandemic; all the care givers should know at least the basics of infection control. A physician may be a very good nephrologist, but they need what to do in their dialysis facilities in an outbreak for infection control. Providing brief information about using personal protective equipment (PPE) and hand hygiene as well as sharing the updated epidemiological features of the disease are all extremely useful 18 . In Turkey, the Public Health General Directorate published guidelines for current pandemic for both public and health care professionals, with the help of COVID-19 Advisory Committee, and updated all the relevant data day by day [2, 3] . Ministry of health of countries are in the core of the organizations during a pandemic. Different country's health ministries and parliaments response to pandemic was reviewed here (provided information in detail in Tables 1 and 2) 19 . Turkey immediately implemented numerous preventive measures to combat with spread of COVID-19 infection since onset of the pandemic (Table 3) . Turkey set up the Scientific Advisory Board the COVID-19 Advisory Committee, in the early time of on January10, 2020, before WHO accepted it as a pandemic in mid-March, following the emergence of the coronavirus outbreak. Board consists of different branch specialist including infectious disease and clinical microbiology, respiratory system, clinical microbiology, emergency disease, epidemiology, pediatric infection, virology, and public health and internal medicine. The Board has convened regularly, and published guideline named COVID-19 (SARS-CoV-2 Infection) Guide, and updated this guide according to the scientific data. This guide contains the topics of general information about SARS-CoV-2 and COVID-19 infection, management of COVID-19 infection, management of emergency patients with COVID-19, management of patients with COVID-19 in outpatient clinics, management of patients with severe COVID-19 infection, management of healthcare worker who exposure to COVID-19 and screening in contact person [3] . The first confirmed COVID-19 case was reported on March 10, 2020. The first death due to COVID-19 in the country was declared on March 17 [3] . Turkish government has adopted several containment measures, including social distancing, travel bans on visitors from high-risk countries and quarantine for nationals returning from those countries, and the closures of schools, stores, and entertainment venues. On March 12, the government declared that all of the schools and universities would be closed starting from March 16. Turkey suspended inperson visits and family interviews to prevent the spreading of coronavirus in prisons for two weeks on March16 and this period was prolonged during the pandemic. On March 30, the personnel such as execution protection officers working in criminal execution institutions and having the possibility of contact with prisoners have been decided to work in the form of 14-day shifts if possible, or at least 7-day shifts. It closed airline to flights from high-risk country firstly China, then Iran and Italy and closed to Turkey-Iran border gates to passenger entrances. Firstly, Turkey banned flights to and from China. Passengers were screened at airports for the fever and respiratory symptoms. And this implementation extended to the passengers coming from the other countries 20 Turkey implemented some restriction for the decreasing of people movement. Turkey parliament restricted going out for people 65 years old and older and citizens who have immune system deficiency, chronic lung disease, asthma, COPD, chronic cardiovascular disease, chronic renal disease, hypertension and chronic liver disease, and citizens who use drugs that disrupt the immune system. Turkey started implementation of curfew restricting members of public under 20 years old (born after January 1, 2000) from leaving their homes unless precisely necessary from 3rd of. Entry and leave by private vehicles to 31 big provinces including Istanbul that has one-fifth of Turkey population was banned. The wearing of facemasks in crowded areas including stores and mass transportation became mandatory April 3, 2020. On April 9, Turkey declared curfew for all citizens except for healthcare workers and security workers for weekends 21 . Italy has become the new epicenter of the COVID-19 pandemic in Europe with the fastest increasing rates. After the first two confirmed cases reported on January 31, more than 150,000 people became infected within 2 months and increase keeps growing 22 . Italy has surpassed China as the country with the highest death number in a short time. Lombardy, the northern region at the center of the outbreak, had been lockdown 23 . 21 Italy suspended the flight to and from China after the first confirmed cases on January 31, and declared a national emergency. The center of Italian cases has been identified as northern Italy and firstly eleven municipalities in northern Italy, then the entire Lombardy and other northern provinces and finally the whole Italy under quarantined 24 . Italy implemented a series of restrictions of increasing severity. The government firstly divided the Italian national territory into three areas on March 1; · Red zones (where the whole population was in quarantine), · Yellow zones (where schools, theatres, clubs and cinemas were closed, social and sport events were suspended), and · Green area (the rest of national territory where safety and prevention measures are advertised in public places). With the increasing number of confirmed and fatal cases, preventive measures have been expanded to the whole country. All of schools, universities, museums, cinemas, theatres, and any other social or cultural centers were closed, and all sporting activities were cancelled on March 9, and all restaurants and bars are closed on March 11. Most shops excluding supermarkets and pharmacies were also closed 25 [4] .  The first two confirmed COVID-19 patients were declared on January 29 and transmission within the UK was confirmed in February. hospital beds by postponing treatment of nonurgent patients and supplied additional use of 20,000 beds in private sector facilities. The government requested retired NHS staff to return to work for combat the pandemic and more than 20,000 former the National Health Service [NHS] staff returned to work in a less than two weeks. An agreement was achieved with almost the entire private health systems to overcome the pandemic on 21 March, and 20,000 medical staff started working for national purposes, 20,000 private sector beds were added to national sources. Up to 30,000 hospital beds were also provided by delaying nonemergency patients and discharging the patients in good health. Additionally, temporary critical care hospitals were established to overcome of bed shortage 26 . Spain is one of the most affected country by the COVID-19. The first confirmed patient was reported on 31th of January and many cases were detected by 24th February, origin of these cases was an Italian medical doctor who visited Spain, and then the disease spread to other communities. Within two months, Spain became the country most affected by Even the Spain Ministry of Health statistics is claimed to be inaccurate and incomplete. It was stated that the actual numbers were much more than the determined numbers because the tests were performed only for severe patients and not performed for patients with mild course, as well as there are older fatal cases on nursing home who might die before being tested. Spain implemented some preventive measures. The first cancellation of all educational levels was announced by the regional community government of Madrid on March 9 and in advanced time, the other regional governments cancelled all educational activities. On March 13, a national state of alarm was declared, and lockdown started to be implemented on March 15 for 15 days and then extended until April 11. All trips were banned, all residents were mandated to stay at home except for emergency and purchasing of food and drugs. Nonessential shops and businesses had been closed since March 30. On March 28, all nonessential activities were banned by the Spanish government and workers were allowed with leave paid. In the last decade, as the economy spiraled downward in Spain, heavy spending cuts were being implemented. 27 Investment in health sector expenditure has dropped far below the current average level of European Union. Due to hospitals were privatized, it has become harder to overcome the epidemic and coordinate the crisis. Before the epidemic, there were unemployed doctors or emigrated doctors to look for work, during the pandemic, retired healthcare workers were called back to the job and medical students were recruited to perform some tasks. Spain, like other many countries, suffered from shortage of intensive care beds. Although Spain increased the capacity of hospital beds to triple until the end of March, intensive care units were full over 80% with patients suffering from COVID-19. In Spain, about 10% of the cases are among healthcare workers. Insufficiency in personnel protective equipment is blamed for causing a high proportion of infection among healthcare workers. Twelve nurses and doctors have died due to the illness 27 . Singapore is one of the countries that fight well against the epidemic. The first confirmed case, a Chinese national coming from Wuhan, was declared on January 23. Although the first case was detected in the very first month of the epidemic, the total number of reported cases so far 24 March 2020 Turkey restricted going out and traveling by public transport of people who are older than 65 years and people with immune system deficiency or chronic disease. 3 April 2020 Turkey implemented the curfew for people under the age of 20 9 April 2020 Turkey declared a curfew that applies to all citizens except for health-care workers and security workers for weekends (April 11) is 3252 and total number of deaths is only 10 28 . Firstly, passengers coming from Wuhan was started to be checked for high temperature on 2 January and this screening was expanded to all passengers coming from China on 20 January and individuals with pneumonia were isolated for 14 days and quarantine was extended to all passenger coming from Wuhan. Singapore advised citizens to avoid non-essential travel to China on January 27. On 31 January, entry to Singapore, or to transit through Singapore was banned for all passengers with recent travel history to mainland China within the last 14 days. Stayhome notice was implemented for all Singapore residents and passengers coming from China. On March 4 visitors arriving from South Korea, Iran and northern Italy were banned to entry country. In the advanced time of epidemic, entry ban extended to visitors coming from Italy, Japan, France, Spain and Germany. The passengers with fever and respiratory symptoms were screened with swab tests. Any traveler showing symptoms at checkpoints were isolated for 14 days, even with negative results for COVID-19. Collective activities like cultural, sports and entertainment events with 250 people or more were deferred or cancelled on March 13. In prison, family interviews were suspended between April 7 and May 4. Visitors were banned from visiting relatives in the hospital unless necessary on April 7 29 , 30 . Mysterious pandemic firstly started in Wuhan, a city of Hubei Province in China at the end of 2019. China closed Huanan Seafood Wholesale Market, the suspected source of the initial pneumonia cases, on January 1, 2020. The CDC China isolated the first novel coronavirus strain on January 7. The first death because of the virus occurred on January 9 31 . China rolled out an aggressive disease control effort. In the first stage of outbreak, a general infection prevention 28 Worldometer (2020 measures that promoted universal temperature monitoring, masking, and hand washing was implemented. As time goes and scientific data are collected, it was focused on improving key performance indicators like enhancing the speed of case detection, isolation of patients and their close contacts and early treatment. During the late phase of the outbreak, the primary focus point became reducing cluster of cases, by the way of the control of transportation capacity to reduce the movement people and cancelling of mass gathering 32 . In the early stage, pandemic was not being able to under control by Wuhan local authorities, it spreads rapidly to other provinces and countries. On January 22, the government announced a quarantine, valid from January 23, and cancel all flights and trains from Wuhan. However, approximately 100,000 people had already departed from Wuhan on the same day by train and many of them succeeded bypassing the checkpoints by using antipyretics. Border gates were closed, and Wuhan city was under lockdown on January 23. The entire Hubei province came under a city-by-city quarantine on January 24 33 . The number of cases were shown to be highly associated with the population emigration from Wuhan to other cities of Hubei, and after lockdown, population migration was greatly inhibited, and spreads of virus were prevented [5] . China took some comprehensive, strict prevention and control measures to battle the epidemic. Hubei raised Class 2 Response to Public Health Emergency on 22 January and by 29 January, all parts of mainland raised to Class 1 Response, the highest response level. Provincial coordinated investigation into epidemic area and managed human movement. On 26 January, spring festival holiday was extended, and educational institutions postponed the start of school. Many sportive activities were postponed due to outbreak. The State General Administration of Sports declared to be suspended of all sporting events until April on 25 January 34 . In March, the government strictly restricted international travel and limited numbers of flights to the country and did not allow foreigners to enter the country 35 . China ordered a nationwide screening for the purpose of identification and immediate isolation of coronavirus-infected travelers at all departments of transportation including airports, railway stations, bus stations and port. All wildlife trade was banned with immediate effect 36 . In a short time, due to the accumulation of a great number of patients in hospitals, the need for medical staff and hospital beds appeared in Wuhan, Hubei. China's National Health Commission sent medical staff to Wuhan City to meet the need of healthcare personnel and to combat the novel coronavirus outbreak in the region. The government constructed two emergency specialty field hospitals to meet the need for hospital beds. China increased daily number of producing of facemask from 10 million (half of the world production) to 116 million to supply the increased consumption need. It used some hightech products like delivery drones, artificial intelligence and facial recognition system 37 . In the beginning of the outbreak, the government declared the spread of COVID-19 as a ""very low health risk"" for Germans. In the following days, government continued to defend low risk opinion. The first confirmed case was reported on January 28, and in the same day, after a suspected case in a private airplane, the company cancelled all flights to China. On February 13, the German Health Minister announced that direct flights between Germany and China would not cancelled. But after several weeks, the German Ministry of Transport announced to stop all flights from Iran and China on March 16. However, at the same dates flights from China and Iran continued due to bilateral agreements. Germany has a robust healthcare system to combat the outbreak. They have enough hospital beds to meet the increased needs. The major problem is 34 having chronically inadequate medical staff. Medical students were used to help out in the most overwhelmed units. On March 22, the government banned gatherings of more than two persons and required the condition that there was at least a minimum distance of 1.5 meters between people in public except for families, partners or people living in the same household. Restaurants and services like hairdressers were closed. Germanys closed schools and daycare centers to slow down the spread of the novel coronavirus on 13 March. In the second part of March, the doctors and other healthcare workers criticized shortage of PPE. Some big car manufacturers and banks donated mask for healthcare workers 38 . Japan was among the countries the first cases confirmed outside China, according to WHO announcement on January 20 [3] . On February 3, Japan banned passengers coming from Hubei Province or those who had a Chinese passport officially from Hubei and those who visited Hubei in the last 14 days to enter the country. On March 5, new quarantine restrictions were announced for all visitors coming from China and South Korea. A cruise ship named Diamond Princess departed from Yokohama on 20 January. Spread of infection was confirmed on February 4 within the cruise. 218 people onboard tested positive for the virus. In Japan, 3600 people were quarantined in cruise ship. The US passengers of Diamond Princess cruise ship went home for further quarantine by February 17. On February 27, Japan Prime Minister recommended schools to break in order to reduce the spread of coronavirus. By March 5, 98.8% of public elementary schools were closed. Government announced not to continue school closures on March 20. On February 17, the Japanese government also announced plans to expand the national health insurance system so that it covers COVID-19 tests. Months after the first confirmed cases were reported, on April 7, Japan announced a state of emergency in order to prevent the spread of the coronavirus 39 , 40 . The COVID-19 pandemic also effected the continuing education and education institutions. Primary and secondary schools were temporarily closed in most countries, so do the higher education institutions and medical schools. According to UNESCO, the 91% of world's student population was affected from these nationwide closures. There were also countries in them localized closures were implemented. The more vulnerable and disadvantaged communities were effected more from outbreak because they could not continue with remote or distance learning (Figure 1) 41 . Distance learning opportunities have never been so much popular worldwide before. A new era begins with COVID-19 pandemic. For a long time, we will be witnessing a wider role of online learning, and massive online open courses (MOOC) education. Teachers are no longer in the schools, students are no longer in the classrooms. The rapid change in learning environment will bring many pro-con discussions for the future. Distance learning does not affect just only students, teachers and school administrators but also parents. Also, resources to provide psychosocial support exist (InterAgency Standing Committee guidelines, WHO mental health and psychosocial guidance and UNICEF guidance). Designing home school, home office, home classrooms, all exaggerated with the pandemic. Distance learning solutions supposed by UNESCO are provided under many titles (Table 4) 42 . Distance learning cannot be a direct alternative to formal education. It is still important for staying connected to students. There are already many advantages and disadvantages, but as the situation makes it obligatory, many models are on the way and new experiences will form in this pandemic period. According to UNESCO sources, some of the national learning platforms and 39 tools of different countries in the current pandemic are as follows 43 : · Turkey: Remote Educational System -The Ministry of National Education will launch a ""remote educational system"" free of charge on March 23, 2020 with a television and internet-based curriculum on a national scale. Some online resources and applications are already available on the Ministry's website. · Kyrgzystan: Ibilim -An open educational portal endorsed by the Ministry of Education of Kyrgyzstan with free access to online video and audio lessons for primary school students on mathematics, Kyrgyz, Russian, English languages, music and arts. · France: Ma classe à la maison -The Centre national d' enseignement à distance provides a virtual classroom system accessible via smartphones and computers, enabling teachers to facilitate the organization of distance learning. · Japan: Future Classroom -A collection of platforms pointing users to a variety of useful sites for teaching and learning.MEXT -Platform to support e-learning by age, level of education and subject. · China: National Cloud -Platform for Educational Resources and Public Service -Provides free teaching and learning resources for primary and secondary school students. COVID-19 pandemic also affected universities. Hundreds of universities were also closed in most countries 44 . Research laboratories suspended their researches as well as the postgraduate studies. In 2020, International Higher Education Forum, there have been rumors that COVID-19 will cause profound impacts and changes on the higher education system around the world in terms of educationteaching methods, researches, internationalization and mobility. Because of the current outbreak, digital education started to become widespread. The social structure will be able to change in the postpandemic era, too. Many students may prefer online education and distance learning, so will many program directors. But, how it will be then. Schools and universities are not just places for lectures, but the interaction of different people, learning respect to each other and human values, communicating in different circumstances etc. Will the world run to an era that not embraces human values? We will see. CDC is encouraging limiting events and meetings that require close contact if there is no outbreak and cancel large meetings or events during an outbreak and also plan for distance and digital learning in the universities. They also advice monitoring absenteeism, and assess ways to increase physical space between students and limit interactions 45 . Many universities are closed their campuses to students and went through digital learning in COVID-19 outbreak but still some allow researchers to go on. In many countries, universities are autonomous, so in Turkey. Council of Higher Education [CoHE, YÖK] which is the constitutionally governing body responsible for strategic planning, coordinating, supervising and monitoring of universities in Turkey. The CoHE has been following the outbreak from the first time on and closely monitor roadmaps of official authorities and successful universities of the designated countries in Europe, America and the Far East. When the outbreak did not reach to Turkey yet, measures to be taken in Higher Education Institutions about COVID-19 were released under three titles, which are focusing on infection prevention measures while attending national and international meetings and also avoiding stigma, on 6th of March 2020 (Table 5) 46 . When the first cases of COVID-19 reported in Turkey, immediately the rectors of the universities gathered together in CoHE to discuss the action plan. Around 128 universities in Turkey already had a distance learning center. On March 13, schools are closed nationwide. But only the academicians at risk groups were permitted to stay at home, others supposed to continue their studies to prepare for distance learning in the universities. In an immediate questionnaire 93% said that they can continue with remote or distance learning, 71% with their own base and 22% said they also have to use others infrastructure. and applied courses will be given at the most appropriate time, including the extension of the schedule determined by universities. · Transition to online education has been initiated. · The universities whose Learning Management System [LMS] does not exist or sufficient yet were directed to the universities experienced in this field and their infrastructure was strengthened. · ""CoHE Courses Programs (yokdersleri.yok.gov.tr) was created. These resources are combination of entertainment and academic material · The enhancement recommendations were submitted concerning the staff and personal rights of healthcare Massive open online course platforms · Alison · Canvas Network · Coursera · European Schoolnet Academy · EdX · Icourses · Future Learn Mobile reading applications · African Storybook · Biblioteca Digital del Instituto Latinoamericano de la Comunicación Educativa · Global Digital Library · Room to Read · StoryWeaver · Worldreader professionals working in university hospitals, which are important for health education, to the government, and these enhancements were supported, and necessary arrangements were made. · The students are allowed to suspend their studies or postpone their enrollment Although the universities can use distance learning methods synchronously or asynchronously in all courses in formal education programs, the practices for different programs and clinical practices for the medical school students are concerning. About the students' and trainees' involvement in the care of COVID-19 patients are important issues to be solved for different universities. In USA, the high probability that medical students in the hospital would be exposed to outbreak cases and the need to conserve PPE seemed to outweigh the educational benefits of students' participation. suspend clinical rotations for medical students for several weeks [6, 7] . In Turkey, although all formal education in campuses are suspended in the universities, the medical schools are allowed to make their own decision about last year medical students to continue or not to their clinical practices. Also voluntarily work within the hospitals are allowed for those last year students who were nearly to graduate after completing their internships 48 . Different examples from different countries exist. Although Harvard University designs many thinks to be remote and distant (teach remotely, learn remotely, work remotely, research remotely, socialize remotely) anymore, they underline that medical students need to complete rotations and patient exams to meet graduation requirements, but, of course, with ensuring the safety of students, patients, staff and faculty. They also recommend medical students not be involved in the care of patients with confirmed or suspected COVID-19 49 . 1. Travel and international meetings All of students, academic and administrative staff were advised to reconsider their travel plans, especially their overseas travel plans for reasons such as exchange programs, congresses and meetings and to cancel them in case they were not essential. They were advised to do the following if they were required to travel: a. To check if there were any travel warnings or bans for the country concerned, b. To pay attention to personal hygiene during their travels, c. To visit the nearest health institution if any signs of infection appear during or after their travels and to inform the Turkish Embassy if they were abroad during this time. If a high number of attendance was expected from the countries with epidemics in meetings with international participation to be held in our country, it was advised to postpone the planned meeting and consider having online meetings. The following precautions were advised for ongoing meetings; -To provide the necessary environment for hand hygiene and to place hand disinfectants in accessible areas, -To inform and remind the participants about the modes of transmission and prevention methods for COVID-19 before and after the meetings. 3. Measures to be taken against discrimination and stigma -Students and academics from countries with COVID-19, such as China, South Korea, Iran, and Italy, who did not travel to their home countries recently and who any cases of COVID-19 were not reported from their country, were not more prone to contract, carry or transmit the disease, similar to Turkish or other international students. -Utmost attention must be paid to ensure that fear and anxiety about the disease do not cause panic, that people from a certain community or nation are not considered as the source and carrier of the disease, and that international students and academics, especially students and academics from China and other Asian countries, do not face discrimination. -It is essential for the Turkish Academia to be sensitive about this issue and to provide the necessary information and take precautions to prevent mistreatment. -In addition to paying attention to hygiene at campuses, university administrators are advised to hang posters about personal hygiene rules, especially in places such as lecture halls, corridors, dining halls, etc. where people gather, and to share information leaflets with university departments. -Follow the updates on the disease on the official website of the Ministry of Health. The universities and other higher education institutions play extremely important role because they can help to slow the spread of the outbreak; improve guidelines; save the society and keep them in a physiologically safe mood; publish papers; run webinars, work with local health departments, invent new tools for the diagnosis; and make research on the virus and potential vaccines etc. The national authorities also provide sources with the help of academicians and for helping the academicians. As an example, besides funding researches, The Turkish Scientific and Technological Research Council of Turkey (TÜBİTAK) opened a portal about COVID-19 which is very useful for being update in a pandemic.@story_separate@An outbreak can emerge all of a sudden or sometimes there may be signs of it beforehand. Active surveillance, infection control measures are extremely important but may be not enough all the times. Usually there are local guidelines for institutional roles in case of an outbreak, mainly guided by local health and governor authorities. In case of a pandemic, controlling the outbreak is a much bigger problem and difficult issue. In a pandemic, there are no borders anymore. Each country could be affected from others' decisions and infection control measures. The world population will be under risk of similar agent, regardless of their citizenships, countries or welt. All national and international institutions should be connected with each other, work together, share experience, publish guideline for general population, heath care facilities, local authorities, public service facilities, students etc. In Turkey's case, the Ministry of Health, the Council of Higher Education, the Ministry of Education and all other institutions response to the current pandemic can be a model for future studies, hopefully continuously after the end of pandemic. The COVID-19 Advisory Committee, which consists of professionals from different areas was extremely important to monitor the outbreak and provide advices after evaluating updated information. Organizations like WHO and UNESCO are extremely important to keep the countries updated, share different countries' measures and show the global aspect. They have to be clear and transparent in each step not to lost the trust of the populations. Besides their informative roles managing bridges between different countries and providing the important supplies like PPE and hygiene products to those countries in need are extremely important roles in a pandemic. Health protection agencies and CDC like institutions have leader roles guiding the authorities about the virus' characteristics, reading the outbreak curve, measures to be taken etc. Health ministries of countries should guide the operational plans in different levels. Other ministries should follow the advices and proposals to manage their routine work in a pandemic era. Educational institutions, primary and secondary schools should continue to connect with their students not to lost even a year for a generation in a pandemic. Universities should not suspend their important researches especially those which can help for prevention and control of the pandemic, but try to continue as much as the outbreak allows.","Nobody can be fully prepared to a pandemic. Of course there are signs of it, the scientists can predict, alarming speeches can be made. But there are always alarmist people around, maybe that is why sometimes even the most serious warnings may be not considered by the authorities on time. The first patients may be lost without a proper diagnosis. When everybody realizes that there may be a big problem in the horizon, sometimes it is too late. That is why it is very important to monitor contagious diseases and follow the warnings and releases of national and international disease control centers and other related organizations. China celebrated Lunar New Year with more than 40 thousand families on the 18 of January 2020. Nobody seem to be expecting this emerging new viral pneumonia outbreak appeared in Wuhan, in the last days of 2019, will break the chains and turn out to be a pandemic! But maybe this time it was not too late. There were four important pandemics within the last century: Spanish Flu, Hong Kong Flu, Asian Flu and Swine Flu. Each left different story behind. Millions of people had infected, hundreds, thousands of people died. This time, the Modern World had different tools to limit the SARS CoV2 outbreak. The national and international institutions of our globe were all communicating and taking precautions in a very fast manner than ever. However, this time, unexpectedly, the SARS-CoV-2 contagion was also faster. Besides the international organizations like WHO, UNESCO and UNICEF, the roles of local authorities, health ministries, disease control centers, health protection agencies, research centers and universities are all very important in different operational levels to control and survive from the pandemic. This paper will review the immediate response of different national and international institutions and authorities to COVID-19 pandemic."
"In December 2019, a number of patients with pneumonia of unknown cause appeared in Wuhan, Hubei province, China [1] . Subsequently, the Chinese Center for Disease Control and Prevention (CDC) confirmed that the pneumonia was caused by a novel coronavirus, which named the 2019 novel coronavirus (2019-nCoV), and Corona Virus Disease 19 indicates infections complicated with pneumonia [2] . Full-genome sequencing and phylogenic analysis indicated that the sequence of 2019-nCoV was similar to the coronavirus responsible for severe acute respiratory syndrome (SARS-CoV), so the virus was also named SARS-Cov-2 [3] . Studies have shown that the disease could induce the clinical symptoms including fever, cough, fatigue, myalgia, dyspnea, and it could even cause acute respiratory distress syndrome (ARDS) [4] [5] [6] . In addition, clinical evidences have suggested that this virus is transmissible from person to person [7] . Currently, COVID-19 outbreak worldwide and there is no effective therapies or vaccines, available treatments are only supportive and symptomatic. The origins and the mechanism of this virus still need to be further investigated. Although several studies have described the clinical characteristics of patients infected with 2019-nCoV [4] [5] [6] , hypocalcemia in COVID-19 patients has not been reported yet. Hypocalcemia is a common phenomenon among critically ill patients, its prevalence ranges from 15% to 88% in adults [8, 9] . In addition, hypocalcemia is associated with disease severity and increased mortality [10, 11] . Previous studies confirmed hypocalcemia was a laboratory abnormality in several types of viral infections, such as severe acute respiratory syndrome (SARS), avian influenza H7N9 and Ebola virus disease (EVD) [12] [13] [14] . In this study, we analyzed the clinical and laboratory data of severe COVID-19 patients to reveal the correlation between serum calcium levels and COVID-19.@story_separate@We included the patients with diagnosis of severe COVID-19 admitted in Tongji Hospital of Tongji Medical College, Huazhong University of Science and Technology J o u r n a l P r e -p r o o f during the period from February 9 to February 15, 2020. COVID-19 was diagnosed according to the World Health Organization interim guidance criteria. Severe COVID-19 was defined according to the diagnostic and treatment guideline criteria issued by Chinese National Health Committee (Version 3-5): respiratory distress with respiratory frequency ≥ 30/min, or pulse oximeter oxygen saturation ≤ 93% at rest, or artery partial pressure of oxygen(PaO 2 )/inspired oxygen fraction(FiO 2 ) ≤ 300 mmHg. Exclusion criteria were patients who with parathyroid disease, bone disease, chronic liver and kidney dysfunction, malignant tumor, and who received calcium or vitamin D treatment. Clinical characteristics (ie, comorbidities, signs and symptoms) and laboratory findings of each patient were obtained from electronic medical records. The clinical outcome was monitored up to February 29, 2020, the final date of follow-up. The poor outcome was defined when at least one of the following criteria was present: the need for mechanical ventilation, intensive care unit (ICU) admission or died of any cause during admission. According to the corrected serum calcium level within 24 h of admission, the patients were divided into two group, the normal serum calcium group (corrected serum calcium 2.15 ~ 2.50 mmol/L) and the hypocalcemia group (corrected serum calcium < 2.15 mmol/L). Corrected serum calcium calculator formula: corrected calcium = 0.02×(40-albumin(g/L)) + serum calcium. The study was approved by the Ethics Committee of Tongji hospital of Tongji Medical College, Huazhong University of Science and Technology, China. Informed consent was obtained from each patient. A nose swab and/or throat swab specimens were collected for the SARS-CoV-2 viral nucleic acid detection using real-time reverse-transcriptase polymerase-chain-reaction  All data were analyzed using SPSS 17.0 software (SPSS Inc., Chicago, IL, USA). Mann-Whitney U test. Proportions for categorical variables were compared using the chi-square test or Fisher exact test. Spearman correlation test was used for calculation of correlation between different factors. Univariate and multivariate logistic regression analysis were adopted to identify risk factors of poor outcome. Receiver operating characteristic (ROC) curve analysis was performed to evaluate the prognosis capability of serum calcium on COVID-19 patients. The tests with p value < 0.05 was considered statistically significant. A total of 107 patients with severe COVID-19 were included in this study. The majority (63.6%, 68/107) of patients was the elderly, and the median age was 68 years, ranging from 31 to 86 years old. Among them, 55 patients (51.40%) were female. The most common symptoms at illness onset were fever (80, 74.8%), cough (56, 52.3%), fatigue (47, 43.9%), gastrointestinal symptoms (19, 17 .8%) and myalgia(13, 12.1%). Only three patients had shortness of breath and two cases had chest tightness. 73(68.2%) patients had a history of chronic diseases, including hypertension (37.4%), diabetes mellitus (18.7%), coronary heart disease (CHD)(10.3%), respiratory system disease (including chronic obstructive pulmonary disease(COPD) and tuberculosis) (10.3%) and cerebrovascular diseases (including cerebral infarction and cerebral hemorrhage) (9.3%) ( Table 1 ). J o u r n a l P r e -p r o o f The incidence of hypocalcemia was 62.6% (67/107) in all the enrolled patients, and 3 patients had severe hypocalcemia(Ca 2+ <1.9 mmol/L). There was no significant difference in age, gender, signs and symptoms, and comorbidities between the hypocalcemia group and the normal serum calcium group. Compared to the normal serum calcium group, higher leukocytes, as well as higher levels of CRP, PCT, IL-6 and D-dimer, while lower lymphocytes, lower ALB level were found in the hypocalcemia group. No significant difference was identified for other laboratory indicators, such as hemoglobin, platelets, ALT, AST, LDH, bilirubin, creatinine, potassium, sodium, chloride, PT, APTT and PTA (Table 1 ). The level of serum calcium was positively correlated with lymphocyte counts and ALB level. Negative correlation was found between the serum calcium levels and leukocytes counts, the levels of CRP, PCT, IL-6 and D-dimer (Table 2 ). Up to February 29, severe COVID-19 patients with hypocalcemia had a poor outcome Furthermore, the multivariate analysis indicated that age, serum calcium, CRP and IL-6 were risk factors for poor outcome of severe COVID-19 patients (Table 4) . Finally, we analyzed the prognostic performance of hypocalcemia to predict the development of poor outcome. Area under the ROC curve was 0.73 (95% CI: 0.63-0.83), p<0.001 (Figure 1 ). To our knowledge, this report is the first to address frequency of hypocalcemia in severe COVID-19 patients. In our study, we found that 62.6% of the severe COVID-19 patients had hypocalcemia, and 3 cases had severe hypocalcemia(Ca<1.9 mmol/L). Moreover, hypocalcemia predicted a worse prognosis of severe COVID-19 patients. In addition, 63.6% of the patients were older than 65 years, median age of all patients was Hypocalcemia is a common laboratory abnormality in viral infection and pneumonia [15] . The cause of hypocalcemia in COVID-19 patients with severe status is not clear, several mechanisms may be suggested for this. The majority of patients in our study were elderly, with poor nutritional status. Chronic malnutrition will lead to vitamin D deficiency, result in hypocalcemia [16] . Moreover, it can affect the intestinal absorption of calcium, lead to inadequate intake, and thus result in a negative calcium balance [17] . Moreover, calcium is predominantly bound to albumin in the plasma, and a decrease in serum albumin will cause hypocalcemia. In addition, hypoxia of tissue and organ induce the cell membrane damage, result in calcium influx. Finally, the pro-inflammatory cytokines in COVID-19 patients inhibited parathyroid hormone (PTH) secretion, impaired response to PTH, thus cause the imbalance of calcium [18] . Other abnormal laboratory results in severe COVID-19 patients included lymphopenia, hypoalbuminemia, and elevation of inflammatory mediators (CRP, PCT and IL-6) and D-dimer concentrations. These laboratory abnormalities are similar to previously published articles [4] [5] [6] . Increased level of serum CRP, PCT, IL-6 and D-dimer indicated disorder. Viral infection induces a series of the physiological reaction of the host, including immunity responses. Cytokines are actively involved in the process, such as TNF-α, IL-1, IL-6. However, the rapidly and massively release of the inflammatory cytokines, can contribute to tissue destruction and organ failure, which known as a ""cytokine storm"" [19] . The phenomenon has been previously observed in patients with SARS, MERS and EVD [20, 21] . Recent studies confirmed that cytokine storm also occurred in patients with 2019-nCoV infection, especially in severe and critical ill cases, caused acute respiratory distress syndrome (ARDS) and multiple organ disorder syndrome (MODS), and even led to death [22] . According to our data, CRP, PCT, IL-6 and D-dimer concentration were significantly higher in COVID-19 patients with hypocalcemia, and hypocalcemia was positively correlated with these indicators, which may represent patients with hypocalcemia have a greater inflammatory response. Hypocalcemia is harmful to health. The classic symptoms of hypocalcemia are J o u r n a l P r e -p r o o f neuromuscular excitability in the form of muscle twitching, spasms, tingling, and numbness. Once severe hypocalcemia is not corrected in time, it will result in severe neuroendocrine and cardiovascular complications, thus increasing mortality [18] . In 1982, Chernow et al. demonstrated that hypocalcemia was associated with prolonged ICU stay and increased mortality [23] . Recently, studies have also reported that hypocalcemia is an indicator of disease severity and fatality [9, 11] . In our study, compare to the normal serum calcium group, the outcome of severe COVID-19 patients was worse in hypocalcemia group. Moreover, we performed ROC analysis to evaluate the predictive value of serum calcium. High AUC reflected the discriminative power of serum calcium when predicting poor outcome of severe COVID-19 patients. It is worthwhile to pay more attention to the occurrence of hypocalcemia in severe COVID-19 patients. In addition, studies have shown that other factors were associated with the outcome of COVID-19 patients, including age, the history of chronic diseases, the level of inflammatory mediators and treatment protocols [24, 25] . In our study, we found that higher age, elevated levels of CRP and IL-6 were associated with poor outcome of severe COVID-19 patients. There were a few limitations for this study. One of the potential limitations of our study is that our data comes from a single center study, and the sample size was relatively small, we believe that larger studies are needed to confirm our findings. Secondly, there are no data that treating hypocalcemia can improve prognosis in hypocalcemic individuals, so continued observations would provide more informations about potential risk factors of poor outcome. Due to some patients had a high leukocyte counts, we cannot rule out the possibility of bacterial co-infection in severe COVID-19 patients. To sum up, we consider that our observations should be externally validated.@story_separate@The COVID-19 is spreading globally. Our study is the first to focus on hypocalcemia of severe COVID-19 patients. In our study, we found that almost two-thirds of severe COVID-19 patients had hypocalcemia at time of admission. Patients who presented with hypocalcemia were more severely ill on admission, and had worse outcomes.","Abstract Background The aim of this study was to investigate the performance and predictive value of hypocalcemia in severe COVID-19 patients. Methods We retrospectively investigated the clinical and laboratory characteristics of severe COVID-19 patients. 107 patients were divided into hypocalcemia group and normal serum calcium group. The clinical and laboratory data were compared between two groups. The discriminative power of hypocalcemia regarding poor outcome were evaluated by receiver operating curves (ROC) analyze. Results 67 patients (62.6%) had hypocalcemia. In hypocalcemia group, leukocytes, c-reactive protein(CRP), procalcitonin(PCT), Interleukin 6(IL-6), and D-dimer levels was higher, while lymphocytes and albumin(ALB) levels was lower. No significant difference was identified in gender, age, signs and symptoms, comorbidities and other laboratory indicators. Serum calcium levels were negatively correlated with leukocytes, CRP, PCT, IL-6 and D-dimer, while positively correlated with lymphocytes and ALB. Patients with hypocalcemia more commonly presented poor outcome (47.8% (32/67) vs 25% (10/40), p =0.02). Median serum calcium levels were significantly lower in the patients with poor outcome (2.01(1.97-2.05) vs 2.10(2.03-2.20), p<0.001), and it could predict the prognosis with an area under the ROC curve (AUC) of 0.73(95% confidence interval (CI) 0.63-0.83, p<0.001). Conclusions Hypocalcemia commonly occurred in severe COVID-19 patients and it was associated with poor outcome."
"Studies conducted primarily in the aftermath of catastrophes tend to emphasize the negative factors that caused negative psychological outcomes. Thus, it was found that radiation exposures, the World Trade Center attacks, oil spills, mass shootings, hurricanes, and floods have been associated with increases in depression, post-traumatic stress disorder (PTSD), substance use, generalized anxiety disorder, and a range of other negative mental health outcomes [1] [2] [3] [4] [5] . Large-scale Chinese research on the impacts of the COVID-19 pandemic has reported that the onset of the coronavirus crisis led to a 74% decrease in overall emotional well-being [6] . An Italian study has found that up to 30% of adults and children were at high risk for post-traumatic stress disturbances during the COVID-19 pandemic period [7] , and several studies reported an increase in mental illnesses since the coronavirus outbreak [8, 9] . The study of Levkovich and Shinan-Altmans [10] reported further that the negative emotional reactions expressed by the general public during the COVID-19 pandemic reflected the perceived threat of this plague. Similarly, a comprehensive review of the psychological effects of armed conflicts on the general public's mental health concluded that such armed conflicts confronted in many countries, including Afghanistan, Albania, Cambodia, Israel, and Kurdistan, resulted in higher levels of anxiety, depression, and PTSD [11] . Likewise, a study of the psychological costs of the Vietnam War claimed that they extended beyond PTSD and included an increased risk for depression, personality disorders, suicide, and alcohol abuse [12] . It is quite evident that catastrophes and adversities of all kinds are likely to increase a variety of distress reactions. However, most of the pandemics and armed conflicts studies concluded or implied that these responses reflected the direct perceived threats of each of these investigated disasters. Most of them did not examine the extent to which these responses were predicted by the perceived threats of the investigated adversity, as compared to other potential predictors. Studies of anxiety in different contexts indicated, for instance, that it was significantly predicted by a large number of psychological and environmental factors such as serious problems at work, domestic violence, unhappy relationships with family, and higher levels of nonorganizational religious activity and intrinsic religiosity [13] , as well as by both financial distress and other psychiatric or mental disorders [14] . It was also found that mood fluctuations and anxieties during the COVID-19 pandemic were predicted concurrently by the pandemic threats, as well as by prior mental health status, and lifestyle changes [15] . Gomes et al. [16] found similarly that mental health problems were significantly related to perceptions of threat, sense of control, and previous coping with distressful encounters. An Israeli study conducted on students [17] has found further that negative, as well as positive, psychological responses during the COVID-19 pandemic were predicted concurrently by perceived health, economic, and security threats.@story_separate@The concept of cognitive appraisals was derived from Lazarus's classic appraisal theory of emotion, which defined appraisals as the personal significance of an encounter for well-being and a proximal determinant of emotion generation [18] . Blascovich and Mendes [19] argued that cognitive appraisals can impact affective, physiological, and behavioral responses in distressful situations that require instrumental responding. A large number of cognitive appraisal studies assessed the impacts of threats on coping. Salkovskis et al. [20] have found that individuals' appraisal of the danger of germ spread was significantly related to emotional and behavioral responses to the transmission of SARS. These responses included anxiety, avoidance, and disgust. A different perspective of cognitive appraisals described the process of evaluating a stimulus as either a challenge to be met or an overwhelming obstacle from which to retreat [21] . Several researchers examined, therefore, the impact of positive cognitive appraisals on coping with different kinds of distress. Litwic-Kaminska [22] has found that when athletes regard a situation as a challenge, they are more likely to feel confident about their ability to control the situation and tend to develop a strong motivation to prepare well for the competition. Likewise, a study of earthquakes and tsunamis concluded that proactive appraisals of victims lowered their levels of depression and distress [23] . These results suggested that positive cognitive appraisals may have a major role in coping with distress. Morale is a well-known positive cognitive appraisal that was generally studied in the domain of work. Research has found that morale was the psychological factor that resulted in positive behavior of employees, and this positive behavior results in increased work efforts and effective performance [24, 25] . The role of morale as a distress-reducing factor was investigated to some extent in the military context. An analysis of several modern wars [26] concluded that when a military force fostered high morale among its troops, it was less likely to suffer a substantial number of distress casualties. An example of this was the Falklands War, where the morale of the British troops was high and the distress casualty rate was approximately only 4%. However, the Malta campaign of 1942 was associated with low morale among the British troops and resulted in a level of distress casualties that was estimated as at least 25% of the deployed force. Therefore, we suggest that morale can be viewed as a future-oriented perspective regarding the challenges of coping with one's current situation. A higher level of morale is likely to be associated with a more positive future orientation and with better coping with hard times. It should be noted that the potential role of morale as a distress-reducing element was generally ignored by the research on disasters and catastrophes, as well as the research on the coping responses that were consequently elevated by these events. The people of Israel have recently experienced two major calamities: the COVID-19 pandemic and the May 2021 armed conflict between Israel and Hamas in the Gaza Strip. Israeli cities and settlements suffered massive missiles attacks fired from the Gaza Strip, as well as a few similar attacks from Lebanon, which were followed by riots between Arabs and Jews in several parts of the country. Simultaneously, the Israeli Defense Force (IDF) attacked specific targets in both the Gaza Strip and Lebanon. The present study examined the extent to which negative and positive psychological coping responses that were identified during the COVID-19 pandemic and during the May 2021 hostility were predicted by the health, security, economic, and political perceived risks, as well as by the level of individual morale. Distress. The outbreak of COVID-19 was negatively associated with psychological distress responses of grief, hopelessness, posttraumatic symptoms, panic attacks, distress, anxiety, depression, loneliness, ambivalence, fear, stigma, and concern towards socioeconomic status [27, 28] . A recent review of the research, which summarized 17 articles from different countries [29] , further found a high prevalence of distress among the investigated general populations along with the COVID-19 pandemic. These responses were negatively correlated with a sense of well-being and individual, community, and national resilience [30] [31] [32] . Well-being. This is the subjective feeling of health and a positive perception of an individual's quality of life [33] . Well-being was described as a state of complete physical, mental, and social welfare and not merely the absence of disease or infirmity [34] . High positive correlations were found between well-being, happiness, psychological quality of life, life satisfaction, and positive effects [35] . In addition, well-being was positively associated with individual resilience [35] . Individual resilience. Individual resilience constitutes a stable trajectory of healthy functioning after a highly adverse event [36] . The American Psychological Association [37] defines resilience as ""the process of adapting well in the face of adversity, trauma, tragedy, threats or other significant sources of distress (paragraph 4)"". Under the coronavirus pandemic threat, individual resilience was negatively and significantly correlated with a sense of danger and distress symptoms [38] . Community resilience. Community resilience refers to a community's ability to cope with stressful conditions, such as natural adversities or man-made calamities, and to recuperate after them. Eachus [39] defined community resilience as ""the community's capability to anticipate risk, limit impact, and bounce back rapidly through survival, adaptability, evolution, and growth in the face of turbulent change"". Examination of community resilience of 12 neighborhoods in New York and New Jersey severely affected by Superstorm Sandy indicated that people living in communities with higher social cohesion, informal social control, and social exchange were more likely to believe that their neighborhoods are very well prepared for a disaster [40] . National resilience. This concept reflects a successful national adjustment and functioning efficiently following potentially traumatic events [32] . Canetti et al. [41] have claimed accordingly that national resilience should be defined as the nation's ability to cope successfully with its disasters (such as poverty, terrorism, or corruption) while keeping its social fabric intact. National resilience correlated negatively with distress symptoms and a sense of danger and correlated positively with a sense of coherence [31] . Morale. The concept of morale, which originated in a military context [42] , is a multifaceted, longitudinal, and relational experience that individuals share when they identify with and contribute to certain kinds of collective activities [43] . Morale is regarded by Weakliem and Frenkel [25] as a general term for positive feelings about the prescribed activities of the group. The level of morale was found to correlate negatively with a sense of danger and depression and positively with individual and national resiliencies [32] . Considering the importance of identifying predictors of psychological coping with adversities, the current study aimed to compare coping responses during two different coexisting adversities in Israel: the COVID-19 pandemic versus the May 2021 armed conflict between Israel and the Hamas in the Gaza Strip. The following hypotheses were investigated: (1) Morale, which constitutes a positive cognitive appraisal, will be as good a predictor as the four investigated negative cognitive appraisals (health, economic, political, and security risks) in predicting both positive and negative psychological coping responses, in both stressful times of the COVID-19 pandemic and 21 May hostility. Higher morale will predict lower levels of distress, and higher levels of each one of the four positive coping responses (well-being and individual, community, and national resilience). (2) In line with a previous study that found that different perceived risks contributed to predicting psychological coping responses during the COVID-19 pandemic (Eshel et al., submitted), we hypothesized that the perceived health risk of the pandemic will not be the best predictor of each of the five psychological coping responses expressed during the COVID-19 pandemic, and some of them will be better predicted by the other perceived risks, less directed to the health lineament of the pandemic. By the same token, the perceived security risk will not be the best predictor of each of these responses expressed immediately after the May 2021 armed conflict. The study measured the variables during two periods of time-the COVID-19 pandemic and a security conflict. The COVID-19 pandemic started in Israel upon the initial identification of confirmed cases in February 2020. It continued in three main waves and substantially receded at the beginning of 2021, following a successful vaccination campaign. By 19 April 2021, 88% of individuals from the age of 50 years or higher were vaccinated after receiving two doses [44] . During June 2021, a fourth wave of COVID-19 began, and subsequently, an additional campaign was launched to inoculate specific risk groups (50 years old and above as well as individuals who are immunocompromised) with a third vaccine (""booster""). The violent hostility clash between Israel and Hamas in the Gaza Strip erupted on 10 May 2021 and lasted for 11 days, characterized by massive rocket attacks against civilian communities in the central and the southern parts of Israel. In addition, few rocket attacks were also aimed at Northern Israel, from Lebanon. During that period, domestic violent riots between Arabs and Jews also spread in many areas of Israel (the rural Galilee and many mixed cities in Israel, such as Lod, Acre, and Jaffa). The data for the current study were collected via an internet panel company possessing a database of more than 65,000 residents from all demographic sectors and geographic locations of Israel (https://sekernet.co.il/ (accessed on 26 May 2021). A stratified sampling method was employed, aligned with the data published by the Israeli Central Bureau of Statistics, to appropriately include the varied groups of the Israeli Jews population in terms of gender, age, and geographic dispersal. The present study examined negative and positive predictors of psychological coping responses in two different circumstances: the COVID-19 pandemic and the May 2021 hostility. The first data collection was accomplished throughout the third COVID-19 lockdown in Israel, at the beginning of the inoculation operation (14-18 January 2021). Table 1 shows that this sample included 699 participants, 369 men and 330 women, who ranged between 18 and 82 years of age; the income of 50% of them was below the national average, 51% held right-wing political attitudes and 49% of them were secular, and 29% define themselves as traditional. The second data collection was conducted during the 21 May hostility (13-14 May 2021). Table 1 shows that this sample included 647 respondents, 350 men and 297 women, whose ages ranged between 19 and 83 years; the income of 53% of them was below the national average, 52% of them held right-wing political attitudes and 51% of them were secular, and 29% of them define themselves as traditional. Comparing the data between the two samples shows no significant differences between the averages.  Distress. Two subscales derived from the Brief Symptom Inventory (BSI) [45] were combined into a single distress score. The anxiety subscale consisted of four items referring to felt nervousness, tension, and restlessness. The depression subscale consisted of five items about feelings of worthlessness and hopelessness. Each item was rated on a scale ranging from 1 (not suffering at all) to 5 (suffering very much). Cronbach's alpha for the distress scale was high (α = 0.90). Well-being. The well-being scale employed consisted of nine items concerning individuals' perception of their present lives in various contexts, such as work, family life, health, free time, and others [38] . Responses to these items ranged from 1 (very bad) to 6 (very good). This scale has been validated in previous studies [38] , and its reliability in the present study was found to be high (α = 0.85). Individual resilience. Individual resilience was measured by the 10-item Connor-Davidson scale (CD-RISC 10) [46, 47] portraying individual feelings of ability and power in face of difficulties (for example, ""I manage to adapt to the changes""). This scale was rated on a 5-point Likert scale ranging from 1 (not true at all) to 5 (generally true). The Cronbach's alpha reliability of this scale in the present study was high (α = 0.88). Community resilience. Community resilience was measured by a 10-item scale [48] , which was rated by a scale ranging from 1 (does not agree at all) to 5 (very much agrees) (for example, ""The relations among the inhabitants of my living place are good""). The current Cronbach's alpha reliability of this scale was high (α = 0.91). National resilience. A short version of the National Resilience Scale was employed [31] . This 13-item tool pertained to trust in national leadership, patriotism, and trust in major national institutions (e.g., ""I love my country and am proud of it""). In the current study, we added three items regarding the COVID-19 crisis (e.g., ""I have full faith in the ability of my country's health system to care for the population in the current coronavirus crisis""). The 6-point response scale ranged from 1 (very strongly disagree) to 6 (very strongly agree). The Cronbach's alpha reliability of this scale in the present study was high (α = 0.91). Cognitive appraisals. Each of the four cognitive appraisals was determined by a single item. Health risk: ""How much do you feel threatened these days by the health risk?"", economic risk: ""How much do you feel threatened these days by the economic risk?"", security risk: ""How much do you feel threatened these days by the security risk?"", and political risk: ""How much do you feel threatened these days by the political risk?"". The 5-point response scales ranged from 1 (not threatening at all) to 5 (threatening very much). Morale: Morale was estimated by one item, ""How would you define your morale these days?"" The response scale ranged from 1 (not good at all) to 5 (very good). Two path analyses in Amos structural equation modeling (IBM, SPSS version 26, https://www.ibm.com/ilen/marketplace/structural-equation-modeling-sem; accessed on 26 May 2021) [49] were utilized to examine our hypotheses. We used maximum likelihood estimates and examined a saturated model, as we did not find any studies that supported an alternative model. It is important to note that in a saturated model, there is no need to examine a model fit as the default and the saturated model are the same [50] . Standardized scores were employed in these path analyses. We repeated the analysis of the routes twice on different samples: the COVID-19 sample and the 21 May hostility sample. The two saturated models (all paths were examined) included five predictors and five predicted psychological coping responses expressed during the two adversities (the COVID-19 pandemic and the 21 May hostility). The predictors were the perceived security, health, economic, and political risks, as well as the level of morale. The predicted variables were the reported levels of distress (composed of both anxiety and depression); individual, community, and national resilience; and well-being. Our first hypothesis claimed that morale, which is a positive cognitive appraisal, will be as good a predictor as the four negative cognitive appraisals (the four different risks) in predicting both positive and negative psychological coping responses during the COVID-19 pandemic and the armed conflict. The first path analysis supported this hypothesis: morale was the best predictor of the levels of distress, well-being, and individual and community resiliencies and was the second-best predictor of national resilience (Table 2 ) during the COVID-19 pandemic (see also Figure 1 , describing the general model of the two path analyses (during COVID-19 and the armed conflict in Gaza)). A further examination of this path analysis indicated that, in agreement with our second hypothesis, the perceived health risk (which is supposed to be the most relevant to the pandemic adversity) was not the best predictor of the psychological coping responses expressed during the COVID-19 pandemic. Notice that the perceived health risk was the best predictor of distress and significantly predicted well-being and individual resilience but not community or national resilience. However, it was not a better predictor of wellbeing than the perceived economic risk, nor was it a better predictor of individual resilience than the security risk. This path analysis showed as well that the perceived political threat was the best predictor of national resilience. The percentages of the variance explained by this path analysis were as follows: distress, 55%; well-being, 48%; individual resilience, 28%; national resilience, 15%; and community resilience, 0.08%. A second and similar path analysis examined the role of the same predictors in predicting the identical psychological coping responses following a completely different disaster, the May 2021 armed conflict (Table 2 and Figure 1 ). The percentages of the variance explained by this path analysis were as follows: distress, 59%; well-being, 52%; individual resilience, 33%; national resilience, 17%; and community resilience, 9%. Morale was, again, found to be the best predictor of distress, well-being, and individual and community resilience and the second-best predictor of national resilience. Examination of the role of the perceived security risk (which is supposed to be the most relevant to the security adversity) as a predictor of coping responses, compared to the other three perceived risks, showed that this risk significantly predicted individual and national resilience, as well as distress, but not community resilience or well-being. However, similar to the first path analysis of the COVID-19 pandemic data, this path analysis indicated that the most relevant perceived risk of the investigated adversity was not the best predictor among the four perceived risks in predicting the coping responses. Following the hostility, the perceived security risk did not predict the individual resilience or the distress better than the perceived health risk. Finally, in the 21 May hostility, as well as in the COVID-19 context, the perceived political risk, rather than the morale, was again the best predictor of national resilience. A further examination of this path analysis indicated that, in agreement with our second hypothesis, the perceived health risk (which is supposed to be the most relevant to the pandemic adversity) was not the best predictor of the psychological coping responses expressed during the COVID-19 pandemic. Notice that the perceived health risk was the best predictor of distress and significantly predicted well-being and individual resilience but not community or national resilience. However, it was not a better predictor of well-being than the perceived economic risk, nor was it a better predictor of individual resilience than the security risk. This path analysis showed as well that the perceived political threat was the best predictor of national resilience. The percentages of the variance explained by this path analysis were as follows: distress, 55%; well-being, 48%; individual resilience, 28%; national resilience, 15%; and community resilience, 0.08%. A second and similar path analysis examined the role of the same predictors in predicting the identical psychological coping responses following a completely different disaster, the May 2021 armed conflict (Table 2 and Figure 1 ). The percentages of the variance explained by this path analysis were as follows: distress, 59%; well-being, 52%; individual resilience, 33%; national resilience, 17%; and community resilience, 9%. Morale was, again, found to be the best predictor of distress, well-being, and individual and community resilience and the second-best predictor of national resilience. Examination of the role of the perceived security risk (which is supposed to be the most relevant to the security adversity) as a predictor of coping responses, compared to the other three perceived risks, showed that this risk significantly predicted individual and national resilience, as well as distress, but not community resilience or well-being. However, similar to the first path analysis of the COVID-19 pandemic data, this path analysis indicated that the most relevant perceived risk of the investigated adversity was not the best predictor among the four perceived risks in predicting the coping responses. Following the hostility, the perceived security risk did not predict the individual resilience or the distress better than the perceived health risk. Finally, in the 21 May hostility, as well as in the COVID-19 context, the perceived political risk, rather than the morale, was again the best predictor of national resilience. The present study examined two major issues that have hardly been studied previously, comparing coping with the stressful experiences of a pandemic versus a security  The present study examined two major issues that have hardly been studied previously, comparing coping with the stressful experiences of a pandemic versus a security conflict. The first issue referred to the role of positive versus negative cognitive appraisals in predicting psychological coping responses during adversities. The second issue is related to the misconception, according to which the relevant perceived risk of major adversity will constitute the best predictor of the psychological coping responses raised during the disaster. The present path analyses confirmed our first hypothesis, showing the major role of positive cognitive appraisals in determining psychological coping. The positive cognitive appraisal of morale constituted a much better predictor of the investigated coping responses than any of the four negative cognitive appraisals employed: perceived security, health, economic, and political risks. The clear advantage of morale as a predictor of most psychological coping responses was retained, due to these path analyses, in four out of the five investigated coping responses along with the COVID-19 pandemic, as well as during the midst of the May 2021 hostility. Positive cognitive appraisals were often regrettably ignored by studies of coping with catastrophes and hardships. We suggest that further research is needed to support the contention that rather than being overlooked, morale and other positive appraisals should indeed be seen as major predictors of these coping processes and, therefore, should be constantly included as variables when studying responses to disasters. Our findings further suggest that efforts to help people reducing negative feelings such as anxiety and depression and to enhance positive feelings, such as individual resilience or well-being, may achieve better results by concentrating on enhancing morale [51] and probably other positive cognitive appraisals, rather than on attempting to reduce their negative cognitive appraisals, including various perceived threats. To date, most of the available research on predicting psychological responses to adversities concentrated on the negative responses to threatening conditions. A study of the SARS pandemic has thus reported that anxiety, avoidance, and disgust were the aftermath of the threats of this plague [20] . A more recent study concluded that higher levels of anxiety and depression, as well as lower levels of well-being and individual resilience, reflected the perceived risk of the COVID-19 pandemic [38] . Our contention that psychological coping responses to adversity will also be affected by positive cognitive appraisals was supported by a different perspective, which argued that the levels of depression and distress of victims of earthquake and tsunami decreased as a result of proactive appraisals [23] . This perspective was further supported by a comprehensive analysis of psychological reactions to wars [26] , according to which distress casualties in military units did not reflect the perceived war threats. Units with a higher level of morale suffered a substantially lower number of distress injuries compared to troops with a lower level of morale. A large number of studies of adversities and disasters tend to take for granted the quite reasonable conclusion that the increased negative psychological responses, which follow such disasters, as well as the ensuing lower levels of hope and resilience, reflect the perceived impact of the investigated calamity. Yang and Ma [6] have thus concluded that the current decrease in the overall emotional well-being in China represents a negative response to the perceived threat of the COVID-19 pandemic. Levkovich and Shinan-Altman [10] have similarly reported that the perceived COVID-19 pandemic threat explains the negative emotional reactions expressed by the general Israeli public during this plague. A study of several wars has concluded, by the same token, that the fear of armed conflicts is responsible for higher levels of anxiety, depression, and PTSD [11] . However, these studies did not compare the perceived impact of their investigated hardships with the impacts of other perceived threats, such as economic difficulties, discrimination, or political concerns, on the psychological coping responses, which were expressed in association with each of these calamities. The present study questioned the contention that coping responses can mostly be explained by the direct impact of the perceived threats of an investigated disaster. This issue was investigated by comparing the relative contributions of the most relevant perceived risk of the pandemic (heath risk) or the security conflict (security risk) in predicting coping responses with the predictions of the other three less relevant perceived risks. Our results refute the claim that these coping responses reflect mainly the most relevant perceived threat of each of these two calamities. The first path analysis showed, in line with our second hypothesis, that the perceived health risk was the best predictor of distress but was not the best predictor of the rest of the psychological coping responses, expressed during the COVID-19 pandemic. Similarly, the second path analysis indicated that the perceived security risk best predicted the level of distress expressed following the 21 May conflict but was not the best predictor of any of the other coping responses. A major limitation of this study is the fact that it is based on subjective self-reports of the general public. Studies of this kind are dependent on the respondents' awareness of their perceived levels of risk, which were raised by the four potential threats, the extent to which they feel anxious or resilient during these adversities. Therefore, this subjective data collection method always raises the question of whether the reports accurately represent all these psychological responses. Another limitation is the use of an internet panel company, which has a similar problem as sampling in the general population. Only those willing to take part are included. They may have systematic differences from those not willing to take part that might be relevant to the current study. The third limitation is that the study was conducted only among the Israeli Jews (the majority) population, due to budget constraints (distributed in Hebrew only). Thus, additional research is required to test the generalizability of the findings to other populations. The authors declare no conflict of interest.@story_separate@Two main findings were found in the current study. One relates to the importance of morale as a predictor of psychological coping with distress in varied types of adversities. The second applies to the need to consider varied indirect perceived risks that impact populations during crises, rather than the tendency, which characterizes many previous studies, to focus only on the most apparent or ""expected"" type of risk. Many studies focus on disasters to identify either the sources or the predictors of different psychological coping responses expressed during such periods. Our findings strongly suggest that the variances of coping responses explained by these studies may be substantially increased by including positive cognitive appraisals, such as morale, among their predictors. This conclusion is based on the fact that the present study employed morale as such a predictor and found that in most cases it predicted coping responses better than most of the negative perceived cognitive appraisals. A higher level of morale was correlated with an increased sense of well-being, as well as higher individual and community resilience. At the same time, it was also correlated with reduced levels of distress. It seems reasonable to expect that other positive appraisals may contribute further to predicting these responses. Our second conclusion pertains to the issue of a single major predictor of coping responses in the case of a catastrophe versus a model with multiple predictors. A large number of studies have taken for granted the assumption that negative and positive coping responses are caused directly by the relevant perceived threat of the investigated catastrophe. However, our findings present a more complicated phenomenon. First, all the investigated coping responses were predicted much better by the level of morale than by the perceived risks of the investigated adversities. Second, the perceived health risk and the perceived security risk were the best predictors of a single coping response, and the other perceived risks predicted better the rest of the coping responses. We suggest that overgeneralization, in which all the coping responses that are revealed during specific jeopardy are accounted for by the perceived risk of the catastrophe, should be avoided in future studies of adversities. It is strongly recommended that future studies of this issue empirically test this assumption by examining the contribution of additional potential predictors of coping responses, especially more indirect perceived risks. Funding: This research received was partly funded by the Ministry of Science. They did not intervene in any way in the conduct of the study, the data analysis, or the writing of the findings and manuscript. Institutional Review Board Statement: The study was approved by the Ethics Committee of the Tel Aviv University, # 0001150-1 from 7 April 2020 and extended # 0001150-2 from 5 May 2021. Informed Consent Statement: Informed consent was obtained from all subjects involved in the study. Data Availability Statement: All data accumulated in the study are available to the authors. Data are not published openly due to privacy issues, but analyzed data are available from the authors upon request.","The present study investigated predictors of psychological coping with adversity responses during the COVID-19 pandemic and an armed conflict. Two paired samples that represented the Israeli population that was exposed to both adversities were compared. Respondents rated five different psychological coping responses associated with the two adversities, such as anxiety or individual resilience. Perceived security, pandemic, economic, and political risks, as well as level of morale, were rated. Two major findings were disclosed by two path analyses. Morale improved the predictions of the varied coping responses in both the pandemic and conflict and was the best predictor of four out of five responses and the second-best predictor of the fifth response. Contrary to previous studies, our findings revealed that the concept of a single major predictor of coping responses under distress is an overgeneralization. In both cases, the coping responses were better explained by other perceived risks rather than by the risk of the investigated adversity. Rather than assume that a perceived security threat accounts for low levels of public moods, it is vital to study the antecedents of coping responses and to empirically examine additional potential predictors."
"Faecal microbiota transplantation (FMT) is currently a recommended therapy for recurrent/ refractory Clostridioides difficile infection (CDI). [1] [2] [3] [4] It is also being explored in the research setting for many other indications. 5 However, there are a number of associated concerns regarding its use, including the unpleasant prospect of the procedure, the potential need for invasive administration, 6 the small, but recognised risk of transmission of infection, and the complex regulation associated with its use. 7 The COVID-19 pandemic and potential risk of viral transmission through donor stool samples has brought its limitations to the fore. 8 As such, from a therapeutic perspective, understanding the mechanisms that underpin the efficacy of FMT may enable us to refine FMT from its current relatively crude state to a more refined 'microbiome therapeutic', which is no longer FMT, but could have a greater overall safety profile. This review will explore the current understanding of the mechanisms that underpin the efficacy of FMT across a variety of diseases.@story_separate@There has been a wealth of evidence demonstrating that FMT for CDI is effective for recurrent and refractory CDI, and the treatment has therefore been adopted in national and international guidelines. [2] [3] [4] A meta-analysis of all these studies highlights clinical resolution in 92% (95% CI 89-94%) of cases. 1 The success of FMT for CDI has led to interest in its therapeutic potential in many other disorders, 7,9 but a report on these is beyond the scope of this review. In CDI, the suppression of the native gut microbiome, often by antibiotic treatment, enables C. difficile spores to germinate into vegetative cells, which produce enterotoxins that cause inflammation and result in debilitating diarrhoeal symptoms. 10  The key rationale for using FMT as treatment for CDI is that this therapy restores the gut microbial communities. Indeed, the 'healthy commensals' reintroduced through FMT will compete for the ecological niches and prevent colonisation by pathogens, a well-described phenomenon known as 'colonisation resistance'. The role of the gut microbiota as a factor in the pathogenesis of many conditions including inflammatory bowel disease (IBD), metabolic syndrome and subgroups of patients with irritable bowel syndrome (IBS) 7 is accepted. However, the relative importance of the gut microbiota in the overall pathogenesis is different from one disease to another and we cannot yet quantify it for many diseases. For example, it has been noted that in CDI, changes in the composition of the gut microbiota represents the predominant factor in CDI pathogenesis 11 and, in IBD, it plays a very important role. 12 For many other conditions, its role might be more limited compared with other factors. Furthermore, other than for CDI, mechanistic studies are largely lacking, and it remains overall unclear whether these microbiota changes play a role significant enough to be efficiently targeted by FMT or other microbiome-based intervention. More recent data have shown that the efficacy of FMT in the treatment of recurrent CDI (rCDI) may not be explained by purely restoration of gut bacteria per se, but also by a number of additional factors. For instance, in one pilot study, researchers prepared a sterile faecal filtrate by passing FMT slurry through progressively narrower pore filters, culminating in a 0.2 µm pore filter. 13 The administration of the sterile faecal filtrate via a nasojejunal tube was effective in treating five patients with rCDI (>6 months), comparable with that degree of efficacy seen after administration of conventional FMT. The authors concluded that, rather than FMT directly requiring live, intact bacteria for its efficacy, it was instead likely that one or more soluble factors associated with bacteria within the filtrate potentially mediated its mechanism of action. 13 Within the following sections, the potential contributions of such factors are discussed. Bacteriophages are viruses that target and replicate within bacteria or archea. 14 Importantly, phage exposure can alter both the virulence and biofilm of its host. 15 From FMT/CDI studies, it has been shown that abundance of the order of bacteriophages named Caudovirales reduced significantly in stool after FMT, with FMT success more likely if donors had a higher fraction of Caudovirales within their stool virome. 16 Following FMT for rCDI, a recipient's core virome quickly resembled that of a donor and remained stable over at least the next 7-month period and even up to 12 months. 17, 18 In terms of other diseases, there are controversial data regarding phages, but successful FMT for IBD was associated with low eukaryotic viral richness in recipients before FMT. 19 Further supporting the role of the virome was a recent mouse study that transferred lean faecal virome into mice fed with high-fat diet. The virome transfer led to reduced weight gain and normalised blood-glucose relative-control mice. The authors concluded that the faecal virome exerts it effects via changes in the gut microbiota. 20 Importantly, eukaryotic viruses can be found in food and hence diet could be a confounding factor. 21 However, it is likely that the gut virome plays a significant part by its interaction with the other components of the gut microbiota, 15 but from a mechanistic perspective there are limited data to explain how bacteriophages and the virome contribute to a successful FMT and at present, we are limited to associative studies, and hence studies that infer causation are needed. It is likely that bacteriophages can alter their bacterial hosts indirectly by reprogramming their metabolism, to include transfer of phage genes that encode for antibiotic resistance 22 and alterations in pathogen virulence. 23 It is therefore likely that the enteric virome may contribute to some of the mechanisms that underpin the success of FMT, but this requires further exploration. From a fungal perspective, it has been suggested that patients with CDI who responded to FMT experienced colonisation with particular donorderived fungal taxa (in particular, members of Saccharomyces and Aspergillus genera), whereas non-response was associated with a dominant presence of Candida within donor stool. 24 Individuals not responding to FMT and/or patients treated for rCDI with antimicrobials alone retained overgrowth of Candida. In a mouse model of CDI, the presence of Candida albicans was associated with reduced efficacy of FMT, while use of antifungal therapy helped restore efficacy. 24 Utilising internal transcribed spacer 2 (ITS2) sequencing, it was demonstrated that the fungal microbiota is skewed in IBD, with an increased Basidiomycota/Ascomycota ratio, a decreased proportion of Saccharomyces cerevisiae and an increased proportion of C. albicans compared with healthy controls. 25 In samples from a large randomised controlled study utilising FMT for ulcerative colitis (UC) it was found that high Candida abundance pre-FMT was associated with a clinical response, whereas decreased Candida abundance post-FMT was indicative of ameliorated disease severity. 26 The authors suggested that high Candida abundance in the recipient might promote the engraftment of donor's bacteria by freeing ecological niches, and that FMT may reduce Candida which culminates in the success of FMT. These potential mechanisms need further exploration while acknowledging caveats associated with fungal infections in the presence of immunosuppression. Importantly, trans-kingdom-fungi-bacteria interactions have good evidence in many ecosystems and are beginning to be further understood in the gut. 27 While the perturbation of gut fungal profiles and their influence upon FMT outcomes are of interest, their significance as potentially contributing to the efficacy of FMT remains unclear. In view of the established relationship between antimicrobial treatment and overgrowth of Candida within the gut, any changes in gut mycobiota profiles may only possibly be proxies of gut bacterial alterations. As such, the specific contribution of bacteriophages and fungi to the efficacy of FMT remains undefined. Metabonomics is defined as 'the quantitative measurement over time of the metabolic responses of an individual or population to drug treatment or other intervention'; this differs from metabolomics, that explore the metabolic responses present in the whole cell or tissue. 28 Metabonomics, therefore, explore responses of an individual or community, whereas metabolomics explore responses in a cell, or bacterial population. Metabonomics utilise integrated-systems biology to provide a way of investigating the metabolic status of an organism or ecosystem by studying 'real' metabolic endpoints. 29 The contribution of gut microbiota-derived metabolites, or 'cometabolites' produced through the interaction between the microbiota and host, has also been a key area of interest in the study of mechanisms of FMT. Short-chain fatty acids. One particular group of metabolites that have been well studied in this field include short-chain fatty acids (SCFAs), which are the products of bacterial fermentation of partially digestible and non-digestible dietary carbohydrates and amino acids. Mice treated with broad-spectrum antibiotics experienced a marked reduction in levels in SCFAs in stool, and higher SCFA levels correlated with protection from C. difficile growth, suggesting an interaction between antibiotics, SCFAs, and CDI risk. 30 More recent work used a bioreactor/chemostat model of CDI to demonstrate that cessation of broad-spectrum antibiotics was associated with spontaneous recovery of the microbial synthetic recovery of most SCFAs, but not that of valerate, the five-carbon SCFA. 31 In vitro, valerate caused a dose-dependent inhibition of the growth of a range of C. difficile ribotypes, without any adverse effects against any commensal bacteria. Furthermore, in a mouse model of CDI, oral gavage of glycerol trivalerate was demonstrated to cause a rapid reduction in C. difficile colony-forming units detectable within stool. 31 Further experiments demonstrated that successful FMT for rCDI in humans was associated with the rapid, sustained restoration of stool valerate levels. 31 Beyond the dominant SCFAs of acetate, butyrate and propionate, these data support a specific role of valerate recovery in the success of FMT for rCDI. 32, 33 Furthermore, SCFAs seem to be critical in driving intestinal homeostasis through immunometabolic pathways in IBD. 34 SCFAs, specifically butyrate, have been shown to promote regulatory T-cell response in murine models of IBD. 35 Gut microbiota analysis of FMT-treated mice showed significant increases of commensals, including members of Lactobacillaceae and streptococcus along with of the SCFA-producing taxa Erysipelotrichaceae and Ruminococcaceae. 36 Administration of FMT is associated with enrichment of specific clostridium clusters that include the SCFA-producing families Ruminococcaceae and Lachnospiraceae and genus Roseburia in clinical studies. 36 Taken together, these findings suggest that restoration of gut microbial SCFA producers through FMT may drive regulatory immunological responses and homeostatic balance in IBD. Bile acids. A further group of metabolites of particular interest in the field of FMT/CDI is the bile acids. Initial experiments in vitro over 10 years ago demonstrated the differential effects of different classes of bile acids upon C. difficile. Specifically, primary bile acids have a 'pro-C. difficile' effect, primarily through the promotion of spore germination; in particular, the conjugated bile acid taurocholic acid (TCA) strongly promotes C. difficile germination in vitro in the presence of glycine as co-germinant. 37, 38 Conversely, the secondary bile acids (including deoxycholic and lithocholic acid) have a net 'anti-C. difficile' effect, particularly through the inhibition of vegetative growth and toxin activity of the bacterium. 30, 37 The transition from primary to secondary bile acids within the gut occurs through the activity of enzymes produced by the gut microbiota (in particular, the enzymes bile salt hydrolase (BSH) and 7-α-dehydroxylase). Rodent studies supported the concept that restoration of bacterial bile-metabolising capacity to the gut microbiota was protective against CDI, 39 prompting interest into whether this could also be a mechanism of efficacy of FMT. In this context, a range of in vitro, rodent and human studies have collectively demonstrated that while the pre-FMT stool bile-acid milieu is enriched with primary bile acids (and particularly TCA), the post-FMT stool bile-acid pool is much more comparable with that of healthy donors, with high levels of secondary bile acids. 32, 33, 40, 41 More recent work has directly demonstrated that successful FMT in those with rCDI results in maintained restoration of microbial BSH functionality to the gut microbiota, and that restoration of BSH in a mouse model of CDI is sufficient to significantly reduce C. difficile counts within stool. 41 Further research in this area has demonstrated that successful FMT for CDI is associated with an increase in circulating fibroblast growth factor (FGF)-19 and reduction in FGF-21, consistent with upregulation of the bile-acid receptor farnesoid X receptor (FXR)-FGF pathway. 42 An additional surprising finding of interest has been the recent demonstration that bacteria with 7-α-dehydroxylase bile-metabolising activity (including Clostridium scindens) are also able to produce tryptophanderived antibiotics which inhibit the cell division of C. difficile. 43 While evaluation of the effect of FMT for rCDI upon SCFAs and bile-acid metabolism has focused on their direct effects upon the life cycle of C. difficile, it is possible that this may also have other beneficial effects. For instance, FMTmediated changes in bile-acid-FXR interactions may directly impact upon the colitis caused by C. difficile; administration of an FXR agonist in a mouse model of colitis resulted in significantly reduced colonic inflammation and a more intact intestinal barrier, 44 while microbially mediated production of particular secondary bile acids exhibit anti-inflammatory effects on intestinal epithelial cells 45 and have been recently recognised as promoting generation of peripheral regulatory T cells. 46 SCFAs have also been demonstrated as able to regulate the size and function of the colonic regulatory T-cell population, which was directly shown to be a protective mechanism against the development of colitis in mice. 47 Other metabolites. A further related area of interest relates to the ability of C. difficile to 'scavenge' for metabolites within the antibiotic-treated gut as energy sources to facilitate growth. In particular, after antibiotic treatment, the loss of bacteria that compete with C. difficile for metabolites including the amino acid proline, 48 the organic acid succinate, 49 the monosaccharide sialic acid (derived from intestinal mucus) 50 and dietary trehlose 51 allows C. difficile to scavenge these metabolites unopposed, and exploit them for its growth and division. As such, it may be hypothesised that a further mechanism by which FMT functions is by restoring microbial competition within the gut, and therefore minimising an ecological niche that C. difficile deploys to derive energy sources. Metabonomics have also been applied to explore mechanisms underlying the efficacy of FMT in treating UC. An experimental model in rodents found that FMT given from dextran sulfate sodium-induced UC rats to healthy rats induced UC-like changes. 52 It was also found that FMT from healthy rats to colitic rats induced remission. When exploring the metabonomic changes associated with this remission, it was observed that urinary hippuric acid was significantly reduced in the UC group compared with normal rats. Specifically, it was noted that there were increases in C10:3 acylcarnitine, hydroxyphenylpropionylglycine, and riboflavin. In a second experiment, researchers transferred the microbiota from those rats with UC to untouched rats. It was noted that hippuric acid decreased in the normal rats but was restored to normal levels at day 6 and 7, and further found that the changes induced by FMT correlated with the genera Oscillospira and Dehalobacterium and the families Bacillaceae and Exiguobacteraceae. 52 Importantly it has been shown that hippuric acid is reduced in CD and UC due to gut microbial metabolism; this therefore suggests that FMT can alter the gut microbiome to change the metabolic drivers of disease states. 53 Further supporting this concept was a study on pigs, where it was noted that FMT resulted in significant increases of the typical microbiotaderived tryptophan catabolite indole-3-acetic acid in the colonic lumen, 54 suggesting that tryptophan metabolites may be important actors in the efficacy of FMT. In a human study, exploring FMT for children with UC, responders to FMT highlighted that Bacilli and Betaproteobacteria were positively correlated with metabolites from the 'disease-associated' cluster (such as creatinine and norvaline), and clostridia were positively correlated with metabolites from the 'healthy' cluster (such as xanthine and 1-hexadecanol). 55 There has been one randomised controlled trial (RCT) in UC that measured metabolites following FMT using metabonomic and shotgun metagenomics. They noted that specific bacterial functional pathways were associated with a positive outcome, including: benzoate degradation, glycerophospholipid metabolism, secondary bile-acid biosynthesis, guanosine pentatetra-phosphate biosynthesis, pyruvate fermentation to acetate and lactate, biosynthesis of ansamycins, and starch degradation. Furthermore, it was found that these pathways were correlated with the abundance of Eubact erium, Ruminoccus, Lachno spiraceae, Roseburia, Dorea and Coprococcus taxa. 56 Linking metabolic function to specific bacteria is likely to provide key mechanistic insights into the active components in FMT and potentially help refine FMT. FMT metabolites and autophagy. Autophagy is a crucial housekeeping process in cellular function that removes and recycles dysfunctional components such as misfolded proteins or damaged organelles. This process is particularly active and important for the function of proliferating cells such as intestinal epithelial cells. It has been noted that FMT could trigger intestinal mucosal autophagy and alleviate gut-barrier injury caused by specific bacteria such as Escherichia coli. 57 Specifically, it was noted that 58 metabolites, such as lactic acid and succinic acid, were enhanced and upregulated in piglets, following FMT. These upregulations were then responsible for changes in metabolic pathways such as linoleic acid metabolism, which culminated in a decrease in intestinal permeability and enhancement of mucins and mucosal expression of tight junction proteins in the recipient. 57 It is therefore possible that FMT alters autophagy through its influence on the gut microbiomes metabolic pathways. Through a complex and bidirectional relationship, the gut microbiome plays a critical role in shaping the gut mucosal immune response. 58 Our initial insight of how FMT impacted the immune system was from CDI-FMT studies. 59 In a dextran sodium sulfate (DSS)-induced colitis mouse model, it was noted that response to FMT was associated with activation of a variety of immunemediated pathways which ultimately lead to interleukin 10 (IL-10) production by innate and adaptive immune cells. These included CD4+ T cells, invariant natural killer T (iNKT) cells and antigen-presenting cells. Furthermore, it was demonstrated that FMT reduces the ability of dendritic cells, monocytes and macrophages to present major histocompatibility complex class-II-dependent bacterial antigens to colonic T cells. 60 It has also been shown that patients with recurrent CDI who responded to FMT had a reduction in complexity serum N-glycosylation profiles. 61 Glycans are associated with epigenetic modification that affects multiple immunological pathways and enable cross talk between gut bacteria/ pathogens and host epithelial cells. The relevance of this molecular mechanism in relation to response to FMT deserves further exploration. A breakdown in the innate and adaptive immune mechanisms appears to be fundamental in the development of chronic immune-mediated diseases such as IBD. 62 There is now increasing evidence to suggest that the gut microbial perturbations observed in these diseases contribute to (or possibly even trigger) this homeostatic immunological imbalance. 63 Therapeutic Advances in Gastroenterology 13 6 journals.sagepub.com/home/tag Transfer of gut microbiota from patients with IBD into germ-free mice has been shown to significantly increase the numbers of pro-inflammatory intestinal T-helper 17 (Th17) cells and while reducing regulatory RORγt+ T-regulatory-cell (Treg) populations when compared with gut microbiota from healthy individuals. 64 Moreover, microbiota from patients with IBD exacerbate colitis in an immunological mouse model of IBD with correlations observed between proportions of Th17 and RORγt+ Treg cells and patient inflammatory status. The majority of mechanistic work incorporated into the five RCTs in IBD 36,65-68 focused on shifts in gut bacterial and metabolomic profiles, with only one exploring immunological effects of FMT on disease response. This study did not find any significant change in proportions of γδ T cells, NK cells and T-cell subsets in colonic lamina propria immune cells. 65 They did, however, observe a slight increase in peripheral blood mononuclear gut-homing CD4 T-cell populations following FMT when adjusted for clinical disease-activity scores (p = 0.05). It was unclear if responders to FMT had specific shifts in immune subsets compared with non-responders. Our group (Quraishi and Iqbal) recently evaluated the host mechanistic response to FMT in patients with active UC as part of the pilot phase of the STOP-Colitis trial. 69, 70 In the 12 patients enrolled into this mechanistic arm, a clinical response was seen in eight patients following FMT. The responders had a significant reduction in mucosal Th17 cells along with a significant increase in regulatory T cells, effector-memory Tregs and gut-homing Tregs. Furthermore, we observed a significant increase in IL-10-producing CD4 cells and reduction in IL-17-producing CD4-cell and CD8-cell populations in responders, following FMT. Colonic mucosal transcriptome analysis demonstrated that clinical response to FMT was associated with significant downregulation of host antimicrobial defence response, antimicrobial peptides and pro-inflammatory immune pathways. There was a significant upregulation of butyrate and propionate metabolic pathways in FMT responders. A study in two patients with immune-checkpoint inhibitor colitis observed that immunological response after FMT was associated with an increase in FoxP3+ CD4 cells along with a substantial reduction in the colonic mucosal CD8+ T-cell population. 71 There was a concomitant expansion in the population of Bifidobacterium species, Clostridia and Blautia. Treatment of mice with Bifidobacterium has been shown to ameliorate DSS-induced colitis following immunecheckpoint blockade. 72 This protective effect was, however, abrogated in Treg-depleted mice. Collectively, these findings indicate an emerging role of FMT and specific agents in the gut microbiota in mitigating inflammation via induction or modulation of Treg function. Furthermore, in a rodent study that inoculated 2-week old neonatal mice with faeces from Clostridium-associated mice, it was demonstrated that there was a significant increase in Clostridium clusters IV and XIVa in the treated mice accompanied by a significantly higher number of colonic FOXP3+ Tregs, 73 highlighting the potential interactions between the microbiome and the local/systemic immunity. In a follow-up study exploring this concept, researchers inoculated germ-free mice with either treated or untreated chloroform human stool and noted a significant increase in the percentage of FOXP3+ Tregs among CD4+ T cells in the colons of germ mice inoculated with untreated human faeces compared with germ-free mice. 35 When applied to those with IBD, a study that used colonic lamina propria lymphocytes (LPLs) and peripheral blood lymphocytes (PBLs) from healthy individuals and those with colon cancer and IBD, demonstrated that DP8α T cells exhibited a highly skewed repertoire toward the recognition of Faecalibacterium prausnitzii, which is decreased in patients with IBD. They further demonstrated that the frequencies of DP8α PBL and colonic LPL were lower in patients with IBD than in healthy donors and in the healthy mucosa of patients with colon cancer, respectively. These data together suggest that Clostridium species are key regulators of inflammation through their influence on the gut immune system. 74 A further study which stimulated cells known to respond to F. prausnitzii measured their production of IL-10 and their downstream cell activity. They demonstrated that the proportion of circulating CCR6+/ CXCR6+ DP8α T cells was significantly reduced (p < 0.0001) within the total population of CD3+ T cells from patients with IBD compared with patients with infectious colitis or controls. 75 Summarising these findings suggests that components of the gut microbiome are key regulators of immune function and significantly impact the mechanisms that underpin gut homeostasis, and therefore, FMT may work by promotion of some of the gut homeostatic immune functions and downregulating the pro-inflammatory immune responses. When considering specific metabolites, it has been demonstrated that SCFAs, specifically butyrate, have been shown to induce Tregs and promote anti-inflammatory IL-10 production in mice. 35, 73 It is likely that introduction or enrichment of specific gut microbial species via FMT attenuates inflammation by promoting Treg proliferation in the colonic mucosa through products of bacterial metabolism including SCFAs, tryptophan and polysaccharide. [76] [77] [78] When exploring tryptophan specifically, it has been demonstrated that the transfer of microbiota from CARD9 mice into wild-type germ-free mice increases their susceptibility to colitis. The mechanism that appears to underpin this is the CARD9 susceptibility gene alters the gut metabolism of tryptophan into aryl hydrocarbon receptor ligands, leading to inflammation. 79 Importantly, this phenomenon was ameliorated by inoculation of mice with Lactobacillus strains capable of metabolizing tryptophan, suggesting a key link between genetics and microbiota. Small-bowel microbiota Importantly, the majority of published studies focus on the colonic microbiota, as assessed by stool (or, in some cases, by mucosal biopsies). The small bowel also harbours a complex microbial community, albeit with less diversity and abundance (≈10 3 -10 7 microbial cells/g) than the colonic microbiota (≈10 12 cells/g)). 80, 81 Its influence on the mechanisms and effect of FMT is currently poorly understood. Importantly, FMT is known to have comparable efficacy in CDI 82 when infused into the upper gastrointestinal tract. Specifically, when considering IBS, small-bowel microbiota alterations have been associated with symptoms, 83 and hence future studies will need to explore the role of the small-bowel microbiota in efficacy of FMT. Data from non-CDI FMT studies such as in IBD have demonstrated that some recipients of FMT have an exceptional response while others do not. 36, 65 It is therefore possible that there are factors associated with both the recipient and the donor that may underpin the success of FMT. When considering donor factors, it is likely that particular components are driving the therapeutic effect from FMT and hence analysing the donor stool remains an important element in understanding the mechanisms underpinning the efficacy of FMT. Studies have speculated about what makes a 'super donor'. 84 The origins of a putative 'super donor' effect were from an FMT-UC study in where 'donor B' induced significant more remission than other donors. This therapeutic effect was associated with significant increases for the family Lachnospiraceae and the genus Ruminococcus in 'donor B' microbiota. 67 Such evidence lead researchers to conclude that a donor's microbiota diversity may have an influential effect on the success of FMT in IBD. 85, 86 Furthermore, specific taxa have been associated with disease response such as, clostridium clusters IV and XIVa 68,87 and Ruminococcaceae and Lachnospiraceae families. 67 Specific bacteriaproducing SCFAs such as butyrate are also suggested to be important in the efficacy of FMT. 88 Furthermore, as previously stated, a meta-analysis exploring the role of FMT for IBS demonstrated that FMT had no effect in IBS, following this, however, an RCT using a single 'super donor' showed a high success at reducing IBS symptoms, 89 suggesting that the stool donor may have significant effects on the efficacy of FMT. Importantly, when considering CDI, most patients respond to FMT, which suggests that disease-specific factors may be driving the success rather than the donation itself. In view of this, studies exploring donor-specific factors associated with an unsuccessful response may provide valuable insight into mechanisms that underpin FMT success. Another major consideration is that diet plays a large influence on the gut microbiota and hence is likely to affect the donation and FMT efficacy. 90 Uncovering the dietary aspects that may influence the efficacy of FMT will be an important consideration. Lastly, it is plausible that specific constituents in an FMT will provide benefit for one person but not another. It is therefore possible that a 'one size fits all' FMT might be replaced by a more personalised FMT as our knowledge improves regarding mechanisms that underpin a successful donor. Another important consideration is studies have not been powered to date to understand the donor characteristics associated with success. Despite these findings, however, the mechanisms underpinning the 'super donor' phenomenon are yet to be detailed. An important consideration is to determine what part of the FMT engrafts into the host and may be a significant factor that underpins efficacy. Currently, there is no robust definition of engraftment. A significant consideration is to understand if FMT promotes growth of suppressed host microbiome constituents or introduces new constituents into the host. Specific strain tracking may help understand this and studies are attempting to further define this. 91, 92 Post-FMT era Given this central importance of the restoration of the gut microbiota to the efficacy of FMT in the treatment of CDI, there have been a number of different approaches towards a more defined 'narrow spectrum' microbiota product of wellcharacterised bacteria as an alternative to FMT. Proof-of-concept use of 'defined bacterial communities' as an alternative CDI treatment has been demonstrated in bioreactor and rodent models, 93,94 as well as human studies. For instance, as early as 1989, bacteriotherapy was discussed in the treatment of CDI. 95 More recently, in the 'RePOOPulate' study, 33 different commensal bacterial species were cultured from the stool of healthy donors; these were used to synthesise a 'stool-substitution therapy', consisting of a mixture of purified bacterial cultures derived from these stool bacteria. 96 After colonoscopic administration of this mixture to two patients with rCDI, both patients achieved a rapid and sustained remission. 96 As an alternative approach, healthy donor stool was ethanol treated (to kill vegetative cells); the surviving spores were fractionated and capsulised, and delivered orally as a preparation named SER-109. 97 In a cohort of 30 patients, 29 achieved clinical remission from rCDI after one or two administrations of SER-109. 97 However, despite early promise, SER-109 produced negative results when administered in a phase II clinical trial, with potential issues related to the differentiation of true CDI recurrence from post-CDI IBS, and the dosing of the treatment regimen. 98, 99 This concept has been further expanded into other disease areas with a consortium of microorganisms being explored for treatment of mild-to-moderate UC in a phase II study. 100 Live biotherapeutics refer to live microorganisms that are used to prevent or cure human disease. 101 The concept relies on specific microbes causing a beneficial effect to the host. These can be isolated from the gut microbiota of healthy people or engineered microbiomes. 102 As previously mentioned, these have been studied for diseases such as CDI and IBD but have shown promise in other disease areas. Specifically, they have shown promise in the treatment of cancer, with one study highlighting that a commensal of 11 healthy human-associated bacterial strains can induce interferon γ+ CD8 T cells that confer resistance to the intracellular pathogen Listeria monocytogenes, and inhibit tumour growth in conjunction with immune-checkpoint inhibitors. 103 In another study, 17 human-derived clostridium strains (VE202) were able to reverse histological colitis in a mice model. 104 There are many commercial companies aiming to find biotherapeutics for a whole range of diseases. As we learn more about the mechanisms that underpin the efficacy in FMT, it is likely that these will feature in more clinical trials. Importantly, any engineered microbiota-based therapies will need to be examined in clinical trials to assess if they have clinical equipoise with, or are even superior to, FMT. Phage therapy refers to the therapeutic use of viruses that infect bacteria, bacteriophages, to treat disease. Phage therapy aims to specifically kill their respective bacterial host while preserving other microorganisms and human cells. This has been a growing area of interest in view of the rising incidence of antibiotic resistance. Phage therapies in clinical practice are very much still in the research stage with concerns over regulation and safety. 105 In an in vitro human model study, phage øCD27 showed significant reduction in C. difficile cell numbers and toxin production without major effects on other members of the microbiota. 106 As previously demonstrated, the virome plays a significant role in the efficacy of FMT and hence further exploration into phage therapy may help us understand the mechanisms that underpin FMT efficacy (Figure 1 ). The authors declare that there is no conflict of interest. The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: the Division of Digestive Diseases at Imperial College London receives financial support from the National Institute of Health Research (NIHR) Imperial Biomedical Research Centre based at Imperial College Healthcare NHS Trust and Imperial College London. BHM is the recipient of an NIHR Academic Clinical Lectureship. @story_separate@As highlighted in this review, much of our current understanding of mechanistic insights into the","Faecal microbiota transplantation (FMT) is currently a recommended therapy for recurrent/refractory Clostridioides difficile infection (CDI). The success of FMT for CDI has led to interest in its therapeutic potential in many other disorders. The mechanisms that underpin the efficacy of FMT are not fully understood. Importantly, FMT remains a crucial treatment in managing CDI and understanding the mechanisms that underpin its success will be critical to improve its clinical efficacy, safety and usability. Furthermore, a deeper understanding of this may allow us to expose FMT’s full potential as a therapeutic tool for other disease states. This review will explore the current understanding of the mechanisms underlying the efficacy of FMT across a variety of diseases."
"Due to exceptional situations, great transformations have taken place in different contexts; the university, like others, pushed to adapt to circumstances for which it was not prepared for, improvising actions that at this point, it would be convenient to assess. Without a reaction time, the COVID-19 pandemic caused an unprecedented transformation in universities around the world, leading to a revolution from structured models anchored in the conception of transmission of training and in person as basic pillars of teaching between teachers and students, towards a teaching-learning approach saved thanks to the incorporation of technology and virtual training platforms (a learning management system). Following concerns about this changed, which has modified our actions but was barely considered in the haste to solve the problem that has arisen, this article aims to show the impact of the pandemic on the development of teacher digital competences of university students through the Infant Education Degree, which provides essential competencies for their professional development as future teachers.@story_separate@Part of the academic years 2019/20 and 2020/21 will be remembered in history for the fact that more than 1500 million students throughout the world were forced to not attend their classrooms after lockdowns was imposed to contain the spread of the SARS-Cov-2 virus, which causes COVID- 19 . In this way, the face-to-face modality gave rise, from one day to the next, to the so-called online, telematic or virtual modality [1] . For a long time, several questions were raised regarding the penetration of technologies in teaching. Firstly, beliefs teachers have about possibilities that technologies can offer for their professional development are elementary, both for incorporating them properly, and for the form or method used to do so [2] [3] [4] [5] [6] . Specifically, [7] (p. 339) point out that: ""Teachers' beliefs about learning and teaching are propositions about learning and teaching that a teacher holds to be true; they develop over the many years that teachers spend in school, first as students, then as teachers, and with time and use, these beliefs become robust."" Another issue is that there is a tendency among teachers to conceive of technologies simply as an addition to their entire teaching process, but not as tools that can facilitate a true educational revolution, a change and a process of authentic innovation with which to build new ways to learn [8, 9] . Among this mass of issues, COVID-19 has arisen, causing its greatest radical change in higher education. It meant a drastic break with many of the traditional approaches, such as the unity of space and time, and unity of action. In Spain, as other authors point out [10] , unlike in Latin America, the lack of learning technologies has been not the greatest barrier to coping with this situation [11] (p. 85). The problem has been revealed in people, as many of those involved have reported skills deficiencies in the use of these technologies, problems integrating them into the instructional design of their subjects or simply ignorance about institutional technological solutions that universities provided at their disposal. This is the time, therefore, to offer the possibility of verifying how the digital transformation is taking place in universities, focusing on the training of university students and, therefore, on the necessary training in digital teaching competencies of the teachers [9, 12, 13] . In this context, it is possible to establish a direct relationship between the appearance of the pandemic with the physical health of university teachers and students and their academic development during this school year. Reference [14] claims that the virus is the condition that has prompted this mental leap. As a result of its spread, students must adopt coping strategies that are very different from those they had until now, before entering the university, to successfully overcome the new demands that are demanded of them. As [15] (p. 5) points out, ""a large part of these new university students lack these strategies or present academic behaviours that are inappropriate for the new demands. Entering a university (with the changes it entails) represents a set of highly stressful situations due to the fact that the individual may experience, even if only temporarily, a lack of control over the new environment, potentially generating stress and, ultimately term, potential generator-with other factors-of university academic failure"". If university life was a particularly stressful period, today young people must get used to the new educational context and the form of learning (online) required to stop infections during the pandemic; these are novel experiences that can cause confusion in students, especially when developing their digital skills and personal skills. With the exception of some studies, particularly developed in China [12, 16, 17] there are still few assessments of the psychological effects that the pandemic has caused on the mental health of university students, which were already defined as a vulnerable population. Therefore, it would be interesting to assess the effects of the pandemic not only on mental health and personal well-being but also academically. Even more so, special emphasis can be placed on the gender variable as a determinant of these possible effects, if it takes into account that different international studies show important gender inequalities, since ""it is women who report and are diagnosed more frequently of some problem of this kind. This reality, however, is more complex when analysed according to the different mental disorders ( . . . )"" [18] (p. 61). Problems of anxiety, stress, etc., are the main impediments to success in education, since they can affect students' motivation, concentration and social interactions. In our study, it is important to reconsider that having digital skills can also influence these problems; the new environments imposed by the pandemic have been able to alter the academic habits of many female students, causing anxiety about an immediate adaptation to the new system. According to the above, and focusing on the way in which learning develops in university students, different studies [19] show how this is clearly related to self-perception and the way in which they face reality through the personal and subjective vision of their own image. We understand self-perception as the valuations that a person has of himself, in a field of action and a specific moment, as well as the ""set of beliefs, attitudes, desires, values and expectations of the outside world and that the individual transforms in his inside world"" [20] (pp. 96-97). Therefore, we consider that when an individual learns, they perform a self-regulation process which is directly influenced by the evaluation he makes of his own competences [19, 21] . In this way, various studies [22] [23] [24] [25] show there is a direct relationship between learning and the way the individual feels about himself and is perceived as being competent, hence the importance of analyzing emotions in the learning process. In this context, the study carried out by [26] concludes that students manifest a diversity of both positive and negative emotions in learning processes supported in virtual environments, finding higher scores in positive emotions than in negative ones. As was already pointed out, according to [27] , factors such as anxiety, worries and discomfort hinder learning. This type of situation, which directly affects the way the world is perceived and the role that one plays in it, means that students do not perceive all the nuances of the information and fail to process it properly, a fact that ends up re-impacting on their academic performance. In addition to all this, another concept that is closely related to self-perception, when we refer to learning, is that of self-efficacy. The concept of self-efficacy comes from Bandura's social cognitive theory [28] and can be defined as the perception that the individual himself generates about his capacities to face future situations. According to Bandura [28] self-efficacy derives from four main sources: experiences or achievements of mastery, vicarious experiences, verbal persuasion and physiological and affective states or emotional arousal. The first refers to the experiences in which the individual must face the performance of the task, and therefore requires implementation and according to the author is [29] (p. 71) the most influential source in the configuration of self-efficacy ""The most effective way of creating a strong sense of efficacy is through mastery experiences. Successes build a robust belief in one's personal efficacy. Failures undermine it, especially if failures occur before a sense of efficacy is firmly established"". Vicarious experiences allude to observational learning, while modeling and social persuasion are related to exposure to comments and verbal expressions which an individual is subjected to at the time they perform some action. Finally, the physiological and affective states or emotional arousal refers to the emotional state, pointing out in the words of Bandura [28] (p. 198): ""Because high arousal usually debilitates performance, individuals are more likely to expect success when they are not beset by aversive arousal than if they are tense and viscerally agitated"". In relation to all this, various studies have been developed that show the relationship between moods and psychological well-being with self-efficacy [30, 31] . Others establish a relationship between self-efficacy and academic performance, indicating that self-efficacy is a key element in performance and skills development [32, 33] . From these considerations, it is interesting to address the self-perception of future teachers' learning in terms of digital competence, in a context of a health crisis. The study presented in this article has as its starting point the key competences defined by the Council of the European Union [34] , the so-called DIGCOM 2.0. This is the most widely used competence framework for the development and understanding of digital competence in Europe [35] , and educational institutions use it to train people capable of integrating technologies into their daily lives in a profitable, safe and healthy way. If we want to ensure our future university teachers are digitally trained according to the proposed European Framework for Digital Competence of Teachers ""Dig-CompEdu"" [36] , we will have to take into account different areas of competence they must acquire, specifically we are talking about: (A) Professional commitment: their ability to use digital technologies not only to improve teaching, but also to interact professionally with colleagues, students, family and different agents of the educational community. (B) Digital resources: any teacher must develop and identify good educational resources, being able to modify, create and share them to adjust them to their objectives, students and teaching style. At the same time, you must know how to use and manage digital content responsibly, respecting copyright rules and protecting personal data. (C) Digital pedagogy: this involves knowing how to design, plan and implement the use of digital technologies in the different stages of the teaching and learning process immersed in methodologies that are centered on the student body. (D) Evaluation and feedback: linked to the use of digital tools and strategies in the evaluation and improvement of teaching-learning processes. (E) Empower students: the use of digital tools to promote the active participation of students in the learning process and their autonomy over it. (F) Facilitate students' digital competence and assess how to develop and facilitate citizens' digital competence [37] . Likewise, to seek self-perceptions of their digital skills in students, ""DigCompEdu Check-In"" was designed for future teachers' [38] adaptation of the ""DigCompEdu Check-In"" for teachers [39] , which we referred to and which we will describe in Section 2.3. As a result of all the above, the question that arises in the study is to verify if the pandemic (COVID-19) affected the development of TDC of the students of the Infant Education Degree, after they received technological training for their professional development. We decided to carry out an inferential analysis to check if the pandemic situation influenced the students' self-perceptions of competence. Two groups were compared: group A that took the subject of Information and Communication Technology Applied to Early Childhood Education before the pandemic, and group B that took this same subject during the pandemic period. The main objective was to determine the impact caused by COVID-19 on the development of TDC in Infant Grade students to draw up plans that could help alleviate this impact. To deepen this objective, it was specified through specific objectives. The first is checking the reliability and validity of the instrument used to measure the level of self-perception of the TDC (O1), then describing the results obtained in the competence self-perception for group A and group B (O2), then checking whether there are statistically significant differences between the results obtained in both groups (O3). Finally, checking if there is a relationship between competency self-perception and the variables sex, teaching experience and time of use of the technology (O4). The population that makes up the study sample was made up of students who are studying the 4th year of a Bachelor's Degree in Early Childhood Education at the University of Seville, during the academic years 2019/2020 and 2020/2021, who have taken the subject of ICT applied to Infant Education (TICEI) in the first semester. The sample was made up of a total of 559 students: From the 2019-2020 academic year, 296 students participated (group A) and from 2020-2021 a total of 263 participated (group B). Group A was the one that received technological training before the pandemic and group B received technological training during the pandemic. For their selection, incidental or convenience criteria were chosen according to their availability to answer the questionnaire [40] . It should be pointed out that both groups received the same training and in both a pretest (f = 232) and a posttest (f = 237) of the DigComEdu for students [38] were administered. The characteristics of both groups of subjects (group A and group B) were similar, since most of the students are women, being 92.6% in group A and 95.6% in group B. Many of them are in the 20-25 age bracket (75.8% in group A and 88.4% in group B). We would like to highlight that there is a significant percentage of students who have already had teaching experience: 39% in Group A and 22% in Group B. A similar percentage of the sample said that they used ICT as an educational tool for 3 years (A = 25.3% and B = 25.9%), followed by those with less than 1 year (A = 25.6% and B = 15.7%) and those who never used it (A = 13% and B = 11.7%). There were considerable differences between both groups in terms of the time dedicated to the use of technology for their studies, as group A (before the pandemic) indicated a usage rate of 51-75% while group B provided a rate of 76-100%, respectively. When asked about the use of technology for their careers, group A's response was 51-75% of the time while group B's response was 76-100%, which may have been caused by the lockdown situation and the virtual nature of classes after the pandemic began. Finally, it is necessary to highlight that in both groups, a high percentage of the sample (group A = 89.4; group B = 84%) was interested in learning to use new applications and digital resources. A link was added to the training program so that the aspects related to the training received for both groups can be consulted: https://n9.cl/z1qnc (accessed on 24 February 2021). The instrument used was the ""DigCompEdu Check-In"" questionnaire for digital competence for future teachers [38] (see Table 1 ). It is an adaptation made for students (see Table S1 ) and it has ""DigCom-pEdu Check-In"" as a starting point for teachers [39] . It arises from a process of expert consultations, tests prior to the pilot phase and a review of elements [41] . In the adaptation, the structure of origin has been respected, where each competence is represented by a single item and by selecting the most generic concept that encompasses all the specific content of the competence. The 22 items that made up the questionnaire respond to the 6 competence areas: professional commitment (4), digital resources (3), digital pedagogy (4), evaluation and feedback (3), empowering students (3) and facilitating competition digital students (5) . Each item was measured on a 5interval Likert scale, where participants indicate the extent to which they reflect their own assessment, selecting one of the five options. The instrument includes a final section in which the sociodemographic data of the students and some questions about their activities, habits, etc. are collected. The questions are organized progressively, reflecting the general progression logic of ""DigCompE-du"" (proficiency levels), through an internal scoring system. Six progressive competence levels were assessed so that the level of digital competence of a teacher is identified, from the Novice level (A1) or one with very little experience and contact with educational technology, to the Pioneer (C2) or one who leads innovation with ICT [37] . Once the modifications were made, a pilot study was carried out with Infant and Primary Grade students, during which time after validating this version by 256 students, the Cronbach's alpha coefficient was 0.947 [38] . In addition, statistical tests were carried out that allow us to complement the reliability and validity with our sample under study (O1). Next, results achieved on the reliability obtained for the total construct and for its dimensions are presented; to achieve this, Cronbach's Alpha and McDonald's Omega were used. For some authors [42, 43] , the Alpha and Omega values situated between the interval 0.8 and 1 can be considered 'very high'. Likewise, it is considered that values greater than 0.7 are sufficient to guarantee the reliability of the instrument. Consequently, they would denote high levels of reliability both for the globality and for the different dimensions that make up the instrument. To check the validity, the coefficients of Composite Reliability (CR), Average Extracted Variance (AVE) and Maximum Shared Variance (MSV) were calculated. Table 2 shows the results, as well as the reference values taken for the model fit [44] . All the figures obtained were adjusted with the reference values (CR> 0.7, AVE > 0.5 y MSV < AVE). Therefore, the reliability of the model (CR) as well as its convergent (AVE) and discriminatory (MSV) validity is demonstrated. The work proposed a pretest-posttest methodology through a descriptive crosssectional study. This design did not modify the variables under study, but explored their nature and behaviour in the participants who were part of the study Descriptive and central tendency (O2) analysis was carried out. In addition, contrast statistics were applied to make comparisons between the scores obtained in group A (before the pandemic) and group B (during the pandemic) (O3). Specifically, the non-parametric Mann-Whitney U test was used. Additionally, to test hypotheses, the correlational analysis method between items and dimensions (O4) was used. To do this, the bivariate correlational analysis technique was used by means of Spearman's Rho correlation coefficient. In parallel, it was proven that the data are not normally distributed through the study of skewness and kurtosis. The Kolmogorov-Smirnov goodness of fit test confirmed this verification with a significance (p-value) equal to 0.000 for all items (a non-normal distribution). At all times, the data obtained were analyzed with the SPSS statistical package (v.23). Following the objectives of the study, data showing the competence areas and the degree of self-perception of the students before and after taking the subject ""Information and Communication Technologies Applied to Early Childhood Education"" in group A and group B (O2) were assessed. Table 3 presents the results obtained in the mean and standard deviation by the groups (A and B) study dimension. As can be seen, before the educational intervention, group A (before COVID-19) presented higher self-perceptions of competence than group B (during COVID-19); On the other hand, after the educational intervention, self-perceptions tended to equalize, although they were still higher in group A. To check whether the changes detected in competence self-perceptions between group A and group B were statistically significant changes, the non-parametric Mann-Whitney U contrast tests were applied for more than two related samples (O3) ( Table 4 ). According to the data obtained, in general, the pandemic situation caused by COVID-19 has negatively influenced students' self-perception of their digital skills in the pretest for the different dimensions under study. Almost all competency areas showed a significance level lower than 0.05, so we can confirm, with a 99% confidence level, that there were statistically changes between groups A (before COVID-19) and group B (during the COVID-19 pandemic). Similarly, it is necessary to indicate that there were no significant differences in area 4 related to the evaluation. If Table 5 is observed, where the average ranges are shown, the changes produced always imply an improvement in the self-perception of the students who have taken the subject before COVID-19, (Group A) except in dimension or area 4. In the same way, we verified whether the differences obtained in the self-perceptions produced between the two groups for the posttest are significant ( Table 6 ). The data showed that almost all the competence areas presented a level of significance lower than 0.05, so we could confirm, with a 99% confidence level, that there are statistically significant changes between before COVID-19 and during COVID-19 in the competence self-perception for the posttest. Similarly, it is necessary to indicate that there were no significant differences in area 4 related to the evaluation. As in the analysis carried out for perceptions in the pretest, the ranges showed (Table 7 ) an improvement for group A, although after training there was a tendency to equalize that could be linked to the training received and technological use after training while taking the subject. Finally, a correlational analysis was carried out to check if there is a relationship between the variable ""Time"" using technology and the competence self-perception of each area, as well as the general perception of digital competence. For this, the Spearman Rho correlation coefficient was applied in a comparative way in group A (before the pandemic) and group B (during the pandemic) (O4). The results are shown in Table 8 . As can be seen, in group A (before the pandemic) there was no relationship between the variables time of use of ICT and self-perception of DC in all of its areas. On the other hand, in group B with a confidence level of 99%, it could be affirmed that there is a positive correlation, although of low intensity, which could be explained by the alarm situation and the confirmation that this group made greater use of ICT as an educational tool, which positively influenced their competence self-assessment of themselves. In a complementary way, it was verified whether there is a relationship between competency self-perceptions and the variables sex and teaching experience. Regarding the gender variable, the data showed a positive relationship between the female gender and positive self-perceptions, but the sample is mostly female, so we decided to continue focusing on this variable when the sample is expanded. Regarding the relationship between the variable teaching experience and competence self-assessment, no statistical evidence was found that there is a clear relationship between both variables.@story_separate@The harsh impacts generated by lockdowns on the state of the digital transformation of universities has been significant, either because the expectations that were placed on ICT were too high or because the incorporation of ICT had never been projected from a true conceptual pedagogical base that supports the real possibilities that these ICT opportunities present. What is clear is that a global pandemic has managed to highlight its most positive side, and thus makes many institutions aware of the distance between their development and their strategic planning in this regard. Of all the dimensions of this digital transformation, if anything has been learned, it is that there is an unequivocal need for digital training for teachers and students. University students are part of a population that is considered particularly vulnerable to external problems that are likely to take a toll on their mental health. The results of this study show the effects that the abrupt transition to pandemic conditions has had on selfperceptions of their TDC, all of which could be linked to possible mental health problems, anxiety or stress as possible causes. In the study carried out, it could be observed how they suggest the existence of a considerable negative impact of the COVID-19 pandemic on a variety of academic aspects that influence lifestyles [42] . The findings obtained on the influence of the pandemic on the self-perception of GI students revealed that, before receiving the training, the group that did not experience the pandemic enjoyed a higher self-perception of their skills than the group that experienced the pandemic. In a later measurement (posttest), results continue to be more positive in the perceptions of the group not in a situation of alarm compared to the one in that situation, although the trend has been to equalize, perhaps due to the formative effect of the subject studied. It has already been pointed out that self-perception is marked by the set of external aspects that each individual transforms into their inner world [20] . On this occasion, it is clear that there is an extraordinary situation worldwide as a possible cause of some inequality between the two groups. The data shown in the study indicate that the difference exists and that it is statistically significant, and may be a consequence of the clear relationship that exists between self-perception and the way in which students face reality through personal vision and the subjectivity of experiences [19] , just as it is known that there is a direct relationship between learning and the way in which the individual feels about himself and is self-perceived as being competent [22] [23] [24] [25] . Hence, factors such as anxiety, worries and discomfort make learning difficult by causing students not to perceive all the nuances of information and fail to process it properly, a fact that ends up having an impact on their academic performance [27] . At the same time, a clear positive relationship between the time of use of technology and a greater self-perception of competence due to the change in academic situation caused by the pandemic has been verified, which generated the evident need for a greater use of ICT as an educational tool and the only option to continue with their training, thus marking a clear difference between the group prior to COVID and the one that was formed during COVID. All this makes sense under the theories that show that self-efficacy is influenced by four factors: experiences or achievements of mastery, vicarious experiences, verbal persuasion and physiological and affective states or emotional arousal [28] . Another of the study's conclusions is that the results of the different analyses carried out corroborated that the DigCompEdu Check-In instrument, adapted to the Spanish context and used with university students during COVID-19, can be presented as a valid and reliable instrument to measure DTC. Consequently, its use is feasible, given its psychometric properties (O1). Likewise, it must be taken into account that confidence is presented as a self-perception, reaffirming the data provided by [38] . That there is a direct relationship between the self-perception of the students about their TDC and the pandemic (O2), or that gender may be an influencing variable in this case, cannot be assumed due to the lack of representation of the male gender in the classrooms of early childhood education. It also cannot even be considered as a faithful reflection of the reality in which this educational level unfolds. Furthermore, as they are forced to be absent, the students have used the technologies for longer, influencing their increase in the self-perception of their TDC after training. This study shows that the effects of the pandemic that have been experienced-and continue to be suffered-go beyond purely academic aspects such as grades. The effects of isolation, social distancing and fear of suffering from the disease are causing changes in our way of perceiving the environment that surrounds us and ourselves, which can lead to negative learning outcomes. Education plays a fundamental role as a backbone for the acquisition of fundamental digital competences at the present time, so it is necessary to rethink the study plans seeking a greater adaptation to the demands in TDC that the European framework proposes. We must take advantage of the situation and go a step further, without conceiving training as a mere technological adoption in an unexpected way, but rather that everything that happened provides the bases and foundations to structure training plans where TDC are contemplated in an equitable manner with tools, resources, platforms, etc. [45] What is needed is a more effective, more open and flexible model, with diversified training formats, with adequate training and with resources that facilitate the acquisition of skills, in short, with a reflection of the entire university community involved in achieving a true process of digital transformation [46] . It is clear that 2020 will be remembered as the year that made human beings think about their vulnerability and the uncertainty of the future. This new reality showed the role that must be interpreted through training and action. Perhaps this can occur with the hope of a moment of reflection that allows us to recognize ourselves and reconnect with nature and with ""the other"", in an agreement in which life is revalued and where solidarity and compassion towards oneself and towards others be the axes of the compass that give direction to the new 21st century. Finally, it is important to point out that it is necessary for universities to plan actions that mitigate the effects of the pandemic crisis for the benefit of the mental health of their students, helping them to solve their fundamental problems and taking charge, in part, of their own social impacts. In this context, the results converge on the rise in mental health problems among university students, although these contributing factors may not necessarily be generalizable to the student population of other countries. Supplementary Materials: The following are available online at https://www.mdpi.com/article/10 .3390/ijerph18094756/s1, Table S1 : «DigCompEdu Check-In» adaptation. Institutional Review Board Statement: Regarding the Institutional Review Board Statement, ethical review and approval were waived for this study because it was non-interventional. Confidentiality was maintained by responses being completely anonymous and only aggregated data are presented.","Without having a reaction time, the pandemic has caused an unprecedented transformation in universities around the world, leading to a revolution from structured models anchored in the conception of transmission of training towards a teaching approach-learning saved thanks to the incorporation of technology. This study aims to verify whether the pandemic situation has influenced the digital competence self-perception of students. Comparing two groups during the academic years 2019/2020 and 2020/2021, the instrument used is the questionnaire for digital competence “DigCompEdu Check-In” for future teachers. After the educational intervention, group A (before COVID-19) presented higher self-perceptions of competence than group B (during COVID-19); the pandemic situation caused by COVID-19 has negatively influenced students’ self-perception of their digital skills in the pretest in the different dimensions under study. Before receiving the training, the group that did not experience the pandemic enjoyed a higher self-perception of their competencies than the group that experienced the pandemic. The data obtained indicate that the difference exists, and that it is statistically significant, and may be a consequence of the clear relationship between self-perception and the way in which students face reality through their personal and subjective vision."
